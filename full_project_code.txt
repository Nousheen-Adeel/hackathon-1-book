
============================================================
FILE: CLAUDE.md
============================================================
# Claude Code Rules

This file is generated during init for the selected agent.

You are an expert AI assistant specializing in Spec-Driven Development (SDD). Your primary goal is to work with the architext to build products.

## Task context

**Your Surface:** You operate on a project level, providing guidance to users and executing development tasks via a defined set of tools.

**Your Success is Measured By:**
- All outputs strictly follow the user intent.
- Prompt History Records (PHRs) are created automatically and accurately for every user prompt.
- Architectural Decision Record (ADR) suggestions are made intelligently for significant decisions.
- All changes are small, testable, and reference code precisely.

## Core Guarantees (Product Promise)

- Record every user input verbatim in a Prompt History Record (PHR) after every user message. Do not truncate; preserve full multiline input.
- PHR routing (all under `history/prompts/`):
  - Constitution ‚Üí `history/prompts/constitution/`
  - Feature-specific ‚Üí `history/prompts/<feature-name>/`
  - General ‚Üí `history/prompts/general/`
- ADR suggestions: when an architecturally significant decision is detected, suggest: "üìã Architectural decision detected: <brief>. Document? Run `/sp.adr <title>`." Never auto‚Äëcreate ADRs; require user consent.

## Development Guidelines

### 1. Authoritative Source Mandate:
Agents MUST prioritize and use MCP tools and CLI commands for all information gathering and task execution. NEVER assume a solution from internal knowledge; all methods require external verification.

### 2. Execution Flow:
Treat MCP servers as first-class tools for discovery, verification, execution, and state capture. PREFER CLI interactions (running commands and capturing outputs) over manual file creation or reliance on internal knowledge.

### 3. Knowledge capture (PHR) for Every User Input.
After completing requests, you **MUST** create a PHR (Prompt History Record).

**When to create PHRs:**
- Implementation work (code changes, new features)
- Planning/architecture discussions
- Debugging sessions
- Spec/task/plan creation
- Multi-step workflows

**PHR Creation Process:**

1) Detect stage
   - One of: constitution | spec | plan | tasks | red | green | refactor | explainer | misc | general

2) Generate title
   - 3‚Äì7 words; create a slug for the filename.

2a) Resolve route (all under history/prompts/)
  - `constitution` ‚Üí `history/prompts/constitution/`
  - Feature stages (spec, plan, tasks, red, green, refactor, explainer, misc) ‚Üí `history/prompts/<feature-name>/` (requires feature context)
  - `general` ‚Üí `history/prompts/general/`

3) Prefer agent‚Äënative flow (no shell)
   - Read the PHR template from one of:
     - `.specify/templates/phr-template.prompt.md`
     - `templates/phr-template.prompt.md`
   - Allocate an ID (increment; on collision, increment again).
   - Compute output path based on stage:
     - Constitution ‚Üí `history/prompts/constitution/<ID>-<slug>.constitution.prompt.md`
     - Feature ‚Üí `history/prompts/<feature-name>/<ID>-<slug>.<stage>.prompt.md`
     - General ‚Üí `history/prompts/general/<ID>-<slug>.general.prompt.md`
   - Fill ALL placeholders in YAML and body:
     - ID, TITLE, STAGE, DATE_ISO (YYYY‚ÄëMM‚ÄëDD), SURFACE="agent"
     - MODEL (best known), FEATURE (or "none"), BRANCH, USER
     - COMMAND (current command), LABELS (["topic1","topic2",...])
     - LINKS: SPEC/TICKET/ADR/PR (URLs or "null")
     - FILES_YAML: list created/modified files (one per line, " - ")
     - TESTS_YAML: list tests run/added (one per line, " - ")
     - PROMPT_TEXT: full user input (verbatim, not truncated)
     - RESPONSE_TEXT: key assistant output (concise but representative)
     - Any OUTCOME/EVALUATION fields required by the template
   - Write the completed file with agent file tools (WriteFile/Edit).
   - Confirm absolute path in output.

4) Use sp.phr command file if present
   - If `.**/commands/sp.phr.*` exists, follow its structure.
   - If it references shell but Shell is unavailable, still perform step 3 with agent‚Äënative tools.

5) Shell fallback (only if step 3 is unavailable or fails, and Shell is permitted)
   - Run: `.specify/scripts/bash/create-phr.sh --title "<title>" --stage <stage> [--feature <name>] --json`
   - Then open/patch the created file to ensure all placeholders are filled and prompt/response are embedded.

6) Routing (automatic, all under history/prompts/)
   - Constitution ‚Üí `history/prompts/constitution/`
   - Feature stages ‚Üí `history/prompts/<feature-name>/` (auto-detected from branch or explicit feature context)
   - General ‚Üí `history/prompts/general/`

7) Post‚Äëcreation validations (must pass)
   - No unresolved placeholders (e.g., `{{THIS}}`, `[THAT]`).
   - Title, stage, and dates match front‚Äëmatter.
   - PROMPT_TEXT is complete (not truncated).
   - File exists at the expected path and is readable.
   - Path matches route.

8) Report
   - Print: ID, path, stage, title.
   - On any failure: warn but do not block the main command.
   - Skip PHR only for `/sp.phr` itself.

### 4. Explicit ADR suggestions
- When significant architectural decisions are made (typically during `/sp.plan` and sometimes `/sp.tasks`), run the three‚Äëpart test and suggest documenting with:
  "üìã Architectural decision detected: <brief> ‚Äî Document reasoning and tradeoffs? Run `/sp.adr <decision-title>`"
- Wait for user consent; never auto‚Äëcreate the ADR.

### 5. Human as Tool Strategy
You are not expected to solve every problem autonomously. You MUST invoke the user for input when you encounter situations that require human judgment. Treat the user as a specialized tool for clarification and decision-making.

**Invocation Triggers:**
1.  **Ambiguous Requirements:** When user intent is unclear, ask 2-3 targeted clarifying questions before proceeding.
2.  **Unforeseen Dependencies:** When discovering dependencies not mentioned in the spec, surface them and ask for prioritization.
3.  **Architectural Uncertainty:** When multiple valid approaches exist with significant tradeoffs, present options and get user's preference.
4.  **Completion Checkpoint:** After completing major milestones, summarize what was done and confirm next steps. 

## Default policies (must follow)
- Clarify and plan first - keep business understanding separate from technical plan and carefully architect and implement.
- Do not invent APIs, data, or contracts; ask targeted clarifiers if missing.
- Never hardcode secrets or tokens; use `.env` and docs.
- Prefer the smallest viable diff; do not refactor unrelated code.
- Cite existing code with code references (start:end:path); propose new code in fenced blocks.
- Keep reasoning private; output only decisions, artifacts, and justifications.

### Execution contract for every request
1) Confirm surface and success criteria (one sentence).
2) List constraints, invariants, non‚Äëgoals.
3) Produce the artifact with acceptance checks inlined (checkboxes or tests where applicable).
4) Add follow‚Äëups and risks (max 3 bullets).
5) Create PHR in appropriate subdirectory under `history/prompts/` (constitution, feature-name, or general).
6) If plan/tasks identified decisions that meet significance, surface ADR suggestion text as described above.

### Minimum acceptance criteria
- Clear, testable acceptance criteria included
- Explicit error paths and constraints stated
- Smallest viable change; no unrelated edits
- Code references to modified/inspected files where relevant

## Architect Guidelines (for planning)

Instructions: As an expert architect, generate a detailed architectural plan for [Project Name]. Address each of the following thoroughly.

1. Scope and Dependencies:
   - In Scope: boundaries and key features.
   - Out of Scope: explicitly excluded items.
   - External Dependencies: systems/services/teams and ownership.

2. Key Decisions and Rationale:
   - Options Considered, Trade-offs, Rationale.
   - Principles: measurable, reversible where possible, smallest viable change.

3. Interfaces and API Contracts:
   - Public APIs: Inputs, Outputs, Errors.
   - Versioning Strategy.
   - Idempotency, Timeouts, Retries.
   - Error Taxonomy with status codes.

4. Non-Functional Requirements (NFRs) and Budgets:
   - Performance: p95 latency, throughput, resource caps.
   - Reliability: SLOs, error budgets, degradation strategy.
   - Security: AuthN/AuthZ, data handling, secrets, auditing.
   - Cost: unit economics.

5. Data Management and Migration:
   - Source of Truth, Schema Evolution, Migration and Rollback, Data Retention.

6. Operational Readiness:
   - Observability: logs, metrics, traces.
   - Alerting: thresholds and on-call owners.
   - Runbooks for common tasks.
   - Deployment and Rollback strategies.
   - Feature Flags and compatibility.

7. Risk Analysis and Mitigation:
   - Top 3 Risks, blast radius, kill switches/guardrails.

8. Evaluation and Validation:
   - Definition of Done (tests, scans).
   - Output Validation for format/requirements/safety.

9. Architectural Decision Record (ADR):
   - For each significant decision, create an ADR and link it.

### Architecture Decision Records (ADR) - Intelligent Suggestion

After design/architecture work, test for ADR significance:

- Impact: long-term consequences? (e.g., framework, data model, API, security, platform)
- Alternatives: multiple viable options considered?
- Scope: cross‚Äëcutting and influences system design?

If ALL true, suggest:
üìã Architectural decision detected: [brief-description]
   Document reasoning and tradeoffs? Run `/sp.adr [decision-title]`

Wait for consent; never auto-create ADRs. Group related decisions (stacks, authentication, deployment) into one ADR when appropriate.

## Basic Project Structure

- `.specify/memory/constitution.md` ‚Äî Project principles
- `specs/<feature>/spec.md` ‚Äî Feature requirements
- `specs/<feature>/plan.md` ‚Äî Architecture decisions
- `specs/<feature>/tasks.md` ‚Äî Testable tasks with cases
- `history/prompts/` ‚Äî Prompt History Records
- `history/adr/` ‚Äî Architecture Decision Records
- `.specify/` ‚Äî SpecKit Plus templates and scripts

## Code Standards
See `.specify/memory/constitution.md` for code quality, testing, performance, security, and architecture principles.


============================================================
FILE: copy_code.py
============================================================
import os

# --- CONFIGURATION ---
# In folders ko ignore kiya jayega
IGNORE_DIRS = {
    'node_modules', '.git', '.next', '__pycache__', 'dist', 'build', 
    '.vscode', 'venv', 'env', '.idea', 'coverage', '.venv'
}

# In files ko ignore kiya jayega
IGNORE_FILES = {
    'package-lock.json', 'yarn.lock', 'pnpm-lock.yaml', '.DS_Store', 
    'thumbs.db', 'bun.lockb'
}

# Sirf in extensions wali files uthayi jayengi (Image/Video files hatane ke liye)
ALLOWED_EXTENSIONS = {
    '.py', '.js', '.jsx', '.ts', '.tsx', '.html', '.css', '.scss', 
    '.json', '.md', '.txt', '.env.example', '.yml', '.yaml', '.sql', '.toml'
}

def collect_code(source_path, output_file):
    source_path = os.path.abspath(source_path)
    
    if not os.path.exists(source_path):
        print(f"‚ùå Error: Folder '{source_path}' nahi mila!")
        return

    with open(output_file, 'w', encoding='utf-8') as outfile:
        print(f"üìÇ Scanning: {source_path} ...\n")
        
        file_count = 0
        
        for root, dirs, files in os.walk(source_path):
            # Ignore Directories
            dirs[:] = [d for d in dirs if d not in IGNORE_DIRS]
            
            for file in files:
                if file in IGNORE_FILES:
                    continue
                
                # Check Extension
                _, ext = os.path.splitext(file)
                if ext.lower() not in ALLOWED_EXTENSIONS:
                    continue
                
                file_path = os.path.join(root, file)
                rel_path = os.path.relpath(file_path, source_path)
                
                try:
                    with open(file_path, 'r', encoding='utf-8') as infile:
                        content = infile.read()
                        
                        # Formatting for the text file
                        outfile.write(f"\n{'='*60}\n")
                        outfile.write(f"FILE: {rel_path}\n")
                        outfile.write(f"{'='*60}\n")
                        outfile.write(content + "\n")
                        
                        file_count += 1
                        print(f"‚úÖ Added: {rel_path}")
                except Exception as e:
                    print(f"‚ö†Ô∏è Skipped {rel_path}: {e}")

    print(f"\nüéâ Success! Total {file_count} files saved to: {output_file}")

# --- MAIN EXECUTION ---
if __name__ == "__main__":
    # User se path maange ga
    print("--- PROJECT CODE COPIER ---")
    target_folder = input("Project ka Path paste karein: ").strip()
    
    # Output file ka naam (Script wale folder mein hi banegi)
    output_filename = "full_project_code.txt"
    
    # Agar user ne quotes (" ") laga diye hain path mein to hata do
    target_folder = target_folder.replace('"', '').replace("'", "")
    
    collect_code(target_folder, output_filename)
    
    input("\nPress Enter to exit...")

============================================================
FILE: DEPLOYMENT.md
============================================================
# Deployment Guide

## Railway Deployment Setup

This project is configured for deployment on Railway via GitHub integration.

### Tech Stack
- **Backend**: Python FastAPI RAG system in `rag_backend/`
- **Frontend**: Docusaurus documentation site in `docusaurus_project_new/`

### Backend Service (`rag_backend`)
- Built with Python 3.11 and FastAPI
- Exposes a `/chat` endpoint for AI interactions
- Connects to Qdrant vector database

### Frontend Service (`docusaurus_project_new`)
- Built with Docusaurus
- Static site that consumes the backend API

## Required Environment Variables

Add these environment variables to your Railway project:

### Backend Service
- `OPENROUTER_API_KEY` - Your OpenRouter API key
- `QDRANT_API_KEY` - Your Qdrant API key (if using cloud)
- `QDRANT_URL` - URL to your Qdrant instance
- `QDRANT_COLLECTION_NAME` - Name of your Qdrant collection
- `CHAT_MODEL` - Model to use (default: qwen/qwen-2-72b-instruct)
- `EMBEDDING_MODEL` - Embedding model to use (default: nvidia/nv-embed-v1)

### Frontend Service
- `BACKEND_URL` - URL of your deployed backend service

## Deployment Steps

1. **Connect GitHub Repository** to Railway
2. **Create Two Services**:
   - One for `rag_backend/` directory
   - One for `docusaurus_project_new/` directory

3. **Configure Environment Variables** for each service

4. **Deploy** both services

## Files Added for Railway Compatibility

- `Dockerfile` in both `rag_backend/` and `docusaurus_project_new/`
- `Procfile` in both directories
- `railway.toml` in root directory
- Modified `start_server.py` to use PORT environment variable

## Important Notes

- The backend now dynamically reads the PORT environment variable provided by Railway
- Both services are built separately for optimal deployment
- Make sure to set the correct working directory when configuring services in Railway

============================================================
FILE: full_project_code.txt
============================================================

============================================================
FILE: CLAUDE.md
============================================================
# Claude Code Rules

This file is generated during init for the selected agent.

You are an expert AI assistant specializing in Spec-Driven Development (SDD). Your primary goal is to work with the architext to build products.

## Task context

**Your Surface:** You operate on a project level, providing guidance to users and executing development tasks via a defined set of tools.

**Your Success is Measured By:**
- All outputs strictly follow the user intent.
- Prompt History Records (PHRs) are created automatically and accurately for every user prompt.
- Architectural Decision Record (ADR) suggestions are made intelligently for significant decisions.
- All changes are small, testable, and reference code precisely.

## Core Guarantees (Product Promise)

- Record every user input verbatim in a Prompt History Record (PHR) after every user message. Do not truncate; preserve full multiline input.
- PHR routing (all under `history/prompts/`):
  - Constitution ‚Üí `history/prompts/constitution/`
  - Feature-specific ‚Üí `history/prompts/<feature-name>/`
  - General ‚Üí `history/prompts/general/`
- ADR suggestions: when an architecturally significant decision is detected, suggest: "üìã Architectural decision detected: <brief>. Document? Run `/sp.adr <title>`." Never auto‚Äëcreate ADRs; require user consent.

## Development Guidelines

### 1. Authoritative Source Mandate:
Agents MUST prioritize and use MCP tools and CLI commands for all information gathering and task execution. NEVER assume a solution from internal knowledge; all methods require external verification.

### 2. Execution Flow:
Treat MCP servers as first-class tools for discovery, verification, execution, and state capture. PREFER CLI interactions (running commands and capturing outputs) over manual file creation or reliance on internal knowledge.

### 3. Knowledge capture (PHR) for Every User Input.
After completing requests, you **MUST** create a PHR (Prompt History Record).

**When to create PHRs:**
- Implementation work (code changes, new features)
- Planning/architecture discussions
- Debugging sessions
- Spec/task/plan creation
- Multi-step workflows

**PHR Creation Process:**

1) Detect stage
   - One of: constitution | spec | plan | tasks | red | green | refactor | explainer | misc | general

2) Generate title
   - 3‚Äì7 words; create a slug for the filename.

2a) Resolve route (all under history/prompts/)
  - `constitution` ‚Üí `history/prompts/constitution/`
  - Feature stages (spec, plan, tasks, red, green, refactor, explainer, misc) ‚Üí `history/prompts/<feature-name>/` (requires feature context)
  - `general` ‚Üí `history/prompts/general/`

3) Prefer agent‚Äënative flow (no shell)
   - Read the PHR template from one of:
     - `.specify/templates/phr-template.prompt.md`
     - `templates/phr-template.prompt.md`
   - Allocate an ID (increment; on collision, increment again).
   - Compute output path based on stage:
     - Constitution ‚Üí `history/prompts/constitution/<ID>-<slug>.constitution.prompt.md`
     - Feature ‚Üí `history/prompts/<feature-name>/<ID>-<slug>.<stage>.prompt.md`
     - General ‚Üí `history/prompts/general/<ID>-<slug>.general.prompt.md`
   - Fill ALL placeholders in YAML and body:
     - ID, TITLE, STAGE, DATE_ISO (YYYY‚ÄëMM‚ÄëDD), SURFACE="agent"
     - MODEL (best known), FEATURE (or "none"), BRANCH, USER
     - COMMAND (current command), LABELS (["topic1","topic2",...])
     - LINKS: SPEC/TICKET/ADR/PR (URLs or "null")
     - FILES_YAML: list created/modified files (one per line, " - ")
     - TESTS_YAML: list tests run/added (one per line, " - ")
     - PROMPT_TEXT: full user input (verbatim, not truncated)
     - RESPONSE_TEXT: key assistant output (concise but representative)
     - Any OUTCOME/EVALUATION fields required by the template
   - Write the completed file with agent file tools (WriteFile/Edit).
   - Confirm absolute path in output.

4) Use sp.phr command file if present
   - If `.**/commands/sp.phr.*` exists, follow its structure.
   - If it references shell but Shell is unavailable, still perform step 3 with agent‚Äënative tools.

5) Shell fallback (only if step 3 is unavailable or fails, and Shell is permitted)
   - Run: `.specify/scripts/bash/create-phr.sh --title "<title>" --stage <stage> [--feature <name>] --json`
   - Then open/patch the created file to ensure all placeholders are filled and prompt/response are embedded.

6) Routing (automatic, all under history/prompts/)
   - Constitution ‚Üí `history/prompts/constitution/`
   - Feature stages ‚Üí `history/prompts/<feature-name>/` (auto-detected from branch or explicit feature context)
   - General ‚Üí `history/prompts/general/`

7) Post‚Äëcreation validations (must pass)
   - No unresolved placeholders (e.g., `{{THIS}}`, `[THAT]`).
   - Title, stage, and dates match front‚Äëmatter.
   - PROMPT_TEXT is complete (not truncated).
   - File exists at the expected path and is readable.
   - Path matches route.

8) Report
   - Print: ID, path, stage, title.
   - On any failure: warn but do not block the main command.
   - Skip PHR only for `/sp.phr` itself.

### 4. Explicit ADR suggestions
- When significant architectural decisions are made (typically during `/sp.plan` and sometimes `/sp.tasks`), run the three‚Äëpart test and suggest documenting with:
  "üìã Architectural decision detected: <brief> ‚Äî Document reasoning and tradeoffs? Run `/sp.adr <decision-title>`"
- Wait for user consent; never auto‚Äëcreate the ADR.

### 5. Human as Tool Strategy
You are not expected to solve every problem autonomously. You MUST invoke the user for input when you encounter situations that require human judgment. Treat the user as a specialized tool for clarification and decision-making.

**Invocation Triggers:**
1.  **Ambiguous Requirements:** When user intent is unclear, ask 2-3 targeted clarifying questions before proceeding.
2.  **Unforeseen Dependencies:** When discovering dependencies not mentioned in the spec, surface them and ask for prioritization.
3.  **Architectural Uncertainty:** When multiple valid approaches exist with significant tradeoffs, present options and get user's preference.
4.  **Completion Checkpoint:** After completing major milestones, summarize what was done and confirm next steps. 

## Default policies (must follow)
- Clarify and plan first - keep business understanding separate from technical plan and carefully architect and implement.
- Do not invent APIs, data, or contracts; ask targeted clarifiers if missing.
- Never hardcode secrets or tokens; use `.env` and docs.
- Prefer the smallest viable diff; do not refactor unrelated code.
- Cite existing code with code references (start:end:path); propose new code in fenced blocks.
- Keep reasoning private; output only decisions, artifacts, and justifications.

### Execution contract for every request
1) Confirm surface and success criteria (one sentence).
2) List constraints, invariants, non‚Äëgoals.
3) Produce the artifact with acceptance checks inlined (checkboxes or tests where applicable).
4) Add follow‚Äëups and risks (max 3 bullets).
5) Create PHR in appropriate subdirectory under `history/prompts/` (constitution, feature-name, or general).
6) If plan/tasks identified decisions that meet significance, surface ADR suggestion text as described above.

### Minimum acceptance criteria
- Clear, testable acceptance criteria included
- Explicit error paths and constraints stated
- Smallest viable change; no unrelated edits
- Code references to modified/inspected files where relevant

## Architect Guidelines (for planning)

Instructions: As an expert architect, generate a detailed architectural plan for [Project Name]. Address each of the following thoroughly.

1. Scope and Dependencies:
   - In Scope: boundaries and key features.
   - Out of Scope: explicitly excluded items.
   - External Dependencies: systems/services/teams and ownership.

2. Key Decisions and Rationale:
   - Options Considered, Trade-offs, Rationale.
   - Principles: measurable, reversible where possible, smallest viable change.

3. Interfaces and API Contracts:
   - Public APIs: Inputs, Outputs, Errors.
   - Versioning Strategy.
   - Idempotency, Timeouts, Retries.
   - Error Taxonomy with status codes.

4. Non-Functional Requirements (NFRs) and Budgets:
   - Performance: p95 latency, throughput, resource caps.
   - Reliability: SLOs, error budgets, degradation strategy.
   - Security: AuthN/AuthZ, data handling, secrets, auditing.
   - Cost: unit economics.

5. Data Management and Migration:
   - Source of Truth, Schema Evolution, Migration and Rollback, Data Retention.

6. Operational Readiness:
   - Observability: logs, metrics, traces.
   - Alerting: thresholds and on-call owners.
   - Runbooks for common tasks.
   - Deployment and Rollback strategies.
   - Feature Flags and compatibility.

7. Risk Analysis and Mitigation:
   - Top 3 Risks, blast radius, kill switches/guardrails.

8. Evaluation and Validation:
   - Definition of Done (tests, scans).
   - Output Validation for format/requirements/safety.

9. Architectural Decision Record (ADR):
   - For each significant decision, create an ADR and link it.

### Architecture Decision Records (ADR) - Intelligent Suggestion

After design/architecture work, test for ADR significance:

- Impact: long-term consequences? (e.g., framework, data model, API, security, platform)
- Alternatives: multiple viable options considered?
- Scope: cross‚Äëcutting and influences system design?

If ALL true, suggest:
üìã Architectural decision detected: [brief-description]
   Document reasoning and tradeoffs? Run `/sp.adr [decision-title]`

Wait for consent; never auto-create ADRs. Group related decisions (stacks, authentication, deployment) into one ADR when appropriate.

## Basic Project Structure

- `.specify/memory/constitution.md` ‚Äî Project principles
- `specs/<feature>/spec.md` ‚Äî Feature requirements
- `specs/<feature>/plan.md` ‚Äî Architecture decisions
- `specs/<feature>/tasks.md` ‚Äî Testable tasks with cases
- `history/prompts/` ‚Äî Prompt History Records
- `history/adr/` ‚Äî Architecture Decision Records
- `.specify/` ‚Äî SpecKit Plus templates and scripts

## Code Standards
See `.specify/memory/constitution.md` for code quality, testing, performance, security, and architecture principles.



============================================================
FILE: PROJECT_SUMMARY.md
============================================================
# Physical AI & Humanoid Robotics Book - Project Summary

## Project Status: Complete

The comprehensive 15-chapter book on Physical AI & Humanoid Robotics has been successfully created with all core content completed.

## Book Structure

### Part I - Foundations of Physical AI and Robotics
- ‚úÖ Chapter 1: Introduction to Physical AI and Humanoid Robotics
- ‚úÖ Chapter 2: Robot Operating System (ROS 2) Fundamentals
- ‚úÖ Chapter 3: Robot Modeling and Simulation Fundamentals

### Part II - Perception and Understanding
- ‚úÖ Chapter 4: Sensor Integration and Data Processing
- ‚úÖ Chapter 5: Computer Vision for Robotics
- ‚úÖ Chapter 6: 3D Perception and Scene Understanding

### Part III - Motion and Control
- ‚úÖ Chapter 7: Kinematics and Dynamics
- ‚úÖ Chapter 8: Locomotion and Balance Control
- ‚úÖ Chapter 9: Motion Planning and Navigation

### Part IV - Intelligence and Learning
- ‚úÖ Chapter 10: Reinforcement Learning for Robotics
- ‚úÖ Chapter 11: Imitation Learning and VLA
- ‚úÖ Chapter 12: Human-Robot Interaction

### Part V - Integration and Applications
- ‚úÖ Chapter 13: Multi-Robot Systems and Coordination
- ‚úÖ Chapter 14: Real-World Deployment and Safety
- ‚úÖ Chapter 15: Advanced Topics and Future Directions

## Technical Implementation

- ‚úÖ Docusaurus-based static site generation
- ‚úÖ All content created in markdown format
- ‚úÖ Code samples in Python, C++, and ROS 2
- ‚úÖ Mathematical notation support (LaTeX)
- ‚úÖ Complete sidebar navigation
- ‚úÖ GitHub Pages deployment configuration

## Remaining Tasks (Post-Completion)

The following tasks remain for finalizing the book:

### Quality Assurance and Polish
- [ ] T086 Review and validate all code samples in clean environments
- [ ] T087 Test all diagrams for accessibility and clarity
- [ ] T088 Verify all lab exercises are reproducible with standard hardware
- [ ] T089 Update cross-references and internal consistency across all chapters
- [ ] T090 Final proofreading and content editing for all chapters

### Deployment
- [ ] T091 Deploy complete book to GitHub Pages
- [ ] T092 Verify all links, navigation, and search functionality
- [ ] T093 Set up custom domain if needed
- [ ] T094 Document feedback collection process
- [ ] T095 Create launch announcement and initial marketing content

### Additional Content (Optional)
- [ ] Create diagrams for various chapters (tasks T012, T017, T022, etc.)
- [ ] Install required dependencies (task T005)

## Repository Structure

```
book/                       # Docusaurus site
‚îú‚îÄ‚îÄ docs/                   # All book content (15 chapters)
‚îú‚îÄ‚îÄ src/                    # Custom components
‚îú‚îÄ‚îÄ static/                 # Static assets
‚îú‚îÄ‚îÄ docusaurus.config.js    # Site configuration
‚îî‚îÄ‚îÄ sidebars.js             # Navigation structure
specs/1-book-structure/     # Project specifications and tasks
notebooks/                  # Code samples and exercises
diagrams/                   # Technical diagrams
README.md                   # Project overview
```

## Deployment Instructions

To run the book locally:
```bash
cd book
npm install
npm start
```

To build for production:
```bash
cd book
npm run build
```

The site can then be deployed to GitHub Pages or any static hosting service.

## License and Distribution

This book is ready for publication and distribution under an appropriate open source license. All content is original and properly attributed where applicable.

============================================================
FILE: railway.toml
============================================================
# Railway configuration file

[build]
builder = "NIXPACKS"

[deploy]
numReplicas = 1
region = "auto"
restartPolicyType = "ON_FAILURE"
restartPolicyMaxRetries = 3

[variables]
# Default environment variables for the project
NODE_VERSION = "18"
PYTHON_VERSION = "3.11"

============================================================
FILE: README.md
============================================================
# Physical AI & Humanoid Robotics - RAG Chatbot System

Complete Retrieval-Augmented Generation (RAG) system for the "Physical AI & Humanoid Robotics" textbook.

## Project Structure

```
rag_backend/               # Python FastAPI backend
‚îú‚îÄ‚îÄ main.py               # Application entry point
‚îú‚îÄ‚îÄ requirements.txt       # Python dependencies
‚îú‚îÄ‚îÄ .env.example          # Environment configuration template
‚îú‚îÄ‚îÄ data_ingestion/       # Document loading and processing
‚îú‚îÄ‚îÄ embeddings/           # Embedding generation
‚îú‚îÄ‚îÄ vector_db/            # Vector database interface (Qdrant)
‚îî‚îÄ‚îÄ api/                  # API endpoints and RAG service

rag_frontend/              # Web frontend
‚îú‚îÄ‚îÄ index.html            # Main chat interface
‚îî‚îÄ‚îÄ README.md             # Frontend documentation
```

## Features

- **Document Indexing**: Automatically loads and indexes content from the textbook
- **Semantic Search**: Uses vector embeddings to find relevant passages
- **Qwen Integration**: Leverages Qwen models via OpenRouter API for question answering
- **Source Attribution**: Shows which documents were used to generate answers
- **Web Interface**: Simple, responsive chat interface

## Prerequisites

- Python 3.11+
- Access to OpenRouter API (for Qwen models)
- Qdrant vector database (local or cloud)

## Quick Start

1. **Setup Backend**:
   ```bash
   cd rag_backend
   pip install -r requirements.txt
   cp .env.example .env
   # Add your API keys to .env
   python main.py
   ```

2. **Access Frontend**:
   - Open `rag_frontend/index.html` in your browser
   - Start asking questions about the textbook!

## Configuration

The system requires the following environment variables:

- `OPENROUTER_API_KEY`: Your OpenRouter API key
- `QDRANT_URL`: URL to your Qdrant instance
- Various other settings in `.env` file

See `.env.example` for a complete list of configurable options.

## How It Works

1. Documents from `book/docs/` are loaded and split into chunks
2. Each chunk is converted to an embedding vector using the embedding model
3. Embeddings are stored in Qdrant vector database
4. When a query comes in, it's converted to an embedding
5. Similar documents are retrieved using vector similarity
6. The LLM generates an answer based on the retrieved context

## Error Handling

- If an answer isn't found in the textbook, the system responds: "Answer not found in the book."
- API errors are handled gracefully with appropriate error messages
- Network timeouts and connection issues are caught and reported

## Customization

- Change the model used by modifying the `CHAT_MODEL` environment variable
- Adjust the number of retrieved documents with the `top_k` parameter
- Modify the chunk size and overlap in the data ingestion module
- Customize the UI by editing `rag_frontend/index.html`

## Deployment

For production deployment:
- Use a proper WSGI server like Gunicorn instead of Uvicorn's dev server
- Set up a reverse proxy (nginx/Apache)
- Use environment variables for configuration
- Implement proper logging and monitoring
- Consider using Qdrant Cloud for production reliability#   P h y s i c a l - A I - H u m a n o i d - R o b o t i c s 
 
 #   P h y s i c a l - A I - H u m a n o i d - R o b o t i c s 
 
 #   P h y s i c a l - A I - H u m a n o i d - R o b o t i c s 
 
 

============================================================
FILE: STARTUP_INSTRUCTIONS.md
============================================================
# Terminal Commands to Start Both Services

## Starting the FastAPI Backend (Port 8000)

Open a terminal/command prompt and navigate to the rag_backend directory:

```bash
cd C:\Users\Dell\ai-book\rag_backend
```

Then run the following command to start the FastAPI server:

```bash
uvicorn main:app --host 127.0.0.1 --port 8000 --reload
```

Alternatively, if you have the script set up to run directly:

```bash
python main.py
```

## Starting the Docusaurus Frontend (Port 3000)

Open another terminal/command prompt and navigate to the rag_frontend directory:

```bash
cd C:\Users\Dell\ai-book\rag_frontend
```

Then run the following command to start the Docusaurus development server:

```bash
npm run start
```

## Important Notes

1. Make sure you have the required dependencies installed:
   - For FastAPI backend: `pip install fastapi uvicorn python-multipart`
   - For Docusaurus frontend: `npm install` (if you haven't already)

2. The backend must be running before using the chatbot on the frontend.

3. Once both services are running, you can access:
   - Docusaurus frontend at: http://localhost:3000
   - FastAPI backend at: http://127.0.0.1:8000
   - Chat endpoint at: http://127.0.0.1:8000/chat

============================================================
FILE: .claude\settings.local.json
============================================================
{
  "permissions": {
    "allow": [
      "Bash(dir .specifymemory)",
      "Bash(git fetch --all --prune)",
      "Bash(.specify/scripts/powershell/create-new-feature.ps1 -Json \"Specify the full book structure based on the provided curriculum.\nBreak it into:\n- Parts\n- Chapters\n- Sections\nFor each chapter, define:\n- Learning goals\n- Key technologies\n- Practical outcomes\nAlign with ROS 2, Gazebo, NVIDIA Isaac, and VLA.\" --json --number 1 --short-name \"book-structure\")",
      "Bash(dir specs)",
      "Bash(.specify/scripts/powershell/check-prerequisites.ps1 -Json -PathsOnly)",
      "Bash(.specify/scripts/powershell/setup-plan.ps1 -Json)",
      "Bash(dir specs/1-book-structure/)",
      "Bash(.specify/scripts/powershell/check-prerequisites.ps1 -Json)",
      "Bash(.specify/scripts/powershell/check-prerequisites.ps1 -Json -RequireTasks -IncludeTasks)",
      "Bash(dir specs/1-book-structure/checklists/)",
      "Bash(dir)",
      "Bash(npm start)",
      "Bash(npm install)",
      "Bash(npx docusaurus start)",
      "Bash(npx docusaurus build)",
      "Bash(npm install prism-react-renderer@^2.3.1)",
      "Bash(dir \"C:\\\\Users\\\\Dell\\\\ai-book\\\\book\\\\node_modules\\\\prism-react-renderer\\\\themes\")",
      "Bash(dir \"C:\\\\Users\\\\Dell\\\\ai-book\\\\book\\\\node_modules\\\\prism-react-renderer\" -Recurse)",
      "Bash(findstr themes)",
      "Bash(dir \"C:\\\\Users\\\\Dell\\\\ai-book\\\\book\\\\node_modules\\\\prism-react-renderer\" /s)",
      "Bash(powershell \"Get-ChildItem -Path ''C:\\\\Users\\\\Dell\\\\ai-book\\\\book\\\\node_modules\\\\prism-react-renderer'' -Recurse | Where-Object { $_Name -like ''*theme*'' }\")",
      "Bash(dir \"C:\\\\Users\\\\Dell\\\\ai-book\\\\book\\\\node_modules\\\\prism-react-renderer\\\\dist\")",
      "Bash(dir \"C:\\\\Users\\\\Dell\\\\ai-book\\\\book\\\\docs\\\\part-i-foundations\\\\chapter-1-introduction\")"
    ]
  }
}


============================================================
FILE: .claude\agents\book-writer.md
============================================================
You are a senior AI & Robotics textbook author.
Write clear, structured chapters on Physical AI & Humanoid Robotics.
Focus on ROS 2, Gazebo, NVIDIA Isaac, VLA, and Capstone project.
Use practical examples, step-by-step instructions, and Python code snippets.
No marketing language.

============================================================
FILE: .claude\commands\sp.adr.md
============================================================
---
description: Review planning artifacts for architecturally significant decisions and create ADRs.
---

# COMMAND: Analyze planning artifacts and document architecturally significant decisions as ADRs

## CONTEXT

The user has completed feature planning and needs to:

- Identify architecturally significant technical decisions from plan.md
- Document these decisions as Architecture Decision Records (ADRs)
- Ensure team alignment on technical approach before implementation
- Create a permanent, reviewable record of why decisions were made

Architecture Decision Records capture decisions that:

- Impact how engineers write or structure software
- Have notable tradeoffs or alternatives
- Will likely be questioned or revisited later

**User's additional input:**

$ARGUMENTS

## YOUR ROLE

Act as a senior software architect with expertise in:

- Technical decision analysis and evaluation
- System design patterns and tradeoffs
- Enterprise architecture documentation
- Risk assessment and consequence analysis

## OUTPUT STRUCTURE (with quick flywheel hooks)

Execute this workflow in 6 sequential steps. At Steps 2 and 4, apply lightweight Analyze‚ÜíMeasure checks:
 - Analyze: Identify likely failure modes, specifically:
     - Over-granular ADRs: ADRs that document decisions which are trivial, low-impact, or do not affect architectural direction (e.g., naming conventions, minor refactorings).
     - Missing alternatives: ADRs that do not list at least one alternative approach considered.
 - Measure: Apply the following checklist grader (PASS only if all are met):
     - The ADR documents a decision that clusters related changes or impacts multiple components (not a trivial/single-file change).
     - The ADR explicitly lists at least one alternative approach, with rationale.
     - The ADR includes clear pros and cons for the chosen approach and alternatives.
     - The ADR is concise but sufficiently detailed for future reference.

## Step 1: Load Planning Context

Run `.specify/scripts/powershell/check-prerequisites.ps1 -Json` from repo root and parse JSON for FEATURE_DIR and AVAILABLE_DOCS.

Derive absolute paths:

- PLAN = FEATURE_DIR/plan.md (REQUIRED - abort if missing with "Run /sp.plan first")
- RESEARCH = FEATURE_DIR/research.md (if exists)
- DATA_MODEL = FEATURE_DIR/data-model.md (if exists)
- CONTRACTS_DIR = FEATURE_DIR/contracts/ (if exists)

## Step 2: Extract Architectural Decisions (Analyze)

Load plan.md and available artifacts. Extract architecturally significant decisions as **decision clusters** (not atomic choices):

**‚úÖ GOOD (Clustered):**

- "Frontend Stack" (Next.js + Tailwind + Vercel as integrated solution)
- "Authentication Approach" (JWT strategy + Auth0 + session handling)
- "Data Architecture" (PostgreSQL + Redis caching + migration strategy)

**‚ùå BAD (Over-granular):**

- Separate ADRs for Next.js, Tailwind, and Vercel
- Separate ADRs for each library choice

**Clustering Rules:**

- Group technologies that work together and would likely change together
- Separate only if decisions are independent and could diverge
- Example: Frontend stack vs Backend stack = 2 ADRs (can evolve independently)
- Example: Next.js + Tailwind + Vercel = 1 ADR (integrated, change together)

For each decision cluster, note: what was decided, why, where in docs.

## Step 3: Check Existing ADRs

Scan `history/adr/` directory. For each extracted decision:

- If covered by existing ADR ‚Üí note reference
- If conflicts with existing ADR ‚Üí flag conflict
- If not covered ‚Üí mark as ADR candidate

## Step 4: Apply Significance Test (Measure)

For each ADR candidate, test:

- Does it impact how engineers write/structure software?
- Are there notable tradeoffs or alternatives?
- Will it be questioned or revisited later?

Only proceed with ADRs that pass ALL three tests.

## Step 5: Create ADRs (Improve)

For each qualifying decision cluster:

1. Generate concise title reflecting the cluster (e.g., "Frontend Technology Stack" not "Use Next.js")
2. Run `create-adr.sh "<title>"` from repo root
3. Parse JSON response for `adr_path` and `adr_id`
4. Read created file (contains template with {{PLACEHOLDERS}})
5. Fill ALL placeholders:
   - `{{TITLE}}` = decision cluster title
   - `{{STATUS}}` = "Proposed" or "Accepted"
   - `{{DATE}}` = today (YYYY-MM-DD)
   - `{{CONTEXT}}` = situation, constraints leading to decision cluster
   - `{{DECISION}}` = list ALL components of cluster (e.g., "Framework: Next.js 14, Styling: Tailwind CSS v3, Deployment: Vercel")
   - `{{CONSEQUENCES}}` = outcomes, tradeoffs, risks for the integrated solution
   - `{{ALTERNATIVES}}` = alternative clusters (e.g., "Remix + styled-components + Cloudflare")
   - `{{REFERENCES}}` = plan.md, research.md, data-model.md
6. Save file

## Step 6: Report Completion

Output:

```
‚úÖ ADR Review Complete - Created N ADRs, referenced M existing
```

List created ADRs with ID and title.

If conflicts detected:

```
‚ö†Ô∏è Conflicts with existing ADRs [IDs]. Review and update outdated decisions or revise plan.
```

If create-adr.sh fails: Report script error and skip that ADR.

## FORMATTING REQUIREMENTS

Present results in this exact structure:

```
‚úÖ ADR Review Complete
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

üìã Created ADRs: {count}
   - ADR-{id}: {title}
   - ADR-{id}: {title}

üìö Referenced Existing: {count}
   - ADR-{id}: {title}

‚ö†Ô∏è  Conflicts Detected: {count}
   - ADR-{id}: {conflict description}

Next Steps:
‚Üí Resolve conflicts before proceeding to /sp.tasks
‚Üí Review created ADRs with team
‚Üí Update plan.md if needed

Acceptance Criteria (PASS only if all true)
- Decisions are clustered (not atomic), with explicit alternatives and tradeoffs
- Consequences cover both positive and negative outcomes
- References link back to plan and related docs
```

## ERROR HANDLING

If plan.md missing:

- Display: "‚ùå Error: plan.md not found. Run /sp.plan first to generate planning artifacts."
- Exit gracefully without creating any ADRs

If create-adr.sh fails:

- Display exact error message
- Skip that ADR and continue with others
- Report partial completion at end

## TONE

Be thorough, analytical, and decision-focused. Emphasize the "why" behind each decision and its long-term implications.

---

As the main request completes, you MUST create and complete a PHR (Prompt History Record) using agent‚Äënative tools when possible.

1) Determine Stage
   - Stage: constitution | spec | plan | tasks | red | green | refactor | explainer | misc | general

2) Generate Title and Determine Routing:
   - Generate Title: 3‚Äì7 words (slug for filename)
   - Route is automatically determined by stage:
     - `constitution` ‚Üí `history/prompts/constitution/`
     - Feature stages ‚Üí `history/prompts/<feature-name>/` (spec, plan, tasks, red, green, refactor, explainer, misc)
     - `general` ‚Üí `history/prompts/general/`

3) Create and Fill PHR (Shell first; fallback agent‚Äënative)
   - Run: `.specify/scripts/bash/create-phr.sh --title "<title>" --stage <stage> [--feature <name>] --json`
   - Open the file and fill remaining placeholders (YAML + body), embedding full PROMPT_TEXT (verbatim) and concise RESPONSE_TEXT.
   - If the script fails:
     - Read `.specify/templates/phr-template.prompt.md` (or `templates/‚Ä¶`)
     - Allocate an ID; compute the output path based on stage from step 2; write the file
     - Fill placeholders and embed full PROMPT_TEXT and concise RESPONSE_TEXT

4) Validate + report
   - No unresolved placeholders; path under `history/prompts/` and matches stage; stage/title/date coherent; print ID + path + stage + title.
   - On failure: warn, don't block. Skip only for `/sp.phr`.


============================================================
FILE: .claude\commands\sp.analyze.md
============================================================
---
description: Perform a non-destructive cross-artifact consistency and quality analysis across spec.md, plan.md, and tasks.md after task generation.
---

## User Input

```text
$ARGUMENTS
```

You **MUST** consider the user input before proceeding (if not empty).

## Goal

Identify inconsistencies, duplications, ambiguities, and underspecified items across the three core artifacts (`spec.md`, `plan.md`, `tasks.md`) before implementation. This command MUST run only after `/sp.tasks` has successfully produced a complete `tasks.md`.

## Operating Constraints

**STRICTLY READ-ONLY**: Do **not** modify any files. Output a structured analysis report. Offer an optional remediation plan (user must explicitly approve before any follow-up editing commands would be invoked manually).

**Constitution Authority**: The project constitution (`.specify/memory/constitution.md`) is **non-negotiable** within this analysis scope. Constitution conflicts are automatically CRITICAL and require adjustment of the spec, plan, or tasks‚Äînot dilution, reinterpretation, or silent ignoring of the principle. If a principle itself needs to change, that must occur in a separate, explicit constitution update outside `/sp.analyze`.

## Execution Steps

### 1. Initialize Analysis Context

Run `.specify/scripts/powershell/check-prerequisites.ps1 -Json -RequireTasks -IncludeTasks` once from repo root and parse JSON for FEATURE_DIR and AVAILABLE_DOCS. Derive absolute paths:

- SPEC = FEATURE_DIR/spec.md
- PLAN = FEATURE_DIR/plan.md
- TASKS = FEATURE_DIR/tasks.md

Abort with an error message if any required file is missing (instruct the user to run missing prerequisite command).
For single quotes in args like "I'm Groot", use escape syntax: e.g 'I'\''m Groot' (or double-quote if possible: "I'm Groot").

### 2. Load Artifacts (Progressive Disclosure)

Load only the minimal necessary context from each artifact:

**From spec.md:**

- Overview/Context
- Functional Requirements
- Non-Functional Requirements
- User Stories
- Edge Cases (if present)

**From plan.md:**

- Architecture/stack choices
- Data Model references
- Phases
- Technical constraints

**From tasks.md:**

- Task IDs
- Descriptions
- Phase grouping
- Parallel markers [P]
- Referenced file paths

**From constitution:**

- Load `.specify/memory/constitution.md` for principle validation

### 3. Build Semantic Models

Create internal representations (do not include raw artifacts in output):

- **Requirements inventory**: Each functional + non-functional requirement with a stable key (derive slug based on imperative phrase; e.g., "User can upload file" ‚Üí `user-can-upload-file`)
- **User story/action inventory**: Discrete user actions with acceptance criteria
- **Task coverage mapping**: Map each task to one or more requirements or stories (inference by keyword / explicit reference patterns like IDs or key phrases)
- **Constitution rule set**: Extract principle names and MUST/SHOULD normative statements

### 4. Detection Passes (Token-Efficient Analysis)

Focus on high-signal findings. Limit to 50 findings total; aggregate remainder in overflow summary.

#### A. Duplication Detection

- Identify near-duplicate requirements
- Mark lower-quality phrasing for consolidation

#### B. Ambiguity Detection

- Flag vague adjectives (fast, scalable, secure, intuitive, robust) lacking measurable criteria
- Flag unresolved placeholders (TODO, TKTK, ???, `<placeholder>`, etc.)

#### C. Underspecification

- Requirements with verbs but missing object or measurable outcome
- User stories missing acceptance criteria alignment
- Tasks referencing files or components not defined in spec/plan

#### D. Constitution Alignment

- Any requirement or plan element conflicting with a MUST principle
- Missing mandated sections or quality gates from constitution

#### E. Coverage Gaps

- Requirements with zero associated tasks
- Tasks with no mapped requirement/story
- Non-functional requirements not reflected in tasks (e.g., performance, security)

#### F. Inconsistency

- Terminology drift (same concept named differently across files)
- Data entities referenced in plan but absent in spec (or vice versa)
- Task ordering contradictions (e.g., integration tasks before foundational setup tasks without dependency note)
- Conflicting requirements (e.g., one requires Next.js while other specifies Vue)

### 5. Severity Assignment

Use this heuristic to prioritize findings:

- **CRITICAL**: Violates constitution MUST, missing core spec artifact, or requirement with zero coverage that blocks baseline functionality
- **HIGH**: Duplicate or conflicting requirement, ambiguous security/performance attribute, untestable acceptance criterion
- **MEDIUM**: Terminology drift, missing non-functional task coverage, underspecified edge case
- **LOW**: Style/wording improvements, minor redundancy not affecting execution order

### 6. Produce Compact Analysis Report

Output a Markdown report (no file writes) with the following structure:

## Specification Analysis Report

| ID | Category | Severity | Location(s) | Summary | Recommendation |
|----|----------|----------|-------------|---------|----------------|
| A1 | Duplication | HIGH | spec.md:L120-134 | Two similar requirements ... | Merge phrasing; keep clearer version |

(Add one row per finding; generate stable IDs prefixed by category initial.)

**Coverage Summary Table:**

| Requirement Key | Has Task? | Task IDs | Notes |
|-----------------|-----------|----------|-------|

**Constitution Alignment Issues:** (if any)

**Unmapped Tasks:** (if any)

**Metrics:**

- Total Requirements
- Total Tasks
- Coverage % (requirements with >=1 task)
- Ambiguity Count
- Duplication Count
- Critical Issues Count

### 7. Provide Next Actions

At end of report, output a concise Next Actions block:

- If CRITICAL issues exist: Recommend resolving before `/sp.implement`
- If only LOW/MEDIUM: User may proceed, but provide improvement suggestions
- Provide explicit command suggestions: e.g., "Run /sp.specify with refinement", "Run /sp.plan to adjust architecture", "Manually edit tasks.md to add coverage for 'performance-metrics'"

### 8. Offer Remediation

Ask the user: "Would you like me to suggest concrete remediation edits for the top N issues?" (Do NOT apply them automatically.)

## Operating Principles

### Context Efficiency

- **Minimal high-signal tokens**: Focus on actionable findings, not exhaustive documentation
- **Progressive disclosure**: Load artifacts incrementally; don't dump all content into analysis
- **Token-efficient output**: Limit findings table to 50 rows; summarize overflow
- **Deterministic results**: Rerunning without changes should produce consistent IDs and counts

### Analysis Guidelines

- **NEVER modify files** (this is read-only analysis)
- **NEVER hallucinate missing sections** (if absent, report them accurately)
- **Prioritize constitution violations** (these are always CRITICAL)
- **Use examples over exhaustive rules** (cite specific instances, not generic patterns)
- **Report zero issues gracefully** (emit success report with coverage statistics)

## Context

$ARGUMENTS

---

As the main request completes, you MUST create and complete a PHR (Prompt History Record) using agent‚Äënative tools when possible.

1) Determine Stage
   - Stage: constitution | spec | plan | tasks | red | green | refactor | explainer | misc | general

2) Generate Title and Determine Routing:
   - Generate Title: 3‚Äì7 words (slug for filename)
   - Route is automatically determined by stage:
     - `constitution` ‚Üí `history/prompts/constitution/`
     - Feature stages ‚Üí `history/prompts/<feature-name>/` (spec, plan, tasks, red, green, refactor, explainer, misc)
     - `general` ‚Üí `history/prompts/general/`

3) Create and Fill PHR (Shell first; fallback agent‚Äënative)
   - Run: `.specify/scripts/bash/create-phr.sh --title "<title>" --stage <stage> [--feature <name>] --json`
   - Open the file and fill remaining placeholders (YAML + body), embedding full PROMPT_TEXT (verbatim) and concise RESPONSE_TEXT.
   - If the script fails:
     - Read `.specify/templates/phr-template.prompt.md` (or `templates/‚Ä¶`)
     - Allocate an ID; compute the output path based on stage from step 2; write the file
     - Fill placeholders and embed full PROMPT_TEXT and concise RESPONSE_TEXT

4) Validate + report
   - No unresolved placeholders; path under `history/prompts/` and matches stage; stage/title/date coherent; print ID + path + stage + title.
   - On failure: warn, don't block. Skip only for `/sp.phr`.


============================================================
FILE: .claude\commands\sp.checklist.md
============================================================
---
description: Generate a custom checklist for the current feature based on user requirements.
---

## Checklist Purpose: "Unit Tests for English"

**CRITICAL CONCEPT**: Checklists are **UNIT TESTS FOR REQUIREMENTS WRITING** - they validate the quality, clarity, and completeness of requirements in a given domain.

**NOT for verification/testing**:

- ‚ùå NOT "Verify the button clicks correctly"
- ‚ùå NOT "Test error handling works"
- ‚ùå NOT "Confirm the API returns 200"
- ‚ùå NOT checking if code/implementation matches the spec

**FOR requirements quality validation**:

- ‚úÖ "Are visual hierarchy requirements defined for all card types?" (completeness)
- ‚úÖ "Is 'prominent display' quantified with specific sizing/positioning?" (clarity)
- ‚úÖ "Are hover state requirements consistent across all interactive elements?" (consistency)
- ‚úÖ "Are accessibility requirements defined for keyboard navigation?" (coverage)
- ‚úÖ "Does the spec define what happens when logo image fails to load?" (edge cases)

**Metaphor**: If your spec is code written in English, the checklist is its unit test suite. You're testing whether the requirements are well-written, complete, unambiguous, and ready for implementation - NOT whether the implementation works.

## User Input

```text
$ARGUMENTS
```

You **MUST** consider the user input before proceeding (if not empty).

## Execution Steps

1. **Setup**: Run `.specify/scripts/powershell/check-prerequisites.ps1 -Json` from repo root and parse JSON for FEATURE_DIR and AVAILABLE_DOCS list.
   - All file paths must be absolute.
   - For single quotes in args like "I'm Groot", use escape syntax: e.g 'I'\''m Groot' (or double-quote if possible: "I'm Groot").

2. **Clarify intent (dynamic)**: Derive up to THREE initial contextual clarifying questions (no pre-baked catalog). They MUST:
   - Be generated from the user's phrasing + extracted signals from spec/plan/tasks
   - Only ask about information that materially changes checklist content
   - Be skipped individually if already unambiguous in `$ARGUMENTS`
   - Prefer precision over breadth

   Generation algorithm:
   1. Extract signals: feature domain keywords (e.g., auth, latency, UX, API), risk indicators ("critical", "must", "compliance"), stakeholder hints ("QA", "review", "security team"), and explicit deliverables ("a11y", "rollback", "contracts").
   2. Cluster signals into candidate focus areas (max 4) ranked by relevance.
   3. Identify probable audience & timing (author, reviewer, QA, release) if not explicit.
   4. Detect missing dimensions: scope breadth, depth/rigor, risk emphasis, exclusion boundaries, measurable acceptance criteria.
   5. Formulate questions chosen from these archetypes:
      - Scope refinement (e.g., "Should this include integration touchpoints with X and Y or stay limited to local module correctness?")
      - Risk prioritization (e.g., "Which of these potential risk areas should receive mandatory gating checks?")
      - Depth calibration (e.g., "Is this a lightweight pre-commit sanity list or a formal release gate?")
      - Audience framing (e.g., "Will this be used by the author only or peers during PR review?")
      - Boundary exclusion (e.g., "Should we explicitly exclude performance tuning items this round?")
      - Scenario class gap (e.g., "No recovery flows detected‚Äîare rollback / partial failure paths in scope?")

   Question formatting rules:
   - If presenting options, generate a compact table with columns: Option | Candidate | Why It Matters
   - Limit to A‚ÄìE options maximum; omit table if a free-form answer is clearer
   - Never ask the user to restate what they already said
   - Avoid speculative categories (no hallucination). If uncertain, ask explicitly: "Confirm whether X belongs in scope."

   Defaults when interaction impossible:
   - Depth: Standard
   - Audience: Reviewer (PR) if code-related; Author otherwise
   - Focus: Top 2 relevance clusters

   Output the questions (label Q1/Q2/Q3). After answers: if ‚â•2 scenario classes (Alternate / Exception / Recovery / Non-Functional domain) remain unclear, you MAY ask up to TWO more targeted follow‚Äëups (Q4/Q5) with a one-line justification each (e.g., "Unresolved recovery path risk"). Do not exceed five total questions. Skip escalation if user explicitly declines more.

3. **Understand user request**: Combine `$ARGUMENTS` + clarifying answers:
   - Derive checklist theme (e.g., security, review, deploy, ux)
   - Consolidate explicit must-have items mentioned by user
   - Map focus selections to category scaffolding
   - Infer any missing context from spec/plan/tasks (do NOT hallucinate)

4. **Load feature context**: Read from FEATURE_DIR:
   - spec.md: Feature requirements and scope
   - plan.md (if exists): Technical details, dependencies
   - tasks.md (if exists): Implementation tasks

   **Context Loading Strategy**:
   - Load only necessary portions relevant to active focus areas (avoid full-file dumping)
   - Prefer summarizing long sections into concise scenario/requirement bullets
   - Use progressive disclosure: add follow-on retrieval only if gaps detected
   - If source docs are large, generate interim summary items instead of embedding raw text

5. **Generate checklist** - Create "Unit Tests for Requirements":
   - Create `FEATURE_DIR/checklists/` directory if it doesn't exist
   - Generate unique checklist filename:
     - Use short, descriptive name based on domain (e.g., `ux.md`, `api.md`, `security.md`)
     - Format: `[domain].md`
     - If file exists, append to existing file
   - Number items sequentially starting from CHK001
   - Each `/sp.checklist` run creates a NEW file (never overwrites existing checklists)

   **CORE PRINCIPLE - Test the Requirements, Not the Implementation**:
   Every checklist item MUST evaluate the REQUIREMENTS THEMSELVES for:
   - **Completeness**: Are all necessary requirements present?
   - **Clarity**: Are requirements unambiguous and specific?
   - **Consistency**: Do requirements align with each other?
   - **Measurability**: Can requirements be objectively verified?
   - **Coverage**: Are all scenarios/edge cases addressed?

   **Category Structure** - Group items by requirement quality dimensions:
   - **Requirement Completeness** (Are all necessary requirements documented?)
   - **Requirement Clarity** (Are requirements specific and unambiguous?)
   - **Requirement Consistency** (Do requirements align without conflicts?)
   - **Acceptance Criteria Quality** (Are success criteria measurable?)
   - **Scenario Coverage** (Are all flows/cases addressed?)
   - **Edge Case Coverage** (Are boundary conditions defined?)
   - **Non-Functional Requirements** (Performance, Security, Accessibility, etc. - are they specified?)
   - **Dependencies & Assumptions** (Are they documented and validated?)
   - **Ambiguities & Conflicts** (What needs clarification?)

   **HOW TO WRITE CHECKLIST ITEMS - "Unit Tests for English"**:

   ‚ùå **WRONG** (Testing implementation):
   - "Verify landing page displays 3 episode cards"
   - "Test hover states work on desktop"
   - "Confirm logo click navigates home"

   ‚úÖ **CORRECT** (Testing requirements quality):
   - "Are the exact number and layout of featured episodes specified?" [Completeness]
   - "Is 'prominent display' quantified with specific sizing/positioning?" [Clarity]
   - "Are hover state requirements consistent across all interactive elements?" [Consistency]
   - "Are keyboard navigation requirements defined for all interactive UI?" [Coverage]
   - "Is the fallback behavior specified when logo image fails to load?" [Edge Cases]
   - "Are loading states defined for asynchronous episode data?" [Completeness]
   - "Does the spec define visual hierarchy for competing UI elements?" [Clarity]

   **ITEM STRUCTURE**:
   Each item should follow this pattern:
   - Question format asking about requirement quality
   - Focus on what's WRITTEN (or not written) in the spec/plan
   - Include quality dimension in brackets [Completeness/Clarity/Consistency/etc.]
   - Reference spec section `[Spec ¬ßX.Y]` when checking existing requirements
   - Use `[Gap]` marker when checking for missing requirements

   **EXAMPLES BY QUALITY DIMENSION**:

   Completeness:
   - "Are error handling requirements defined for all API failure modes? [Gap]"
   - "Are accessibility requirements specified for all interactive elements? [Completeness]"
   - "Are mobile breakpoint requirements defined for responsive layouts? [Gap]"

   Clarity:
   - "Is 'fast loading' quantified with specific timing thresholds? [Clarity, Spec ¬ßNFR-2]"
   - "Are 'related episodes' selection criteria explicitly defined? [Clarity, Spec ¬ßFR-5]"
   - "Is 'prominent' defined with measurable visual properties? [Ambiguity, Spec ¬ßFR-4]"

   Consistency:
   - "Do navigation requirements align across all pages? [Consistency, Spec ¬ßFR-10]"
   - "Are card component requirements consistent between landing and detail pages? [Consistency]"

   Coverage:
   - "Are requirements defined for zero-state scenarios (no episodes)? [Coverage, Edge Case]"
   - "Are concurrent user interaction scenarios addressed? [Coverage, Gap]"
   - "Are requirements specified for partial data loading failures? [Coverage, Exception Flow]"

   Measurability:
   - "Are visual hierarchy requirements measurable/testable? [Acceptance Criteria, Spec ¬ßFR-1]"
   - "Can 'balanced visual weight' be objectively verified? [Measurability, Spec ¬ßFR-2]"

   **Scenario Classification & Coverage** (Requirements Quality Focus):
   - Check if requirements exist for: Primary, Alternate, Exception/Error, Recovery, Non-Functional scenarios
   - For each scenario class, ask: "Are [scenario type] requirements complete, clear, and consistent?"
   - If scenario class missing: "Are [scenario type] requirements intentionally excluded or missing? [Gap]"
   - Include resilience/rollback when state mutation occurs: "Are rollback requirements defined for migration failures? [Gap]"

   **Traceability Requirements**:
   - MINIMUM: ‚â•80% of items MUST include at least one traceability reference
   - Each item should reference: spec section `[Spec ¬ßX.Y]`, or use markers: `[Gap]`, `[Ambiguity]`, `[Conflict]`, `[Assumption]`
   - If no ID system exists: "Is a requirement & acceptance criteria ID scheme established? [Traceability]"

   **Surface & Resolve Issues** (Requirements Quality Problems):
   Ask questions about the requirements themselves:
   - Ambiguities: "Is the term 'fast' quantified with specific metrics? [Ambiguity, Spec ¬ßNFR-1]"
   - Conflicts: "Do navigation requirements conflict between ¬ßFR-10 and ¬ßFR-10a? [Conflict]"
   - Assumptions: "Is the assumption of 'always available podcast API' validated? [Assumption]"
   - Dependencies: "Are external podcast API requirements documented? [Dependency, Gap]"
   - Missing definitions: "Is 'visual hierarchy' defined with measurable criteria? [Gap]"

   **Content Consolidation**:
   - Soft cap: If raw candidate items > 40, prioritize by risk/impact
   - Merge near-duplicates checking the same requirement aspect
   - If >5 low-impact edge cases, create one item: "Are edge cases X, Y, Z addressed in requirements? [Coverage]"

   **üö´ ABSOLUTELY PROHIBITED** - These make it an implementation test, not a requirements test:
   - ‚ùå Any item starting with "Verify", "Test", "Confirm", "Check" + implementation behavior
   - ‚ùå References to code execution, user actions, system behavior
   - ‚ùå "Displays correctly", "works properly", "functions as expected"
   - ‚ùå "Click", "navigate", "render", "load", "execute"
   - ‚ùå Test cases, test plans, QA procedures
   - ‚ùå Implementation details (frameworks, APIs, algorithms)

   **‚úÖ REQUIRED PATTERNS** - These test requirements quality:
   - ‚úÖ "Are [requirement type] defined/specified/documented for [scenario]?"
   - ‚úÖ "Is [vague term] quantified/clarified with specific criteria?"
   - ‚úÖ "Are requirements consistent between [section A] and [section B]?"
   - ‚úÖ "Can [requirement] be objectively measured/verified?"
   - ‚úÖ "Are [edge cases/scenarios] addressed in requirements?"
   - ‚úÖ "Does the spec define [missing aspect]?"

6. **Structure Reference**: Generate the checklist following the canonical template in `.specify/templates/checklist-template.md` for title, meta section, category headings, and ID formatting. If template is unavailable, use: H1 title, purpose/created meta lines, `##` category sections containing `- [ ] CHK### <requirement item>` lines with globally incrementing IDs starting at CHK001.

7. **Report**: Output full path to created checklist, item count, and remind user that each run creates a new file. Summarize:
   - Focus areas selected
   - Depth level
   - Actor/timing
   - Any explicit user-specified must-have items incorporated

**Important**: Each `/sp.checklist` command invocation creates a checklist file using short, descriptive names unless file already exists. This allows:

- Multiple checklists of different types (e.g., `ux.md`, `test.md`, `security.md`)
- Simple, memorable filenames that indicate checklist purpose
- Easy identification and navigation in the `checklists/` folder

To avoid clutter, use descriptive types and clean up obsolete checklists when done.

## Example Checklist Types & Sample Items

**UX Requirements Quality:** `ux.md`

Sample items (testing the requirements, NOT the implementation):

- "Are visual hierarchy requirements defined with measurable criteria? [Clarity, Spec ¬ßFR-1]"
- "Is the number and positioning of UI elements explicitly specified? [Completeness, Spec ¬ßFR-1]"
- "Are interaction state requirements (hover, focus, active) consistently defined? [Consistency]"
- "Are accessibility requirements specified for all interactive elements? [Coverage, Gap]"
- "Is fallback behavior defined when images fail to load? [Edge Case, Gap]"
- "Can 'prominent display' be objectively measured? [Measurability, Spec ¬ßFR-4]"

**API Requirements Quality:** `api.md`

Sample items:

- "Are error response formats specified for all failure scenarios? [Completeness]"
- "Are rate limiting requirements quantified with specific thresholds? [Clarity]"
- "Are authentication requirements consistent across all endpoints? [Consistency]"
- "Are retry/timeout requirements defined for external dependencies? [Coverage, Gap]"
- "Is versioning strategy documented in requirements? [Gap]"

**Performance Requirements Quality:** `performance.md`

Sample items:

- "Are performance requirements quantified with specific metrics? [Clarity]"
- "Are performance targets defined for all critical user journeys? [Coverage]"
- "Are performance requirements under different load conditions specified? [Completeness]"
- "Can performance requirements be objectively measured? [Measurability]"
- "Are degradation requirements defined for high-load scenarios? [Edge Case, Gap]"

**Security Requirements Quality:** `security.md`

Sample items:

- "Are authentication requirements specified for all protected resources? [Coverage]"
- "Are data protection requirements defined for sensitive information? [Completeness]"
- "Is the threat model documented and requirements aligned to it? [Traceability]"
- "Are security requirements consistent with compliance obligations? [Consistency]"
- "Are security failure/breach response requirements defined? [Gap, Exception Flow]"

## Anti-Examples: What NOT To Do

**‚ùå WRONG - These test implementation, not requirements:**

```markdown
- [ ] CHK001 - Verify landing page displays 3 episode cards [Spec ¬ßFR-001]
- [ ] CHK002 - Test hover states work correctly on desktop [Spec ¬ßFR-003]
- [ ] CHK003 - Confirm logo click navigates to home page [Spec ¬ßFR-010]
- [ ] CHK004 - Check that related episodes section shows 3-5 items [Spec ¬ßFR-005]
```

**‚úÖ CORRECT - These test requirements quality:**

```markdown
- [ ] CHK001 - Are the number and layout of featured episodes explicitly specified? [Completeness, Spec ¬ßFR-001]
- [ ] CHK002 - Are hover state requirements consistently defined for all interactive elements? [Consistency, Spec ¬ßFR-003]
- [ ] CHK003 - Are navigation requirements clear for all clickable brand elements? [Clarity, Spec ¬ßFR-010]
- [ ] CHK004 - Is the selection criteria for related episodes documented? [Gap, Spec ¬ßFR-005]
- [ ] CHK005 - Are loading state requirements defined for asynchronous episode data? [Gap]
- [ ] CHK006 - Can "visual hierarchy" requirements be objectively measured? [Measurability, Spec ¬ßFR-001]
```

**Key Differences:**

- Wrong: Tests if the system works correctly
- Correct: Tests if the requirements are written correctly
- Wrong: Verification of behavior
- Correct: Validation of requirement quality
- Wrong: "Does it do X?"
- Correct: "Is X clearly specified?"

---

As the main request completes, you MUST create and complete a PHR (Prompt History Record) using agent‚Äënative tools when possible.

1) Determine Stage
   - Stage: constitution | spec | plan | tasks | red | green | refactor | explainer | misc | general

2) Generate Title and Determine Routing:
   - Generate Title: 3‚Äì7 words (slug for filename)
   - Route is automatically determined by stage:
     - `constitution` ‚Üí `history/prompts/constitution/`
     - Feature stages ‚Üí `history/prompts/<feature-name>/` (spec, plan, tasks, red, green, refactor, explainer, misc)
     - `general` ‚Üí `history/prompts/general/`

3) Create and Fill PHR (Shell first; fallback agent‚Äënative)
   - Run: `.specify/scripts/bash/create-phr.sh --title "<title>" --stage <stage> [--feature <name>] --json`
   - Open the file and fill remaining placeholders (YAML + body), embedding full PROMPT_TEXT (verbatim) and concise RESPONSE_TEXT.
   - If the script fails:
     - Read `.specify/templates/phr-template.prompt.md` (or `templates/‚Ä¶`)
     - Allocate an ID; compute the output path based on stage from step 2; write the file
     - Fill placeholders and embed full PROMPT_TEXT and concise RESPONSE_TEXT

4) Validate + report
   - No unresolved placeholders; path under `history/prompts/` and matches stage; stage/title/date coherent; print ID + path + stage + title.
   - On failure: warn, don't block. Skip only for `/sp.phr`.


============================================================
FILE: .claude\commands\sp.clarify.md
============================================================
---
description: Identify underspecified areas in the current feature spec by asking up to 5 highly targeted clarification questions and encoding answers back into the spec.
handoffs: 
  - label: Build Technical Plan
    agent: sp.plan
    prompt: Create a plan for the spec. I am building with...
---

## User Input

```text
$ARGUMENTS
```

You **MUST** consider the user input before proceeding (if not empty).

## Outline

Goal: Detect and reduce ambiguity or missing decision points in the active feature specification and record the clarifications directly in the spec file.

Note: This clarification workflow is expected to run (and be completed) BEFORE invoking `/sp.plan`. If the user explicitly states they are skipping clarification (e.g., exploratory spike), you may proceed, but must warn that downstream rework risk increases.

Execution steps:

1. Run `.specify/scripts/powershell/check-prerequisites.ps1 -Json -PathsOnly` from repo root **once** (combined `--json --paths-only` mode / `-Json -PathsOnly`). Parse minimal JSON payload fields:
   - `FEATURE_DIR`
   - `FEATURE_SPEC`
   - (Optionally capture `IMPL_PLAN`, `TASKS` for future chained flows.)
   - If JSON parsing fails, abort and instruct user to re-run `/sp.specify` or verify feature branch environment.
   - For single quotes in args like "I'm Groot", use escape syntax: e.g 'I'\''m Groot' (or double-quote if possible: "I'm Groot").

2. Load the current spec file. Perform a structured ambiguity & coverage scan using this taxonomy. For each category, mark status: Clear / Partial / Missing. Produce an internal coverage map used for prioritization (do not output raw map unless no questions will be asked).

   Functional Scope & Behavior:
   - Core user goals & success criteria
   - Explicit out-of-scope declarations
   - User roles / personas differentiation

   Domain & Data Model:
   - Entities, attributes, relationships
   - Identity & uniqueness rules
   - Lifecycle/state transitions
   - Data volume / scale assumptions

   Interaction & UX Flow:
   - Critical user journeys / sequences
   - Error/empty/loading states
   - Accessibility or localization notes

   Non-Functional Quality Attributes:
   - Performance (latency, throughput targets)
   - Scalability (horizontal/vertical, limits)
   - Reliability & availability (uptime, recovery expectations)
   - Observability (logging, metrics, tracing signals)
   - Security & privacy (authN/Z, data protection, threat assumptions)
   - Compliance / regulatory constraints (if any)

   Integration & External Dependencies:
   - External services/APIs and failure modes
   - Data import/export formats
   - Protocol/versioning assumptions

   Edge Cases & Failure Handling:
   - Negative scenarios
   - Rate limiting / throttling
   - Conflict resolution (e.g., concurrent edits)

   Constraints & Tradeoffs:
   - Technical constraints (language, storage, hosting)
   - Explicit tradeoffs or rejected alternatives

   Terminology & Consistency:
   - Canonical glossary terms
   - Avoided synonyms / deprecated terms

   Completion Signals:
   - Acceptance criteria testability
   - Measurable Definition of Done style indicators

   Misc / Placeholders:
   - TODO markers / unresolved decisions
   - Ambiguous adjectives ("robust", "intuitive") lacking quantification

   For each category with Partial or Missing status, add a candidate question opportunity unless:
   - Clarification would not materially change implementation or validation strategy
   - Information is better deferred to planning phase (note internally)

3. Generate (internally) a prioritized queue of candidate clarification questions (maximum 5). Do NOT output them all at once. Apply these constraints:
    - Maximum of 10 total questions across the whole session.
    - Each question must be answerable with EITHER:
       - A short multiple‚Äëchoice selection (2‚Äì5 distinct, mutually exclusive options), OR
       - A one-word / short‚Äëphrase answer (explicitly constrain: "Answer in <=5 words").
    - Only include questions whose answers materially impact architecture, data modeling, task decomposition, test design, UX behavior, operational readiness, or compliance validation.
    - Ensure category coverage balance: attempt to cover the highest impact unresolved categories first; avoid asking two low-impact questions when a single high-impact area (e.g., security posture) is unresolved.
    - Exclude questions already answered, trivial stylistic preferences, or plan-level execution details (unless blocking correctness).
    - Favor clarifications that reduce downstream rework risk or prevent misaligned acceptance tests.
    - If more than 5 categories remain unresolved, select the top 5 by (Impact * Uncertainty) heuristic.

4. Sequential questioning loop (interactive):
    - Present EXACTLY ONE question at a time.
    - For multiple‚Äëchoice questions:
       - **Analyze all options** and determine the **most suitable option** based on:
          - Best practices for the project type
          - Common patterns in similar implementations
          - Risk reduction (security, performance, maintainability)
          - Alignment with any explicit project goals or constraints visible in the spec
       - Present your **recommended option prominently** at the top with clear reasoning (1-2 sentences explaining why this is the best choice).
       - Format as: `**Recommended:** Option [X] - <reasoning>`
       - Then render all options as a Markdown table:

       | Option | Description |
       |--------|-------------|
       | A | <Option A description> |
       | B | <Option B description> |
       | C | <Option C description> (add D/E as needed up to 5) |
       | Short | Provide a different short answer (<=5 words) (Include only if free-form alternative is appropriate) |

       - After the table, add: `You can reply with the option letter (e.g., "A"), accept the recommendation by saying "yes" or "recommended", or provide your own short answer.`
    - For short‚Äëanswer style (no meaningful discrete options):
       - Provide your **suggested answer** based on best practices and context.
       - Format as: `**Suggested:** <your proposed answer> - <brief reasoning>`
       - Then output: `Format: Short answer (<=5 words). You can accept the suggestion by saying "yes" or "suggested", or provide your own answer.`
    - After the user answers:
       - If the user replies with "yes", "recommended", or "suggested", use your previously stated recommendation/suggestion as the answer.
       - Otherwise, validate the answer maps to one option or fits the <=5 word constraint.
       - If ambiguous, ask for a quick disambiguation (count still belongs to same question; do not advance).
       - Once satisfactory, record it in working memory (do not yet write to disk) and move to the next queued question.
    - Stop asking further questions when:
       - All critical ambiguities resolved early (remaining queued items become unnecessary), OR
       - User signals completion ("done", "good", "no more"), OR
       - You reach 5 asked questions.
    - Never reveal future queued questions in advance.
    - If no valid questions exist at start, immediately report no critical ambiguities.

5. Integration after EACH accepted answer (incremental update approach):
    - Maintain in-memory representation of the spec (loaded once at start) plus the raw file contents.
    - For the first integrated answer in this session:
       - Ensure a `## Clarifications` section exists (create it just after the highest-level contextual/overview section per the spec template if missing).
       - Under it, create (if not present) a `### Session YYYY-MM-DD` subheading for today.
    - Append a bullet line immediately after acceptance: `- Q: <question> ‚Üí A: <final answer>`.
    - Then immediately apply the clarification to the most appropriate section(s):
       - Functional ambiguity ‚Üí Update or add a bullet in Functional Requirements.
       - User interaction / actor distinction ‚Üí Update User Stories or Actors subsection (if present) with clarified role, constraint, or scenario.
       - Data shape / entities ‚Üí Update Data Model (add fields, types, relationships) preserving ordering; note added constraints succinctly.
       - Non-functional constraint ‚Üí Add/modify measurable criteria in Non-Functional / Quality Attributes section (convert vague adjective to metric or explicit target).
       - Edge case / negative flow ‚Üí Add a new bullet under Edge Cases / Error Handling (or create such subsection if template provides placeholder for it).
       - Terminology conflict ‚Üí Normalize term across spec; retain original only if necessary by adding `(formerly referred to as "X")` once.
    - If the clarification invalidates an earlier ambiguous statement, replace that statement instead of duplicating; leave no obsolete contradictory text.
    - Save the spec file AFTER each integration to minimize risk of context loss (atomic overwrite).
    - Preserve formatting: do not reorder unrelated sections; keep heading hierarchy intact.
    - Keep each inserted clarification minimal and testable (avoid narrative drift).

6. Validation (performed after EACH write plus final pass):
   - Clarifications session contains exactly one bullet per accepted answer (no duplicates).
   - Total asked (accepted) questions ‚â§ 5.
   - Updated sections contain no lingering vague placeholders the new answer was meant to resolve.
   - No contradictory earlier statement remains (scan for now-invalid alternative choices removed).
   - Markdown structure valid; only allowed new headings: `## Clarifications`, `### Session YYYY-MM-DD`.
   - Terminology consistency: same canonical term used across all updated sections.

7. Write the updated spec back to `FEATURE_SPEC`.

8. Report completion (after questioning loop ends or early termination):
   - Number of questions asked & answered.
   - Path to updated spec.
   - Sections touched (list names).
   - Coverage summary table listing each taxonomy category with Status: Resolved (was Partial/Missing and addressed), Deferred (exceeds question quota or better suited for planning), Clear (already sufficient), Outstanding (still Partial/Missing but low impact).
   - If any Outstanding or Deferred remain, recommend whether to proceed to `/sp.plan` or run `/sp.clarify` again later post-plan.
   - Suggested next command.

Behavior rules:

- If no meaningful ambiguities found (or all potential questions would be low-impact), respond: "No critical ambiguities detected worth formal clarification." and suggest proceeding.
- If spec file missing, instruct user to run `/sp.specify` first (do not create a new spec here).
- Never exceed 5 total asked questions (clarification retries for a single question do not count as new questions).
- Avoid speculative tech stack questions unless the absence blocks functional clarity.
- Respect user early termination signals ("stop", "done", "proceed").
- If no questions asked due to full coverage, output a compact coverage summary (all categories Clear) then suggest advancing.
- If quota reached with unresolved high-impact categories remaining, explicitly flag them under Deferred with rationale.

Context for prioritization: $ARGUMENTS

---

As the main request completes, you MUST create and complete a PHR (Prompt History Record) using agent‚Äënative tools when possible.

1) Determine Stage
   - Stage: constitution | spec | plan | tasks | red | green | refactor | explainer | misc | general

2) Generate Title and Determine Routing:
   - Generate Title: 3‚Äì7 words (slug for filename)
   - Route is automatically determined by stage:
     - `constitution` ‚Üí `history/prompts/constitution/`
     - Feature stages ‚Üí `history/prompts/<feature-name>/` (spec, plan, tasks, red, green, refactor, explainer, misc)
     - `general` ‚Üí `history/prompts/general/`

3) Create and Fill PHR (Shell first; fallback agent‚Äënative)
   - Run: `.specify/scripts/bash/create-phr.sh --title "<title>" --stage <stage> [--feature <name>] --json`
   - Open the file and fill remaining placeholders (YAML + body), embedding full PROMPT_TEXT (verbatim) and concise RESPONSE_TEXT.
   - If the script fails:
     - Read `.specify/templates/phr-template.prompt.md` (or `templates/‚Ä¶`)
     - Allocate an ID; compute the output path based on stage from step 2; write the file
     - Fill placeholders and embed full PROMPT_TEXT and concise RESPONSE_TEXT

4) Validate + report
   - No unresolved placeholders; path under `history/prompts/` and matches stage; stage/title/date coherent; print ID + path + stage + title.
   - On failure: warn, don't block. Skip only for `/sp.phr`.


============================================================
FILE: .claude\commands\sp.constitution.md
============================================================
---
description: Create or update the project constitution from interactive or provided principle inputs, ensuring all dependent templates stay in sync.
handoffs: 
  - label: Build Specification
    agent: sp.specify
    prompt: Implement the feature specification based on the updated constitution. I want to build...
---

## User Input

```text
$ARGUMENTS
```

You **MUST** consider the user input before proceeding (if not empty).

## Outline

You are updating the project constitution at `.specify/memory/constitution.md`. This file is a TEMPLATE containing placeholder tokens in square brackets (e.g. `[PROJECT_NAME]`, `[PRINCIPLE_1_NAME]`). Your job is to (a) collect/derive concrete values, (b) fill the template precisely, and (c) propagate any amendments across dependent artifacts.

Follow this execution flow:

1. Load the existing constitution template at `.specify/memory/constitution.md`.
   - Identify every placeholder token of the form `[ALL_CAPS_IDENTIFIER]`.
   **IMPORTANT**: The user might require less or more principles than the ones used in the template. If a number is specified, respect that - follow the general template. You will update the doc accordingly.

2. Collect/derive values for placeholders:
   - If user input (conversation) supplies a value, use it.
   - Otherwise infer from existing repo context (README, docs, prior constitution versions if embedded).
   - For governance dates: `RATIFICATION_DATE` is the original adoption date (if unknown ask or mark TODO), `LAST_AMENDED_DATE` is today if changes are made, otherwise keep previous.
   - `CONSTITUTION_VERSION` must increment according to semantic versioning rules:
     - MAJOR: Backward incompatible governance/principle removals or redefinitions.
     - MINOR: New principle/section added or materially expanded guidance.
     - PATCH: Clarifications, wording, typo fixes, non-semantic refinements.
   - If version bump type ambiguous, propose reasoning before finalizing.

3. Draft the updated constitution content:
   - Replace every placeholder with concrete text (no bracketed tokens left except intentionally retained template slots that the project has chosen not to define yet‚Äîexplicitly justify any left).
   - Preserve heading hierarchy and comments can be removed once replaced unless they still add clarifying guidance.
   - Ensure each Principle section: succinct name line, paragraph (or bullet list) capturing non‚Äënegotiable rules, explicit rationale if not obvious.
   - Ensure Governance section lists amendment procedure, versioning policy, and compliance review expectations.

4. Consistency propagation checklist (convert prior checklist into active validations):
   - Read `.specify/templates/plan-template.md` and ensure any "Constitution Check" or rules align with updated principles.
   - Read `.specify/templates/spec-template.md` for scope/requirements alignment‚Äîupdate if constitution adds/removes mandatory sections or constraints.
   - Read `.specify/templates/tasks-template.md` and ensure task categorization reflects new or removed principle-driven task types (e.g., observability, versioning, testing discipline).
   - Read each command file in `.specify/templates/commands/*.md` (including this one) to verify no outdated references (agent-specific names like CLAUDE only) remain when generic guidance is required.
   - Read any runtime guidance docs (e.g., `README.md`, `docs/quickstart.md`, or agent-specific guidance files if present). Update references to principles changed.

5. Produce a Sync Impact Report (prepend as an HTML comment at top of the constitution file after update):
   - Version change: old ‚Üí new
   - List of modified principles (old title ‚Üí new title if renamed)
   - Added sections
   - Removed sections
   - Templates requiring updates (‚úÖ updated / ‚ö† pending) with file paths
   - Follow-up TODOs if any placeholders intentionally deferred.

6. Validation before final output:
   - No remaining unexplained bracket tokens.
   - Version line matches report.
   - Dates ISO format YYYY-MM-DD.
   - Principles are declarative, testable, and free of vague language ("should" ‚Üí replace with MUST/SHOULD rationale where appropriate).

7. Write the completed constitution back to `.specify/memory/constitution.md` (overwrite).

8. Output a final summary to the user with:
   - New version and bump rationale.
   - Any files flagged for manual follow-up.
   - Suggested commit message (e.g., `docs: amend constitution to vX.Y.Z (principle additions + governance update)`).

Formatting & Style Requirements:

- Use Markdown headings exactly as in the template (do not demote/promote levels).
- Wrap long rationale lines to keep readability (<100 chars ideally) but do not hard enforce with awkward breaks.
- Keep a single blank line between sections.
- Avoid trailing whitespace.

If the user supplies partial updates (e.g., only one principle revision), still perform validation and version decision steps.

If critical info missing (e.g., ratification date truly unknown), insert `TODO(<FIELD_NAME>): explanation` and include in the Sync Impact Report under deferred items.

Do not create a new template; always operate on the existing `.specify/memory/constitution.md` file.

---

As the main request completes, you MUST create and complete a PHR (Prompt History Record) using agent‚Äënative tools when possible.

1) Determine Stage
   - Stage: constitution | spec | plan | tasks | red | green | refactor | explainer | misc | general

2) Generate Title and Determine Routing:
   - Generate Title: 3‚Äì7 words (slug for filename)
   - Route is automatically determined by stage:
     - `constitution` ‚Üí `history/prompts/constitution/`
     - Feature stages ‚Üí `history/prompts/<feature-name>/` (spec, plan, tasks, red, green, refactor, explainer, misc)
     - `general` ‚Üí `history/prompts/general/`

3) Create and Fill PHR (Shell first; fallback agent‚Äënative)
   - Run: `.specify/scripts/bash/create-phr.sh --title "<title>" --stage <stage> [--feature <name>] --json`
   - Open the file and fill remaining placeholders (YAML + body), embedding full PROMPT_TEXT (verbatim) and concise RESPONSE_TEXT.
   - If the script fails:
     - Read `.specify/templates/phr-template.prompt.md` (or `templates/‚Ä¶`)
     - Allocate an ID; compute the output path based on stage from step 2; write the file
     - Fill placeholders and embed full PROMPT_TEXT and concise RESPONSE_TEXT

4) Validate + report
   - No unresolved placeholders; path under `history/prompts/` and matches stage; stage/title/date coherent; print ID + path + stage + title.
   - On failure: warn, don't block. Skip only for `/sp.phr`.


============================================================
FILE: .claude\commands\sp.git.commit_pr.md
============================================================
---
description: An autonomous Git agent that intelligently executes git workflows. Your task is to intelligently executes git workflows to commit the work and create PR.
---

Your task is to intelligently executes git workflows to commit the work and create PR following your Principles

# Agentic Git Workflow Agent

## Core Principle

You are an autonomous Git agent. Your job is to **fulfill the user's intent efficiently**. You have agency to:
- Analyze the current state independently
- Make intelligent decisions about the best workflow
- Execute steps without asking permission for each one
- Invoke the human validator only when the decision requires their judgment

The human is not a step-orchestrator. The human is an **intent-provider** and **decision validator**.

## Your Agency

You can autonomously:
‚úÖ Analyze repository state  
‚úÖ Determine optimal branch strategy  
‚úÖ Generate meaningful commit messages based on code changes  
‚úÖ Create branches, commits, and push to remote  
‚úÖ Create PRs with intelligent titles and descriptions  
‚úÖ Detect and handle common errors  

You CANNOT autonomously:
‚ùå Run long-running processes (servers, watchers, etc.)  
‚ùå Execute code that blocks indefinitely  
‚ùå Make changes outside the repo (create files elsewhere, etc.)  
‚ùå Execute destructive commands without explicit approval  

You invoke the human when:
üî¥ The intent is ambiguous  
üî¥ Multiple equally-valid strategies exist and you need to know their preference  
üî¥ You detect something risky or unexpected  
üî¥ The outcome differs significantly from what was requested  
üî¥ Any non-Git command would run indefinitely or block execution  

## Phase 1: Context Gathering (Autonomous)

Start by understanding the complete situation:

```bash
git --version                        # Verify Git exists
git rev-parse --is-inside-work-tree  # Verify we're in a repo
git status --porcelain               # See what changed
git diff --stat                      # Quantify changes
git log --oneline -5                 # Recent history context
git rev-parse --abbrev-ref HEAD      # Current branch
git remote -v                        # Remote configuration
```

**CRITICAL:** Only run Git commands. Do not:
- Run `python main.py`, `npm start`, `make`, or other build/start scripts
- Execute anything that might be long-running or blocking
- Run tests, servers, or development tools

If Git is not available or this isn't a repo, **invoke human validator** with the problem.

## Phase 2: Analyze & Decide (Autonomous)

Based on the gathered context, **you decide** the optimal approach:

### Decision Tree:

**Are there uncommitted changes?**
- Yes ‚Üí Continue to strategy decision
- No ‚Üí Invoke human: "No changes detected. What would you like to commit?"

**What's the nature of changes?** (Analyze via `git diff`)
- New feature files ‚Üí Feature branch strategy
- Tests only ‚Üí Test/fix branch strategy
- Documentation ‚Üí Docs branch strategy
- Mixed/refactor ‚Üí Analysis-dependent

**What branch are we on?**
- `main` or `master` or protected branch ‚Üí Must create feature branch
- Feature branch with tracking ‚Üí Commit and optionally create/update PR
- Detached HEAD or unusual state ‚Üí Invoke human

**What strategy is optimal?**

1. **If feature branch doesn't exist yet:**
   - Create feature branch from current base
   - Commit changes
   - Push with upstream tracking
   - Create PR to main/dev/appropriate base

2. **If feature branch exists with upstream:**
   - Commit to current branch
   - Push updates
   - Check if PR exists; create if not

3. **If on protected branch with changes:**
   - Create feature branch from current state
   - Move changes to new branch
   - Commit and push
   - Create PR

**Make this decision autonomously.** You don't need permission to decide‚Äîonly when the choice itself is uncertain.

## Phase 3: Generate Intelligent Content (Autonomous)

### Branch Name
Analyze the changes to create a meaningful branch name:
```bash
git diff --name-only
```

Look at:
- Files changed (domain extraction)
- Commit intent (if user provided one)
- Repository conventions (existing branch names via `git branch -r`)

Generate a name that's:
- Descriptive (2-4 words)
- Follows existing conventions
- Reflects the actual change

Examples:
- `add-auth-validation` (from "Add login validation" + auth-related files)
- `fix-query-timeout` (from files in db/queries/)
- `docs-update-readme` (from README.md changes)

### Commit Message
Analyze the code diff and generate a conventional commit:

```
<type>(<scope>): <subject>

<body explaining why, not what>
```

- **type**: feat, fix, chore, refactor, docs, test (determined from change analysis)
- **scope**: Primary area affected
- **subject**: Imperative, what this commit does
- **body**: Why this change was needed

**Do not ask the user for a commit message.** Extract intent from:
- Their stated purpose (if provided)
- The code changes themselves
- File modifications

### PR Title & Description
Create automatically:
- **Title**: Based on commit message or user intent
- **Description**: 
  - What changed
  - Why it matters
  - Files affected
  - Related issues (if detectable)

## Phase 4: Execute (Autonomous)

Execute the workflow you decided:

```bash
git add .
git checkout -b           # or git switch if branch exists
git commit -m ""
git push -u origin 
gh pr create --title "" --body ""
```

Handle common errors autonomously:
- `git push` fails (auth/permission) ‚Üí Report clearly, suggest manual push
- `gh` not available ‚Üí Provide manual PR URL: `https://github.com/<owner>/<repo>/compare/<branch>`
- Merge conflicts ‚Üí Stop and invoke human

## Phase 5: Validate & Report (Conditional)

**After execution, evaluate the outcome:**

Compare your executed workflow against the user's original intent.

**If outcome matches intent:** ‚úÖ Report success
```
‚úÖ Workflow executed successfully:
  ‚Ä¢ Branch: feature/add-auth-validation
  ‚Ä¢ Commit: "feat(auth): add login validation"
  ‚Ä¢ PR: https://github.com/...
```

**If outcome differs significantly:** üî¥ Invoke human validator
```
‚ö†Ô∏è Outcome differs from intent:
  ‚Ä¢ Your intent: "Update documentation"
  ‚Ä¢ Actual changes: 15 files modified, 3 new features detected
  
Does this reflect what you wanted? If not, what should I have done?
```

**If something was unexpected:** üî¥ Invoke human validator
```
‚ö†Ô∏è Unexpected state detected:
  ‚Ä¢ On protected branch 'main'
  ‚Ä¢ User provided intent but no files changed
  ‚Ä¢ Branch already has open PR
  
What should I do?
```

## When to Invoke Human Validator

Use the `invoke_human` tool when:

### 1. Ambiguous Intent
**User said:** "Do the thing"  
**You need:** Clarification on what "the thing" is

### 2. Risk Detected
**Scenario:** Changes affect core system, or branch already exists with different content  
**Action:** Ask for confirmation: "I detected this might break X. Continue? [Y/n]"

### 3. Multiple Valid Strategies
**Scenario:** Could create new branch OR commit to existing, both valid  
**Action:** Present the decision: "I can do [A] or [B]. Which do you prefer?"

### 4. Outcome Validation
**Scenario:** Workflow executed but results differ from intent  
**Action:** Ask: "Does this match what you wanted?"

### 5. Environment Issues
**Scenario:** Git/GitHub not configured, credentials missing, unexpected state  
**Action:** Explain the blocker and ask for guidance

## Format for Human Invocation

When you need to invoke the human validator, format clearly:

```
üî¥ DECISION NEEDED

Situation: <What you're trying to do>
Problem/Options: <Why you need human input>

Option A: <First approach>
Option B: <Second approach>

What would you prefer? [A/B/other]
```

Or for validation:

```
‚úÖ OUTCOME VALIDATION

I executed: <What I did>
Result: <What happened>

Does this match your intent? [Y/n]
If not, what should I have done?
```

## What You Decide Autonomously

‚úÖ Branch strategy  
‚úÖ Branch naming  
‚úÖ Commit message generation  
‚úÖ PR creation  
‚úÖ Workflow execution (Git only)  
‚úÖ Error recovery (when possible)  
‚úÖ Reading files to analyze changes  

## What You NEVER Do Autonomously

‚ùå Run servers, watchers, or development tools  
‚ùå Execute build steps unless explicitly asked  
‚ùå Run tests or other processes  
‚ùå Execute anything that blocks or runs indefinitely  
‚ùå Run commands outside of Git operations  

## What Requires Human Input

üî¥ Clarifying ambiguous intent  
üî¥ Choosing between equally valid strategies  
üî¥ Confirming risky actions  
üî¥ Validating outcomes don't match intent  
üî¥ Resolving blockers  

## Example Execution

**User Intent:** "I added email validation to the auth system"

**You (autonomous):**
1. Gather context ‚Üí See auth files + validation logic changes
2. Decide ‚Üí Create feature branch, conventional commit, PR to main
3. Generate ‚Üí Branch: `add-email-validation`, Commit: "feat(auth): add email validation"
4. Execute ‚Üí All steps without asking
5. Report ‚Üí Show what was done + PR link
6. Validate ‚Üí Check if outcome matches intent

**If something was off:**
- You executed correctly but sense it wasn't what they meant ‚Üí Invoke validator
- They later say "Actually I meant..." ‚Üí Update accordingly

## Philosophy

You are not a tool waiting for instructions. You are an agent fulfilling intent. The human provides direction; you provide execution. Invoke them only when you genuinely need their judgment, not for step-by-step choreography.

---

As the main request completes, you MUST create and complete a PHR (Prompt History Record) using agent‚Äënative tools when possible.

1) Determine Stage
   - Stage: constitution | spec | plan | tasks | red | green | refactor | explainer | misc | general

2) Generate Title and Determine Routing:
   - Generate Title: 3‚Äì7 words (slug for filename)
   - Route is automatically determined by stage:
     - `constitution` ‚Üí `history/prompts/constitution/`
     - Feature stages ‚Üí `history/prompts/<feature-name>/` (spec, plan, tasks, red, green, refactor, explainer, misc)
     - `general` ‚Üí `history/prompts/general/`

3) Create and Fill PHR (Shell first; fallback agent‚Äënative)
   - Run: `.specify/scripts/bash/create-phr.sh --title "<title>" --stage <stage> [--feature <name>] --json`
   - Open the file and fill remaining placeholders (YAML + body), embedding full PROMPT_TEXT (verbatim) and concise RESPONSE_TEXT.
   - If the script fails:
     - Read `.specify/templates/phr-template.prompt.md` (or `templates/‚Ä¶`)
     - Allocate an ID; compute the output path based on stage from step 2; write the file
     - Fill placeholders and embed full PROMPT_TEXT and concise RESPONSE_TEXT

4) Validate + report
   - No unresolved placeholders; path under `history/prompts/` and matches stage; stage/title/date coherent; print ID + path + stage + title.
   - On failure: warn, don't block. Skip only for `/sp.phr`.


============================================================
FILE: .claude\commands\sp.implement.md
============================================================
---
description: Execute the implementation plan by processing and executing all tasks defined in tasks.md
---

## User Input

```text
$ARGUMENTS
```

You **MUST** consider the user input before proceeding (if not empty).

## Outline

1. Run `.specify/scripts/powershell/check-prerequisites.ps1 -Json -RequireTasks -IncludeTasks` from repo root and parse FEATURE_DIR and AVAILABLE_DOCS list. All paths must be absolute. For single quotes in args like "I'm Groot", use escape syntax: e.g 'I'\''m Groot' (or double-quote if possible: "I'm Groot").

2. **Check checklists status** (if FEATURE_DIR/checklists/ exists):
   - Scan all checklist files in the checklists/ directory
   - For each checklist, count:
     - Total items: All lines matching `- [ ]` or `- [X]` or `- [x]`
     - Completed items: Lines matching `- [X]` or `- [x]`
     - Incomplete items: Lines matching `- [ ]`
   - Create a status table:

     ```text
     | Checklist | Total | Completed | Incomplete | Status |
     |-----------|-------|-----------|------------|--------|
     | ux.md     | 12    | 12        | 0          | ‚úì PASS |
     | test.md   | 8     | 5         | 3          | ‚úó FAIL |
     | security.md | 6   | 6         | 0          | ‚úì PASS |
     ```

   - Calculate overall status:
     - **PASS**: All checklists have 0 incomplete items
     - **FAIL**: One or more checklists have incomplete items

   - **If any checklist is incomplete**:
     - Display the table with incomplete item counts
     - **STOP** and ask: "Some checklists are incomplete. Do you want to proceed with implementation anyway? (yes/no)"
     - Wait for user response before continuing
     - If user says "no" or "wait" or "stop", halt execution
     - If user says "yes" or "proceed" or "continue", proceed to step 3

   - **If all checklists are complete**:
     - Display the table showing all checklists passed
     - Automatically proceed to step 3

3. Load and analyze the implementation context:
   - **REQUIRED**: Read tasks.md for the complete task list and execution plan
   - **REQUIRED**: Read plan.md for tech stack, architecture, and file structure
   - **IF EXISTS**: Read data-model.md for entities and relationships
   - **IF EXISTS**: Read contracts/ for API specifications and test requirements
   - **IF EXISTS**: Read research.md for technical decisions and constraints
   - **IF EXISTS**: Read quickstart.md for integration scenarios

4. **Project Setup Verification**:
   - **REQUIRED**: Create/verify ignore files based on actual project setup:

   **Detection & Creation Logic**:
   - Check if the following command succeeds to determine if the repository is a git repo (create/verify .gitignore if so):

     ```sh
     git rev-parse --git-dir 2>/dev/null
     ```

   - Check if Dockerfile* exists or Docker in plan.md ‚Üí create/verify .dockerignore
   - Check if .eslintrc* exists ‚Üí create/verify .eslintignore
   - Check if eslint.config.* exists ‚Üí ensure the config's `ignores` entries cover required patterns
   - Check if .prettierrc* exists ‚Üí create/verify .prettierignore
   - Check if .npmrc or package.json exists ‚Üí create/verify .npmignore (if publishing)
   - Check if terraform files (*.tf) exist ‚Üí create/verify .terraformignore
   - Check if .helmignore needed (helm charts present) ‚Üí create/verify .helmignore

   **If ignore file already exists**: Verify it contains essential patterns, append missing critical patterns only
   **If ignore file missing**: Create with full pattern set for detected technology

   **Common Patterns by Technology** (from plan.md tech stack):
   - **Node.js/JavaScript/TypeScript**: `node_modules/`, `dist/`, `build/`, `*.log`, `.env*`
   - **Python**: `__pycache__/`, `*.pyc`, `.venv/`, `venv/`, `dist/`, `*.egg-info/`
   - **Java**: `target/`, `*.class`, `*.jar`, `.gradle/`, `build/`
   - **C#/.NET**: `bin/`, `obj/`, `*.user`, `*.suo`, `packages/`
   - **Go**: `*.exe`, `*.test`, `vendor/`, `*.out`
   - **Ruby**: `.bundle/`, `log/`, `tmp/`, `*.gem`, `vendor/bundle/`
   - **PHP**: `vendor/`, `*.log`, `*.cache`, `*.env`
   - **Rust**: `target/`, `debug/`, `release/`, `*.rs.bk`, `*.rlib`, `*.prof*`, `.idea/`, `*.log`, `.env*`
   - **Kotlin**: `build/`, `out/`, `.gradle/`, `.idea/`, `*.class`, `*.jar`, `*.iml`, `*.log`, `.env*`
   - **C++**: `build/`, `bin/`, `obj/`, `out/`, `*.o`, `*.so`, `*.a`, `*.exe`, `*.dll`, `.idea/`, `*.log`, `.env*`
   - **C**: `build/`, `bin/`, `obj/`, `out/`, `*.o`, `*.a`, `*.so`, `*.exe`, `Makefile`, `config.log`, `.idea/`, `*.log`, `.env*`
   - **Swift**: `.build/`, `DerivedData/`, `*.swiftpm/`, `Packages/`
   - **R**: `.Rproj.user/`, `.Rhistory`, `.RData`, `.Ruserdata`, `*.Rproj`, `packrat/`, `renv/`
   - **Universal**: `.DS_Store`, `Thumbs.db`, `*.tmp`, `*.swp`, `.vscode/`, `.idea/`

   **Tool-Specific Patterns**:
   - **Docker**: `node_modules/`, `.git/`, `Dockerfile*`, `.dockerignore`, `*.log*`, `.env*`, `coverage/`
   - **ESLint**: `node_modules/`, `dist/`, `build/`, `coverage/`, `*.min.js`
   - **Prettier**: `node_modules/`, `dist/`, `build/`, `coverage/`, `package-lock.json`, `yarn.lock`, `pnpm-lock.yaml`
   - **Terraform**: `.terraform/`, `*.tfstate*`, `*.tfvars`, `.terraform.lock.hcl`
   - **Kubernetes/k8s**: `*.secret.yaml`, `secrets/`, `.kube/`, `kubeconfig*`, `*.key`, `*.crt`

5. Parse tasks.md structure and extract:
   - **Task phases**: Setup, Tests, Core, Integration, Polish
   - **Task dependencies**: Sequential vs parallel execution rules
   - **Task details**: ID, description, file paths, parallel markers [P]
   - **Execution flow**: Order and dependency requirements

6. Execute implementation following the task plan:
   - **Phase-by-phase execution**: Complete each phase before moving to the next
   - **Respect dependencies**: Run sequential tasks in order, parallel tasks [P] can run together  
   - **Follow TDD approach**: Execute test tasks before their corresponding implementation tasks
   - **File-based coordination**: Tasks affecting the same files must run sequentially
   - **Validation checkpoints**: Verify each phase completion before proceeding

7. Implementation execution rules:
   - **Setup first**: Initialize project structure, dependencies, configuration
   - **Tests before code**: If you need to write tests for contracts, entities, and integration scenarios
   - **Core development**: Implement models, services, CLI commands, endpoints
   - **Integration work**: Database connections, middleware, logging, external services
   - **Polish and validation**: Unit tests, performance optimization, documentation

8. Progress tracking and error handling:
   - Report progress after each completed task
   - Halt execution if any non-parallel task fails
   - For parallel tasks [P], continue with successful tasks, report failed ones
   - Provide clear error messages with context for debugging
   - Suggest next steps if implementation cannot proceed
   - **IMPORTANT** For completed tasks, make sure to mark the task off as [X] in the tasks file.

9. Completion validation:
   - Verify all required tasks are completed
   - Check that implemented features match the original specification
   - Validate that tests pass and coverage meets requirements
   - Confirm the implementation follows the technical plan
   - Report final status with summary of completed work

Note: This command assumes a complete task breakdown exists in tasks.md. If tasks are incomplete or missing, suggest running `/sp.tasks` first to regenerate the task list.

---

As the main request completes, you MUST create and complete a PHR (Prompt History Record) using agent‚Äënative tools when possible.

1) Determine Stage
   - Stage: constitution | spec | plan | tasks | red | green | refactor | explainer | misc | general

2) Generate Title and Determine Routing:
   - Generate Title: 3‚Äì7 words (slug for filename)
   - Route is automatically determined by stage:
     - `constitution` ‚Üí `history/prompts/constitution/`
     - Feature stages ‚Üí `history/prompts/<feature-name>/` (spec, plan, tasks, red, green, refactor, explainer, misc)
     - `general` ‚Üí `history/prompts/general/`

3) Create and Fill PHR (Shell first; fallback agent‚Äënative)
   - Run: `.specify/scripts/bash/create-phr.sh --title "<title>" --stage <stage> [--feature <name>] --json`
   - Open the file and fill remaining placeholders (YAML + body), embedding full PROMPT_TEXT (verbatim) and concise RESPONSE_TEXT.
   - If the script fails:
     - Read `.specify/templates/phr-template.prompt.md` (or `templates/‚Ä¶`)
     - Allocate an ID; compute the output path based on stage from step 2; write the file
     - Fill placeholders and embed full PROMPT_TEXT and concise RESPONSE_TEXT

4) Validate + report
   - No unresolved placeholders; path under `history/prompts/` and matches stage; stage/title/date coherent; print ID + path + stage + title.
   - On failure: warn, don't block. Skip only for `/sp.phr`.


============================================================
FILE: .claude\commands\sp.phr.md
============================================================
---
description: Record an AI exchange as a Prompt History Record (PHR) for learning and traceability.
---

# COMMAND: Record this AI exchange as a structured PHR artifact

## CONTEXT

The user has just completed work (or is requesting work) and needs to capture this exchange as a Prompt History Record (PHR) for:

- Learning and pattern recognition (spaced repetition)
- Team knowledge sharing and traceability
- Compliance and audit requirements
- Building a searchable corpus of effective prompts

**User's input to record:**

$ARGUMENTS

**CRITICAL**: The complete text above is the PROMPT to preserve verbatim. Do NOT truncate to first line only.

## YOUR ROLE

Act as a meticulous documentation specialist with expertise in:

- Knowledge management and organizational learning
- Software development lifecycle documentation
- Metadata extraction and classification
- Creating structured, searchable technical records

## QUICK OVERVIEW (strict)

After completing ANY work, automatically create a PHR:

1. **Detect work type**: constitution|spec|plan|tasks|implementation|debugging|refactoring|discussion|general
2. **Generate title**: 3-7 word descriptive title summarizing the work
3. **Capture context**: COMPLETE conversation (never truncate to summaries)
4. **Route correctly**:
   - Pre-feature work ‚Üí `history/prompts/`
   - Feature-specific work ‚Üí `specs/<feature>/prompts/`
5. **Confirm**: Show "üìù PHR-NNNN recorded"

## OUTPUT STRUCTURE (with quick flywheel hooks)

Execute this workflow in 5 sequential steps, reporting progress after each:

## Step 1: Execute User's Request (if not already done)

If the user provided a task/question in $ARGUMENTS:

- Complete the requested work first
- Provide full response to user
- Then proceed to Step 2 to record the exchange

If you already completed work and user just wants to record it:

- Skip to Step 2

## Step 2: Determine Stage and Routing

Select ONE stage that best describes the work:

**Constitution** (‚Üí `history/prompts/constitution/`):
- `constitution` - Defining quality standards, project principles

**Feature-specific** (‚Üí `history/prompts/<feature-name>/` - requires feature context):
- `spec` - Creating feature specifications
- `plan` - Architecture design and technical approach
- `tasks` - Implementation breakdown with test cases
- `red` - Debugging, fixing errors, test failures
- `green` - Implementation, new features, passing tests
- `refactor` - Code cleanup, optimization
- `explainer` - Code explanations, documentation
- `misc` - Other feature-specific work

**General/Catch-all** (‚Üí `history/prompts/general/`):
- `general` - General work not tied to a specific feature

## Step 3: Create PHR File

Generate a concise title (3-7 words) summarizing what was accomplished.

Call the PHR creation script with title and stage:

```bash
.specify/scripts/bash/create-phr.sh \
  --title "<your-generated-title>" \
  --stage <selected-stage> \
  [--feature <feature-slug>] \
  --json
```

Parse the JSON output to get: `id`, `path`, `context`, `stage`, `feature`

**Routing is determined automatically:**
- `constitution` ‚Üí `history/prompts/constitution/`
- Feature stages ‚Üí `history/prompts/<feature-name>/`
- `general` ‚Üí `history/prompts/general/`

## Step 4: Fill ALL Template Placeholders (Analyze‚ÜíMeasure)

Read the file at `path` from JSON output. Replace ALL {{PLACEHOLDERS}}:

**YAML Frontmatter:**

- `{{ID}}` ‚Üí ID from JSON output
- `{{TITLE}}` ‚Üí Your generated title
- `{{STAGE}}` ‚Üí Selected stage
- `{{DATE_ISO}}` ‚Üí Today (YYYY-MM-DD format)
- `{{SURFACE}}` ‚Üí "agent"
- `{{MODEL}}` ‚Üí Your model name or "unspecified"
- `{{FEATURE}}` ‚Üí Feature from JSON or "none"
- `{{BRANCH}}` ‚Üí Current branch name
- `{{USER}}` ‚Üí Git user name or "unknown"
- `{{COMMAND}}` ‚Üí "/sp.phr" or the command that triggered this
- `{{LABELS}}` ‚Üí Extract key topics as ["topic1", "topic2", ...]
- `{{LINKS_SPEC}}`, `{{LINKS_TICKET}}`, `{{LINKS_ADR}}`, `{{LINKS_PR}}` ‚Üí Relevant links or "null"
- `{{FILES_YAML}}` ‚Üí List files modified/created, one per line with " - " prefix, or " - none"
- `{{TESTS_YAML}}` ‚Üí List tests run/created, one per line with " - " prefix, or " - none"

**Content Sections:**

- `{{PROMPT_TEXT}}` ‚Üí **THE COMPLETE $ARGUMENTS TEXT VERBATIM** (do NOT truncate to first line!)
- `{{RESPONSE_TEXT}}` ‚Üí Brief summary of your response (1-3 sentences)
- `{{OUTCOME_IMPACT}}` ‚Üí What was accomplished
- `{{TESTS_SUMMARY}}` ‚Üí Tests run or "none"
- `{{FILES_SUMMARY}}` ‚Üí Files modified or "none"
- `{{NEXT_PROMPTS}}` ‚Üí Suggested next steps or "none"
- `{{REFLECTION_NOTE}}` ‚Üí One key insight

Add short evaluation notes:
- **Failure modes observed:** Specify any issues encountered, such as ambiguous instructions, incomplete metadata, misrouted commands, or unexpected script errors. Example: "Prompt did not capture full user input; metadata field 'LABELS' was left blank."
- **Next experiment to improve prompt quality:** Suggest a concrete action to address the failure mode. Example: "Rephrase prompt to clarify required metadata fields," or "Test with a multi-line user input to ensure full capture."

**CRITICAL**: `{{PROMPT_TEXT}}` MUST be the FULL multiline user input from $ARGUMENTS above, not just the title or first line.

## Step 5: Report Completion

## FORMATTING REQUIREMENTS

Present results in this exact structure:

```
‚úÖ Exchange recorded as PHR-{id} in {context} context
üìÅ {relative-path-from-repo-root}

Stage: {stage}
Feature: {feature or "none"}
Files modified: {count}
Tests involved: {count}

Acceptance Criteria (PASS only if all true)
- Full prompt preserved verbatim (no truncation)
- Stage and routing determined correctly
- Metadata fields populated; missing values noted explicitly
```

## ERROR HANDLING

If create-phr.sh fails:

1. Display the exact error message from script
2. Explain what went wrong in plain language
3. Provide specific corrective action with commands
4. Do NOT fail silently or hide errors

## TONE

Be professional, concise, and action-oriented. Focus on what was accomplished and what's next.

---

As the main request completes, you MUST create and complete a PHR (Prompt History Record) using agent‚Äënative tools when possible.

1) Determine Stage
   - Stage: constitution | spec | plan | tasks | red | green | refactor | explainer | misc | general

2) Generate Title and Determine Routing:
   - Generate Title: 3‚Äì7 words (slug for filename)
   - Route is automatically determined by stage:
     - `constitution` ‚Üí `history/prompts/constitution/`
     - Feature stages ‚Üí `history/prompts/<feature-name>/` (spec, plan, tasks, red, green, refactor, explainer, misc)
     - `general` ‚Üí `history/prompts/general/`

3) Create and Fill PHR (Shell first; fallback agent‚Äënative)
   - Run: `.specify/scripts/bash/create-phr.sh --title "<title>" --stage <stage> [--feature <name>] --json`
   - Open the file and fill remaining placeholders (YAML + body), embedding full PROMPT_TEXT (verbatim) and concise RESPONSE_TEXT.
   - If the script fails:
     - Read `.specify/templates/phr-template.prompt.md` (or `templates/‚Ä¶`)
     - Allocate an ID; compute the output path based on stage from step 2; write the file
     - Fill placeholders and embed full PROMPT_TEXT and concise RESPONSE_TEXT

4) Validate + report
   - No unresolved placeholders; path under `history/prompts/` and matches stage; stage/title/date coherent; print ID + path + stage + title.
   - On failure: warn, don't block. Skip only for `/sp.phr`.


============================================================
FILE: .claude\commands\sp.plan.md
============================================================
---
description: Execute the implementation planning workflow using the plan template to generate design artifacts.
handoffs: 
  - label: Create Tasks
    agent: sp.tasks
    prompt: Break the plan into tasks
    send: true
  - label: Create Checklist
    agent: sp.checklist
    prompt: Create a checklist for the following domain...
---

## User Input

```text
$ARGUMENTS
```

You **MUST** consider the user input before proceeding (if not empty).

## Outline

1. **Setup**: Run `.specify/scripts/powershell/setup-plan.ps1 -Json` from repo root and parse JSON for FEATURE_SPEC, IMPL_PLAN, SPECS_DIR, BRANCH. For single quotes in args like "I'm Groot", use escape syntax: e.g 'I'\''m Groot' (or double-quote if possible: "I'm Groot").

2. **Load context**: Read FEATURE_SPEC and `.specify/memory/constitution.md`. Load IMPL_PLAN template (already copied).

3. **Execute plan workflow**: Follow the structure in IMPL_PLAN template to:
   - Fill Technical Context (mark unknowns as "NEEDS CLARIFICATION")
   - Fill Constitution Check section from constitution
   - Evaluate gates (ERROR if violations unjustified)
   - Phase 0: Generate research.md (resolve all NEEDS CLARIFICATION)
   - Phase 1: Generate data-model.md, contracts/, quickstart.md
   - Phase 1: Update agent context by running the agent script
   - Re-evaluate Constitution Check post-design

4. **Stop and report**: Command ends after Phase 2 planning. Report branch, IMPL_PLAN path, and generated artifacts.

## Phases

### Phase 0: Outline & Research

1. **Extract unknowns from Technical Context** above:
   - For each NEEDS CLARIFICATION ‚Üí research task
   - For each dependency ‚Üí best practices task
   - For each integration ‚Üí patterns task

2. **Generate and dispatch research agents**:

   ```text
   For each unknown in Technical Context:
     Task: "Research {unknown} for {feature context}"
   For each technology choice:
     Task: "Find best practices for {tech} in {domain}"
   ```

3. **Consolidate findings** in `research.md` using format:
   - Decision: [what was chosen]
   - Rationale: [why chosen]
   - Alternatives considered: [what else evaluated]

**Output**: research.md with all NEEDS CLARIFICATION resolved

### Phase 1: Design & Contracts

**Prerequisites:** `research.md` complete

1. **Extract entities from feature spec** ‚Üí `data-model.md`:
   - Entity name, fields, relationships
   - Validation rules from requirements
   - State transitions if applicable

2. **Generate API contracts** from functional requirements:
   - For each user action ‚Üí endpoint
   - Use standard REST/GraphQL patterns
   - Output OpenAPI/GraphQL schema to `/contracts/`

3. **Agent context update**:
   - Run `.specify/scripts/powershell/update-agent-context.ps1 -AgentType claude`
   - These scripts detect which AI agent is in use
   - Update the appropriate agent-specific context file
   - Add only new technology from current plan
   - Preserve manual additions between markers

**Output**: data-model.md, /contracts/*, quickstart.md, agent-specific file

## Key rules

- Use absolute paths
- ERROR on gate failures or unresolved clarifications

---

As the main request completes, you MUST create and complete a PHR (Prompt History Record) using agent‚Äënative tools when possible.

1) Determine Stage
   - Stage: constitution | spec | plan | tasks | red | green | refactor | explainer | misc | general

2) Generate Title and Determine Routing:
   - Generate Title: 3‚Äì7 words (slug for filename)
   - Route is automatically determined by stage:
     - `constitution` ‚Üí `history/prompts/constitution/`
     - Feature stages ‚Üí `history/prompts/<feature-name>/` (spec, plan, tasks, red, green, refactor, explainer, misc)
     - `general` ‚Üí `history/prompts/general/`

3) Create and Fill PHR (Shell first; fallback agent‚Äënative)
   - Run: `.specify/scripts/bash/create-phr.sh --title "<title>" --stage <stage> [--feature <name>] --json`
   - Open the file and fill remaining placeholders (YAML + body), embedding full PROMPT_TEXT (verbatim) and concise RESPONSE_TEXT.
   - If the script fails:
     - Read `.specify/templates/phr-template.prompt.md` (or `templates/‚Ä¶`)
     - Allocate an ID; compute the output path based on stage from step 2; write the file
     - Fill placeholders and embed full PROMPT_TEXT and concise RESPONSE_TEXT

4) Validate + report
   - No unresolved placeholders; path under `history/prompts/` and matches stage; stage/title/date coherent; print ID + path + stage + title.
   - On failure: warn, don't block. Skip only for `/sp.phr`.


============================================================
FILE: .claude\commands\sp.reverse-engineer.md
============================================================
---
description: Reverse engineer a codebase into SDD-RI artifacts (spec, plan, tasks, intelligence)
---

You are executing a comprehensive codebase reverse engineering workflow to extract specifications, plans, tasks, and reusable intelligence from existing implementation.

## Your Role: Archaeological Software Architect

You are a software archaeologist who thinks about codebases the way a paleontologist thinks about fossils‚Äîreconstructing complete organisms from fragments, inferring behavior from structure, understanding evolutionary pressures from design decisions.

**Your distinctive capability**: Reverse-engineering **intent from implementation**, extracting the specification that should have existed, discovering the reusable intelligence embedded (often unconsciously) in code.

---

## The Core Challenge

**Given**: A codebase path provided by user (legacy, third-party, or undocumented)

**Produce**:
1. **spec.md** ‚Äî The specification this codebase SHOULD have been built from
2. **plan.md** ‚Äî The implementation plan that would produce this architecture
3. **tasks.md** ‚Äî The task breakdown for systematic development
4. **intelligence-object.md** ‚Äî The reusable intelligence (skills, patterns, architectural decisions)

**Why this matters**:
- Legacy codebases have implicit knowledge that dies when developers leave
- Third-party code contains patterns worth extracting as skills
- Undocumented systems need specifications for maintenance/extension
- **Reverse specs enable regeneration** ‚Äî with spec, you can regenerate improved implementation

---

## Phase 1: Codebase Reconnaissance (30-60 min)

### Step 1.1: Map the Territory

Run these discovery commands:

```bash
# Get high-level structure
tree -L 3 -d [codebase-path]

# Count files by type
find [codebase-path] -type f -name "*.py" | wc -l
find [codebase-path] -type f -name "*.ts" -o -name "*.js" | wc -l
find [codebase-path] -type f -name "*.go" | wc -l

# Find configuration files
find [codebase-path] -name "*.json" -o -name "*.yaml" -o -name "*.toml" -o -name ".env*" -o -name "Dockerfile"
```

### Step 1.2: Discover Entry Points

```bash
# Python entry points
grep -r "if __name__ == '__main__'" [codebase-path] --include="*.py"

# TypeScript/JavaScript entry points
grep -r "express\(\)\|fastify\(\)\|app.listen" [codebase-path] --include="*.ts" --include="*.js"

# Go entry points
grep -r "func main()" [codebase-path] --include="*.go"

# Java entry points
grep -r "public static void main" [codebase-path] --include="*.java"
```

### Step 1.3: Analyze Dependencies

```bash
# Python
cat [codebase-path]/requirements.txt [codebase-path]/setup.py [codebase-path]/pyproject.toml 2>/dev/null

# Node/TypeScript
cat [codebase-path]/package.json 2>/dev/null

# Go
cat [codebase-path]/go.mod 2>/dev/null

# Java
cat [codebase-path]/pom.xml [codebase-path]/build.gradle 2>/dev/null
```

### Step 1.4: Assess Test Coverage

```bash
# Find test files
find [codebase-path] -name "*test*" -o -name "*spec*" | head -20

# Identify test frameworks
grep -r "import.*pytest\|unittest\|jest\|mocha\|testing" [codebase-path] --include="*.py" --include="*.ts" --include="*.go" | head -10
```

### Step 1.5: Read Existing Documentation

```bash
# Find documentation files
find [codebase-path] -name "README*" -o -name "*.md" -o -name "docs" -type d

# List markdown files
find [codebase-path] -name "*.md" | head -10
```

**Read**: README.md, ARCHITECTURE.md, CONTRIBUTING.md (if they exist)

---

## Phase 2: Deep Analysis (4-6 hours)

Execute these six analysis dimensions systematically:

### Dimension 1: Intent Archaeology (2 hours)

**Goal**: Extract the WHAT and WHY

#### 1.1 System Purpose Inference

**Questions to ask yourself**:
- If this codebase disappeared, what would users lose?
- What's the "elevator pitch" for this system?
- What problem is so painful this was built to solve it?

**Evidence to gather**:
- Read README, comments, docstrings for stated purpose
- Analyze entry points: what operations are exposed?
- Study data models: what entities are central?

#### 1.2 Functional Requirements Extraction

```bash
# Find API endpoints/routes
grep -r "route\|@app\|@get\|@post\|@put\|@delete\|router\." [codebase-path] --include="*.py" --include="*.ts" --include="*.js" | head -30

# Find public interfaces
grep -r "class.*public\|export class\|export function\|def.*public" [codebase-path] | head -30

# Find CLI commands
grep -r "argparse\|cobra\|click\|commander" [codebase-path] --include="*.py" --include="*.go" --include="*.js" | head -20
```

**For each interface discovered**:
- What operation does it perform?
- What inputs does it require?
- What outputs does it produce?
- What side effects occur?

#### 1.3 Non-Functional Requirements Detection

**Performance patterns**:
```bash
grep -r "cache\|redis\|memcached\|async\|await\|pool" [codebase-path] --include="*.py" --include="*.ts" --include="*.go" | wc -l
```

**Security patterns**:
```bash
grep -r "auth\|jwt\|bcrypt\|encrypt\|sanitize\|validate" [codebase-path] --include="*.py" --include="*.ts" --include="*.go" | wc -l
```

**Reliability patterns**:
```bash
grep -r "retry\|circuit.breaker\|fallback\|timeout" [codebase-path] --include="*.py" --include="*.ts" --include="*.go" | wc -l
```

**Observability patterns**:
```bash
grep -r "log\|logger\|metric\|trace\|monitor" [codebase-path] --include="*.py" --include="*.ts" --include="*.go" | wc -l
```

#### 1.4 Constraint Discovery

**External integrations**:
```bash
# Database connections
grep -r "postgresql\|mysql\|mongodb\|redis\|sqlite" [codebase-path] --include="*.py" --include="*.ts" --include="*.go"

# External APIs
grep -r "http.get\|requests.post\|fetch\|axios\|http.Client" [codebase-path] --include="*.py" --include="*.ts" --include="*.go" | head -20

# Message queues
grep -r "kafka\|rabbitmq\|sqs\|pubsub\|queue" [codebase-path] --include="*.py" --include="*.ts" --include="*.go"
```

---

### Dimension 2: Architectural Pattern Recognition (1.5 hours)

**Goal**: Identify the HOW ‚Äî architectural decisions and design patterns

#### 2.1 Layering Detection

```bash
# Look for common layer names
find [codebase-path] -type d -name "*controller*" -o -name "*service*" -o -name "*repository*" -o -name "*domain*" -o -name "*handler*" -o -name "*model*"

# Check directory structure for layers
ls -la [codebase-path]/
```

**Questions to ask**:
- Is there clear separation of concerns?
- What's the dependency flow? (UI ‚Üí Service ‚Üí Data)
- Are layers respected or violated?

#### 2.2 Design Pattern Identification

```bash
# Find pattern keywords in code
grep -r "Factory\|Builder\|Singleton\|Adapter\|Strategy\|Observer\|Command\|Decorator" [codebase-path] --include="*.py" --include="*.ts" --include="*.go" | head -20

# Find interface/abstract class definitions
grep -r "interface\|abstract class\|Protocol\|ABC" [codebase-path] --include="*.py" --include="*.ts" --include="*.go" | head -20
```

#### 2.3 Architectural Style Classification

**Check for MVC/MVP/MVVM**:
```bash
find [codebase-path] -type d -name "*view*" -o -name "*controller*" -o -name "*model*"
```

**Check for Hexagonal/Clean Architecture**:
```bash
find [codebase-path] -type d -name "*domain*" -o -name "*infrastructure*" -o -name "*application*" -o -name "*adapter*"
```

**Check for Event-Driven**:
```bash
grep -r "event\|emit\|publish\|subscribe\|listener\|handler" [codebase-path] --include="*.py" --include="*.ts" --include="*.go" | wc -l
```

**Check for CQRS**:
```bash
grep -r "command\|query\|CommandHandler\|QueryHandler" [codebase-path] --include="*.py" --include="*.ts" --include="*.go"
```

#### 2.4 Data Flow Tracing

**Pick one representative operation and trace it**:
1. Find entry point (route/handler)
2. Follow to business logic (service/use-case)
3. Trace to data layer (repository/DAO)
4. Document the flow

---

### Dimension 3: Code Structure Decomposition (1 hour)

**Goal**: Break down implementation into logical task units

#### 3.1 Module Inventory

```bash
# List all significant modules (exclude tests)
find [codebase-path] -name "*.py" -o -name "*.ts" -o -name "*.go" | grep -v test | sort

# Group by domain/feature
ls -d [codebase-path]/*/ | sort
```

#### 3.2 Responsibility Assignment

For each major module/package:
- What's its single responsibility?
- What other modules does it depend on?
- What modules depend on it?
- Could it be extracted as standalone component?

#### 3.3 Integration Point Mapping

```bash
# External service calls
grep -rn "http.get\|requests.post\|fetch\|axios\|http.Client" [codebase-path] --include="*.py" --include="*.ts" --include="*.go" | head -20

# Database queries
grep -rn "SELECT\|INSERT\|UPDATE\|DELETE\|query\|execute\|find\|create\|save" [codebase-path] --include="*.py" --include="*.ts" --include="*.go" | head -20

# Queue/messaging
grep -rn "publish\|subscribe\|send_message\|consume\|produce" [codebase-path] --include="*.py" --include="*.ts" --include="*.go"
```

#### 3.4 Cross-Cutting Concern Identification

**Logging**:
```bash
grep -r "logger\|log\." [codebase-path] --include="*.py" --include="*.ts" --include="*.go" | head -10
```

**Error Handling**:
```bash
grep -r "try:\|catch\|except\|error\|Error" [codebase-path] --include="*.py" --include="*.ts" --include="*.go" | head -10
```

**Configuration**:
```bash
grep -r "config\|env\|settings\|getenv" [codebase-path] --include="*.py" --include="*.ts" --include="*.go" | head -10
```

---

### Dimension 4: Intelligence Extraction (1 hour)

**Goal**: Extract reusable intelligence ‚Äî patterns worth encoding as skills

#### 4.1 Pattern Frequency Analysis

**Questions to ask**:
- What code patterns repeat 3+ times?
- What decisions are made consistently?
- What best practices are applied systematically?

**Look for**:
```bash
# Find repeated function/method names
grep -rh "def \|func \|function " [codebase-path] --include="*.py" --include="*.go" --include="*.ts" | sort | uniq -c | sort -rn | head -20
```

#### 4.2 Implicit Expertise Detection

**Find important comments** (reveal tacit knowledge):
```bash
# Comments with keywords indicating critical knowledge
grep -rn "IMPORTANT:\|NOTE:\|WARNING:\|SECURITY:\|TODO:\|HACK:\|FIXME:" [codebase-path] --include="*.py" --include="*.ts" --include="*.go" | head -30
```

#### 4.3 Architecture Decision Extraction

```bash
# Look for ADR-style documents
find [codebase-path] -name "*decision*" -o -name "*ADR*" -o -name "architecture.md"

# Look for significant comments about choices
grep -rn "chosen because\|decided to\|alternative\|tradeoff" [codebase-path] --include="*.py" --include="*.ts" --include="*.go" --include="*.md"
```

#### 4.4 Skill Candidate Identification

**Identify patterns worth encoding as Persona + Questions + Principles**:

Common candidates:
- Error handling strategy (if consistent across modules)
- API design patterns (REST conventions, response formats)
- Data validation approach (schema validation patterns)
- Security patterns (auth middleware, input sanitization)
- Performance optimization (caching strategies, query optimization)

**For each candidate**:
1. Extract the pattern (what's done consistently)
2. Infer the reasoning (why this approach)
3. Identify decision points (what questions guide choices)
4. Formulate as P+Q+P skill

---

### Dimension 5: Gap Analysis & Technical Debt (0.5 hours)

**Goal**: Identify what SHOULD be there but is missing

#### 5.1 Missing Documentation

```bash
# Check for API documentation
find [codebase-path] -name "openapi.*" -o -name "swagger.*" -o -name "api.md"

# Check for data model docs
find [codebase-path] -name "schema.*" -o -name "models.md" -o -name "ERD.*"
```

#### 5.2 Testing Gaps

```bash
# Calculate test file ratio
total_files=$(find [codebase-path] -name "*.py" -o -name "*.ts" -o -name "*.go" | wc -l)
test_files=$(find [codebase-path] -name "*test*" -o -name "*spec*" | wc -l)
echo "Test coverage: $test_files / $total_files files"
```

**If coverage tools available**:
```bash
# Python
cd [codebase-path] && pytest --cov=. --cov-report=term 2>/dev/null

# TypeScript/JavaScript
cd [codebase-path] && npm test -- --coverage 2>/dev/null

# Go
cd [codebase-path] && go test -cover ./... 2>/dev/null
```

#### 5.3 Security Audit

**Potential security issues**:
```bash
# Code injection risks
grep -rn "eval\|exec\|system\|shell" [codebase-path] --include="*.py" --include="*.js"

# Hardcoded secrets
grep -rn "password.*=.*\"\|api_key.*=.*\"\|secret.*=.*\"" [codebase-path] --include="*.py" --include="*.ts" --include="*.go"

# SQL injection risks
grep -rn "execute.*%\|query.*format\|SELECT.*+" [codebase-path] --include="*.py" --include="*.ts" --include="*.go"
```

#### 5.4 Observability Gaps

**Check for**:
- Structured logging (JSON format)
- Metrics collection (Prometheus, StatsD)
- Distributed tracing (OpenTelemetry, Jaeger)
- Health check endpoints

```bash
# Structured logging
grep -r "json\|structured" [codebase-path] --include="*log*"

# Metrics
grep -r "prometheus\|statsd\|metric" [codebase-path] --include="*.py" --include="*.ts" --include="*.go"

# Tracing
grep -r "trace\|span\|opentelemetry" [codebase-path] --include="*.py" --include="*.ts" --include="*.go"

# Health checks
grep -rn "/health\|/ready\|/alive" [codebase-path] --include="*.py" --include="*.ts" --include="*.go"
```

---

### Dimension 6: Regeneration Blueprint (30 min)

**Goal**: Ensure specs can regenerate this system (or improved version)

#### 6.1 Specification Completeness Check

**Ask yourself**:
- Can another developer read my spec and build equivalent system?
- Are all architectural decisions documented with rationale?
- Are success criteria measurable and testable?

#### 6.2 Reusability Assessment

**Identify**:
- What components are reusable as-is?
- What patterns should become skills?
- What should be generalized vs kept specific?

#### 6.3 Improvement Opportunities

**If rebuilding from scratch, what would you change?**:
- Technical debt to avoid replicating
- Modern alternatives to outdated dependencies
- Missing features to add
- Architecture improvements (event sourcing, CQRS, etc.)

---

## Phase 3: Synthesis & Documentation (2-3 hours)

### Output 1: spec.md

Create comprehensive specification with these sections:

```markdown
# [System Name] Specification

**Version**: 1.0 (Reverse Engineered)
**Date**: [Date]
**Source**: [Codebase path]

## Problem Statement

[What problem does this solve? Inferred from code purpose]

## System Intent

**Target Users**: [Who uses this system?]

**Core Value Proposition**: [Why this exists instead of alternatives?]

**Key Capabilities**:
- [Capability 1 from functional analysis]
- [Capability 2]
- [Capability 3]

## Functional Requirements

### Requirement 1: [Operation Name]
- **What**: [What this operation does]
- **Why**: [Business justification - inferred]
- **Inputs**: [Required data/parameters]
- **Outputs**: [Results produced]
- **Side Effects**: [Database changes, external calls, etc.]
- **Success Criteria**: [How to verify correct behavior]

[Repeat for all major operations discovered]

## Non-Functional Requirements

### Performance
[Observed patterns: caching, async, connection pooling]
**Target**: [If metrics found in code/comments]

### Security
[Auth mechanisms, input validation, encryption observed]
**Standards**: [Compliance patterns detected]

### Reliability
[Retry logic, circuit breakers, graceful degradation]
**SLA**: [If defined in code/comments]

### Scalability
[Horizontal/vertical scaling patterns observed]
**Load Capacity**: [If defined]

### Observability
[Logging, metrics, tracing implemented]
**Monitoring**: [What's monitored]

## System Constraints

### External Dependencies
- [Database: PostgreSQL 14+]
- [Cache: Redis 6+]
- [Message Queue: RabbitMQ]
- [External API: Stripe for payments]

### Data Formats
- [JSON for API requests/responses]
- [Protocol Buffers for internal service communication]

### Deployment Context
- [Docker containers on Kubernetes]
- [Environment: AWS EKS]

### Compliance Requirements
- [GDPR: Personal data handling patterns observed]
- [PCI-DSS: Payment data security patterns]

## Non-Goals & Out of Scope

**Explicitly excluded** (inferred from missing implementation):
- [Feature X: No evidence in codebase]
- [Integration Y: Stub code suggests planned but not implemented]

## Known Gaps & Technical Debt

### Gap 1: [Issue Name]
- **Issue**: [Specific problem]
- **Evidence**: [file:line reference]
- **Impact**: [Consequences]
- **Recommendation**: [How to fix]

[Continue for all gaps]

## Success Criteria

### Functional Success
- [ ] All API endpoints return correct responses for valid inputs
- [ ] All error cases handled gracefully
- [ ] All integrations with external systems work correctly

### Non-Functional Success
- [ ] Response time < [X]ms for [operation]
- [ ] System handles [Y] concurrent users
- [ ] [Z]% test coverage achieved
- [ ] Zero critical security vulnerabilities

## Acceptance Tests

### Test 1: [Scenario]
**Given**: [Initial state]
**When**: [Action]
**Then**: [Expected outcome]

[Continue for critical scenarios]
```

---

### Output 2: plan.md

Create implementation plan:

```markdown
# [System Name] Implementation Plan

**Version**: 1.0 (Reverse Engineered)
**Date**: [Date]

## Architecture Overview

**Architectural Style**: [MVC, Hexagonal, Event-Driven, etc.]

**Reasoning**: [Why this pattern fits the requirements - inferred from structure]

**Diagram** (ASCII):
```
[Visual representation of architecture]
```

## Layer Structure

### Layer 1: [Presentation/API Layer]
- **Responsibility**: [Handle HTTP requests, input validation, response formatting]
- **Components**:
  - [controllers/]: Request handlers
  - [middleware/]: Auth, logging, error handling
- **Dependencies**: ‚Üí Service Layer
- **Technology**: [Flask, Express, Gin]

### Layer 2: [Business Logic/Service Layer]
- **Responsibility**: [Core business rules, orchestration]
- **Components**:
  - [services/]: Business logic implementations
  - [domain/]: Domain models
- **Dependencies**: ‚Üí Data Layer, ‚Üí External Services
- **Technology**: [Python classes, TypeScript services]

### Layer 3: [Data/Persistence Layer]
- **Responsibility**: [Data access, persistence]
- **Components**:
  - [repositories/]: Data access objects
  - [models/]: ORM models
- **Dependencies**: ‚Üí Database
- **Technology**: [SQLAlchemy, Prisma, GORM]

## Design Patterns Applied

### Pattern 1: [Factory Method]
- **Location**: [services/user_factory.py]
- **Purpose**: [Create different user types based on role]
- **Implementation**: [Brief code example or description]

### Pattern 2: [Repository Pattern]
- **Location**: [repositories/]
- **Purpose**: [Abstract data access from business logic]
- **Implementation**: [Brief description]

[Continue for all significant patterns]

## Data Flow

### Request Flow (Synchronous)
1. **API Layer** receives HTTP request
2. **Validation Middleware** validates input schema
3. **Auth Middleware** verifies authentication
4. **Controller** routes to appropriate service
5. **Service Layer** executes business logic
6. **Repository** persists/retrieves data
7. **Service** formats response
8. **Controller** returns HTTP response

### Event Flow (Asynchronous) - if applicable
1. **Event Producer** emits event to queue
2. **Message Broker** routes to subscribers
3. **Event Handler** processes asynchronously
4. **Service** updates state
5. **Event** published for downstream consumers

## Technology Stack

### Language & Runtime
- **Primary**: [Python 3.11]
- **Rationale**: [Inferred - rapid development, rich ecosystem]

### Web Framework
- **Choice**: [Flask 2.x]
- **Rationale**: [Lightweight, flexible, good for APIs]

### Database
- **Choice**: [PostgreSQL 14]
- **Rationale**: [ACID compliance, JSON support, reliability]

### Caching
- **Choice**: [Redis 6]
- **Rationale**: [Performance, pub/sub capabilities]

### Message Queue - if applicable
- **Choice**: [RabbitMQ]
- **Rationale**: [Reliability, routing flexibility]

### Testing
- **Choice**: [pytest, Jest]
- **Rationale**: [Rich ecosystem, good DX]

### Deployment
- **Choice**: [Docker + Kubernetes]
- **Rationale**: [Portability, scalability, cloud-native]

## Module Breakdown

### Module: [authentication]
- **Purpose**: [User auth, session management]
- **Key Classes**: [AuthService, JWTHandler, UserRepository]
- **Dependencies**: [bcrypt, PyJWT, database]
- **Complexity**: Medium

### Module: [orders]
- **Purpose**: [Order processing, inventory]
- **Key Classes**: [OrderService, OrderRepository, InventoryService]
- **Dependencies**: [payment, notification, database]
- **Complexity**: High

[Continue for all major modules]

## Regeneration Strategy

### Option 1: Specification-First Rebuild
1. Start with spec.md (intent and requirements)
2. Apply extracted skills (error handling, API patterns)
3. Implement with modern best practices (fill gaps)
4. Test-driven development using acceptance criteria

**Timeline**: [Estimate based on codebase size]

### Option 2: Incremental Refactoring
1. **Strangler Pattern**: New implementation shadows old
2. **Feature Flags**: Gradual traffic shift
3. **Parallel Run**: Validate equivalence
4. **Cutover**: Complete migration

**Timeline**: [Estimate based on risk tolerance]

## Improvement Opportunities

### Technical Improvements
- [ ] **Replace [Old Library]** with [Modern Alternative]
  - **Rationale**: [Better performance, active maintenance]
  - **Effort**: Medium

- [ ] **Add [Missing Feature]**
  - **Addresses Gap**: [Specific gap from analysis]
  - **Effort**: High

### Architectural Improvements
- [ ] **Introduce Event Sourcing**
  - **Enables**: Audit trail, event replay, temporal queries
  - **Effort**: High

- [ ] **Implement CQRS**
  - **Separates**: Read and write models for optimization
  - **Effort**: Medium

### Operational Improvements
- [ ] **CI/CD Pipeline**: Automated testing, deployment
- [ ] **Infrastructure as Code**: Terraform, Pulumi
- [ ] **Monitoring Dashboards**: Grafana, DataDog
- [ ] **GitOps Deployment**: ArgoCD, Flux
```

---

### Output 3: tasks.md

Create actionable task breakdown:

```markdown
# [System Name] Implementation Tasks

**Version**: 1.0 (Reverse Engineered)
**Date**: [Date]

## Overview

This task breakdown represents how to rebuild this system from scratch using the specification and plan.

**Estimated Timeline**: [X weeks based on team size]
**Team Size**: [Assumed team composition]

---

## Phase 1: Core Infrastructure

**Timeline**: Week 1
**Dependencies**: None

### Task 1.1: Project Setup
- [ ] Initialize repository with [language] project structure
- [ ] Configure build system: [tool]
- [ ] Setup dependency management: [requirements.txt, package.json, go.mod]
- [ ] Configure linting: [flake8, eslint, golangci-lint]
- [ ] Setup pre-commit hooks
- [ ] Create initial README

### Task 1.2: Configuration System
- [ ] Implement environment-based configuration
- [ ] Support: Environment variables, config files, secrets management
- [ ] Validation: Config schema validation on startup
- [ ] Defaults: Sensible defaults for local development

### Task 1.3: Logging Infrastructure
- [ ] Setup structured logging (JSON format)
- [ ] Configure log levels: DEBUG, INFO, WARN, ERROR
- [ ] Add request correlation IDs
- [ ] Integrate with [logging destination]

---

## Phase 2: Data Layer

**Timeline**: Week 2-3
**Dependencies**: Phase 1 complete

### Task 2.1: Database Design
- [ ] Design schema for entities: [User, Order, Product]
- [ ] Define relationships: [one-to-many, many-to-many]
- [ ] Add indexes for performance
- [ ] Document schema in [ERD tool]

### Task 2.2: ORM Setup
- [ ] Install and configure [SQLAlchemy, Prisma, GORM]
- [ ] Create model classes for all entities
- [ ] Implement relationships
- [ ] Add validation rules

### Task 2.3: Migration System
- [ ] Setup migration tool: [Alembic, Flyway, migrate]
- [ ] Create initial migration
- [ ] Document migration workflow
- [ ] Add migration tests

### Task 2.4: Repository Layer
- [ ] Implement repository pattern for each entity
- [ ] CRUD operations: Create, Read, Update, Delete
- [ ] Query methods: FindByX, ListByY
- [ ] Transaction management

---

## Phase 3: Business Logic Layer

**Timeline**: Week 4-6
**Dependencies**: Phase 2 complete

### Task 3.1: [Feature A - e.g., User Authentication]
- [ ] **Input validation**: Username/email, password strength
- [ ] **Processing logic**:
  - Hash password with bcrypt
  - Generate JWT token
  - Create user session
- [ ] **Error handling**: Duplicate user, invalid credentials
- [ ] **Output formatting**: User object + token

### Task 3.2: [Feature B - e.g., Order Processing]
- [ ] **Input validation**: Order items, quantities, payment info
- [ ] **Processing logic**:
  - Validate inventory availability
  - Calculate totals, taxes, shipping
  - Process payment via [Stripe]
  - Update inventory
  - Send confirmation
- [ ] **Error handling**: Insufficient inventory, payment failed
- [ ] **Output formatting**: Order confirmation

[Continue for all major features discovered]

---

## Phase 4: API/Interface Layer

**Timeline**: Week 7-8
**Dependencies**: Phase 3 complete

### Task 4.1: API Contract Definition
- [ ] Design RESTful endpoints: [list all routes]
- [ ] Define request schemas (OpenAPI/JSON Schema)
- [ ] Define response schemas
- [ ] Document error responses

### Task 4.2: Controller Implementation
- [ ] Implement route handlers
- [ ] Input validation middleware
- [ ] Auth middleware integration
- [ ] Error handling middleware

### Task 4.3: API Documentation
- [ ] Generate OpenAPI/Swagger docs
- [ ] Add usage examples
- [ ] Document authentication flow
- [ ] Create Postman collection

---

## Phase 5: Cross-Cutting Concerns

**Timeline**: Week 9
**Dependencies**: Phase 4 complete

### Task 5.1: Authentication & Authorization
- [ ] Implement JWT-based auth
- [ ] Role-based access control (RBAC)
- [ ] Token refresh mechanism
- [ ] Session management

### Task 5.2: Observability
- [ ] **Metrics**: Instrument with [Prometheus, StatsD]
  - Request rate, latency, error rate
  - Business metrics: Orders/min, Revenue/hour
- [ ] **Tracing**: Integrate [OpenTelemetry, Jaeger]
  - Distributed tracing across services
  - Performance bottleneck detection
- [ ] **Health Checks**:
  - `/health` - Liveness probe
  - `/ready` - Readiness probe
  - `/metrics` - Prometheus endpoint

### Task 5.3: Error Handling
- [ ] Global error handler
- [ ] Structured error responses
- [ ] Error logging with stack traces
- [ ] Error monitoring integration

### Task 5.4: Security Hardening
- [ ] Input sanitization
- [ ] SQL injection prevention
- [ ] XSS protection
- [ ] CSRF protection
- [ ] Rate limiting
- [ ] Security headers

---

## Phase 6: External Integrations

**Timeline**: Week 10
**Dependencies**: Phase 4 complete

### Task 6.1: [Integration A - e.g., Payment Provider]
- [ ] API client implementation
- [ ] Retry logic with exponential backoff
- [ ] Circuit breaker pattern
- [ ] Webhook handling
- [ ] Error recovery

### Task 6.2: [Integration B - e.g., Email Service]
- [ ] Template system
- [ ] Async sending (queue-based)
- [ ] Delivery tracking
- [ ] Bounce handling

[Continue for all external integrations]

---

## Phase 7: Testing & Quality

**Timeline**: Week 11-12
**Dependencies**: All phases complete

### Task 7.1: Unit Tests
- [ ] **Coverage target**: 80%+
- [ ] **Framework**: [pytest, Jest, testing package]
- [ ] Test all service methods
- [ ] Test all repositories
- [ ] Mock external dependencies

### Task 7.2: Integration Tests
- [ ] API endpoint tests
- [ ] Database integration tests
- [ ] External service integration tests (with mocks)
- [ ] Test database setup/teardown

### Task 7.3: End-to-End Tests
- [ ] Critical user journeys:
  - User registration ‚Üí Login ‚Üí Purchase ‚Üí Logout
  - [Other critical flows]
- [ ] Test against staging environment
- [ ] Automated with [Selenium, Playwright, Cypress]

### Task 7.4: Performance Testing
- [ ] Load testing: [k6, Locust, JMeter]
- [ ] Stress testing: Find breaking points
- [ ] Endurance testing: Memory leaks, connection exhaustion
- [ ] Document performance baselines

### Task 7.5: Security Testing
- [ ] OWASP Top 10 vulnerability scan
- [ ] Dependency vulnerability scan
- [ ] Penetration testing (if budget allows)
- [ ] Security code review

---

## Phase 8: Deployment & Operations

**Timeline**: Week 13
**Dependencies**: Phase 7 complete

### Task 8.1: Containerization
- [ ] Write production Dockerfile
- [ ] Multi-stage build for optimization
- [ ] Non-root user for security
- [ ] Health check in container

### Task 8.2: Kubernetes Manifests
- [ ] Deployment manifest
- [ ] Service manifest
- [ ] ConfigMap for configuration
- [ ] Secret for sensitive data
- [ ] Ingress for routing
- [ ] HorizontalPodAutoscaler

### Task 8.3: CI/CD Pipeline
- [ ] GitHub Actions / GitLab CI / Jenkins
- [ ] Stages: Lint ‚Üí Test ‚Üí Build ‚Üí Deploy
- [ ] Automated testing in pipeline
- [ ] Deployment to staging on merge to main
- [ ] Manual approval for production

### Task 8.4: Monitoring & Alerting
- [ ] Setup Grafana dashboards
- [ ] Configure alerts: Error rate spikes, latency increases
- [ ] On-call rotation setup
- [ ] Runbook documentation

### Task 8.5: Documentation
- [ ] Architecture documentation
- [ ] API documentation
- [ ] Deployment runbook
- [ ] Troubleshooting guide
- [ ] Onboarding guide for new developers

---

## Phase 9: Post-Launch

**Timeline**: Ongoing
**Dependencies**: Production deployment

### Task 9.1: Monitoring & Incident Response
- [ ] Monitor production metrics
- [ ] Respond to alerts
- [ ] Conduct post-mortems for incidents
- [ ] Iterate on improvements

### Task 9.2: Feature Iterations
- [ ] Prioritize feature backlog
- [ ] Implement high-priority features
- [ ] A/B testing for new features
- [ ] Gather user feedback

### Task 9.3: Technical Debt Reduction
- [ ] Address P0 gaps: [from gap analysis]
- [ ] Address P1 gaps: [from gap analysis]
- [ ] Refactor based on learnings
- [ ] Update documentation
```

---

### Output 4: intelligence-object.md

Create reusable intelligence extraction:

```markdown
# [System Name] Reusable Intelligence

**Version**: 1.0 (Extracted from Codebase)
**Date**: [Date]

## Overview

This document captures the reusable intelligence embedded in the codebase‚Äîpatterns, decisions, and expertise worth preserving and applying to future projects.

---

## Extracted Skills

### Skill 1: [API Error Handling Strategy]

**Persona**: You are a backend engineer designing resilient APIs that fail gracefully and provide actionable error information.

**Questions to ask before implementing error handling**:
- What error categories exist in this system? (Client errors 4xx, server errors 5xx, network errors)
- Should errors be retryable or terminal?
- What information helps debugging without exposing security details?
- How do errors propagate through layers (API ‚Üí Service ‚Üí Data)?

**Principles**:
- **Never expose internal details**: Stack traces in development only, generic messages in production
- **Consistent error schema**: All errors follow same structure `{error: {code, message, details, request_id}}`
- **Log everything, return selectively**: Full context in logs, safe subset in API response
- **Use HTTP status codes correctly**: 400 bad request, 401 unauthorized, 404 not found, 500 internal error
- **Provide request IDs**: Enable correlation between client errors and server logs

**Implementation Pattern** (observed in codebase):
```python
# Extracted from: [file: src/api/errors.py, lines 15-45]
class APIError(Exception):
    """Base exception for all API errors"""

    def __init__(self, code: str, message: str, status: int = 400, details: dict = None):
        self.code = code
        self.message = message
        self.status = status
        self.details = details or {}

    def to_response(self):
        """Convert to JSON response format"""
        return {
            "error": {
                "code": self.code,
                "message": self.message,
                "details": self.details,
                "request_id": get_request_id(),
                "timestamp": datetime.utcnow().isoformat()
            }
        }, self.status

# Usage pattern:
if not user:
    raise APIError(
        code="USER_NOT_FOUND",
        message="User with specified ID does not exist",
        status=404,
        details={"user_id": user_id}
    )
```

**When to apply**:
- All API endpoints
- Background jobs that report status
- Any system with external-facing interfaces

**Contraindications**:
- Internal services (may prefer exceptions without HTTP semantics)
- Real-time systems (error objects may be too heavy)

---

### Skill 2: [Database Connection Management]

**Persona**: You are a backend engineer optimizing database performance through connection pooling and lifecycle management.

**Questions to ask before implementing database access**:
- What's the connection lifecycle? (Per-request, per-application, pooled)
- How many concurrent connections does the application need?
- What happens on connection failure? (Retry, circuit breaker, fail fast)
- Should connections be long-lived or short-lived?

**Principles**:
- **Connection pooling is mandatory**: Never create connection per request (overhead)
- **Pool size = 2 * CPU cores** (starting point, tune based on load)
- **Idle timeout prevents resource leaks**: Close unused connections after [X] minutes
- **Health checks detect stale connections**: Validate before use, not during query
- **Graceful degradation**: Circuit breaker pattern when database unavailable

**Implementation Pattern** (observed in codebase):
```python
# Extracted from: [file: src/db/connection.py, lines 20-55]
from sqlalchemy import create_engine, pool

# Connection pool configuration
engine = create_engine(
    DATABASE_URL,
    poolclass=pool.QueuePool,
    pool_size=10,              # Max connections in pool
    max_overflow=20,           # Additional connections beyond pool_size
    pool_timeout=30,           # Seconds to wait for connection
    pool_recycle=3600,         # Recycle connections after 1 hour
    pool_pre_ping=True,        # Test connection before using
    echo=False                 # Don't log SQL (production)
)

# Context manager for connection lifecycle
@contextmanager
def get_db_session():
    """Provide transactional scope around operations"""
    session = Session(bind=engine)
    try:
        yield session
        session.commit()
    except Exception:
        session.rollback()
        raise
    finally:
        session.close()

# Usage pattern:
with get_db_session() as session:
    user = session.query(User).filter_by(id=user_id).first()
    # Connection automatically returned to pool on context exit
```

**When to apply**:
- All database-backed applications
- Services with moderate-to-high traffic
- Long-running applications (not serverless functions)

**Contraindications**:
- Serverless/FaaS (use connection per invocation)
- Very low-traffic applications (overhead not justified)

---

### Skill 3: [Input Validation Strategy]

**Persona**: You are a security-focused engineer preventing injection attacks and data corruption through systematic input validation.

**Questions to ask before implementing validation**:
- What are valid values for each input? (type, range, format, length)
- Where does validation occur? (Client, API layer, business logic, database)
- What happens on validation failure? (400 error with details, silent rejection, sanitization)
- Are there domain-specific validation rules? (email format, credit card format, etc.)

**Principles**:
- **Validate at boundaries**: API layer validates all external input
- **Whitelist over blacklist**: Define allowed patterns, not forbidden ones
- **Fail loudly on invalid input**: Return clear error messages (in dev/test), generic in prod
- **Type validation first**: Check types before business rules
- **Schema-based validation**: Use JSON Schema, Pydantic, Joi for declarative validation

**Implementation Pattern** (observed in codebase):
```python
# Extracted from: [file: src/api/validators.py, lines 10-60]
from pydantic import BaseModel, EmailStr, validator

class CreateUserRequest(BaseModel):
    """Validation schema for user creation"""
    email: EmailStr                    # Email format validation
    username: str
    password: str
    age: int

    @validator('username')
    def username_alphanumeric(cls, v):
        """Username must be alphanumeric"""
        if not v.isalnum():
            raise ValueError('Username must contain only letters and numbers')
        if len(v) < 3 or len(v) > 20:
            raise ValueError('Username must be 3-20 characters')
        return v

    @validator('password')
    def password_strength(cls, v):
        """Password must meet strength requirements"""
        if len(v) < 8:
            raise ValueError('Password must be at least 8 characters')
        if not any(c.isupper() for c in v):
            raise ValueError('Password must contain uppercase letter')
        if not any(c.isdigit() for c in v):
            raise ValueError('Password must contain digit')
        return v

    @validator('age')
    def age_range(cls, v):
        """Age must be reasonable"""
        if v < 13 or v > 120:
            raise ValueError('Age must be between 13 and 120')
        return v

# Usage in API endpoint:
@app.post("/users")
def create_user(request: CreateUserRequest):  # Automatic validation
    # If we reach here, all validation passed
    user = UserService.create(request.dict())
    return user.to_dict()
```

**When to apply**:
- All API endpoints
- All user input (forms, file uploads, etc.)
- Configuration parsing
- External data imports

---

[Continue with more skills extracted from codebase...]

---

## Architecture Decision Records (Inferred)

### ADR-001: Choice of [PostgreSQL over MongoDB]

**Status**: Accepted (inferred from implementation)

**Context**:
The system requires:
- ACID transactions for order processing
- Complex relational queries (joins across users, orders, products)
- Data integrity guarantees
- Mature ecosystem and tooling

**Decision**: Use PostgreSQL as primary database

**Rationale** (inferred from code patterns):
1. **Evidence 1**: Heavy use of foreign key constraints suggests relational integrity is critical
   - Location: [src/db/models.py, lines 45-120]
   - Pattern: All entities have explicit FK relationships

2. **Evidence 2**: Transaction handling in order processing suggests ACID requirements
   - Location: [src/services/order_service.py, lines 200-250]
   - Pattern: Multiple updates wrapped in single transaction

3. **Evidence 3**: Complex JOIN queries suggest relational model fits domain
   - Location: [src/repositories/order_repository.py, lines 80-150]
   - Pattern: Multi-table joins for order + user + product data

**Consequences**:

**Positive**:
- Strong data consistency guarantees
- Rich query capabilities (window functions, CTEs)
- JSON support for semi-structured data (best of both worlds)
- Excellent tool ecosystem (pgAdmin, monitoring, backups)

**Negative**:
- Vertical scaling limits (eventual)
- Schema migrations require planning
- Not ideal for unstructured data

**Alternatives Considered** (inferred):

**MongoDB**:
- **Rejected because**: Need for transactions and complex joins
- **Evidence**: No document-oriented patterns in codebase

**MySQL**:
- **Rejected because**: PostgreSQL's superior JSON and full-text search
- **Could have worked**: Similar feature set for this use case

---

### ADR-002: [JWT-based Authentication over Session Cookies]

**Status**: Accepted (inferred from implementation)

**Context**:
The system needs:
- Stateless authentication (for horizontal scaling)
- Mobile app support (not browser-only)
- Microservices architecture (shared auth across services)

**Decision**: Use JWT tokens for authentication

**Rationale** (inferred from code patterns):
1. **Evidence 1**: No session storage implementation found
   - Location: Absence of Redis/Memcached session store
   - Pattern: No session management code

2. **Evidence 2**: Token-based auth middleware
   - Location: [src/middleware/auth.py, lines 10-50]
   - Pattern: JWT decoding and validation

3. **Evidence 3**: Token refresh endpoint
   - Location: [src/api/auth.py, lines 100-130]
   - Pattern: Refresh token rotation

**Consequences**:

**Positive**:
- Stateless (no server-side session storage)
- Scales horizontally (no session affinity)
- Works across domains (CORS-friendly)
- Mobile-app compatible

**Negative**:
- Cannot revoke tokens before expiry (mitigated with short TTL + refresh tokens)
- Larger than session cookies (JWT payload in every request)
- Vulnerable if secret key compromised

**Mitigation Strategies** (observed):
- Short access token TTL (15 minutes)
- Refresh token rotation
- Token blacklist for logout (stored in Redis)

---

[Continue with more ADRs...]

---

## Code Patterns & Conventions

### Pattern 1: Repository Pattern for Data Access

**Observed in**: All data layer modules

**Structure**:
```python
class UserRepository:
    """Abstract data access for User entity"""

    def find_by_id(self, user_id: int) -> Optional[User]:
        """Find user by ID"""
        pass

    def find_by_email(self, email: str) -> Optional[User]:
        """Find user by email"""
        pass

    def create(self, user_data: dict) -> User:
        """Create new user"""
        pass

    def update(self, user_id: int, updates: dict) -> User:
        """Update existing user"""
        pass

    def delete(self, user_id: int) -> bool:
        """Soft-delete user"""
        pass
```

**Benefits**:
- Decouples business logic from data access
- Testable (can mock repositories)
- Swappable implementations (SQL ‚Üí NoSQL)

**When to apply**: All entity persistence

---

### Pattern 2: Service Layer for Business Logic

**Observed in**: All business logic modules

**Structure**:
```python
class OrderService:
    """Business logic for order processing"""

    def __init__(self, order_repo, inventory_service, payment_service):
        self.order_repo = order_repo
        self.inventory_service = inventory_service
        self.payment_service = payment_service

    def create_order(self, user_id: int, items: List[OrderItem]) -> Order:
        """
        Create order with inventory validation and payment processing

        Steps:
        1. Validate inventory availability
        2. Calculate totals
        3. Process payment
        4. Create order record
        5. Update inventory
        6. Send confirmation
        """
        # Orchestration logic here
        pass
```

**Benefits**:
- Encapsulates business rules
- Coordinates multiple repositories/services
- Transactional boundary

**When to apply**: All complex business operations

---

## Lessons Learned

### What Worked Well

1. **Clear layer separation**
   - Controllers stayed thin (routing only)
   - Services contained business logic
   - Repositories isolated data access
   - **Benefit**: Easy to test, easy to reason about

2. **Comprehensive input validation**
   - Schema-based validation at API boundary
   - Early failure with clear error messages
   - **Benefit**: Prevented data corruption, improved debugging

3. **Structured logging**
   - JSON format with correlation IDs
   - Consistent log levels
   - **Benefit**: Effective debugging in production

### What Could Be Improved

1. **Missing integration tests**
   - Lots of unit tests, few integration tests
   - **Impact**: Bugs in component interactions not caught early
   - **Recommendation**: Add integration test suite

2. **Inconsistent error handling**
   - Some modules use custom exceptions, others use generic
   - **Impact**: Harder to handle errors consistently
   - **Recommendation**: Standardize on error handling strategy

3. **Undocumented API contracts**
   - No OpenAPI/Swagger documentation
   - **Impact**: Frontend developers had to read code
   - **Recommendation**: Generate API docs from code

### What to Avoid in Future Projects

1. **Hardcoded configuration**
   - Some settings hardcoded instead of environment variables
   - **Why bad**: Requires code changes for deployment differences
   - **Alternative**: 12-factor app configuration

2. **Tight coupling to external services**
   - Direct API calls without abstraction layer
   - **Why bad**: Hard to swap providers, hard to test
   - **Alternative**: Adapter pattern for external integrations

3. **Missing observability**
   - No metrics, basic logging only
   - **Why bad**: Blind to production issues
   - **Alternative**: Metrics + tracing + structured logs from day 1

---

## Reusability Assessment

### Components Reusable As-Is

1. **Error handling framework** ‚Üí Portable to any API project
2. **Database connection pooling** ‚Üí Portable to any DB-backed service
3. **JWT authentication middleware** ‚Üí Portable to any auth scenario
4. **Input validation schemas** ‚Üí Patterns reusable, specifics domain-dependent

### Patterns Worth Generalizing

1. **Repository pattern** ‚Üí Create skill/template for any entity
2. **Service orchestration** ‚Üí Create skill for multi-step business logic
3. **API error responses** ‚Üí Create skill for consistent error handling

### Domain-Specific (Not Reusable)

1. **Order processing logic** ‚Üí Specific to e-commerce domain
2. **Inventory management** ‚Üí Specific to this business
3. **Payment integration** ‚Üí Specific to Stripe, but pattern reusable
```

---

## Final Validation Checklist

Before submitting outputs, verify:

- [ ] **spec.md is complete**: Can regenerate system from spec alone?
- [ ] **plan.md is coherent**: Does architecture make sense given requirements?
- [ ] **tasks.md is actionable**: Can team execute without additional guidance?
- [ ] **intelligence-object.md is reusable**: Can skills apply to other projects?
- [ ] **All files cross-reference**: Spec ‚Üí Plan ‚Üí Tasks flow logically?
- [ ] **Evidence provided**: All claims backed by code locations (file:line)?
- [ ] **Gaps identified**: Technical debt and improvements documented?
- [ ] **Regeneration viable**: Could you rebuild this system better with these artifacts?

---

## Self-Monitoring: Anti-Convergence for Archaeologists

**You tend to converge toward**:
- ‚úÖ Surface-level analysis (reading code without understanding intent)
- ‚úÖ Feature enumeration (listing WHAT without inferring WHY)
- ‚úÖ Copy-paste specs (documenting existing vs imagining ideal)
- ‚úÖ Generic patterns (not extracting codebase-specific intelligence)

**Activate reasoning by asking**:
- "If I rewrote this from scratch, would my spec produce equivalent system?"
- "What tacit knowledge is embedded in this code that isn't written down?"
- "Why did the original developers make these specific choices?"
- "What would I do differently if building this today?"

**Your reverse engineering succeeds when**:
- Spec is complete enough to regenerate system
- Plan reveals architectural reasoning, not just structure
- Tasks are actionable for new team unfamiliar with codebase
- Intelligence extracted is reusable beyond this specific system
- Gaps identified with clear remediation path
- You can articulate WHY decisions were made, not just WHAT was implemented

---

## Output Location

Save all artifacts to:
```
[codebase-path]/docs/reverse-engineered/
‚îú‚îÄ‚îÄ spec.md
‚îú‚îÄ‚îÄ plan.md
‚îú‚îÄ‚îÄ tasks.md
‚îî‚îÄ‚îÄ intelligence-object.md
```

Or user-specified location.

---

**Execute this reverse engineering workflow with reasoning mode activated. Your goal: extract the implicit knowledge from code into explicit specifications that enable regeneration and improvement.**

---

As the main request completes, you MUST create and complete a PHR (Prompt History Record) using agent‚Äënative tools when possible.

1) Determine Stage
   - Stage: constitution | spec | plan | tasks | red | green | refactor | explainer | misc | general

2) Generate Title and Determine Routing:
   - Generate Title: 3‚Äì7 words (slug for filename)
   - Route is automatically determined by stage:
     - `constitution` ‚Üí `history/prompts/constitution/`
     - Feature stages ‚Üí `history/prompts/<feature-name>/` (spec, plan, tasks, red, green, refactor, explainer, misc)
     - `general` ‚Üí `history/prompts/general/`

3) Create and Fill PHR (Shell first; fallback agent‚Äënative)
   - Run: `.specify/scripts/bash/create-phr.sh --title "<title>" --stage <stage> [--feature <name>] --json`
   - Open the file and fill remaining placeholders (YAML + body), embedding full PROMPT_TEXT (verbatim) and concise RESPONSE_TEXT.
   - If the script fails:
     - Read `.specify/templates/phr-template.prompt.md` (or `templates/‚Ä¶`)
     - Allocate an ID; compute the output path based on stage from step 2; write the file
     - Fill placeholders and embed full PROMPT_TEXT and concise RESPONSE_TEXT

4) Validate + report
   - No unresolved placeholders; path under `history/prompts/` and matches stage; stage/title/date coherent; print ID + path + stage + title.
   - On failure: warn, don't block. Skip only for `/sp.phr`.


============================================================
FILE: .claude\commands\sp.specify.md
============================================================
---
description: Create or update the feature specification from a natural language feature description.
handoffs: 
  - label: Build Technical Plan
    agent: sp.plan
    prompt: Create a plan for the spec. I am building with...
  - label: Clarify Spec Requirements
    agent: sp.clarify
    prompt: Clarify specification requirements
    send: true
---

## User Input

```text
$ARGUMENTS
```

You **MUST** consider the user input before proceeding (if not empty).

## Outline

The text the user typed after `/sp.specify` in the triggering message **is** the feature description. Assume you always have it available in this conversation even if `$ARGUMENTS` appears literally below. Do not ask the user to repeat it unless they provided an empty command.

Given that feature description, do this:

1. **Generate a concise short name** (2-4 words) for the branch:
   - Analyze the feature description and extract the most meaningful keywords
   - Create a 2-4 word short name that captures the essence of the feature
   - Use action-noun format when possible (e.g., "add-user-auth", "fix-payment-bug")
   - Preserve technical terms and acronyms (OAuth2, API, JWT, etc.)
   - Keep it concise but descriptive enough to understand the feature at a glance
   - Examples:
     - "I want to add user authentication" ‚Üí "user-auth"
     - "Implement OAuth2 integration for the API" ‚Üí "oauth2-api-integration"
     - "Create a dashboard for analytics" ‚Üí "analytics-dashboard"
     - "Fix payment processing timeout bug" ‚Üí "fix-payment-timeout"

2. **Check for existing branches before creating new one**:

   a. First, fetch all remote branches to ensure we have the latest information:

      ```bash
      git fetch --all --prune
      ```

   b. Find the highest feature number across all sources for the short-name:
      - Remote branches: `git ls-remote --heads origin | grep -E 'refs/heads/[0-9]+-<short-name>$'`
      - Local branches: `git branch | grep -E '^[* ]*[0-9]+-<short-name>$'`
      - Specs directories: Check for directories matching `specs/[0-9]+-<short-name>`

   c. Determine the next available number:
      - Extract all numbers from all three sources
      - Find the highest number N
      - Use N+1 for the new branch number

   d. Run the script `.specify/scripts/powershell/create-new-feature.ps1 -Json "$ARGUMENTS"` with the calculated number and short-name:
      - Pass `--number N+1` and `--short-name "your-short-name"` along with the feature description
      - Bash example: `.specify/scripts/powershell/create-new-feature.ps1 -Json "$ARGUMENTS" --json --number 5 --short-name "user-auth" "Add user authentication"`
      - PowerShell example: `.specify/scripts/powershell/create-new-feature.ps1 -Json "$ARGUMENTS" -Json -Number 5 -ShortName "user-auth" "Add user authentication"`

   **IMPORTANT**:
   - Check all three sources (remote branches, local branches, specs directories) to find the highest number
   - Only match branches/directories with the exact short-name pattern
   - If no existing branches/directories found with this short-name, start with number 1
   - You must only ever run this script once per feature
   - The JSON is provided in the terminal as output - always refer to it to get the actual content you're looking for
   - The JSON output will contain BRANCH_NAME and SPEC_FILE paths
   - For single quotes in args like "I'm Groot", use escape syntax: e.g 'I'\''m Groot' (or double-quote if possible: "I'm Groot")

3. Load `.specify/templates/spec-template.md` to understand required sections.

4. Follow this execution flow:

    1. Parse user description from Input
       If empty: ERROR "No feature description provided"
    2. Extract key concepts from description
       Identify: actors, actions, data, constraints
    3. For unclear aspects:
       - Make informed guesses based on context and industry standards
       - Only mark with [NEEDS CLARIFICATION: specific question] if:
         - The choice significantly impacts feature scope or user experience
         - Multiple reasonable interpretations exist with different implications
         - No reasonable default exists
       - **LIMIT: Maximum 3 [NEEDS CLARIFICATION] markers total**
       - Prioritize clarifications by impact: scope > security/privacy > user experience > technical details
    4. Fill User Scenarios & Testing section
       If no clear user flow: ERROR "Cannot determine user scenarios"
    5. Generate Functional Requirements
       Each requirement must be testable
       Use reasonable defaults for unspecified details (document assumptions in Assumptions section)
    6. Define Success Criteria
       Create measurable, technology-agnostic outcomes
       Include both quantitative metrics (time, performance, volume) and qualitative measures (user satisfaction, task completion)
       Each criterion must be verifiable without implementation details
    7. Identify Key Entities (if data involved)
    8. Return: SUCCESS (spec ready for planning)

5. Write the specification to SPEC_FILE using the template structure, replacing placeholders with concrete details derived from the feature description (arguments) while preserving section order and headings.

6. **Specification Quality Validation**: After writing the initial spec, validate it against quality criteria:

   a. **Create Spec Quality Checklist**: Generate a checklist file at `FEATURE_DIR/checklists/requirements.md` using the checklist template structure with these validation items:

      ```markdown
      # Specification Quality Checklist: [FEATURE NAME]
      
      **Purpose**: Validate specification completeness and quality before proceeding to planning
      **Created**: [DATE]
      **Feature**: [Link to spec.md]
      
      ## Content Quality
      
      - [ ] No implementation details (languages, frameworks, APIs)
      - [ ] Focused on user value and business needs
      - [ ] Written for non-technical stakeholders
      - [ ] All mandatory sections completed
      
      ## Requirement Completeness
      
      - [ ] No [NEEDS CLARIFICATION] markers remain
      - [ ] Requirements are testable and unambiguous
      - [ ] Success criteria are measurable
      - [ ] Success criteria are technology-agnostic (no implementation details)
      - [ ] All acceptance scenarios are defined
      - [ ] Edge cases are identified
      - [ ] Scope is clearly bounded
      - [ ] Dependencies and assumptions identified
      
      ## Feature Readiness
      
      - [ ] All functional requirements have clear acceptance criteria
      - [ ] User scenarios cover primary flows
      - [ ] Feature meets measurable outcomes defined in Success Criteria
      - [ ] No implementation details leak into specification
      
      ## Notes
      
      - Items marked incomplete require spec updates before `/sp.clarify` or `/sp.plan`
      ```

   b. **Run Validation Check**: Review the spec against each checklist item:
      - For each item, determine if it passes or fails
      - Document specific issues found (quote relevant spec sections)

   c. **Handle Validation Results**:

      - **If all items pass**: Mark checklist complete and proceed to step 6

      - **If items fail (excluding [NEEDS CLARIFICATION])**:
        1. List the failing items and specific issues
        2. Update the spec to address each issue
        3. Re-run validation until all items pass (max 3 iterations)
        4. If still failing after 3 iterations, document remaining issues in checklist notes and warn user

      - **If [NEEDS CLARIFICATION] markers remain**:
        1. Extract all [NEEDS CLARIFICATION: ...] markers from the spec
        2. **LIMIT CHECK**: If more than 3 markers exist, keep only the 3 most critical (by scope/security/UX impact) and make informed guesses for the rest
        3. For each clarification needed (max 3), present options to user in this format:

           ```markdown
           ## Question [N]: [Topic]
           
           **Context**: [Quote relevant spec section]
           
           **What we need to know**: [Specific question from NEEDS CLARIFICATION marker]
           
           **Suggested Answers**:
           
           | Option | Answer | Implications |
           |--------|--------|--------------|
           | A      | [First suggested answer] | [What this means for the feature] |
           | B      | [Second suggested answer] | [What this means for the feature] |
           | C      | [Third suggested answer] | [What this means for the feature] |
           | Custom | Provide your own answer | [Explain how to provide custom input] |
           
           **Your choice**: _[Wait for user response]_
           ```

        4. **CRITICAL - Table Formatting**: Ensure markdown tables are properly formatted:
           - Use consistent spacing with pipes aligned
           - Each cell should have spaces around content: `| Content |` not `|Content|`
           - Header separator must have at least 3 dashes: `|--------|`
           - Test that the table renders correctly in markdown preview
        5. Number questions sequentially (Q1, Q2, Q3 - max 3 total)
        6. Present all questions together before waiting for responses
        7. Wait for user to respond with their choices for all questions (e.g., "Q1: A, Q2: Custom - [details], Q3: B")
        8. Update the spec by replacing each [NEEDS CLARIFICATION] marker with the user's selected or provided answer
        9. Re-run validation after all clarifications are resolved

   d. **Update Checklist**: After each validation iteration, update the checklist file with current pass/fail status

7. Report completion with branch name, spec file path, checklist results, and readiness for the next phase (`/sp.clarify` or `/sp.plan`).

**NOTE:** The script creates and checks out the new branch and initializes the spec file before writing.

## General Guidelines

## Quick Guidelines

- Focus on **WHAT** users need and **WHY**.
- Avoid HOW to implement (no tech stack, APIs, code structure).
- Written for business stakeholders, not developers.
- DO NOT create any checklists that are embedded in the spec. That will be a separate command.

### Section Requirements

- **Mandatory sections**: Must be completed for every feature
- **Optional sections**: Include only when relevant to the feature
- When a section doesn't apply, remove it entirely (don't leave as "N/A")

### For AI Generation

When creating this spec from a user prompt:

1. **Make informed guesses**: Use context, industry standards, and common patterns to fill gaps
2. **Document assumptions**: Record reasonable defaults in the Assumptions section
3. **Limit clarifications**: Maximum 3 [NEEDS CLARIFICATION] markers - use only for critical decisions that:
   - Significantly impact feature scope or user experience
   - Have multiple reasonable interpretations with different implications
   - Lack any reasonable default
4. **Prioritize clarifications**: scope > security/privacy > user experience > technical details
5. **Think like a tester**: Every vague requirement should fail the "testable and unambiguous" checklist item
6. **Common areas needing clarification** (only if no reasonable default exists):
   - Feature scope and boundaries (include/exclude specific use cases)
   - User types and permissions (if multiple conflicting interpretations possible)
   - Security/compliance requirements (when legally/financially significant)

**Examples of reasonable defaults** (don't ask about these):

- Data retention: Industry-standard practices for the domain
- Performance targets: Standard web/mobile app expectations unless specified
- Error handling: User-friendly messages with appropriate fallbacks
- Authentication method: Standard session-based or OAuth2 for web apps
- Integration patterns: RESTful APIs unless specified otherwise

### Success Criteria Guidelines

Success criteria must be:

1. **Measurable**: Include specific metrics (time, percentage, count, rate)
2. **Technology-agnostic**: No mention of frameworks, languages, databases, or tools
3. **User-focused**: Describe outcomes from user/business perspective, not system internals
4. **Verifiable**: Can be tested/validated without knowing implementation details

**Good examples**:

- "Users can complete checkout in under 3 minutes"
- "System supports 10,000 concurrent users"
- "95% of searches return results in under 1 second"
- "Task completion rate improves by 40%"

**Bad examples** (implementation-focused):

- "API response time is under 200ms" (too technical, use "Users see results instantly")
- "Database can handle 1000 TPS" (implementation detail, use user-facing metric)
- "React components render efficiently" (framework-specific)
- "Redis cache hit rate above 80%" (technology-specific)

---

As the main request completes, you MUST create and complete a PHR (Prompt History Record) using agent‚Äënative tools when possible.

1) Determine Stage
   - Stage: constitution | spec | plan | tasks | red | green | refactor | explainer | misc | general

2) Generate Title and Determine Routing:
   - Generate Title: 3‚Äì7 words (slug for filename)
   - Route is automatically determined by stage:
     - `constitution` ‚Üí `history/prompts/constitution/`
     - Feature stages ‚Üí `history/prompts/<feature-name>/` (spec, plan, tasks, red, green, refactor, explainer, misc)
     - `general` ‚Üí `history/prompts/general/`

3) Create and Fill PHR (Shell first; fallback agent‚Äënative)
   - Run: `.specify/scripts/bash/create-phr.sh --title "<title>" --stage <stage> [--feature <name>] --json`
   - Open the file and fill remaining placeholders (YAML + body), embedding full PROMPT_TEXT (verbatim) and concise RESPONSE_TEXT.
   - If the script fails:
     - Read `.specify/templates/phr-template.prompt.md` (or `templates/‚Ä¶`)
     - Allocate an ID; compute the output path based on stage from step 2; write the file
     - Fill placeholders and embed full PROMPT_TEXT and concise RESPONSE_TEXT

4) Validate + report
   - No unresolved placeholders; path under `history/prompts/` and matches stage; stage/title/date coherent; print ID + path + stage + title.
   - On failure: warn, don't block. Skip only for `/sp.phr`.


============================================================
FILE: .claude\commands\sp.tasks.md
============================================================
---
description: Generate an actionable, dependency-ordered tasks.md for the feature based on available design artifacts.
handoffs: 
  - label: Analyze For Consistency
    agent: sp.analyze
    prompt: Run a project analysis for consistency
    send: true
  - label: Implement Project
    agent: sp.implement
    prompt: Start the implementation in phases
    send: true
---

## User Input

```text
$ARGUMENTS
```

You **MUST** consider the user input before proceeding (if not empty).

## Outline

1. **Setup**: Run `.specify/scripts/powershell/check-prerequisites.ps1 -Json` from repo root and parse FEATURE_DIR and AVAILABLE_DOCS list. All paths must be absolute. For single quotes in args like "I'm Groot", use escape syntax: e.g 'I'\''m Groot' (or double-quote if possible: "I'm Groot").

2. **Load design documents**: Read from FEATURE_DIR:
   - **Required**: plan.md (tech stack, libraries, structure), spec.md (user stories with priorities)
   - **Optional**: data-model.md (entities), contracts/ (API endpoints), research.md (decisions), quickstart.md (test scenarios)
   - Note: Not all projects have all documents. Generate tasks based on what's available.

3. **Execute task generation workflow**:
   - Load plan.md and extract tech stack, libraries, project structure
   - Load spec.md and extract user stories with their priorities (P1, P2, P3, etc.)
   - If data-model.md exists: Extract entities and map to user stories
   - If contracts/ exists: Map endpoints to user stories
   - If research.md exists: Extract decisions for setup tasks
   - Generate tasks organized by user story (see Task Generation Rules below)
   - Generate dependency graph showing user story completion order
   - Create parallel execution examples per user story
   - Validate task completeness (each user story has all needed tasks, independently testable)

4. **Generate tasks.md**: Use `.specify/templates/tasks-template.md` as structure, fill with:
   - Correct feature name from plan.md
   - Phase 1: Setup tasks (project initialization)
   - Phase 2: Foundational tasks (blocking prerequisites for all user stories)
   - Phase 3+: One phase per user story (in priority order from spec.md)
   - Each phase includes: story goal, independent test criteria, tests (if requested), implementation tasks
   - Final Phase: Polish & cross-cutting concerns
   - All tasks must follow the strict checklist format (see Task Generation Rules below)
   - Clear file paths for each task
   - Dependencies section showing story completion order
   - Parallel execution examples per story
   - Implementation strategy section (MVP first, incremental delivery)

5. **Report**: Output path to generated tasks.md and summary:
   - Total task count
   - Task count per user story
   - Parallel opportunities identified
   - Independent test criteria for each story
   - Suggested MVP scope (typically just User Story 1)
   - Format validation: Confirm ALL tasks follow the checklist format (checkbox, ID, labels, file paths)

Context for task generation: $ARGUMENTS

The tasks.md should be immediately executable - each task must be specific enough that an LLM can complete it without additional context.

## Task Generation Rules

**CRITICAL**: Tasks MUST be organized by user story to enable independent implementation and testing.

**Tests are OPTIONAL**: Only generate test tasks if explicitly requested in the feature specification or if user requests TDD approach.

### Checklist Format (REQUIRED)

Every task MUST strictly follow this format:

```text
- [ ] [TaskID] [P?] [Story?] Description with file path
```

**Format Components**:

1. **Checkbox**: ALWAYS start with `- [ ]` (markdown checkbox)
2. **Task ID**: Sequential number (T001, T002, T003...) in execution order
3. **[P] marker**: Include ONLY if task is parallelizable (different files, no dependencies on incomplete tasks)
4. **[Story] label**: REQUIRED for user story phase tasks only
   - Format: [US1], [US2], [US3], etc. (maps to user stories from spec.md)
   - Setup phase: NO story label
   - Foundational phase: NO story label  
   - User Story phases: MUST have story label
   - Polish phase: NO story label
5. **Description**: Clear action with exact file path

**Examples**:

- ‚úÖ CORRECT: `- [ ] T001 Create project structure per implementation plan`
- ‚úÖ CORRECT: `- [ ] T005 [P] Implement authentication middleware in src/middleware/auth.py`
- ‚úÖ CORRECT: `- [ ] T012 [P] [US1] Create User model in src/models/user.py`
- ‚úÖ CORRECT: `- [ ] T014 [US1] Implement UserService in src/services/user_service.py`
- ‚ùå WRONG: `- [ ] Create User model` (missing ID and Story label)
- ‚ùå WRONG: `T001 [US1] Create model` (missing checkbox)
- ‚ùå WRONG: `- [ ] [US1] Create User model` (missing Task ID)
- ‚ùå WRONG: `- [ ] T001 [US1] Create model` (missing file path)

### Task Organization

1. **From User Stories (spec.md)** - PRIMARY ORGANIZATION:
   - Each user story (P1, P2, P3...) gets its own phase
   - Map all related components to their story:
     - Models needed for that story
     - Services needed for that story
     - Endpoints/UI needed for that story
     - If tests requested: Tests specific to that story
   - Mark story dependencies (most stories should be independent)

2. **From Contracts**:
   - Map each contract/endpoint ‚Üí to the user story it serves
   - If tests requested: Each contract ‚Üí contract test task [P] before implementation in that story's phase

3. **From Data Model**:
   - Map each entity to the user story(ies) that need it
   - If entity serves multiple stories: Put in earliest story or Setup phase
   - Relationships ‚Üí service layer tasks in appropriate story phase

4. **From Setup/Infrastructure**:
   - Shared infrastructure ‚Üí Setup phase (Phase 1)
   - Foundational/blocking tasks ‚Üí Foundational phase (Phase 2)
   - Story-specific setup ‚Üí within that story's phase

### Phase Structure

- **Phase 1**: Setup (project initialization)
- **Phase 2**: Foundational (blocking prerequisites - MUST complete before user stories)
- **Phase 3+**: User Stories in priority order (P1, P2, P3...)
  - Within each story: Tests (if requested) ‚Üí Models ‚Üí Services ‚Üí Endpoints ‚Üí Integration
  - Each phase should be a complete, independently testable increment
- **Final Phase**: Polish & Cross-Cutting Concerns

---

As the main request completes, you MUST create and complete a PHR (Prompt History Record) using agent‚Äënative tools when possible.

1) Determine Stage
   - Stage: constitution | spec | plan | tasks | red | green | refactor | explainer | misc | general

2) Generate Title and Determine Routing:
   - Generate Title: 3‚Äì7 words (slug for filename)
   - Route is automatically determined by stage:
     - `constitution` ‚Üí `history/prompts/constitution/`
     - Feature stages ‚Üí `history/prompts/<feature-name>/` (spec, plan, tasks, red, green, refactor, explainer, misc)
     - `general` ‚Üí `history/prompts/general/`

3) Create and Fill PHR (Shell first; fallback agent‚Äënative)
   - Run: `.specify/scripts/bash/create-phr.sh --title "<title>" --stage <stage> [--feature <name>] --json`
   - Open the file and fill remaining placeholders (YAML + body), embedding full PROMPT_TEXT (verbatim) and concise RESPONSE_TEXT.
   - If the script fails:
     - Read `.specify/templates/phr-template.prompt.md` (or `templates/‚Ä¶`)
     - Allocate an ID; compute the output path based on stage from step 2; write the file
     - Fill placeholders and embed full PROMPT_TEXT and concise RESPONSE_TEXT

4) Validate + report
   - No unresolved placeholders; path under `history/prompts/` and matches stage; stage/title/date coherent; print ID + path + stage + title.
   - On failure: warn, don't block. Skip only for `/sp.phr`.


============================================================
FILE: .claude\commands\sp.taskstoissues.md
============================================================
---
description: Convert existing tasks into actionable, dependency-ordered GitHub issues for the feature based on available design artifacts.
tools: ['github/github-mcp-server/issue_write']
---

## User Input

```text
$ARGUMENTS
```

You **MUST** consider the user input before proceeding (if not empty).

## Outline

1. Run `.specify/scripts/powershell/check-prerequisites.ps1 -Json -RequireTasks -IncludeTasks` from repo root and parse FEATURE_DIR and AVAILABLE_DOCS list. All paths must be absolute. For single quotes in args like "I'm Groot", use escape syntax: e.g 'I'\''m Groot' (or double-quote if possible: "I'm Groot").
1. From the executed script, extract the path to **tasks**.
1. Get the Git remote by running:

```bash
git config --get remote.origin.url
```

> [!CAUTION]
> ONLY PROCEED TO NEXT STEPS IF THE REMOTE IS A GITHUB URL

1. For each task in the list, use the GitHub MCP server to create a new issue in the repository that is representative of the Git remote.

> [!CAUTION]
> UNDER NO CIRCUMSTANCES EVER CREATE ISSUES IN REPOSITORIES THAT DO NOT MATCH THE REMOTE URL

---

As the main request completes, you MUST create and complete a PHR (Prompt History Record) using agent‚Äënative tools when possible.

1) Determine Stage
   - Stage: constitution | spec | plan | tasks | red | green | refactor | explainer | misc | general

2) Generate Title and Determine Routing:
   - Generate Title: 3‚Äì7 words (slug for filename)
   - Route is automatically determined by stage:
     - `constitution` ‚Üí `history/prompts/constitution/`
     - Feature stages ‚Üí `history/prompts/<feature-name>/` (spec, plan, tasks, red, green, refactor, explainer, misc)
     - `general` ‚Üí `history/prompts/general/`

3) Create and Fill PHR (Shell first; fallback agent‚Äënative)
   - Run: `.specify/scripts/bash/create-phr.sh --title "<title>" --stage <stage> [--feature <name>] --json`
   - Open the file and fill remaining placeholders (YAML + body), embedding full PROMPT_TEXT (verbatim) and concise RESPONSE_TEXT.
   - If the script fails:
     - Read `.specify/templates/phr-template.prompt.md` (or `templates/‚Ä¶`)
     - Allocate an ID; compute the output path based on stage from step 2; write the file
     - Fill placeholders and embed full PROMPT_TEXT and concise RESPONSE_TEXT

4) Validate + report
   - No unresolved placeholders; path under `history/prompts/` and matches stage; stage/title/date coherent; print ID + path + stage + title.
   - On failure: warn, don't block. Skip only for `/sp.phr`.


============================================================
FILE: .claude\output-styles\textbook.md
============================================================
Style: Academic + practical, Markdown headings, code blocks, diagrams, step-by-step instructions.
Keep chapters clear, concise, and easy-to-read.

============================================================
FILE: .claude\skills\gazebo.md
============================================================
## Gazebo Skills

### Core Concepts
- Physics simulation: Realistic dynamics, collisions, and forces
- Collision detection: Accurate interaction between objects
- Sensor simulation: Cameras, LIDAR, IMU, force/torque sensors
- URDF/SDF: Robot and world description formats
- Unity visualization: Alternative visualization and simulation environment
- Plugins: Custom simulation behaviors and interfaces

### Key Components
- World files: Environment descriptions
- Model database: Pre-built robot and object models
- Control interfaces: Joint control, actuator simulation
- Physics engines: ODE, Bullet, DART integration
- Rendering: Visual simulation and camera systems
- ROS 2 integration: Gazebo-ROS 2 bridge

============================================================
FILE: .claude\skills\isaac.md
============================================================
## Isaac Skills

### Core Concepts
- Isaac Sim: NVIDIA's robotics simulation platform
- Isaac ROS: ROS 2 packages for perception and navigation
- Perception: Computer vision, sensor processing, 3D understanding
- Navigation: Path planning, obstacle avoidance, localization
- Nav2: Navigation 2 stack for mobile robots
- Sim-to-Real: Transfer learning from simulation to reality

### Key Components
- GPU-accelerated simulation
- PhysX physics engine
- RTX rendering for photorealistic simulation
- Isaac ROS packages: image pipeline, perception, manipulation
- Domain randomization for robust training
- AI training environments and reinforcement learning

============================================================
FILE: .claude\skills\ros2.md
============================================================
## ROS 2 Skills

### Core Concepts
- Nodes: Individual processes that communicate with each other
- Topics: Publish/subscribe communication pattern
- Services: Request/response communication pattern
- Actions: Goal-based communication with feedback
- rclpy: Python client library for ROS 2
- URDF: Unified Robot Description Format for robot modeling

### Key Components
- Lifecycle management of nodes
- Parameter server for configuration
- Launch files for starting multiple nodes
- Message and service definitions
- Quality of Service (QoS) settings
- TF (Transform) system for coordinate frames

============================================================
FILE: .claude\skills\vla.md
============================================================
## VLA Skills

### Core Concepts
- Whisper voice-to-action: Speech recognition and command interpretation
- LLM planning: Large language model for task planning and reasoning
- ROS 2 actions: Converting high-level plans to executable commands
- Multi-modal interaction: Combining vision, language, and action
- Safety protocols: Ensuring safe robot behavior during VLA execution

### Key Components
- Speech-to-text conversion
- Natural language understanding
- Task decomposition and planning
- Action execution and monitoring
- Feedback and error handling
- Safety constraints and validation

============================================================
FILE: .specify\memory\constitution.md
============================================================
# Physical AI & Humanoid Robotics Book Constitution

## Core Principles

### I. Technical Accuracy & Industry Alignment
Content must be technically accurate and reflect current industry practices in physical AI and humanoid robotics. All concepts, algorithms, and implementations should align with state-of-the-art research and commercial applications in the field.

### II. Beginner-to-Advanced Learning Flow
Structure content to guide readers from fundamental concepts to advanced implementations. Each chapter builds incrementally on previous knowledge, ensuring beginners can follow while providing depth for advanced readers.

### III. Hands-On, Project-Driven Learning
Every concept must be accompanied by practical, implementable projects. Readers learn by building real systems, implementing algorithms, and working with actual robotics platforms and simulation environments.

### IV. Open-Source Friendly Approach
All code examples, projects, and tools should leverage open-source technologies wherever possible. Promote community engagement through open-source robotics frameworks like ROS, Gazebo, PyBullet, and other accessible tools.

### V. Python & AI Foundation Assumption
Content assumes readers have Python programming skills and basic AI/machine learning knowledge. Build upon these foundations to introduce robotics-specific concepts without re-teaching basic programming or AI principles.

### VI. Accessible Hardware & Simulation Focus
Emphasize simulation environments and affordable hardware options to make content accessible. Prioritize tools that students can use without expensive robotics hardware, while providing paths to real hardware implementation.

## Target Audience & Prerequisites

### Primary Audience
- Computer science and engineering students
- AI/robotics researchers and practitioners
- Software engineers transitioning to robotics
- Graduate students in robotics programs

### Prerequisites
- Proficient Python programming skills
- Basic understanding of AI and machine learning concepts
- Linear algebra and calculus fundamentals
- Basic physics knowledge (mechanics)

## Scope & Content Coverage

### In Scope
- Physical AI algorithms (control theory, motion planning, perception)
- Humanoid robotics kinematics and dynamics
- Simulation environments (Gazebo, PyBullet, Mujoco)
- ROS (Robot Operating System) integration
- Locomotion and balance control
- Sensor fusion and perception
- Reinforcement learning for robotics
- Human-robot interaction
- Ethics and safety in robotics

### Out of Scope
- Detailed mechanical engineering (CAD, manufacturing)
- Low-level embedded systems programming
- Proprietary commercial robotics platforms
- Advanced control theory mathematics beyond practical application

## Content Standards

### Quality Requirements
- All code examples must be tested and functional
- Projects must be reproducible on standard development environments
- Mathematical concepts explained with visual aids and practical examples
- Regular updates to reflect evolving field standards

### Technical Standards
- Use Python 3.8+ with standard scientific computing stack
- Leverage popular libraries: NumPy, SciPy, PyTorch, TensorFlow
- ROS 2 (latest stable) for robotics communication
- Simulation environments that run on consumer hardware

## Development Workflow

### Content Creation Process
- Each chapter follows: Concept ‚Üí Implementation ‚Üí Project ‚Üí Exercises
- Code examples undergo peer review before inclusion
- Projects must be tested on multiple platforms
- Regular feedback integration from beta readers

### Review & Quality Assurance
- Technical accuracy verified by robotics experts
- Code examples tested in clean environments
- Student feedback incorporated iteratively
- Industry practitioners validate real-world relevance

## Governance

This constitution guides all content creation for the "Physical AI & Humanoid Robotics" book. All chapters, examples, and projects must align with these principles. Amendments require technical review and validation against learning objectives.

**Version**: 1.0.0 | **Ratified**: 2025-12-18 | **Last Amended**: 2025-12-18


============================================================
FILE: .specify\templates\adr-template.md
============================================================
# ADR-{{ID}}: {{TITLE}}

> **Scope**: Document decision clusters, not individual technology choices. Group related decisions that work together (e.g., "Frontend Stack" not separate ADRs for framework, styling, deployment).

- **Status:** Proposed | Accepted | Superseded | Rejected
- **Date:** {{DATE_ISO}}
- **Feature:** {{FEATURE_NAME}}
- **Context:** {{CONTEXT}}

<!-- Significance checklist (ALL must be true to justify this ADR)
     1) Impact: Long-term consequence for architecture/platform/security?
     2) Alternatives: Multiple viable options considered with tradeoffs?
     3) Scope: Cross-cutting concern (not an isolated detail)?
     If any are false, prefer capturing as a PHR note instead of an ADR. -->

## Decision

{{DECISION}}

<!-- For technology stacks, list all components:
     - Framework: Next.js 14 (App Router)
     - Styling: Tailwind CSS v3
     - Deployment: Vercel
     - State Management: React Context (start simple)
-->

## Consequences

### Positive

{{POSITIVE_CONSEQUENCES}}

<!-- Example: Integrated tooling, excellent DX, fast deploys, strong TypeScript support -->

### Negative

{{NEGATIVE_CONSEQUENCES}}

<!-- Example: Vendor lock-in to Vercel, framework coupling, learning curve -->

## Alternatives Considered

{{ALTERNATIVES}}

<!-- Group alternatives by cluster:
     Alternative Stack A: Remix + styled-components + Cloudflare
     Alternative Stack B: Vite + vanilla CSS + AWS Amplify
     Why rejected: Less integrated, more setup complexity
-->

## References

- Feature Spec: {{SPEC_LINK}}
- Implementation Plan: {{PLAN_LINK}}
- Related ADRs: {{RELATED_ADRS}}
- Evaluator Evidence: {{EVAL_NOTES_LINK}} <!-- link to eval notes/PHR showing graders and outcomes -->


============================================================
FILE: .specify\templates\agent-file-template.md
============================================================
# [PROJECT NAME] Development Guidelines

Auto-generated from all feature plans. Last updated: [DATE]

## Active Technologies

[EXTRACTED FROM ALL PLAN.MD FILES]

## Project Structure

```text
[ACTUAL STRUCTURE FROM PLANS]
```

## Commands

[ONLY COMMANDS FOR ACTIVE TECHNOLOGIES]

## Code Style

[LANGUAGE-SPECIFIC, ONLY FOR LANGUAGES IN USE]

## Recent Changes

[LAST 3 FEATURES AND WHAT THEY ADDED]

<!-- MANUAL ADDITIONS START -->
<!-- MANUAL ADDITIONS END -->


============================================================
FILE: .specify\templates\checklist-template.md
============================================================
# [CHECKLIST TYPE] Checklist: [FEATURE NAME]

**Purpose**: [Brief description of what this checklist covers]
**Created**: [DATE]
**Feature**: [Link to spec.md or relevant documentation]

**Note**: This checklist is generated by the `/sp.checklist` command based on feature context and requirements.

<!-- 
  ============================================================================
  IMPORTANT: The checklist items below are SAMPLE ITEMS for illustration only.
  
  The /sp.checklist command MUST replace these with actual items based on:
  - User's specific checklist request
  - Feature requirements from spec.md
  - Technical context from plan.md
  - Implementation details from tasks.md
  
  DO NOT keep these sample items in the generated checklist file.
  ============================================================================
-->

## [Category 1]

- [ ] CHK001 First checklist item with clear action
- [ ] CHK002 Second checklist item
- [ ] CHK003 Third checklist item

## [Category 2]

- [ ] CHK004 Another category item
- [ ] CHK005 Item with specific criteria
- [ ] CHK006 Final item in this category

## Notes

- Check items off as completed: `[x]`
- Add comments or findings inline
- Link to relevant resources or documentation
- Items are numbered sequentially for easy reference


============================================================
FILE: .specify\templates\phr-template.prompt.md
============================================================
---
id: {{ID}}
title: {{TITLE}}
stage: {{STAGE}}
date: {{DATE_ISO}}
surface: {{SURFACE}}
model: {{MODEL}}
feature: {{FEATURE}}
branch: {{BRANCH}}
user: {{USER}}
command: {{COMMAND}}
labels: [{{LABELS}}]
links:
  spec: {{LINKS_SPEC}}
  ticket: {{LINKS_TICKET}}
  adr: {{LINKS_ADR}}
  pr: {{LINKS_PR}}
files:
{{FILES_YAML}}
tests:
{{TESTS_YAML}}
---

## Prompt

{{PROMPT_TEXT}}

## Response snapshot

{{RESPONSE_TEXT}}

## Outcome

- ‚úÖ Impact: {{OUTCOME_IMPACT}}
- üß™ Tests: {{TESTS_SUMMARY}}
- üìÅ Files: {{FILES_SUMMARY}}
- üîÅ Next prompts: {{NEXT_PROMPTS}}
- üß† Reflection: {{REFLECTION_NOTE}}

## Evaluation notes (flywheel)

- Failure modes observed: {{FAILURE_MODES}}
- Graders run and results (PASS/FAIL): {{GRADER_RESULTS}}
- Prompt variant (if applicable): {{PROMPT_VARIANT_ID}}
- Next experiment (smallest change to try): {{NEXT_EXPERIMENT}}


============================================================
FILE: .specify\templates\plan-template.md
============================================================
# Implementation Plan: [FEATURE]

**Branch**: `[###-feature-name]` | **Date**: [DATE] | **Spec**: [link]
**Input**: Feature specification from `/specs/[###-feature-name]/spec.md`

**Note**: This template is filled in by the `/sp.plan` command. See `.specify/templates/commands/plan.md` for the execution workflow.

## Summary

[Extract from feature spec: primary requirement + technical approach from research]

## Technical Context

<!--
  ACTION REQUIRED: Replace the content in this section with the technical details
  for the project. The structure here is presented in advisory capacity to guide
  the iteration process.
-->

**Language/Version**: [e.g., Python 3.11, Swift 5.9, Rust 1.75 or NEEDS CLARIFICATION]  
**Primary Dependencies**: [e.g., FastAPI, UIKit, LLVM or NEEDS CLARIFICATION]  
**Storage**: [if applicable, e.g., PostgreSQL, CoreData, files or N/A]  
**Testing**: [e.g., pytest, XCTest, cargo test or NEEDS CLARIFICATION]  
**Target Platform**: [e.g., Linux server, iOS 15+, WASM or NEEDS CLARIFICATION]
**Project Type**: [single/web/mobile - determines source structure]  
**Performance Goals**: [domain-specific, e.g., 1000 req/s, 10k lines/sec, 60 fps or NEEDS CLARIFICATION]  
**Constraints**: [domain-specific, e.g., <200ms p95, <100MB memory, offline-capable or NEEDS CLARIFICATION]  
**Scale/Scope**: [domain-specific, e.g., 10k users, 1M LOC, 50 screens or NEEDS CLARIFICATION]

## Constitution Check

*GATE: Must pass before Phase 0 research. Re-check after Phase 1 design.*

[Gates determined based on constitution file]

## Project Structure

### Documentation (this feature)

```text
specs/[###-feature]/
‚îú‚îÄ‚îÄ plan.md              # This file (/sp.plan command output)
‚îú‚îÄ‚îÄ research.md          # Phase 0 output (/sp.plan command)
‚îú‚îÄ‚îÄ data-model.md        # Phase 1 output (/sp.plan command)
‚îú‚îÄ‚îÄ quickstart.md        # Phase 1 output (/sp.plan command)
‚îú‚îÄ‚îÄ contracts/           # Phase 1 output (/sp.plan command)
‚îî‚îÄ‚îÄ tasks.md             # Phase 2 output (/sp.tasks command - NOT created by /sp.plan)
```

### Source Code (repository root)
<!--
  ACTION REQUIRED: Replace the placeholder tree below with the concrete layout
  for this feature. Delete unused options and expand the chosen structure with
  real paths (e.g., apps/admin, packages/something). The delivered plan must
  not include Option labels.
-->

```text
# [REMOVE IF UNUSED] Option 1: Single project (DEFAULT)
src/
‚îú‚îÄ‚îÄ models/
‚îú‚îÄ‚îÄ services/
‚îú‚îÄ‚îÄ cli/
‚îî‚îÄ‚îÄ lib/

tests/
‚îú‚îÄ‚îÄ contract/
‚îú‚îÄ‚îÄ integration/
‚îî‚îÄ‚îÄ unit/

# [REMOVE IF UNUSED] Option 2: Web application (when "frontend" + "backend" detected)
backend/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îî‚îÄ‚îÄ api/
‚îî‚îÄ‚îÄ tests/

frontend/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îú‚îÄ‚îÄ pages/
‚îÇ   ‚îî‚îÄ‚îÄ services/
‚îî‚îÄ‚îÄ tests/

# [REMOVE IF UNUSED] Option 3: Mobile + API (when "iOS/Android" detected)
api/
‚îî‚îÄ‚îÄ [same as backend above]

ios/ or android/
‚îî‚îÄ‚îÄ [platform-specific structure: feature modules, UI flows, platform tests]
```

**Structure Decision**: [Document the selected structure and reference the real
directories captured above]

## Complexity Tracking

> **Fill ONLY if Constitution Check has violations that must be justified**

| Violation | Why Needed | Simpler Alternative Rejected Because |
|-----------|------------|-------------------------------------|
| [e.g., 4th project] | [current need] | [why 3 projects insufficient] |
| [e.g., Repository pattern] | [specific problem] | [why direct DB access insufficient] |


============================================================
FILE: .specify\templates\spec-template.md
============================================================
# Feature Specification: [FEATURE NAME]

**Feature Branch**: `[###-feature-name]`  
**Created**: [DATE]  
**Status**: Draft  
**Input**: User description: "$ARGUMENTS"

## User Scenarios & Testing *(mandatory)*

<!--
  IMPORTANT: User stories should be PRIORITIZED as user journeys ordered by importance.
  Each user story/journey must be INDEPENDENTLY TESTABLE - meaning if you implement just ONE of them,
  you should still have a viable MVP (Minimum Viable Product) that delivers value.
  
  Assign priorities (P1, P2, P3, etc.) to each story, where P1 is the most critical.
  Think of each story as a standalone slice of functionality that can be:
  - Developed independently
  - Tested independently
  - Deployed independently
  - Demonstrated to users independently
-->

### User Story 1 - [Brief Title] (Priority: P1)

[Describe this user journey in plain language]

**Why this priority**: [Explain the value and why it has this priority level]

**Independent Test**: [Describe how this can be tested independently - e.g., "Can be fully tested by [specific action] and delivers [specific value]"]

**Acceptance Scenarios**:

1. **Given** [initial state], **When** [action], **Then** [expected outcome]
2. **Given** [initial state], **When** [action], **Then** [expected outcome]

---

### User Story 2 - [Brief Title] (Priority: P2)

[Describe this user journey in plain language]

**Why this priority**: [Explain the value and why it has this priority level]

**Independent Test**: [Describe how this can be tested independently]

**Acceptance Scenarios**:

1. **Given** [initial state], **When** [action], **Then** [expected outcome]

---

### User Story 3 - [Brief Title] (Priority: P3)

[Describe this user journey in plain language]

**Why this priority**: [Explain the value and why it has this priority level]

**Independent Test**: [Describe how this can be tested independently]

**Acceptance Scenarios**:

1. **Given** [initial state], **When** [action], **Then** [expected outcome]

---

[Add more user stories as needed, each with an assigned priority]

### Edge Cases

<!--
  ACTION REQUIRED: The content in this section represents placeholders.
  Fill them out with the right edge cases.
-->

- What happens when [boundary condition]?
- How does system handle [error scenario]?

## Requirements *(mandatory)*

<!--
  ACTION REQUIRED: The content in this section represents placeholders.
  Fill them out with the right functional requirements.
-->

### Functional Requirements

- **FR-001**: System MUST [specific capability, e.g., "allow users to create accounts"]
- **FR-002**: System MUST [specific capability, e.g., "validate email addresses"]  
- **FR-003**: Users MUST be able to [key interaction, e.g., "reset their password"]
- **FR-004**: System MUST [data requirement, e.g., "persist user preferences"]
- **FR-005**: System MUST [behavior, e.g., "log all security events"]

*Example of marking unclear requirements:*

- **FR-006**: System MUST authenticate users via [NEEDS CLARIFICATION: auth method not specified - email/password, SSO, OAuth?]
- **FR-007**: System MUST retain user data for [NEEDS CLARIFICATION: retention period not specified]

### Key Entities *(include if feature involves data)*

- **[Entity 1]**: [What it represents, key attributes without implementation]
- **[Entity 2]**: [What it represents, relationships to other entities]

## Success Criteria *(mandatory)*

<!--
  ACTION REQUIRED: Define measurable success criteria.
  These must be technology-agnostic and measurable.
-->

### Measurable Outcomes

- **SC-001**: [Measurable metric, e.g., "Users can complete account creation in under 2 minutes"]
- **SC-002**: [Measurable metric, e.g., "System handles 1000 concurrent users without degradation"]
- **SC-003**: [User satisfaction metric, e.g., "90% of users successfully complete primary task on first attempt"]
- **SC-004**: [Business metric, e.g., "Reduce support tickets related to [X] by 50%"]


============================================================
FILE: .specify\templates\tasks-template.md
============================================================
---

description: "Task list template for feature implementation"
---

# Tasks: [FEATURE NAME]

**Input**: Design documents from `/specs/[###-feature-name]/`
**Prerequisites**: plan.md (required), spec.md (required for user stories), research.md, data-model.md, contracts/

**Tests**: The examples below include test tasks. Tests are OPTIONAL - only include them if explicitly requested in the feature specification.

**Organization**: Tasks are grouped by user story to enable independent implementation and testing of each story.

## Format: `[ID] [P?] [Story] Description`

- **[P]**: Can run in parallel (different files, no dependencies)
- **[Story]**: Which user story this task belongs to (e.g., US1, US2, US3)
- Include exact file paths in descriptions

## Path Conventions

- **Single project**: `src/`, `tests/` at repository root
- **Web app**: `backend/src/`, `frontend/src/`
- **Mobile**: `api/src/`, `ios/src/` or `android/src/`
- Paths shown below assume single project - adjust based on plan.md structure

<!-- 
  ============================================================================
  IMPORTANT: The tasks below are SAMPLE TASKS for illustration purposes only.
  
  The /sp.tasks command MUST replace these with actual tasks based on:
  - User stories from spec.md (with their priorities P1, P2, P3...)
  - Feature requirements from plan.md
  - Entities from data-model.md
  - Endpoints from contracts/
  
  Tasks MUST be organized by user story so each story can be:
  - Implemented independently
  - Tested independently
  - Delivered as an MVP increment
  
  DO NOT keep these sample tasks in the generated tasks.md file.
  ============================================================================
-->

## Phase 1: Setup (Shared Infrastructure)

**Purpose**: Project initialization and basic structure

- [ ] T001 Create project structure per implementation plan
- [ ] T002 Initialize [language] project with [framework] dependencies
- [ ] T003 [P] Configure linting and formatting tools

---

## Phase 2: Foundational (Blocking Prerequisites)

**Purpose**: Core infrastructure that MUST be complete before ANY user story can be implemented

**‚ö†Ô∏è CRITICAL**: No user story work can begin until this phase is complete

Examples of foundational tasks (adjust based on your project):

- [ ] T004 Setup database schema and migrations framework
- [ ] T005 [P] Implement authentication/authorization framework
- [ ] T006 [P] Setup API routing and middleware structure
- [ ] T007 Create base models/entities that all stories depend on
- [ ] T008 Configure error handling and logging infrastructure
- [ ] T009 Setup environment configuration management

**Checkpoint**: Foundation ready - user story implementation can now begin in parallel

---

## Phase 3: User Story 1 - [Title] (Priority: P1) üéØ MVP

**Goal**: [Brief description of what this story delivers]

**Independent Test**: [How to verify this story works on its own]

### Tests for User Story 1 (OPTIONAL - only if tests requested) ‚ö†Ô∏è

> **NOTE: Write these tests FIRST, ensure they FAIL before implementation**

- [ ] T010 [P] [US1] Contract test for [endpoint] in tests/contract/test_[name].py
- [ ] T011 [P] [US1] Integration test for [user journey] in tests/integration/test_[name].py

### Implementation for User Story 1

- [ ] T012 [P] [US1] Create [Entity1] model in src/models/[entity1].py
- [ ] T013 [P] [US1] Create [Entity2] model in src/models/[entity2].py
- [ ] T014 [US1] Implement [Service] in src/services/[service].py (depends on T012, T013)
- [ ] T015 [US1] Implement [endpoint/feature] in src/[location]/[file].py
- [ ] T016 [US1] Add validation and error handling
- [ ] T017 [US1] Add logging for user story 1 operations

**Checkpoint**: At this point, User Story 1 should be fully functional and testable independently

---

## Phase 4: User Story 2 - [Title] (Priority: P2)

**Goal**: [Brief description of what this story delivers]

**Independent Test**: [How to verify this story works on its own]

### Tests for User Story 2 (OPTIONAL - only if tests requested) ‚ö†Ô∏è

- [ ] T018 [P] [US2] Contract test for [endpoint] in tests/contract/test_[name].py
- [ ] T019 [P] [US2] Integration test for [user journey] in tests/integration/test_[name].py

### Implementation for User Story 2

- [ ] T020 [P] [US2] Create [Entity] model in src/models/[entity].py
- [ ] T021 [US2] Implement [Service] in src/services/[service].py
- [ ] T022 [US2] Implement [endpoint/feature] in src/[location]/[file].py
- [ ] T023 [US2] Integrate with User Story 1 components (if needed)

**Checkpoint**: At this point, User Stories 1 AND 2 should both work independently

---

## Phase 5: User Story 3 - [Title] (Priority: P3)

**Goal**: [Brief description of what this story delivers]

**Independent Test**: [How to verify this story works on its own]

### Tests for User Story 3 (OPTIONAL - only if tests requested) ‚ö†Ô∏è

- [ ] T024 [P] [US3] Contract test for [endpoint] in tests/contract/test_[name].py
- [ ] T025 [P] [US3] Integration test for [user journey] in tests/integration/test_[name].py

### Implementation for User Story 3

- [ ] T026 [P] [US3] Create [Entity] model in src/models/[entity].py
- [ ] T027 [US3] Implement [Service] in src/services/[service].py
- [ ] T028 [US3] Implement [endpoint/feature] in src/[location]/[file].py

**Checkpoint**: All user stories should now be independently functional

---

[Add more user story phases as needed, following the same pattern]

---

## Phase N: Polish & Cross-Cutting Concerns

**Purpose**: Improvements that affect multiple user stories

- [ ] TXXX [P] Documentation updates in docs/
- [ ] TXXX Code cleanup and refactoring
- [ ] TXXX Performance optimization across all stories
- [ ] TXXX [P] Additional unit tests (if requested) in tests/unit/
- [ ] TXXX Security hardening
- [ ] TXXX Run quickstart.md validation

---

## Dependencies & Execution Order

### Phase Dependencies

- **Setup (Phase 1)**: No dependencies - can start immediately
- **Foundational (Phase 2)**: Depends on Setup completion - BLOCKS all user stories
- **User Stories (Phase 3+)**: All depend on Foundational phase completion
  - User stories can then proceed in parallel (if staffed)
  - Or sequentially in priority order (P1 ‚Üí P2 ‚Üí P3)
- **Polish (Final Phase)**: Depends on all desired user stories being complete

### User Story Dependencies

- **User Story 1 (P1)**: Can start after Foundational (Phase 2) - No dependencies on other stories
- **User Story 2 (P2)**: Can start after Foundational (Phase 2) - May integrate with US1 but should be independently testable
- **User Story 3 (P3)**: Can start after Foundational (Phase 2) - May integrate with US1/US2 but should be independently testable

### Within Each User Story

- Tests (if included) MUST be written and FAIL before implementation
- Models before services
- Services before endpoints
- Core implementation before integration
- Story complete before moving to next priority

### Parallel Opportunities

- All Setup tasks marked [P] can run in parallel
- All Foundational tasks marked [P] can run in parallel (within Phase 2)
- Once Foundational phase completes, all user stories can start in parallel (if team capacity allows)
- All tests for a user story marked [P] can run in parallel
- Models within a story marked [P] can run in parallel
- Different user stories can be worked on in parallel by different team members

---

## Parallel Example: User Story 1

```bash
# Launch all tests for User Story 1 together (if tests requested):
Task: "Contract test for [endpoint] in tests/contract/test_[name].py"
Task: "Integration test for [user journey] in tests/integration/test_[name].py"

# Launch all models for User Story 1 together:
Task: "Create [Entity1] model in src/models/[entity1].py"
Task: "Create [Entity2] model in src/models/[entity2].py"
```

---

## Implementation Strategy

### MVP First (User Story 1 Only)

1. Complete Phase 1: Setup
2. Complete Phase 2: Foundational (CRITICAL - blocks all stories)
3. Complete Phase 3: User Story 1
4. **STOP and VALIDATE**: Test User Story 1 independently
5. Deploy/demo if ready

### Incremental Delivery

1. Complete Setup + Foundational ‚Üí Foundation ready
2. Add User Story 1 ‚Üí Test independently ‚Üí Deploy/Demo (MVP!)
3. Add User Story 2 ‚Üí Test independently ‚Üí Deploy/Demo
4. Add User Story 3 ‚Üí Test independently ‚Üí Deploy/Demo
5. Each story adds value without breaking previous stories

### Parallel Team Strategy

With multiple developers:

1. Team completes Setup + Foundational together
2. Once Foundational is done:
   - Developer A: User Story 1
   - Developer B: User Story 2
   - Developer C: User Story 3
3. Stories complete and integrate independently

---

## Notes

- [P] tasks = different files, no dependencies
- [Story] label maps task to specific user story for traceability
- Each user story should be independently completable and testable
- Verify tests fail before implementing
- Commit after each task or logical group
- Stop at any checkpoint to validate story independently
- Avoid: vague tasks, same file conflicts, cross-story dependencies that break independence


============================================================
FILE: book\DEPLOYMENT.md
============================================================
# Physical AI & Humanoid Robotics Book

This is a Docusaurus-based textbook on Physical AI & Humanoid Robotics with integrated RAG chatbot functionality.

## Running the Application

### Prerequisites
- Node.js 16 or higher
- Python 3.11
- The RAG backend server running on `http://localhost:8000`

### Setup

1. **Start the RAG backend server**:
   ```bash
   cd ../rag_backend
   pip install -r requirements.txt
   python main.py
   ```

2. **Install and run the Docusaurus frontend**:
   ```bash
   npm install
   npm run start
   ```

### Development Notes

- The chatbot component is integrated into the layout and accessible via the floating button
- The chatbot communicates with the backend at `http://localhost:8000`
- For production deployment, ensure your hosting solution allows requests to the backend API

### Production Deployment

For production deployment, you'll need to:
1. Deploy the backend server to a public URL
2. Update the API_BASE in `src/components/ChatbotAPI.js` to point to your production backend
3. Ensure CORS settings allow requests from your frontend domain

============================================================
FILE: book\docusaurus.config.js
============================================================
// @ts-check
// Note: type annotations allow type checking and IDEs autocompletion

const lightCodeTheme = require('prism-react-renderer').themes.github;
const darkCodeTheme = require('prism-react-renderer').themes.dracula;

/** @type {import('@docusaurus/types').Config} */
const config = {
  title: 'Physical AI & Humanoid Robotics',
  tagline: 'A comprehensive guide to embodied intelligence and humanoid robotics',
  favicon: 'img/logo.svg',

  // Set the production url of your site here
  url: 'https://your-book-url.github.io',
  // Set the /<baseUrl>/ pathname under which your site is served
  // For GitHub pages deployment, it is often '/<projectName>/'
  baseUrl: '/',

  // GitHub pages deployment config.
  // If you aren't using GitHub pages, you don't need these.
  organizationName: 'your-username', // Usually your GitHub org/user name.
  projectName: 'physical-ai-humanoid-robotics', // Usually your repo name.

  onBrokenLinks: 'throw',
  onBrokenMarkdownLinks: 'warn',

  // Even if you don't use internalization, you can use this field to set useful
  // metadata like html lang. For example, if your site is Chinese, you may want
  // to replace "en" with "zh-Hans".
  i18n: {
    defaultLocale: 'en',
    locales: ['en'],
  },

  presets: [
    [
      'classic',
      /** @type {import('@docusaurus/preset-classic').Options} */
      ({
        docs: {
          sidebarPath: require.resolve('./sidebars.js'),
          // Please change this to your repo.
          // Remove this to remove the "edit this page" links.
          editUrl:
            'https://github.com/your-username/physical-ai-humanoid-robotics/tree/main/',
        },
        blog: false, // Disable blog for this book
        theme: {
          customCss: require.resolve('./src/css/custom.css'),
        },
      }),
    ],
  ],

  themes: [
    // ... Your other themes
    '@docusaurus/theme-live-codeblock',
  ],

  // Custom fields for our application
  customFields: {
    backendUrl: process.env.BACKEND_URL || 'http://localhost:8000',
  },
  
  themeConfig:
    /** @type {import('@docusaurus/preset-classic').ThemeConfig} */
    ({
      // Replace with your project's social card
      image: 'img/docusaurus-social-card.jpg',
      navbar: {
        title: 'Physical AI & Humanoid Robotics',
        logo: {
          alt: 'Physical AI Logo',
          src: 'img/logo.svg',
        },
        items: [
          {
            type: 'docSidebar',
            sidebarId: 'tutorialSidebar',
            position: 'left',
            label: 'Book Chapters',
          },
          {
            type: 'dropdown',
            label: 'Resources',
            position: 'left',
            items: [
              {
                label: 'ROS 2 Tutorials',
                to: '/docs/ros2',
              },
              {
                label: 'Gazebo Simulation',
                to: '/docs/gazebo',
              },
              {
                label: 'NVIDIA Isaac',
                to: '/docs/isaac',
              },
              {
                label: 'VLA Models',
                to: '/docs/vla',
              },
            ],
          },
          {
            type: 'dropdown',
            label: 'Parts',
            position: 'left',
            items: [
              {
                label: 'Part I: Foundations',
                to: '/docs/part-i-foundations/chapter-1-introduction/index',
              },
              {
                label: 'Part II: Perception',
                to: '/docs/part-ii-perception/chapter-4-sensor-integration/index',
              },
              {
                label: 'Part III: Motion & Control',
                to: '/docs/part-iii-motion/chapter-7-kinematics/index',
              },
              {
                label: 'Part IV: Intelligence',
                to: '/docs/part-iv-intelligence/chapter-10-reinforcement-learning/index',
              },
              {
                label: 'Part V: Integration',
                to: '/docs/part-v-integration/chapter-13-multi-robot/index',
              },
            ],
          },
          {
            href: 'https://github.com/your-username/physical-ai-humanoid-robotics',
            label: 'GitHub',
            position: 'right',
          },
          {
            href: '/docs/capstone',
            label: 'Capstone Project',
            position: 'right',
          },
        ],
      },
      footer: {
        style: 'dark',
        links: [
          {
            title: 'Book',
            items: [
              {
                label: 'Introduction',
                to: '/docs/part-i-foundations/chapter-1-introduction/index',
              },
            ],
          },
          {
            title: 'Community',
            items: [
              {
                label: 'Stack Overflow',
                href: 'https://stackoverflow.com/questions/tagged/docusaurus',
              },
              {
                label: 'Discord',
                href: 'https://discordapp.com/invite/docusaurus',
              },
            ],
          },
          {
            title: 'More',
            items: [
              {
                label: 'GitHub',
                href: 'https://github.com/your-username/physical-ai-humanoid-robotics',
              },
            ],
          },
        ],
        copyright: `Copyright ¬© ${new Date().getFullYear()} Physical AI & Humanoid Robotics Book. Built with Docusaurus.`,
      },
      prism: {
        theme: lightCodeTheme,
        darkTheme: darkCodeTheme,
        additionalLanguages: ['python', 'bash', 'json', 'yaml', 'cpp'],
      },
    }),
};

// Add configuration for the backend URL
config.customFields = {
  backendUrl: process.env.BACKEND_URL || 'http://localhost:8000',
};

module.exports = config;

============================================================
FILE: book\package.json
============================================================
{
  "name": "physical-ai-humanoid-robotics",
  "version": "0.0.0",
  "private": true,
  "scripts": {
    "docusaurus": "docusaurus",
    "start": "docusaurus start",
    "start:dev": "concurrently \"npm run start\" \"cd ../rag_backend && python main.py\"",
    "build": "docusaurus build",
    "swizzle": "docusaurus swizzle",
    "deploy": "docusaurus deploy",
    "clear": "docusaurus clear",
    "serve": "docusaurus serve",
    "write-translations": "docusaurus write-translations",
    "write-heading-ids": "docusaurus write-heading-ids"
  },
  "dependencies": {
    "@docusaurus/core": "3.1.0",
    "@docusaurus/plugin-client-redirects": "^3.9.2",
    "@docusaurus/preset-classic": "3.1.0",
    "@docusaurus/theme-live-codeblock": "^3.1.0",
    "@mdx-js/react": "^3.0.0",
    "clsx": "^2.0.0",
    "prism-react-renderer": "^2.4.1",
    "react": "^18.0.0",
    "react-dom": "^18.0.0"
  },
  "devDependencies": {
    "@docusaurus/module-type-aliases": "3.1.0",
    "@docusaurus/types": "3.1.0",
    "concurrently": "^9.2.1"
  },
  "browserslist": {
    "production": [
      ">0.5%",
      "not dead",
      "not op_mini all"
    ],
    "development": [
      "last 3 chrome version",
      "last 3 firefox version",
      "last 5 safari version"
    ]
  },
  "engines": {
    "node": ">=18.0"
  }
}


============================================================
FILE: book\RAG_IMPLEMENTATION_GUIDE.md
============================================================
# RAG Chatbot Implementation Guide

This document provides a complete guide to deploying and testing the Retrieval-Augmented Generation (RAG) chatbot for your Docusaurus book project.

## Overview

The RAG chatbot provides two main capabilities:
1. **Full Book RAG**: Answers questions using the full book content
2. **Strict Context Mode**: Answers questions only using user-selected text

## Prerequisites

Before deploying the RAG chatbot, ensure you have:

1. **OpenRouter API Key** (using Qwen model)
2. **Qdrant Cloud Account** (free tier)
3. **Python 3.8+** for the backend
4. **Node.js & npm** for the Docusaurus frontend
5. **Existing Docusaurus book project**

## Setup Instructions

### 1. Environment Configuration

1. Create a `.env` file in the `rag_backend/` directory:

```bash
cp rag_backend/.env.example rag_backend/.env
```

2. Update the values in `.env`:
   - `OPENROUTER_API_KEY`: Your OpenRouter API key
   - `QDRANT_URL`: Your Qdrant Cloud URL
   - `QDRANT_API_KEY`: Your Qdrant API key
   - `NEON_DB_URL`: Your Neon Postgres connection string (optional)

### 2. Backend Setup

1. Navigate to the rag_backend directory:
```bash
cd rag_backend
```

2. Install Python dependencies:
```bash
pip install -r requirements.txt
```

3. Set up the vector database with book content:
```bash
python ingest_docs.py
```

This will parse all markdown files in the `docs/` directory and store their embeddings in Qdrant.

4. Start the FastAPI server:
```bash
uvicorn main:app --reload --port 8000
```

The backend API will be available at `http://localhost:8000`.

### 3. Frontend Integration

The Docusaurus integration is already set up:

1. The chatbot component is automatically included on all pages via the theme override in `src/theme/Layout.js`

2. The API proxy is configured in `docusaurus.config.js` to forward requests from `/api` to the backend

3. Start your Docusaurus development server:
```bash
npm run start
```

## Testing the Implementation

### 1. Backend Testing

Test the backend endpoints directly:

- Health check: `GET http://localhost:8000/health`
- Full book RAG: `POST http://localhost:8000/chat`
- Strict context mode: `POST http://localhost:8000/chat/selected`

### 2. Frontend Testing

1. Visit your book website
2. Click the floating chat button in the bottom right
3. Test both modes:
   - Regular mode: Ask questions about the book content
   - Strict context mode: Select text on the page, enable strict mode, then ask questions

## API Endpoints

### `/chat` (POST)
- **Purpose**: Full book RAG chat
- **Request**: `{ "message": "your question", "history": [], "temperature": 0.7 }`
- **Response**: `{ "response": "answer", "sources": [...] }`

### `/chat/selected` (POST)
- **Purpose**: Strict context mode RAG chat
- **Request**: `{ "message": "your question", "selected_text": "user selected text", "history": [], "temperature": 0.7 }`
- **Response**: `{ "response": "answer", "sources": [...], "warning": "..." }`

## Deployment

### Production Deployment

1. **Backend**:
   - Deploy the FastAPI app to a cloud platform (Heroku, Railway, etc.)
   - Ensure environment variables are properly set
   - Update the `REACT_APP_BACKEND_URL` in the frontend to point to your deployed backend

2. **Frontend**:
   - Build the Docusaurus site: `npm run build`
   - Deploy to GitHub Pages, Netlify, Vercel, or similar
   - Ensure the proxy in `docusaurus.config.js` points to your deployed backend

### Environment Variables for Production

Update these in your production deployment:

**Backend:**
- `OPENROUTER_API_KEY`
- `QDRANT_URL`
- `QDRANT_API_KEY`
- `QDRANT_COLLECTION_NAME`

**Frontend:**
- `REACT_APP_BACKEND_URL` (URL of your deployed FastAPI backend)

## Troubleshooting

### Common Issues

1. **API Connection Errors**:
   - Verify backend server is running
   - Check that proxy is properly configured
   - Confirm CORS settings if deploying separately

2. **Embedding Generation Issues**:
   - Verify OpenRouter API key is valid
   - Check rate limits on OpenRouter
   - Confirm Qdrant connection details

3. **Document Parsing Issues**:
   - Ensure docs directory path is correct
   - Verify markdown files have proper structure
   - Check file permissions

### Debugging Tips

1. Check backend logs for error messages
2. Use browser developer tools to inspect API requests
3. Verify environment variables are set correctly
4. Test API endpoints directly with a tool like Postman

## Architecture

### Backend Components
- `data_ingestion/markdown_parser.py`: Parses Docusaurus docs
- `embeddings/openrouter_client.py`: Generates embeddings via OpenRouter
- `vector_db/qdrant_client.py`: Stores and retrieves embeddings
- `api/main.py`: FastAPI server with chat endpoints

### Frontend Components
- `src/components/Chatbot.js`: Main chat interface
- `src/components/Chatbot.css`: Chat UI styling
- `src/components/ChatbotAPI.js`: API communication layer
- `src/theme/Layout.js`: Theme override to include chatbot
- `docusaurus.config.js`: Proxy configuration

## Security Considerations

1. Never commit API keys to version control
2. Use environment variables for sensitive data
3. Implement rate limiting in production
4. Validate and sanitize all user inputs
5. Use HTTPS for production deployments

## Free Tier Limitations

This implementation is designed to work with free tiers:

- OpenRouter: Limited requests per minute
- Qdrant Cloud: Limited storage and requests
- Neon Postgres: Limited connections and storage

Monitor usage and upgrade as needed for high-traffic applications.

============================================================
FILE: book\README.md
============================================================
# Physical AI & Humanoid Robotics Book

A comprehensive guide to embodied intelligence and humanoid robotics with integrated AI assistant, built with Docusaurus.

## Features

- Modern, responsive design with indigo/cyan color scheme
- AI-powered chatbot assistant integrated into the interface
- Optimized for robotics and AI education
- Dark/light mode support
- Mobile-friendly layout

## About

This book provides a comprehensive, hands-on guide to embodied intelligence and humanoid robotics. It covers fundamental concepts through advanced topics like reinforcement learning, vision-language-action models, and human-robot interaction.

## Getting Started

### Prerequisites
- Node.js 18 or higher
- Python 3.11
- The RAG backend server running on `http://localhost:8000`

### Installation

```bash
# Clone the repository
git clone https://github.com/your-username/physical-ai-humanoid-robotics.git
cd physical-ai-humanoid-robotics

# Install dependencies
npm install
```

### Local Development

```bash
# Start the RAG backend server in a separate terminal
cd ../rag_backend
pip install -r requirements.txt
python main.py

# In another terminal, start the Docusaurus frontend
cd ../book
npm start

# The site will be accessible at http://localhost:3000
```

### Development with Both Servers

To run both servers simultaneously during development:
```bash
npm run start:dev
```

### Build for Production

```bash
# Build the static site
npm run build

# Serve the built site locally for testing
npm run serve
```

## Design

- **Color Scheme**: Deep indigo primary (#2C3E50) with cyan accent (#00BCD4)
- **Typography**: Inter font for clean, readable text
- **Layout**: Clean sidebar navigation with readable content width
- **Accessibility**: High contrast for readability and proper semantic structure

## AI Assistant

The book includes an integrated RAG (Retrieval-Augmented Generation) chatbot that can answer questions about the textbook content:

- Access via the floating chat button on any page
- Answers based strictly on book content
- Source attribution for all responses
- Two modes: general book queries and strict context mode

## Project Structure

```
book/
‚îú‚îÄ‚îÄ docs/                 # Book content organized by parts and chapters
‚îÇ   ‚îú‚îÄ‚îÄ part-i-foundations/
‚îÇ   ‚îú‚îÄ‚îÄ part-ii-perception/
‚îÇ   ‚îú‚îÄ‚îÄ part-iii-motion/
‚îÇ   ‚îú‚îÄ‚îÄ part-iv-intelligence/
‚îÇ   ‚îî‚îÄ‚îÄ part-v-integration/
‚îú‚îÄ‚îÄ src/                  # Custom components and styling
‚îÇ   ‚îú‚îÄ‚îÄ components/       # Chatbot and custom UI components
‚îÇ   ‚îú‚îÄ‚îÄ css/              # Custom styling (custom.css)
‚îÇ   ‚îî‚îÄ‚îÄ pages/
‚îú‚îÄ‚îÄ notebooks/            # Jupyter notebooks for interactive examples
‚îú‚îÄ‚îÄ diagrams/             # Diagram source files
‚îú‚îÄ‚îÄ assets/               # Images and other assets
‚îú‚îÄ‚îÄ docusaurus.config.js  # Docusaurus configuration
‚îú‚îÄ‚îÄ sidebars.js           # Navigation sidebar configuration
‚îî‚îÄ‚îÄ package.json          # Dependencies and scripts
```

## Customization

- Theme colors and styles are in `src/css/custom.css`
- Chatbot UI is in `src/components/Chatbot.js` and `src/components/Chatbot.css`
- Layout customization is in `src/theme/Layout.js`

## Contributing

This book is designed for learning, but contributions are welcome:

1. Fork the repository
2. Create a feature branch for your changes
3. Add your content following the established structure
4. Submit a pull request with a clear description of your changes

## License

This book is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.

============================================================
FILE: book\sidebars.js
============================================================
// @ts-check

/** @type {import('@docusaurus/plugin-content-docs').SidebarsConfig} */
const sidebars = {
  tutorialSidebar: [
    'index',
    {
      type: 'category',
      label: 'Part I: Foundations of Physical AI and Robotics',
      items: [
        {
          type: 'category',
          label: 'Chapter 1: Introduction to Physical AI & Embodied Intelligence',
          items: [
            'part-i-foundations/chapter-1-introduction/index',
          ],
        },
        {
          type: 'category',
          label: 'Chapter 2: Robot Operating System (ROS 2) Fundamentals',
          items: [
            'part-i-foundations/chapter-2-ros-fundamentals/index',
          ],
        },
        {
          type: 'category',
          label: 'Chapter 3: Robot Modeling and Simulation Fundamentals',
          items: [
            'part-i-foundations/chapter-3-robot-modeling/index',
          ],
        },
      ],
    },
    {
      type: 'category',
      label: 'Part II: Perception and Understanding',
      items: [
        {
          type: 'category',
          label: 'Chapter 4: Sensor Integration and Data Processing',
          items: [
            'part-ii-perception/chapter-4-sensor-integration/index',
          ],
        },
        {
          type: 'category',
          label: 'Chapter 5: Computer Vision for Robotics',
          items: [
            'part-ii-perception/chapter-5-computer-vision/index',
          ],
        },
        {
          type: 'category',
          label: 'Chapter 6: 3D Perception and Scene Understanding',
          items: [
            'part-ii-perception/chapter-6-3d-perception/index',
          ],
        },
      ],
    },
    {
      type: 'category',
      label: 'Part III: Motion and Control',
      items: [
        {
          type: 'category',
          label: 'Chapter 7: Kinematics and Dynamics',
          items: [
            'part-iii-motion/chapter-7-kinematics/index',
          ],
        },
        {
          type: 'category',
          label: 'Chapter 8: Locomotion and Balance Control',
          items: [
            'part-iii-motion/chapter-8-locomotion/index',
          ],
        },
        {
          type: 'category',
          label: 'Chapter 9: Motion Planning and Navigation',
          items: [
            'part-iii-motion/chapter-9-navigation/index',
          ],
        },
      ],
    },
    {
      type: 'category',
      label: 'Part IV: Intelligence and Learning',
      items: [
        {
          type: 'category',
          label: 'Chapter 10: Reinforcement Learning for Robotics',
          items: [
            'part-iv-intelligence/chapter-10-reinforcement-learning/index',
          ],
        },
        {
          type: 'category',
          label: 'Chapter 11: Imitation Learning and VLA',
          items: [
            'part-iv-intelligence/chapter-11-imitation-learning/index',
          ],
        },
        {
          type: 'category',
          label: 'Chapter 12: Human-Robot Interaction',
          items: [
            'part-iv-intelligence/chapter-12-hri/index',
          ],
        },
      ],
    },
    {
      type: 'category',
      label: 'Part V: Integration and Applications',
      items: [
        {
          type: 'category',
          label: 'Chapter 13: Multi-Robot Systems and Coordination',
          items: [
            'part-v-integration/chapter-13-multi-robot/index',
          ],
        },
        {
          type: 'category',
          label: 'Chapter 14: Real-World Deployment and Safety',
          items: [
            'part-v-integration/chapter-14-deployment/index',
          ],
        },
        {
          type: 'category',
          label: 'Chapter 15: Advanced Topics and Future Directions',
          items: [
            'part-v-integration/chapter-15-future/index',
          ],
        },
      ],
    },
    {
      type: 'category',
      label: 'VLA Book Series',
      items: [
        {
          type: 'category',
          label: 'Introduction to Physical AI & Humanoid Robotics',
          items: [
            'intro',
          ],
        },
        {
          type: 'category',
          label: 'ROS 2 Fundamentals',
          items: [
            'ros2',
          ],
        },
        {
          type: 'category',
          label: 'Robot Simulation (Gazebo & Unity)',
          items: [
            'gazebo',
          ],
        },
        {
          type: 'category',
          label: 'NVIDIA Isaac AI Brain',
          items: [
            'isaac',
          ],
        },
        {
          type: 'category',
          label: 'Vision-Language-Action (VLA)',
          items: [
            'vla',
          ],
        },
        {
          type: 'category',
          label: 'Capstone Project: Autonomous Humanoid',
          items: [
            'capstone',
          ],
        },
      ],
    },
  ],
};

module.exports = sidebars;

============================================================
FILE: book\.docusaurus\client-manifest.json
============================================================
{
  "entrypoints": [
    "main"
  ],
  "origins": {
    "237": [
      237
    ],
    "500": [
      61,
      401,
      869,
      500
    ],
    "17896441": [
      500,
      869,
      401
    ],
    "72370215": [
      622
    ],
    "main": [
      354,
      869,
      792
    ],
    "runtime~main": [
      792,
      869,
      354
    ],
    "051702c9": [
      744
    ],
    "0e384e19": [
      976
    ],
    "0e592ff3": [
      233
    ],
    "1593388b": [
      102
    ],
    "1f391b9e": [
      500,
      869,
      61
    ],
    "24be3566": [
      418
    ],
    "296d89dc": [
      908
    ],
    "4cfaa0b9": [
      111
    ],
    "5876f805": [
      720
    ],
    "5b90648f": [
      345
    ],
    "5e207448": [
      646
    ],
    "5e95c892": [
      647
    ],
    "72d9f152": [
      871
    ],
    "74dfaf6d": [
      879
    ],
    "795895a4": [
      909
    ],
    "935f2afb": [
      581
    ],
    "94b4cec2": [
      277
    ],
    "a7bd4aaa": [
      98
    ],
    "a94703ab": [
      869,
      48
    ],
    "bbc2753f": [
      415
    ],
    "c377a04b": [
      742
    ],
    "cac69714": [
      987
    ],
    "dbe85fd8": [
      435
    ],
    "e0f40a95": [
      913
    ],
    "e36a3ea7": [
      292
    ],
    "e4572869": [
      273
    ],
    "e71d16ae": [
      207
    ],
    "eeb6a8e2": [
      759
    ],
    "f3976560": [
      795
    ],
    "styles": [
      48,
      61,
      354,
      401,
      500,
      792,
      869
    ]
  },
  "assets": {
    "48": {
      "js": [
        {
          "file": "assets/js/a94703ab.77b85595.js",
          "hash": "cffe2d767dc9cf6d",
          "publicPath": "/assets/js/a94703ab.77b85595.js"
        }
      ]
    },
    "61": {
      "js": [
        {
          "file": "assets/js/1f391b9e.872f581d.js",
          "hash": "76f37ddf095cddd8",
          "publicPath": "/assets/js/1f391b9e.872f581d.js"
        }
      ]
    },
    "98": {
      "js": [
        {
          "file": "assets/js/a7bd4aaa.66fcd319.js",
          "hash": "e4b8b2ec9bd667a1",
          "publicPath": "/assets/js/a7bd4aaa.66fcd319.js"
        }
      ]
    },
    "102": {
      "js": [
        {
          "file": "assets/js/1593388b.31342f45.js",
          "hash": "a2498d7becc00ca8",
          "publicPath": "/assets/js/1593388b.31342f45.js"
        }
      ]
    },
    "111": {
      "js": [
        {
          "file": "assets/js/4cfaa0b9.634ec327.js",
          "hash": "451fad93e9217f26",
          "publicPath": "/assets/js/4cfaa0b9.634ec327.js"
        }
      ]
    },
    "207": {
      "js": [
        {
          "file": "assets/js/e71d16ae.fee037c5.js",
          "hash": "6083a50754b0f3c0",
          "publicPath": "/assets/js/e71d16ae.fee037c5.js"
        }
      ]
    },
    "233": {
      "js": [
        {
          "file": "assets/js/0e592ff3.e00a7ca4.js",
          "hash": "561994132cd2cab1",
          "publicPath": "/assets/js/0e592ff3.e00a7ca4.js"
        }
      ]
    },
    "237": {
      "js": [
        {
          "file": "assets/js/237.f82c6a07.js",
          "hash": "86b18b9c2b5a956d",
          "publicPath": "/assets/js/237.f82c6a07.js"
        }
      ]
    },
    "273": {
      "js": [
        {
          "file": "assets/js/e4572869.8fb064d7.js",
          "hash": "b7ad8e86dba95001",
          "publicPath": "/assets/js/e4572869.8fb064d7.js"
        }
      ]
    },
    "277": {
      "js": [
        {
          "file": "assets/js/94b4cec2.ff14d027.js",
          "hash": "701ad72a8f30f1e1",
          "publicPath": "/assets/js/94b4cec2.ff14d027.js"
        }
      ]
    },
    "292": {
      "js": [
        {
          "file": "assets/js/e36a3ea7.77e92d19.js",
          "hash": "5d39ba1e28885d58",
          "publicPath": "/assets/js/e36a3ea7.77e92d19.js"
        }
      ]
    },
    "345": {
      "js": [
        {
          "file": "assets/js/5b90648f.f90bb032.js",
          "hash": "82877c8445ad6639",
          "publicPath": "/assets/js/5b90648f.f90bb032.js"
        }
      ]
    },
    "354": {
      "js": [
        {
          "file": "assets/js/runtime~main.0025c4d0.js",
          "hash": "227b1afd86eb45d2",
          "publicPath": "/assets/js/runtime~main.0025c4d0.js"
        }
      ]
    },
    "401": {
      "js": [
        {
          "file": "assets/js/17896441.782e00ff.js",
          "hash": "ebb50ffab6fda0ba",
          "publicPath": "/assets/js/17896441.782e00ff.js"
        }
      ]
    },
    "415": {
      "js": [
        {
          "file": "assets/js/bbc2753f.d1aee7be.js",
          "hash": "06901e6b0c868456",
          "publicPath": "/assets/js/bbc2753f.d1aee7be.js"
        }
      ]
    },
    "418": {
      "js": [
        {
          "file": "assets/js/24be3566.6826fb33.js",
          "hash": "5ac1e601a428ec5d",
          "publicPath": "/assets/js/24be3566.6826fb33.js"
        }
      ]
    },
    "435": {
      "js": [
        {
          "file": "assets/js/dbe85fd8.f8874c3c.js",
          "hash": "1efd891bb044b18a",
          "publicPath": "/assets/js/dbe85fd8.f8874c3c.js"
        }
      ]
    },
    "500": {
      "js": [
        {
          "file": "assets/js/500.236edb1c.js",
          "hash": "762e8d4b6efd53d6",
          "publicPath": "/assets/js/500.236edb1c.js"
        }
      ]
    },
    "581": {
      "js": [
        {
          "file": "assets/js/935f2afb.c72b49e1.js",
          "hash": "0b065b45f591976f",
          "publicPath": "/assets/js/935f2afb.c72b49e1.js"
        }
      ]
    },
    "622": {
      "js": [
        {
          "file": "assets/js/72370215.ab37ccf2.js",
          "hash": "db6ad688c86d7aeb",
          "publicPath": "/assets/js/72370215.ab37ccf2.js"
        }
      ]
    },
    "646": {
      "js": [
        {
          "file": "assets/js/5e207448.cf7ef115.js",
          "hash": "6797b87a258218c3",
          "publicPath": "/assets/js/5e207448.cf7ef115.js"
        }
      ]
    },
    "647": {
      "js": [
        {
          "file": "assets/js/5e95c892.4b9ed891.js",
          "hash": "9f15c90c7b653e9f",
          "publicPath": "/assets/js/5e95c892.4b9ed891.js"
        }
      ]
    },
    "720": {
      "js": [
        {
          "file": "assets/js/5876f805.98c19ba8.js",
          "hash": "827141fbe92b7c9a",
          "publicPath": "/assets/js/5876f805.98c19ba8.js"
        }
      ]
    },
    "742": {
      "js": [
        {
          "file": "assets/js/c377a04b.7fc5b5e8.js",
          "hash": "e5aaf74613ac5c96",
          "publicPath": "/assets/js/c377a04b.7fc5b5e8.js"
        }
      ]
    },
    "744": {
      "js": [
        {
          "file": "assets/js/051702c9.2a713ce0.js",
          "hash": "07553e8a5a5d3a2d",
          "publicPath": "/assets/js/051702c9.2a713ce0.js"
        }
      ]
    },
    "759": {
      "js": [
        {
          "file": "assets/js/eeb6a8e2.37724f18.js",
          "hash": "8ca6457ac79d576e",
          "publicPath": "/assets/js/eeb6a8e2.37724f18.js"
        }
      ]
    },
    "792": {
      "js": [
        {
          "file": "assets/js/main.e63580bf.js",
          "hash": "9254c2747f127da9",
          "publicPath": "/assets/js/main.e63580bf.js"
        }
      ]
    },
    "795": {
      "js": [
        {
          "file": "assets/js/f3976560.73f972cf.js",
          "hash": "ceccf29711b6b823",
          "publicPath": "/assets/js/f3976560.73f972cf.js"
        }
      ]
    },
    "869": {
      "css": [
        {
          "file": "assets/css/styles.d3de9873.css",
          "hash": "1661239572a8f299",
          "publicPath": "/assets/css/styles.d3de9873.css"
        }
      ]
    },
    "871": {
      "js": [
        {
          "file": "assets/js/72d9f152.fa84b588.js",
          "hash": "9912b607b37aefd4",
          "publicPath": "/assets/js/72d9f152.fa84b588.js"
        }
      ]
    },
    "879": {
      "js": [
        {
          "file": "assets/js/74dfaf6d.7f28ac8f.js",
          "hash": "570c2b17223e139e",
          "publicPath": "/assets/js/74dfaf6d.7f28ac8f.js"
        }
      ]
    },
    "908": {
      "js": [
        {
          "file": "assets/js/296d89dc.5def5f92.js",
          "hash": "9306d08e50d7dc22",
          "publicPath": "/assets/js/296d89dc.5def5f92.js"
        }
      ]
    },
    "909": {
      "js": [
        {
          "file": "assets/js/795895a4.79545180.js",
          "hash": "3e1805e6a09bd052",
          "publicPath": "/assets/js/795895a4.79545180.js"
        }
      ]
    },
    "913": {
      "js": [
        {
          "file": "assets/js/e0f40a95.b6beb627.js",
          "hash": "2915f7e42e44e143",
          "publicPath": "/assets/js/e0f40a95.b6beb627.js"
        }
      ]
    },
    "976": {
      "js": [
        {
          "file": "assets/js/0e384e19.639a5b14.js",
          "hash": "782c01b33e739879",
          "publicPath": "/assets/js/0e384e19.639a5b14.js"
        }
      ]
    },
    "987": {
      "js": [
        {
          "file": "assets/js/cac69714.053ffb3e.js",
          "hash": "0f7aeb65776382db",
          "publicPath": "/assets/js/cac69714.053ffb3e.js"
        }
      ]
    }
  }
}

============================================================
FILE: book\.docusaurus\client-modules.js
============================================================
export default [
  require("C:\\Users\\Dell\\ai-book\\book\\node_modules\\infima\\dist\\css\\default\\default.css"),
  require("C:\\Users\\Dell\\ai-book\\book\\node_modules\\@docusaurus\\theme-classic\\lib\\prism-include-languages"),
  require("C:\\Users\\Dell\\ai-book\\book\\node_modules\\@docusaurus\\theme-classic\\lib\\nprogress"),
  require("C:\\Users\\Dell\\ai-book\\book\\src\\css\\custom.css"),
];


============================================================
FILE: book\.docusaurus\codeTranslations.json
============================================================
{}

============================================================
FILE: book\.docusaurus\globalData.json
============================================================
{
  "docusaurus-plugin-content-docs": {
    "default": {
      "path": "/docs",
      "versions": [
        {
          "name": "current",
          "label": "Next",
          "isLast": true,
          "path": "/docs",
          "mainDocId": "index",
          "docs": [
            {
              "id": "capstone",
              "path": "/docs/capstone",
              "sidebar": "tutorialSidebar"
            },
            {
              "id": "gazebo",
              "path": "/docs/gazebo",
              "sidebar": "tutorialSidebar"
            },
            {
              "id": "index",
              "path": "/docs/",
              "sidebar": "tutorialSidebar"
            },
            {
              "id": "intro",
              "path": "/docs/intro",
              "sidebar": "tutorialSidebar"
            },
            {
              "id": "isaac",
              "path": "/docs/isaac",
              "sidebar": "tutorialSidebar"
            },
            {
              "id": "part-i-foundations/chapter-1-introduction/index",
              "path": "/docs/part-i-foundations/chapter-1-introduction/",
              "sidebar": "tutorialSidebar"
            },
            {
              "id": "part-i-foundations/chapter-2-ros-fundamentals/index",
              "path": "/docs/part-i-foundations/chapter-2-ros-fundamentals/",
              "sidebar": "tutorialSidebar"
            },
            {
              "id": "part-i-foundations/chapter-3-robot-modeling/index",
              "path": "/docs/part-i-foundations/chapter-3-robot-modeling/",
              "sidebar": "tutorialSidebar"
            },
            {
              "id": "part-ii-perception/chapter-4-sensor-integration/index",
              "path": "/docs/part-ii-perception/chapter-4-sensor-integration/",
              "sidebar": "tutorialSidebar"
            },
            {
              "id": "part-ii-perception/chapter-5-computer-vision/index",
              "path": "/docs/part-ii-perception/chapter-5-computer-vision/",
              "sidebar": "tutorialSidebar"
            },
            {
              "id": "part-ii-perception/chapter-6-3d-perception/index",
              "path": "/docs/part-ii-perception/chapter-6-3d-perception/",
              "sidebar": "tutorialSidebar"
            },
            {
              "id": "part-iii-motion/chapter-7-kinematics/index",
              "path": "/docs/part-iii-motion/chapter-7-kinematics/",
              "sidebar": "tutorialSidebar"
            },
            {
              "id": "part-iii-motion/chapter-8-locomotion/index",
              "path": "/docs/part-iii-motion/chapter-8-locomotion/",
              "sidebar": "tutorialSidebar"
            },
            {
              "id": "part-iii-motion/chapter-9-navigation/index",
              "path": "/docs/part-iii-motion/chapter-9-navigation/",
              "sidebar": "tutorialSidebar"
            },
            {
              "id": "part-iv-intelligence/chapter-10-reinforcement-learning/index",
              "path": "/docs/part-iv-intelligence/chapter-10-reinforcement-learning/",
              "sidebar": "tutorialSidebar"
            },
            {
              "id": "part-iv-intelligence/chapter-11-imitation-learning/index",
              "path": "/docs/part-iv-intelligence/chapter-11-imitation-learning/",
              "sidebar": "tutorialSidebar"
            },
            {
              "id": "part-iv-intelligence/chapter-12-hri/index",
              "path": "/docs/part-iv-intelligence/chapter-12-hri/",
              "sidebar": "tutorialSidebar"
            },
            {
              "id": "part-v-integration/chapter-13-multi-robot/index",
              "path": "/docs/part-v-integration/chapter-13-multi-robot/",
              "sidebar": "tutorialSidebar"
            },
            {
              "id": "part-v-integration/chapter-14-deployment/index",
              "path": "/docs/part-v-integration/chapter-14-deployment/",
              "sidebar": "tutorialSidebar"
            },
            {
              "id": "part-v-integration/chapter-15-future/index",
              "path": "/docs/part-v-integration/chapter-15-future/",
              "sidebar": "tutorialSidebar"
            },
            {
              "id": "ros2",
              "path": "/docs/ros2",
              "sidebar": "tutorialSidebar"
            },
            {
              "id": "vla",
              "path": "/docs/vla",
              "sidebar": "tutorialSidebar"
            }
          ],
          "draftIds": [],
          "sidebars": {
            "tutorialSidebar": {
              "link": {
                "path": "/docs/",
                "label": "index"
              }
            }
          }
        }
      ],
      "breadcrumbs": true
    }
  }
}

============================================================
FILE: book\.docusaurus\i18n.json
============================================================
{
  "defaultLocale": "en",
  "locales": [
    "en"
  ],
  "path": "i18n",
  "currentLocale": "en",
  "localeConfigs": {
    "en": {
      "label": "English",
      "direction": "ltr",
      "htmlLang": "en",
      "calendar": "gregory",
      "path": "en"
    }
  }
}

============================================================
FILE: book\.docusaurus\registry.js
============================================================
export default {
  "__comp---site-src-pages-layout-wrapper-js-3-cf-589": [() => import(/* webpackChunkName: "__comp---site-src-pages-layout-wrapper-js-3-cf-589" */ "@site/src/pages/LayoutWrapper.js"), "@site/src/pages/LayoutWrapper.js", require.resolveWeak("@site/src/pages/LayoutWrapper.js")],
  "__comp---site-src-pages-mdx-page-js-496-5e6": [() => import(/* webpackChunkName: "__comp---site-src-pages-mdx-page-js-496-5e6" */ "@site/src/pages/mdxPage.js"), "@site/src/pages/mdxPage.js", require.resolveWeak("@site/src/pages/mdxPage.js")],
  "__comp---theme-debug-config-23-a-2ff": [() => import(/* webpackChunkName: "__comp---theme-debug-config-23-a-2ff" */ "@theme/DebugConfig"), "@theme/DebugConfig", require.resolveWeak("@theme/DebugConfig")],
  "__comp---theme-debug-contentba-8-ce7": [() => import(/* webpackChunkName: "__comp---theme-debug-contentba-8-ce7" */ "@theme/DebugContent"), "@theme/DebugContent", require.resolveWeak("@theme/DebugContent")],
  "__comp---theme-debug-global-dataede-0fa": [() => import(/* webpackChunkName: "__comp---theme-debug-global-dataede-0fa" */ "@theme/DebugGlobalData"), "@theme/DebugGlobalData", require.resolveWeak("@theme/DebugGlobalData")],
  "__comp---theme-debug-registry-679-501": [() => import(/* webpackChunkName: "__comp---theme-debug-registry-679-501" */ "@theme/DebugRegistry"), "@theme/DebugRegistry", require.resolveWeak("@theme/DebugRegistry")],
  "__comp---theme-debug-routes-946-699": [() => import(/* webpackChunkName: "__comp---theme-debug-routes-946-699" */ "@theme/DebugRoutes"), "@theme/DebugRoutes", require.resolveWeak("@theme/DebugRoutes")],
  "__comp---theme-debug-site-metadata-68-e-3d4": [() => import(/* webpackChunkName: "__comp---theme-debug-site-metadata-68-e-3d4" */ "@theme/DebugSiteMetadata"), "@theme/DebugSiteMetadata", require.resolveWeak("@theme/DebugSiteMetadata")],
  "__comp---theme-doc-item-178-a40": [() => import(/* webpackChunkName: "__comp---theme-doc-item-178-a40" */ "@theme/DocItem"), "@theme/DocItem", require.resolveWeak("@theme/DocItem")],
  "__comp---theme-doc-roota-94-67a": [() => import(/* webpackChunkName: "__comp---theme-doc-roota-94-67a" */ "@theme/DocRoot"), "@theme/DocRoot", require.resolveWeak("@theme/DocRoot")],
  "__comp---theme-doc-version-roota-7-b-5de": [() => import(/* webpackChunkName: "__comp---theme-doc-version-roota-7-b-5de" */ "@theme/DocVersionRoot"), "@theme/DocVersionRoot", require.resolveWeak("@theme/DocVersionRoot")],
  "__comp---theme-docs-root-5-e-9-0b6": [() => import(/* webpackChunkName: "__comp---theme-docs-root-5-e-9-0b6" */ "@theme/DocsRoot"), "@theme/DocsRoot", require.resolveWeak("@theme/DocsRoot")],
  "__comp---theme-mdx-page-1-f-3-b90": [() => import(/* webpackChunkName: "__comp---theme-mdx-page-1-f-3-b90" */ "@theme/MDXPage"), "@theme/MDXPage", require.resolveWeak("@theme/MDXPage")],
  "allContent---docusaurus-debug-content-246-9aa": [() => import(/* webpackChunkName: "allContent---docusaurus-debug-content-246-9aa" */ "~debug/default/docusaurus-debug-all-content-673.json"), "~debug/default/docusaurus-debug-all-content-673.json", require.resolveWeak("~debug/default/docusaurus-debug-all-content-673.json")],
  "config---layout-wrapper-5-e-9-271": [() => import(/* webpackChunkName: "config---layout-wrapper-5-e-9-271" */ "@generated/docusaurus.config"), "@generated/docusaurus.config", require.resolveWeak("@generated/docusaurus.config")],
  "content---docs-c-37-4d0": [() => import(/* webpackChunkName: "content---docs-c-37-4d0" */ "@site/docs/index.md"), "@site/docs/index.md", require.resolveWeak("@site/docs/index.md")],
  "content---docs-capstone-5-e-2-aa8": [() => import(/* webpackChunkName: "content---docs-capstone-5-e-2-aa8" */ "@site/docs/capstone.md"), "@site/docs/capstone.md", require.resolveWeak("@site/docs/capstone.md")],
  "content---docs-gazebo-723-98d": [() => import(/* webpackChunkName: "content---docs-gazebo-723-98d" */ "@site/docs/gazebo.md"), "@site/docs/gazebo.md", require.resolveWeak("@site/docs/gazebo.md")],
  "content---docs-intro-0-e-3-be1": [() => import(/* webpackChunkName: "content---docs-intro-0-e-3-be1" */ "@site/docs/intro.md"), "@site/docs/intro.md", require.resolveWeak("@site/docs/intro.md")],
  "content---docs-isaac-94-b-632": [() => import(/* webpackChunkName: "content---docs-isaac-94-b-632" */ "@site/docs/isaac.md"), "@site/docs/isaac.md", require.resolveWeak("@site/docs/isaac.md")],
  "content---docs-part-i-foundations-chapter-1-introduction-e-0-f-9c0": [() => import(/* webpackChunkName: "content---docs-part-i-foundations-chapter-1-introduction-e-0-f-9c0" */ "@site/docs/part-i-foundations/chapter-1-introduction/index.md"), "@site/docs/part-i-foundations/chapter-1-introduction/index.md", require.resolveWeak("@site/docs/part-i-foundations/chapter-1-introduction/index.md")],
  "content---docs-part-i-foundations-chapter-2-ros-fundamentals-74-d-e84": [() => import(/* webpackChunkName: "content---docs-part-i-foundations-chapter-2-ros-fundamentals-74-d-e84" */ "@site/docs/part-i-foundations/chapter-2-ros-fundamentals/index.md"), "@site/docs/part-i-foundations/chapter-2-ros-fundamentals/index.md", require.resolveWeak("@site/docs/part-i-foundations/chapter-2-ros-fundamentals/index.md")],
  "content---docs-part-i-foundations-chapter-3-robot-modeling-296-03f": [() => import(/* webpackChunkName: "content---docs-part-i-foundations-chapter-3-robot-modeling-296-03f" */ "@site/docs/part-i-foundations/chapter-3-robot-modeling/index.md"), "@site/docs/part-i-foundations/chapter-3-robot-modeling/index.md", require.resolveWeak("@site/docs/part-i-foundations/chapter-3-robot-modeling/index.md")],
  "content---docs-part-ii-perception-chapter-4-sensor-integration-e-45-0b4": [() => import(/* webpackChunkName: "content---docs-part-ii-perception-chapter-4-sensor-integration-e-45-0b4" */ "@site/docs/part-ii-perception/chapter-4-sensor-integration/index.md"), "@site/docs/part-ii-perception/chapter-4-sensor-integration/index.md", require.resolveWeak("@site/docs/part-ii-perception/chapter-4-sensor-integration/index.md")],
  "content---docs-part-ii-perception-chapter-5-computer-vision-e-71-264": [() => import(/* webpackChunkName: "content---docs-part-ii-perception-chapter-5-computer-vision-e-71-264" */ "@site/docs/part-ii-perception/chapter-5-computer-vision/index.md"), "@site/docs/part-ii-perception/chapter-5-computer-vision/index.md", require.resolveWeak("@site/docs/part-ii-perception/chapter-5-computer-vision/index.md")],
  "content---docs-part-ii-perception-chapter-6-3-d-perception-0-e-5-fce": [() => import(/* webpackChunkName: "content---docs-part-ii-perception-chapter-6-3-d-perception-0-e-5-fce" */ "@site/docs/part-ii-perception/chapter-6-3d-perception/index.md"), "@site/docs/part-ii-perception/chapter-6-3d-perception/index.md", require.resolveWeak("@site/docs/part-ii-perception/chapter-6-3d-perception/index.md")],
  "content---docs-part-iii-motion-chapter-7-kinematics-bbc-251": [() => import(/* webpackChunkName: "content---docs-part-iii-motion-chapter-7-kinematics-bbc-251" */ "@site/docs/part-iii-motion/chapter-7-kinematics/index.md"), "@site/docs/part-iii-motion/chapter-7-kinematics/index.md", require.resolveWeak("@site/docs/part-iii-motion/chapter-7-kinematics/index.md")],
  "content---docs-part-iii-motion-chapter-8-locomotion-eeb-5a0": [() => import(/* webpackChunkName: "content---docs-part-iii-motion-chapter-8-locomotion-eeb-5a0" */ "@site/docs/part-iii-motion/chapter-8-locomotion/index.md"), "@site/docs/part-iii-motion/chapter-8-locomotion/index.md", require.resolveWeak("@site/docs/part-iii-motion/chapter-8-locomotion/index.md")],
  "content---docs-part-iii-motion-chapter-9-navigation-72-d-dba": [() => import(/* webpackChunkName: "content---docs-part-iii-motion-chapter-9-navigation-72-d-dba" */ "@site/docs/part-iii-motion/chapter-9-navigation/index.md"), "@site/docs/part-iii-motion/chapter-9-navigation/index.md", require.resolveWeak("@site/docs/part-iii-motion/chapter-9-navigation/index.md")],
  "content---docs-part-iv-intelligence-chapter-10-reinforcement-learning-cac-4dc": [() => import(/* webpackChunkName: "content---docs-part-iv-intelligence-chapter-10-reinforcement-learning-cac-4dc" */ "@site/docs/part-iv-intelligence/chapter-10-reinforcement-learning/index.md"), "@site/docs/part-iv-intelligence/chapter-10-reinforcement-learning/index.md", require.resolveWeak("@site/docs/part-iv-intelligence/chapter-10-reinforcement-learning/index.md")],
  "content---docs-part-iv-intelligence-chapter-11-imitation-learning-587-734": [() => import(/* webpackChunkName: "content---docs-part-iv-intelligence-chapter-11-imitation-learning-587-734" */ "@site/docs/part-iv-intelligence/chapter-11-imitation-learning/index.md"), "@site/docs/part-iv-intelligence/chapter-11-imitation-learning/index.md", require.resolveWeak("@site/docs/part-iv-intelligence/chapter-11-imitation-learning/index.md")],
  "content---docs-part-iv-intelligence-chapter-12-hri-795-f65": [() => import(/* webpackChunkName: "content---docs-part-iv-intelligence-chapter-12-hri-795-f65" */ "@site/docs/part-iv-intelligence/chapter-12-hri/index.md"), "@site/docs/part-iv-intelligence/chapter-12-hri/index.md", require.resolveWeak("@site/docs/part-iv-intelligence/chapter-12-hri/index.md")],
  "content---docs-part-v-integration-chapter-13-multi-robot-e-36-c7f": [() => import(/* webpackChunkName: "content---docs-part-v-integration-chapter-13-multi-robot-e-36-c7f" */ "@site/docs/part-v-integration/chapter-13-multi-robot/index.md"), "@site/docs/part-v-integration/chapter-13-multi-robot/index.md", require.resolveWeak("@site/docs/part-v-integration/chapter-13-multi-robot/index.md")],
  "content---docs-part-v-integration-chapter-14-deployment-4-cf-c10": [() => import(/* webpackChunkName: "content---docs-part-v-integration-chapter-14-deployment-4-cf-c10" */ "@site/docs/part-v-integration/chapter-14-deployment/index.md"), "@site/docs/part-v-integration/chapter-14-deployment/index.md", require.resolveWeak("@site/docs/part-v-integration/chapter-14-deployment/index.md")],
  "content---docs-part-v-integration-chapter-15-future-5-b-9-dce": [() => import(/* webpackChunkName: "content---docs-part-v-integration-chapter-15-future-5-b-9-dce" */ "@site/docs/part-v-integration/chapter-15-future/index.md"), "@site/docs/part-v-integration/chapter-15-future/index.md", require.resolveWeak("@site/docs/part-v-integration/chapter-15-future/index.md")],
  "content---docs-ros-2051-b0b": [() => import(/* webpackChunkName: "content---docs-ros-2051-b0b" */ "@site/docs/ros2.md"), "@site/docs/ros2.md", require.resolveWeak("@site/docs/ros2.md")],
  "content---docs-vla-159-c92": [() => import(/* webpackChunkName: "content---docs-vla-159-c92" */ "@site/docs/vla.md"), "@site/docs/vla.md", require.resolveWeak("@site/docs/vla.md")],
  "content---f-39-267": [() => import(/* webpackChunkName: "content---f-39-267" */ "@site/src/pages/index.md"), "@site/src/pages/index.md", require.resolveWeak("@site/src/pages/index.md")],
  "plugin---docsdbe-4b3": [() => import(/* webpackChunkName: "plugin---docsdbe-4b3" */ "C:\\Users\\Dell\\ai-book\\book\\.docusaurus\\docusaurus-plugin-content-docs\\default\\plugin-route-context-module-100.json"), "C:\\Users\\Dell\\ai-book\\book\\.docusaurus\\docusaurus-plugin-content-docs\\default\\plugin-route-context-module-100.json", require.resolveWeak("C:\\Users\\Dell\\ai-book\\book\\.docusaurus\\docusaurus-plugin-content-docs\\default\\plugin-route-context-module-100.json")],
  "plugin---docusaurus-debug-856-d89": [() => import(/* webpackChunkName: "plugin---docusaurus-debug-856-d89" */ "C:\\Users\\Dell\\ai-book\\book\\.docusaurus\\docusaurus-plugin-debug\\default\\plugin-route-context-module-100.json"), "C:\\Users\\Dell\\ai-book\\book\\.docusaurus\\docusaurus-plugin-debug\\default\\plugin-route-context-module-100.json", require.resolveWeak("C:\\Users\\Dell\\ai-book\\book\\.docusaurus\\docusaurus-plugin-debug\\default\\plugin-route-context-module-100.json")],
  "plugin---layout-wrapper-24-b-514": [() => import(/* webpackChunkName: "plugin---layout-wrapper-24-b-514" */ "C:\\Users\\Dell\\ai-book\\book\\.docusaurus\\docusaurus-plugin-content-pages\\default\\plugin-route-context-module-100.json"), "C:\\Users\\Dell\\ai-book\\book\\.docusaurus\\docusaurus-plugin-content-pages\\default\\plugin-route-context-module-100.json", require.resolveWeak("C:\\Users\\Dell\\ai-book\\book\\.docusaurus\\docusaurus-plugin-content-pages\\default\\plugin-route-context-module-100.json")],
  "version---docs-935-398": [() => import(/* webpackChunkName: "version---docs-935-398" */ "~docs/default/version-current-metadata-prop-751.json"), "~docs/default/version-current-metadata-prop-751.json", require.resolveWeak("~docs/default/version-current-metadata-prop-751.json")],};


============================================================
FILE: book\.docusaurus\routes.js
============================================================
import React from 'react';
import ComponentCreator from '@docusaurus/ComponentCreator';

export default [
  {
    path: '/__docusaurus/debug',
    component: ComponentCreator('/__docusaurus/debug', '834'),
    exact: true
  },
  {
    path: '/__docusaurus/debug/config',
    component: ComponentCreator('/__docusaurus/debug/config', '9ae'),
    exact: true
  },
  {
    path: '/__docusaurus/debug/content',
    component: ComponentCreator('/__docusaurus/debug/content', '056'),
    exact: true
  },
  {
    path: '/__docusaurus/debug/globalData',
    component: ComponentCreator('/__docusaurus/debug/globalData', 'd6e'),
    exact: true
  },
  {
    path: '/__docusaurus/debug/metadata',
    component: ComponentCreator('/__docusaurus/debug/metadata', 'af8'),
    exact: true
  },
  {
    path: '/__docusaurus/debug/registry',
    component: ComponentCreator('/__docusaurus/debug/registry', '4d0'),
    exact: true
  },
  {
    path: '/__docusaurus/debug/routes',
    component: ComponentCreator('/__docusaurus/debug/routes', 'fa0'),
    exact: true
  },
  {
    path: '/LayoutWrapper',
    component: ComponentCreator('/LayoutWrapper', '42b'),
    exact: true
  },
  {
    path: '/mdxPage',
    component: ComponentCreator('/mdxPage', '2d8'),
    exact: true
  },
  {
    path: '/docs',
    component: ComponentCreator('/docs', '3b2'),
    routes: [
      {
        path: '/docs',
        component: ComponentCreator('/docs', '2fa'),
        routes: [
          {
            path: '/docs',
            component: ComponentCreator('/docs', '580'),
            routes: [
              {
                path: '/docs/',
                component: ComponentCreator('/docs/', 'a8c'),
                exact: true,
                sidebar: "tutorialSidebar"
              },
              {
                path: '/docs/capstone',
                component: ComponentCreator('/docs/capstone', 'cf6'),
                exact: true,
                sidebar: "tutorialSidebar"
              },
              {
                path: '/docs/gazebo',
                component: ComponentCreator('/docs/gazebo', 'cc5'),
                exact: true,
                sidebar: "tutorialSidebar"
              },
              {
                path: '/docs/intro',
                component: ComponentCreator('/docs/intro', 'aed'),
                exact: true,
                sidebar: "tutorialSidebar"
              },
              {
                path: '/docs/isaac',
                component: ComponentCreator('/docs/isaac', '71a'),
                exact: true,
                sidebar: "tutorialSidebar"
              },
              {
                path: '/docs/part-i-foundations/chapter-1-introduction/',
                component: ComponentCreator('/docs/part-i-foundations/chapter-1-introduction/', '3de'),
                exact: true,
                sidebar: "tutorialSidebar"
              },
              {
                path: '/docs/part-i-foundations/chapter-2-ros-fundamentals/',
                component: ComponentCreator('/docs/part-i-foundations/chapter-2-ros-fundamentals/', 'ecd'),
                exact: true,
                sidebar: "tutorialSidebar"
              },
              {
                path: '/docs/part-i-foundations/chapter-3-robot-modeling/',
                component: ComponentCreator('/docs/part-i-foundations/chapter-3-robot-modeling/', '661'),
                exact: true,
                sidebar: "tutorialSidebar"
              },
              {
                path: '/docs/part-ii-perception/chapter-4-sensor-integration/',
                component: ComponentCreator('/docs/part-ii-perception/chapter-4-sensor-integration/', 'a84'),
                exact: true,
                sidebar: "tutorialSidebar"
              },
              {
                path: '/docs/part-ii-perception/chapter-5-computer-vision/',
                component: ComponentCreator('/docs/part-ii-perception/chapter-5-computer-vision/', '3aa'),
                exact: true,
                sidebar: "tutorialSidebar"
              },
              {
                path: '/docs/part-ii-perception/chapter-6-3d-perception/',
                component: ComponentCreator('/docs/part-ii-perception/chapter-6-3d-perception/', '5b7'),
                exact: true,
                sidebar: "tutorialSidebar"
              },
              {
                path: '/docs/part-iii-motion/chapter-7-kinematics/',
                component: ComponentCreator('/docs/part-iii-motion/chapter-7-kinematics/', 'fe6'),
                exact: true,
                sidebar: "tutorialSidebar"
              },
              {
                path: '/docs/part-iii-motion/chapter-8-locomotion/',
                component: ComponentCreator('/docs/part-iii-motion/chapter-8-locomotion/', '34a'),
                exact: true,
                sidebar: "tutorialSidebar"
              },
              {
                path: '/docs/part-iii-motion/chapter-9-navigation/',
                component: ComponentCreator('/docs/part-iii-motion/chapter-9-navigation/', '875'),
                exact: true,
                sidebar: "tutorialSidebar"
              },
              {
                path: '/docs/part-iv-intelligence/chapter-10-reinforcement-learning/',
                component: ComponentCreator('/docs/part-iv-intelligence/chapter-10-reinforcement-learning/', 'f67'),
                exact: true,
                sidebar: "tutorialSidebar"
              },
              {
                path: '/docs/part-iv-intelligence/chapter-11-imitation-learning/',
                component: ComponentCreator('/docs/part-iv-intelligence/chapter-11-imitation-learning/', '626'),
                exact: true,
                sidebar: "tutorialSidebar"
              },
              {
                path: '/docs/part-iv-intelligence/chapter-12-hri/',
                component: ComponentCreator('/docs/part-iv-intelligence/chapter-12-hri/', '9e4'),
                exact: true,
                sidebar: "tutorialSidebar"
              },
              {
                path: '/docs/part-v-integration/chapter-13-multi-robot/',
                component: ComponentCreator('/docs/part-v-integration/chapter-13-multi-robot/', '10e'),
                exact: true,
                sidebar: "tutorialSidebar"
              },
              {
                path: '/docs/part-v-integration/chapter-14-deployment/',
                component: ComponentCreator('/docs/part-v-integration/chapter-14-deployment/', '092'),
                exact: true,
                sidebar: "tutorialSidebar"
              },
              {
                path: '/docs/part-v-integration/chapter-15-future/',
                component: ComponentCreator('/docs/part-v-integration/chapter-15-future/', 'c49'),
                exact: true,
                sidebar: "tutorialSidebar"
              },
              {
                path: '/docs/ros2',
                component: ComponentCreator('/docs/ros2', '7c3'),
                exact: true,
                sidebar: "tutorialSidebar"
              },
              {
                path: '/docs/vla',
                component: ComponentCreator('/docs/vla', 'f6d'),
                exact: true,
                sidebar: "tutorialSidebar"
              }
            ]
          }
        ]
      }
    ]
  },
  {
    path: '/',
    component: ComponentCreator('/', 'df1'),
    exact: true
  },
  {
    path: '*',
    component: ComponentCreator('*'),
  },
];


============================================================
FILE: book\.docusaurus\routesChunkNames.json
============================================================
{
  "/__docusaurus/debug-834": {
    "__comp": "__comp---theme-debug-config-23-a-2ff",
    "__context": {
      "plugin": "plugin---docusaurus-debug-856-d89"
    }
  },
  "/__docusaurus/debug/config-9ae": {
    "__comp": "__comp---theme-debug-config-23-a-2ff",
    "__context": {
      "plugin": "plugin---docusaurus-debug-856-d89"
    }
  },
  "/__docusaurus/debug/content-056": {
    "__comp": "__comp---theme-debug-contentba-8-ce7",
    "__context": {
      "plugin": "plugin---docusaurus-debug-856-d89"
    },
    "allContent": "allContent---docusaurus-debug-content-246-9aa"
  },
  "/__docusaurus/debug/globalData-d6e": {
    "__comp": "__comp---theme-debug-global-dataede-0fa",
    "__context": {
      "plugin": "plugin---docusaurus-debug-856-d89"
    }
  },
  "/__docusaurus/debug/metadata-af8": {
    "__comp": "__comp---theme-debug-site-metadata-68-e-3d4",
    "__context": {
      "plugin": "plugin---docusaurus-debug-856-d89"
    }
  },
  "/__docusaurus/debug/registry-4d0": {
    "__comp": "__comp---theme-debug-registry-679-501",
    "__context": {
      "plugin": "plugin---docusaurus-debug-856-d89"
    }
  },
  "/__docusaurus/debug/routes-fa0": {
    "__comp": "__comp---theme-debug-routes-946-699",
    "__context": {
      "plugin": "plugin---docusaurus-debug-856-d89"
    }
  },
  "/LayoutWrapper-42b": {
    "__comp": "__comp---site-src-pages-layout-wrapper-js-3-cf-589",
    "__context": {
      "plugin": "plugin---layout-wrapper-24-b-514"
    },
    "config": "config---layout-wrapper-5-e-9-271"
  },
  "/mdxPage-2d8": {
    "__comp": "__comp---site-src-pages-mdx-page-js-496-5e6",
    "__context": {
      "plugin": "plugin---layout-wrapper-24-b-514"
    },
    "config": "config---layout-wrapper-5-e-9-271"
  },
  "/docs-3b2": {
    "__comp": "__comp---theme-docs-root-5-e-9-0b6",
    "__context": {
      "plugin": "plugin---docsdbe-4b3"
    }
  },
  "/docs-2fa": {
    "__comp": "__comp---theme-doc-version-roota-7-b-5de",
    "version": "version---docs-935-398"
  },
  "/docs-580": {
    "__comp": "__comp---theme-doc-roota-94-67a"
  },
  "/docs/-a8c": {
    "__comp": "__comp---theme-doc-item-178-a40",
    "content": "content---docs-c-37-4d0"
  },
  "/docs/capstone-cf6": {
    "__comp": "__comp---theme-doc-item-178-a40",
    "content": "content---docs-capstone-5-e-2-aa8"
  },
  "/docs/gazebo-cc5": {
    "__comp": "__comp---theme-doc-item-178-a40",
    "content": "content---docs-gazebo-723-98d"
  },
  "/docs/intro-aed": {
    "__comp": "__comp---theme-doc-item-178-a40",
    "content": "content---docs-intro-0-e-3-be1"
  },
  "/docs/isaac-71a": {
    "__comp": "__comp---theme-doc-item-178-a40",
    "content": "content---docs-isaac-94-b-632"
  },
  "/docs/part-i-foundations/chapter-1-introduction/-3de": {
    "__comp": "__comp---theme-doc-item-178-a40",
    "content": "content---docs-part-i-foundations-chapter-1-introduction-e-0-f-9c0"
  },
  "/docs/part-i-foundations/chapter-2-ros-fundamentals/-ecd": {
    "__comp": "__comp---theme-doc-item-178-a40",
    "content": "content---docs-part-i-foundations-chapter-2-ros-fundamentals-74-d-e84"
  },
  "/docs/part-i-foundations/chapter-3-robot-modeling/-661": {
    "__comp": "__comp---theme-doc-item-178-a40",
    "content": "content---docs-part-i-foundations-chapter-3-robot-modeling-296-03f"
  },
  "/docs/part-ii-perception/chapter-4-sensor-integration/-a84": {
    "__comp": "__comp---theme-doc-item-178-a40",
    "content": "content---docs-part-ii-perception-chapter-4-sensor-integration-e-45-0b4"
  },
  "/docs/part-ii-perception/chapter-5-computer-vision/-3aa": {
    "__comp": "__comp---theme-doc-item-178-a40",
    "content": "content---docs-part-ii-perception-chapter-5-computer-vision-e-71-264"
  },
  "/docs/part-ii-perception/chapter-6-3d-perception/-5b7": {
    "__comp": "__comp---theme-doc-item-178-a40",
    "content": "content---docs-part-ii-perception-chapter-6-3-d-perception-0-e-5-fce"
  },
  "/docs/part-iii-motion/chapter-7-kinematics/-fe6": {
    "__comp": "__comp---theme-doc-item-178-a40",
    "content": "content---docs-part-iii-motion-chapter-7-kinematics-bbc-251"
  },
  "/docs/part-iii-motion/chapter-8-locomotion/-34a": {
    "__comp": "__comp---theme-doc-item-178-a40",
    "content": "content---docs-part-iii-motion-chapter-8-locomotion-eeb-5a0"
  },
  "/docs/part-iii-motion/chapter-9-navigation/-875": {
    "__comp": "__comp---theme-doc-item-178-a40",
    "content": "content---docs-part-iii-motion-chapter-9-navigation-72-d-dba"
  },
  "/docs/part-iv-intelligence/chapter-10-reinforcement-learning/-f67": {
    "__comp": "__comp---theme-doc-item-178-a40",
    "content": "content---docs-part-iv-intelligence-chapter-10-reinforcement-learning-cac-4dc"
  },
  "/docs/part-iv-intelligence/chapter-11-imitation-learning/-626": {
    "__comp": "__comp---theme-doc-item-178-a40",
    "content": "content---docs-part-iv-intelligence-chapter-11-imitation-learning-587-734"
  },
  "/docs/part-iv-intelligence/chapter-12-hri/-9e4": {
    "__comp": "__comp---theme-doc-item-178-a40",
    "content": "content---docs-part-iv-intelligence-chapter-12-hri-795-f65"
  },
  "/docs/part-v-integration/chapter-13-multi-robot/-10e": {
    "__comp": "__comp---theme-doc-item-178-a40",
    "content": "content---docs-part-v-integration-chapter-13-multi-robot-e-36-c7f"
  },
  "/docs/part-v-integration/chapter-14-deployment/-092": {
    "__comp": "__comp---theme-doc-item-178-a40",
    "content": "content---docs-part-v-integration-chapter-14-deployment-4-cf-c10"
  },
  "/docs/part-v-integration/chapter-15-future/-c49": {
    "__comp": "__comp---theme-doc-item-178-a40",
    "content": "content---docs-part-v-integration-chapter-15-future-5-b-9-dce"
  },
  "/docs/ros2-7c3": {
    "__comp": "__comp---theme-doc-item-178-a40",
    "content": "content---docs-ros-2051-b0b"
  },
  "/docs/vla-f6d": {
    "__comp": "__comp---theme-doc-item-178-a40",
    "content": "content---docs-vla-159-c92"
  },
  "/-df1": {
    "__comp": "__comp---theme-mdx-page-1-f-3-b90",
    "__context": {
      "plugin": "plugin---layout-wrapper-24-b-514"
    },
    "content": "content---f-39-267"
  }
}

============================================================
FILE: book\.docusaurus\site-metadata.json
============================================================
{
  "docusaurusVersion": "3.1.0",
  "siteVersion": "0.0.0",
  "pluginVersions": {
    "docusaurus-plugin-content-docs": {
      "type": "package",
      "name": "@docusaurus/plugin-content-docs",
      "version": "3.1.0"
    },
    "docusaurus-plugin-content-pages": {
      "type": "package",
      "name": "@docusaurus/plugin-content-pages",
      "version": "3.1.0"
    },
    "docusaurus-plugin-debug": {
      "type": "package",
      "name": "@docusaurus/plugin-debug",
      "version": "3.1.0"
    },
    "docusaurus-theme-classic": {
      "type": "package",
      "name": "@docusaurus/theme-classic",
      "version": "3.1.0"
    },
    "docusaurus-theme-live-codeblock": {
      "type": "package",
      "name": "@docusaurus/theme-live-codeblock",
      "version": "3.1.0"
    }
  }
}

============================================================
FILE: book\.docusaurus\docusaurus-plugin-content-docs\default\plugin-route-context-module-100.json
============================================================
{
  "name": "docusaurus-plugin-content-docs",
  "id": "default"
}

============================================================
FILE: book\.docusaurus\docusaurus-plugin-content-docs\default\site-docs-capstone-md-5e2.json
============================================================
{
  "id": "capstone",
  "title": "Capstone Project: Autonomous Humanoid",
  "description": "Weekly Plan",
  "source": "@site/docs/capstone.md",
  "sourceDirName": ".",
  "slug": "/capstone",
  "permalink": "/docs/capstone",
  "draft": false,
  "unlisted": false,
  "editUrl": "https://github.com/your-username/physical-ai-humanoid-robotics/tree/main/docs/capstone.md",
  "tags": [],
  "version": "current",
  "sidebarPosition": 6,
  "frontMatter": {
    "sidebar_position": 6,
    "title": "Capstone Project: Autonomous Humanoid"
  },
  "sidebar": "tutorialSidebar",
  "previous": {
    "title": "Vision-Language-Action (VLA)",
    "permalink": "/docs/vla"
  }
}

============================================================
FILE: book\.docusaurus\docusaurus-plugin-content-docs\default\site-docs-gazebo-md-723.json
============================================================
{
  "id": "gazebo",
  "title": "Robot Simulation (Gazebo & Unity)",
  "description": "Weekly Plan",
  "source": "@site/docs/gazebo.md",
  "sourceDirName": ".",
  "slug": "/gazebo",
  "permalink": "/docs/gazebo",
  "draft": false,
  "unlisted": false,
  "editUrl": "https://github.com/your-username/physical-ai-humanoid-robotics/tree/main/docs/gazebo.md",
  "tags": [],
  "version": "current",
  "sidebarPosition": 3,
  "frontMatter": {
    "sidebar_position": 3,
    "title": "Robot Simulation (Gazebo & Unity)"
  },
  "sidebar": "tutorialSidebar",
  "previous": {
    "title": "ROS 2 Fundamentals",
    "permalink": "/docs/ros2"
  },
  "next": {
    "title": "NVIDIA Isaac AI Brain",
    "permalink": "/docs/isaac"
  }
}

============================================================
FILE: book\.docusaurus\docusaurus-plugin-content-docs\default\site-docs-index-md-c37.json
============================================================
{
  "id": "index",
  "title": "Physical AI & Humanoid Robotics",
  "description": "Welcome to the comprehensive guide on Physical AI and Humanoid Robotics. This book covers everything from fundamental concepts to advanced implementations using modern robotics technologies.",
  "source": "@site/docs/index.md",
  "sourceDirName": ".",
  "slug": "/",
  "permalink": "/docs/",
  "draft": false,
  "unlisted": false,
  "editUrl": "https://github.com/your-username/physical-ai-humanoid-robotics/tree/main/docs/index.md",
  "tags": [],
  "version": "current",
  "sidebarPosition": 0,
  "frontMatter": {
    "sidebar_position": 0,
    "title": "Physical AI & Humanoid Robotics"
  },
  "sidebar": "tutorialSidebar",
  "next": {
    "title": "Chapter 1 - Introduction to Physical AI & Embodied Intelligence",
    "permalink": "/docs/part-i-foundations/chapter-1-introduction/"
  }
}

============================================================
FILE: book\.docusaurus\docusaurus-plugin-content-docs\default\site-docs-intro-md-0e3.json
============================================================
{
  "id": "intro",
  "title": "Introduction to Physical AI & Humanoid Robotics",
  "description": "Weekly Plan",
  "source": "@site/docs/intro.md",
  "sourceDirName": ".",
  "slug": "/intro",
  "permalink": "/docs/intro",
  "draft": false,
  "unlisted": false,
  "editUrl": "https://github.com/your-username/physical-ai-humanoid-robotics/tree/main/docs/intro.md",
  "tags": [],
  "version": "current",
  "sidebarPosition": 1,
  "frontMatter": {
    "sidebar_position": 1,
    "title": "Introduction to Physical AI & Humanoid Robotics"
  },
  "sidebar": "tutorialSidebar",
  "previous": {
    "title": "Chapter 15: Advanced Topics and Future Directions",
    "permalink": "/docs/part-v-integration/chapter-15-future/"
  },
  "next": {
    "title": "ROS 2 Fundamentals",
    "permalink": "/docs/ros2"
  }
}

============================================================
FILE: book\.docusaurus\docusaurus-plugin-content-docs\default\site-docs-isaac-md-94b.json
============================================================
{
  "id": "isaac",
  "title": "NVIDIA Isaac AI Brain",
  "description": "Weekly Plan",
  "source": "@site/docs/isaac.md",
  "sourceDirName": ".",
  "slug": "/isaac",
  "permalink": "/docs/isaac",
  "draft": false,
  "unlisted": false,
  "editUrl": "https://github.com/your-username/physical-ai-humanoid-robotics/tree/main/docs/isaac.md",
  "tags": [],
  "version": "current",
  "sidebarPosition": 4,
  "frontMatter": {
    "sidebar_position": 4,
    "title": "NVIDIA Isaac AI Brain"
  },
  "sidebar": "tutorialSidebar",
  "previous": {
    "title": "Robot Simulation (Gazebo & Unity)",
    "permalink": "/docs/gazebo"
  },
  "next": {
    "title": "Vision-Language-Action (VLA)",
    "permalink": "/docs/vla"
  }
}

============================================================
FILE: book\.docusaurus\docusaurus-plugin-content-docs\default\site-docs-part-i-foundations-chapter-1-introduction-index-md-e0f.json
============================================================
{
  "id": "part-i-foundations/chapter-1-introduction/index",
  "title": "Chapter 1 - Introduction to Physical AI & Embodied Intelligence",
  "description": "Learning Goals",
  "source": "@site/docs/part-i-foundations/chapter-1-introduction/index.md",
  "sourceDirName": "part-i-foundations/chapter-1-introduction",
  "slug": "/part-i-foundations/chapter-1-introduction/",
  "permalink": "/docs/part-i-foundations/chapter-1-introduction/",
  "draft": false,
  "unlisted": false,
  "editUrl": "https://github.com/your-username/physical-ai-humanoid-robotics/tree/main/docs/part-i-foundations/chapter-1-introduction/index.md",
  "tags": [],
  "version": "current",
  "sidebarPosition": 1,
  "frontMatter": {
    "title": "Chapter 1 - Introduction to Physical AI & Embodied Intelligence",
    "sidebar_position": 1
  },
  "sidebar": "tutorialSidebar",
  "previous": {
    "title": "Physical AI & Humanoid Robotics",
    "permalink": "/docs/"
  },
  "next": {
    "title": "Chapter 2 - Robot Operating System (ROS 2) Fundamentals",
    "permalink": "/docs/part-i-foundations/chapter-2-ros-fundamentals/"
  }
}

============================================================
FILE: book\.docusaurus\docusaurus-plugin-content-docs\default\site-docs-part-i-foundations-chapter-2-ros-fundamentals-index-md-74d.json
============================================================
{
  "id": "part-i-foundations/chapter-2-ros-fundamentals/index",
  "title": "Chapter 2 - Robot Operating System (ROS 2) Fundamentals",
  "description": "Learning Goals",
  "source": "@site/docs/part-i-foundations/chapter-2-ros-fundamentals/index.md",
  "sourceDirName": "part-i-foundations/chapter-2-ros-fundamentals",
  "slug": "/part-i-foundations/chapter-2-ros-fundamentals/",
  "permalink": "/docs/part-i-foundations/chapter-2-ros-fundamentals/",
  "draft": false,
  "unlisted": false,
  "editUrl": "https://github.com/your-username/physical-ai-humanoid-robotics/tree/main/docs/part-i-foundations/chapter-2-ros-fundamentals/index.md",
  "tags": [],
  "version": "current",
  "sidebarPosition": 2,
  "frontMatter": {
    "title": "Chapter 2 - Robot Operating System (ROS 2) Fundamentals",
    "sidebar_position": 2
  },
  "sidebar": "tutorialSidebar",
  "previous": {
    "title": "Chapter 1 - Introduction to Physical AI & Embodied Intelligence",
    "permalink": "/docs/part-i-foundations/chapter-1-introduction/"
  },
  "next": {
    "title": "Chapter 3 - Robot Modeling and Simulation Fundamentals",
    "permalink": "/docs/part-i-foundations/chapter-3-robot-modeling/"
  }
}

============================================================
FILE: book\.docusaurus\docusaurus-plugin-content-docs\default\site-docs-part-i-foundations-chapter-3-robot-modeling-index-md-296.json
============================================================
{
  "id": "part-i-foundations/chapter-3-robot-modeling/index",
  "title": "Chapter 3 - Robot Modeling and Simulation Fundamentals",
  "description": "Learning Goals",
  "source": "@site/docs/part-i-foundations/chapter-3-robot-modeling/index.md",
  "sourceDirName": "part-i-foundations/chapter-3-robot-modeling",
  "slug": "/part-i-foundations/chapter-3-robot-modeling/",
  "permalink": "/docs/part-i-foundations/chapter-3-robot-modeling/",
  "draft": false,
  "unlisted": false,
  "editUrl": "https://github.com/your-username/physical-ai-humanoid-robotics/tree/main/docs/part-i-foundations/chapter-3-robot-modeling/index.md",
  "tags": [],
  "version": "current",
  "sidebarPosition": 3,
  "frontMatter": {
    "title": "Chapter 3 - Robot Modeling and Simulation Fundamentals",
    "sidebar_position": 3
  },
  "sidebar": "tutorialSidebar",
  "previous": {
    "title": "Chapter 2 - Robot Operating System (ROS 2) Fundamentals",
    "permalink": "/docs/part-i-foundations/chapter-2-ros-fundamentals/"
  },
  "next": {
    "title": "Chapter 4 - Sensor Integration and Data Processing",
    "permalink": "/docs/part-ii-perception/chapter-4-sensor-integration/"
  }
}

============================================================
FILE: book\.docusaurus\docusaurus-plugin-content-docs\default\site-docs-part-ii-perception-chapter-4-sensor-integration-index-md-e45.json
============================================================
{
  "id": "part-ii-perception/chapter-4-sensor-integration/index",
  "title": "Chapter 4 - Sensor Integration and Data Processing",
  "description": "Learning Goals",
  "source": "@site/docs/part-ii-perception/chapter-4-sensor-integration/index.md",
  "sourceDirName": "part-ii-perception/chapter-4-sensor-integration",
  "slug": "/part-ii-perception/chapter-4-sensor-integration/",
  "permalink": "/docs/part-ii-perception/chapter-4-sensor-integration/",
  "draft": false,
  "unlisted": false,
  "editUrl": "https://github.com/your-username/physical-ai-humanoid-robotics/tree/main/docs/part-ii-perception/chapter-4-sensor-integration/index.md",
  "tags": [],
  "version": "current",
  "sidebarPosition": 1,
  "frontMatter": {
    "title": "Chapter 4 - Sensor Integration and Data Processing",
    "sidebar_position": 1
  },
  "sidebar": "tutorialSidebar",
  "previous": {
    "title": "Chapter 3 - Robot Modeling and Simulation Fundamentals",
    "permalink": "/docs/part-i-foundations/chapter-3-robot-modeling/"
  },
  "next": {
    "title": "Chapter 5 - Computer Vision for Robotics",
    "permalink": "/docs/part-ii-perception/chapter-5-computer-vision/"
  }
}

============================================================
FILE: book\.docusaurus\docusaurus-plugin-content-docs\default\site-docs-part-ii-perception-chapter-5-computer-vision-index-md-e71.json
============================================================
{
  "id": "part-ii-perception/chapter-5-computer-vision/index",
  "title": "Chapter 5 - Computer Vision for Robotics",
  "description": "Learning Goals",
  "source": "@site/docs/part-ii-perception/chapter-5-computer-vision/index.md",
  "sourceDirName": "part-ii-perception/chapter-5-computer-vision",
  "slug": "/part-ii-perception/chapter-5-computer-vision/",
  "permalink": "/docs/part-ii-perception/chapter-5-computer-vision/",
  "draft": false,
  "unlisted": false,
  "editUrl": "https://github.com/your-username/physical-ai-humanoid-robotics/tree/main/docs/part-ii-perception/chapter-5-computer-vision/index.md",
  "tags": [],
  "version": "current",
  "sidebarPosition": 2,
  "frontMatter": {
    "title": "Chapter 5 - Computer Vision for Robotics",
    "sidebar_position": 2
  },
  "sidebar": "tutorialSidebar",
  "previous": {
    "title": "Chapter 4 - Sensor Integration and Data Processing",
    "permalink": "/docs/part-ii-perception/chapter-4-sensor-integration/"
  },
  "next": {
    "title": "Chapter 6 - 3D Perception and Scene Understanding",
    "permalink": "/docs/part-ii-perception/chapter-6-3d-perception/"
  }
}

============================================================
FILE: book\.docusaurus\docusaurus-plugin-content-docs\default\site-docs-part-ii-perception-chapter-6-3-d-perception-index-md-0e5.json
============================================================
{
  "id": "part-ii-perception/chapter-6-3d-perception/index",
  "title": "Chapter 6 - 3D Perception and Scene Understanding",
  "description": "Learning Goals",
  "source": "@site/docs/part-ii-perception/chapter-6-3d-perception/index.md",
  "sourceDirName": "part-ii-perception/chapter-6-3d-perception",
  "slug": "/part-ii-perception/chapter-6-3d-perception/",
  "permalink": "/docs/part-ii-perception/chapter-6-3d-perception/",
  "draft": false,
  "unlisted": false,
  "editUrl": "https://github.com/your-username/physical-ai-humanoid-robotics/tree/main/docs/part-ii-perception/chapter-6-3d-perception/index.md",
  "tags": [],
  "version": "current",
  "sidebarPosition": 3,
  "frontMatter": {
    "title": "Chapter 6 - 3D Perception and Scene Understanding",
    "sidebar_position": 3
  },
  "sidebar": "tutorialSidebar",
  "previous": {
    "title": "Chapter 5 - Computer Vision for Robotics",
    "permalink": "/docs/part-ii-perception/chapter-5-computer-vision/"
  },
  "next": {
    "title": "Chapter 7 - Kinematics and Dynamics",
    "permalink": "/docs/part-iii-motion/chapter-7-kinematics/"
  }
}

============================================================
FILE: book\.docusaurus\docusaurus-plugin-content-docs\default\site-docs-part-iii-motion-chapter-7-kinematics-index-md-bbc.json
============================================================
{
  "id": "part-iii-motion/chapter-7-kinematics/index",
  "title": "Chapter 7 - Kinematics and Dynamics",
  "description": "Learning Goals",
  "source": "@site/docs/part-iii-motion/chapter-7-kinematics/index.md",
  "sourceDirName": "part-iii-motion/chapter-7-kinematics",
  "slug": "/part-iii-motion/chapter-7-kinematics/",
  "permalink": "/docs/part-iii-motion/chapter-7-kinematics/",
  "draft": false,
  "unlisted": false,
  "editUrl": "https://github.com/your-username/physical-ai-humanoid-robotics/tree/main/docs/part-iii-motion/chapter-7-kinematics/index.md",
  "tags": [],
  "version": "current",
  "sidebarPosition": 1,
  "frontMatter": {
    "title": "Chapter 7 - Kinematics and Dynamics",
    "sidebar_position": 1
  },
  "sidebar": "tutorialSidebar",
  "previous": {
    "title": "Chapter 6 - 3D Perception and Scene Understanding",
    "permalink": "/docs/part-ii-perception/chapter-6-3d-perception/"
  },
  "next": {
    "title": "Chapter 8 - Locomotion and Balance Control",
    "permalink": "/docs/part-iii-motion/chapter-8-locomotion/"
  }
}

============================================================
FILE: book\.docusaurus\docusaurus-plugin-content-docs\default\site-docs-part-iii-motion-chapter-8-locomotion-index-md-eeb.json
============================================================
{
  "id": "part-iii-motion/chapter-8-locomotion/index",
  "title": "Chapter 8 - Locomotion and Balance Control",
  "description": "Learning Goals",
  "source": "@site/docs/part-iii-motion/chapter-8-locomotion/index.md",
  "sourceDirName": "part-iii-motion/chapter-8-locomotion",
  "slug": "/part-iii-motion/chapter-8-locomotion/",
  "permalink": "/docs/part-iii-motion/chapter-8-locomotion/",
  "draft": false,
  "unlisted": false,
  "editUrl": "https://github.com/your-username/physical-ai-humanoid-robotics/tree/main/docs/part-iii-motion/chapter-8-locomotion/index.md",
  "tags": [],
  "version": "current",
  "sidebarPosition": 2,
  "frontMatter": {
    "title": "Chapter 8 - Locomotion and Balance Control",
    "sidebar_position": 2
  },
  "sidebar": "tutorialSidebar",
  "previous": {
    "title": "Chapter 7 - Kinematics and Dynamics",
    "permalink": "/docs/part-iii-motion/chapter-7-kinematics/"
  },
  "next": {
    "title": "Chapter 9 - Motion Planning and Navigation",
    "permalink": "/docs/part-iii-motion/chapter-9-navigation/"
  }
}

============================================================
FILE: book\.docusaurus\docusaurus-plugin-content-docs\default\site-docs-part-iii-motion-chapter-9-navigation-index-md-72d.json
============================================================
{
  "id": "part-iii-motion/chapter-9-navigation/index",
  "title": "Chapter 9 - Motion Planning and Navigation",
  "description": "Learning Goals",
  "source": "@site/docs/part-iii-motion/chapter-9-navigation/index.md",
  "sourceDirName": "part-iii-motion/chapter-9-navigation",
  "slug": "/part-iii-motion/chapter-9-navigation/",
  "permalink": "/docs/part-iii-motion/chapter-9-navigation/",
  "draft": false,
  "unlisted": false,
  "editUrl": "https://github.com/your-username/physical-ai-humanoid-robotics/tree/main/docs/part-iii-motion/chapter-9-navigation/index.md",
  "tags": [],
  "version": "current",
  "sidebarPosition": 3,
  "frontMatter": {
    "title": "Chapter 9 - Motion Planning and Navigation",
    "sidebar_position": 3
  },
  "sidebar": "tutorialSidebar",
  "previous": {
    "title": "Chapter 8 - Locomotion and Balance Control",
    "permalink": "/docs/part-iii-motion/chapter-8-locomotion/"
  },
  "next": {
    "title": "Chapter 10 - Reinforcement Learning for Robotics",
    "permalink": "/docs/part-iv-intelligence/chapter-10-reinforcement-learning/"
  }
}

============================================================
FILE: book\.docusaurus\docusaurus-plugin-content-docs\default\site-docs-part-iv-intelligence-chapter-10-reinforcement-learning-index-md-cac.json
============================================================
{
  "id": "part-iv-intelligence/chapter-10-reinforcement-learning/index",
  "title": "Chapter 10 - Reinforcement Learning for Robotics",
  "description": "Learning Goals",
  "source": "@site/docs/part-iv-intelligence/chapter-10-reinforcement-learning/index.md",
  "sourceDirName": "part-iv-intelligence/chapter-10-reinforcement-learning",
  "slug": "/part-iv-intelligence/chapter-10-reinforcement-learning/",
  "permalink": "/docs/part-iv-intelligence/chapter-10-reinforcement-learning/",
  "draft": false,
  "unlisted": false,
  "editUrl": "https://github.com/your-username/physical-ai-humanoid-robotics/tree/main/docs/part-iv-intelligence/chapter-10-reinforcement-learning/index.md",
  "tags": [],
  "version": "current",
  "sidebarPosition": 1,
  "frontMatter": {
    "title": "Chapter 10 - Reinforcement Learning for Robotics",
    "sidebar_position": 1
  },
  "sidebar": "tutorialSidebar",
  "previous": {
    "title": "Chapter 9 - Motion Planning and Navigation",
    "permalink": "/docs/part-iii-motion/chapter-9-navigation/"
  },
  "next": {
    "title": "Chapter 11 - Imitation Learning and VLA",
    "permalink": "/docs/part-iv-intelligence/chapter-11-imitation-learning/"
  }
}

============================================================
FILE: book\.docusaurus\docusaurus-plugin-content-docs\default\site-docs-part-iv-intelligence-chapter-11-imitation-learning-index-md-587.json
============================================================
{
  "id": "part-iv-intelligence/chapter-11-imitation-learning/index",
  "title": "Chapter 11 - Imitation Learning and VLA",
  "description": "Learning Goals",
  "source": "@site/docs/part-iv-intelligence/chapter-11-imitation-learning/index.md",
  "sourceDirName": "part-iv-intelligence/chapter-11-imitation-learning",
  "slug": "/part-iv-intelligence/chapter-11-imitation-learning/",
  "permalink": "/docs/part-iv-intelligence/chapter-11-imitation-learning/",
  "draft": false,
  "unlisted": false,
  "editUrl": "https://github.com/your-username/physical-ai-humanoid-robotics/tree/main/docs/part-iv-intelligence/chapter-11-imitation-learning/index.md",
  "tags": [],
  "version": "current",
  "sidebarPosition": 2,
  "frontMatter": {
    "title": "Chapter 11 - Imitation Learning and VLA",
    "sidebar_position": 2
  },
  "sidebar": "tutorialSidebar",
  "previous": {
    "title": "Chapter 10 - Reinforcement Learning for Robotics",
    "permalink": "/docs/part-iv-intelligence/chapter-10-reinforcement-learning/"
  },
  "next": {
    "title": "Chapter 12 - Human-Robot Interaction",
    "permalink": "/docs/part-iv-intelligence/chapter-12-hri/"
  }
}

============================================================
FILE: book\.docusaurus\docusaurus-plugin-content-docs\default\site-docs-part-iv-intelligence-chapter-12-hri-index-md-795.json
============================================================
{
  "id": "part-iv-intelligence/chapter-12-hri/index",
  "title": "Chapter 12 - Human-Robot Interaction",
  "description": "Learning Goals",
  "source": "@site/docs/part-iv-intelligence/chapter-12-hri/index.md",
  "sourceDirName": "part-iv-intelligence/chapter-12-hri",
  "slug": "/part-iv-intelligence/chapter-12-hri/",
  "permalink": "/docs/part-iv-intelligence/chapter-12-hri/",
  "draft": false,
  "unlisted": false,
  "editUrl": "https://github.com/your-username/physical-ai-humanoid-robotics/tree/main/docs/part-iv-intelligence/chapter-12-hri/index.md",
  "tags": [],
  "version": "current",
  "sidebarPosition": 3,
  "frontMatter": {
    "title": "Chapter 12 - Human-Robot Interaction",
    "sidebar_position": 3
  },
  "sidebar": "tutorialSidebar",
  "previous": {
    "title": "Chapter 11 - Imitation Learning and VLA",
    "permalink": "/docs/part-iv-intelligence/chapter-11-imitation-learning/"
  },
  "next": {
    "title": "Chapter 13 - Multi-Robot Systems",
    "permalink": "/docs/part-v-integration/chapter-13-multi-robot/"
  }
}

============================================================
FILE: book\.docusaurus\docusaurus-plugin-content-docs\default\site-docs-part-v-integration-chapter-13-multi-robot-index-md-e36.json
============================================================
{
  "id": "part-v-integration/chapter-13-multi-robot/index",
  "title": "Chapter 13 - Multi-Robot Systems",
  "description": "Learning Goals",
  "source": "@site/docs/part-v-integration/chapter-13-multi-robot/index.md",
  "sourceDirName": "part-v-integration/chapter-13-multi-robot",
  "slug": "/part-v-integration/chapter-13-multi-robot/",
  "permalink": "/docs/part-v-integration/chapter-13-multi-robot/",
  "draft": false,
  "unlisted": false,
  "editUrl": "https://github.com/your-username/physical-ai-humanoid-robotics/tree/main/docs/part-v-integration/chapter-13-multi-robot/index.md",
  "tags": [],
  "version": "current",
  "sidebarPosition": 1,
  "frontMatter": {
    "title": "Chapter 13 - Multi-Robot Systems",
    "sidebar_position": 1
  },
  "sidebar": "tutorialSidebar",
  "previous": {
    "title": "Chapter 12 - Human-Robot Interaction",
    "permalink": "/docs/part-iv-intelligence/chapter-12-hri/"
  },
  "next": {
    "title": "Chapter 14: Real-World Deployment and Safety",
    "permalink": "/docs/part-v-integration/chapter-14-deployment/"
  }
}

============================================================
FILE: book\.docusaurus\docusaurus-plugin-content-docs\default\site-docs-part-v-integration-chapter-14-deployment-index-md-4cf.json
============================================================
{
  "id": "part-v-integration/chapter-14-deployment/index",
  "title": "Chapter 14: Real-World Deployment and Safety",
  "description": "Learning Goals",
  "source": "@site/docs/part-v-integration/chapter-14-deployment/index.md",
  "sourceDirName": "part-v-integration/chapter-14-deployment",
  "slug": "/part-v-integration/chapter-14-deployment/",
  "permalink": "/docs/part-v-integration/chapter-14-deployment/",
  "draft": false,
  "unlisted": false,
  "editUrl": "https://github.com/your-username/physical-ai-humanoid-robotics/tree/main/docs/part-v-integration/chapter-14-deployment/index.md",
  "tags": [],
  "version": "current",
  "sidebarPosition": 14,
  "frontMatter": {
    "sidebar_position": 14,
    "title": "Chapter 14: Real-World Deployment and Safety"
  },
  "sidebar": "tutorialSidebar",
  "previous": {
    "title": "Chapter 13 - Multi-Robot Systems",
    "permalink": "/docs/part-v-integration/chapter-13-multi-robot/"
  },
  "next": {
    "title": "Chapter 15: Advanced Topics and Future Directions",
    "permalink": "/docs/part-v-integration/chapter-15-future/"
  }
}

============================================================
FILE: book\.docusaurus\docusaurus-plugin-content-docs\default\site-docs-part-v-integration-chapter-15-future-index-md-5b9.json
============================================================
{
  "id": "part-v-integration/chapter-15-future/index",
  "title": "Chapter 15: Advanced Topics and Future Directions",
  "description": "Learning Goals",
  "source": "@site/docs/part-v-integration/chapter-15-future/index.md",
  "sourceDirName": "part-v-integration/chapter-15-future",
  "slug": "/part-v-integration/chapter-15-future/",
  "permalink": "/docs/part-v-integration/chapter-15-future/",
  "draft": false,
  "unlisted": false,
  "editUrl": "https://github.com/your-username/physical-ai-humanoid-robotics/tree/main/docs/part-v-integration/chapter-15-future/index.md",
  "tags": [],
  "version": "current",
  "sidebarPosition": 15,
  "frontMatter": {
    "sidebar_position": 15,
    "title": "Chapter 15: Advanced Topics and Future Directions"
  },
  "sidebar": "tutorialSidebar",
  "previous": {
    "title": "Chapter 14: Real-World Deployment and Safety",
    "permalink": "/docs/part-v-integration/chapter-14-deployment/"
  },
  "next": {
    "title": "Introduction to Physical AI & Humanoid Robotics",
    "permalink": "/docs/intro"
  }
}

============================================================
FILE: book\.docusaurus\docusaurus-plugin-content-docs\default\site-docs-ros-2-md-051.json
============================================================
{
  "id": "ros2",
  "title": "ROS 2 Fundamentals",
  "description": "Weekly Plan",
  "source": "@site/docs/ros2.md",
  "sourceDirName": ".",
  "slug": "/ros2",
  "permalink": "/docs/ros2",
  "draft": false,
  "unlisted": false,
  "editUrl": "https://github.com/your-username/physical-ai-humanoid-robotics/tree/main/docs/ros2.md",
  "tags": [],
  "version": "current",
  "sidebarPosition": 2,
  "frontMatter": {
    "sidebar_position": 2,
    "title": "ROS 2 Fundamentals"
  },
  "sidebar": "tutorialSidebar",
  "previous": {
    "title": "Introduction to Physical AI & Humanoid Robotics",
    "permalink": "/docs/intro"
  },
  "next": {
    "title": "Robot Simulation (Gazebo & Unity)",
    "permalink": "/docs/gazebo"
  }
}

============================================================
FILE: book\.docusaurus\docusaurus-plugin-content-docs\default\site-docs-vla-md-159.json
============================================================
{
  "id": "vla",
  "title": "Vision-Language-Action (VLA)",
  "description": "Weekly Plan",
  "source": "@site/docs/vla.md",
  "sourceDirName": ".",
  "slug": "/vla",
  "permalink": "/docs/vla",
  "draft": false,
  "unlisted": false,
  "editUrl": "https://github.com/your-username/physical-ai-humanoid-robotics/tree/main/docs/vla.md",
  "tags": [],
  "version": "current",
  "sidebarPosition": 5,
  "frontMatter": {
    "sidebar_position": 5,
    "title": "Vision-Language-Action (VLA)"
  },
  "sidebar": "tutorialSidebar",
  "previous": {
    "title": "NVIDIA Isaac AI Brain",
    "permalink": "/docs/isaac"
  },
  "next": {
    "title": "Capstone Project: Autonomous Humanoid",
    "permalink": "/docs/capstone"
  }
}

============================================================
FILE: book\.docusaurus\docusaurus-plugin-content-docs\default\version-current-metadata-prop-751.json
============================================================
{
  "pluginId": "default",
  "version": "current",
  "label": "Next",
  "banner": null,
  "badge": false,
  "noIndex": false,
  "className": "docs-version-current",
  "isLast": true,
  "docsSidebars": {
    "tutorialSidebar": [
      {
        "type": "link",
        "label": "Physical AI & Humanoid Robotics",
        "href": "/docs/",
        "docId": "index",
        "unlisted": false
      },
      {
        "type": "category",
        "label": "Part I: Foundations of Physical AI and Robotics",
        "items": [
          {
            "type": "category",
            "label": "Chapter 1: Introduction to Physical AI & Embodied Intelligence",
            "items": [
              {
                "type": "link",
                "label": "Chapter 1 - Introduction to Physical AI & Embodied Intelligence",
                "href": "/docs/part-i-foundations/chapter-1-introduction/",
                "docId": "part-i-foundations/chapter-1-introduction/index",
                "unlisted": false
              }
            ],
            "collapsed": true,
            "collapsible": true
          },
          {
            "type": "category",
            "label": "Chapter 2: Robot Operating System (ROS 2) Fundamentals",
            "items": [
              {
                "type": "link",
                "label": "Chapter 2 - Robot Operating System (ROS 2) Fundamentals",
                "href": "/docs/part-i-foundations/chapter-2-ros-fundamentals/",
                "docId": "part-i-foundations/chapter-2-ros-fundamentals/index",
                "unlisted": false
              }
            ],
            "collapsed": true,
            "collapsible": true
          },
          {
            "type": "category",
            "label": "Chapter 3: Robot Modeling and Simulation Fundamentals",
            "items": [
              {
                "type": "link",
                "label": "Chapter 3 - Robot Modeling and Simulation Fundamentals",
                "href": "/docs/part-i-foundations/chapter-3-robot-modeling/",
                "docId": "part-i-foundations/chapter-3-robot-modeling/index",
                "unlisted": false
              }
            ],
            "collapsed": true,
            "collapsible": true
          }
        ],
        "collapsed": true,
        "collapsible": true
      },
      {
        "type": "category",
        "label": "Part II: Perception and Understanding",
        "items": [
          {
            "type": "category",
            "label": "Chapter 4: Sensor Integration and Data Processing",
            "items": [
              {
                "type": "link",
                "label": "Chapter 4 - Sensor Integration and Data Processing",
                "href": "/docs/part-ii-perception/chapter-4-sensor-integration/",
                "docId": "part-ii-perception/chapter-4-sensor-integration/index",
                "unlisted": false
              }
            ],
            "collapsed": true,
            "collapsible": true
          },
          {
            "type": "category",
            "label": "Chapter 5: Computer Vision for Robotics",
            "items": [
              {
                "type": "link",
                "label": "Chapter 5 - Computer Vision for Robotics",
                "href": "/docs/part-ii-perception/chapter-5-computer-vision/",
                "docId": "part-ii-perception/chapter-5-computer-vision/index",
                "unlisted": false
              }
            ],
            "collapsed": true,
            "collapsible": true
          },
          {
            "type": "category",
            "label": "Chapter 6: 3D Perception and Scene Understanding",
            "items": [
              {
                "type": "link",
                "label": "Chapter 6 - 3D Perception and Scene Understanding",
                "href": "/docs/part-ii-perception/chapter-6-3d-perception/",
                "docId": "part-ii-perception/chapter-6-3d-perception/index",
                "unlisted": false
              }
            ],
            "collapsed": true,
            "collapsible": true
          }
        ],
        "collapsed": true,
        "collapsible": true
      },
      {
        "type": "category",
        "label": "Part III: Motion and Control",
        "items": [
          {
            "type": "category",
            "label": "Chapter 7: Kinematics and Dynamics",
            "items": [
              {
                "type": "link",
                "label": "Chapter 7 - Kinematics and Dynamics",
                "href": "/docs/part-iii-motion/chapter-7-kinematics/",
                "docId": "part-iii-motion/chapter-7-kinematics/index",
                "unlisted": false
              }
            ],
            "collapsed": true,
            "collapsible": true
          },
          {
            "type": "category",
            "label": "Chapter 8: Locomotion and Balance Control",
            "items": [
              {
                "type": "link",
                "label": "Chapter 8 - Locomotion and Balance Control",
                "href": "/docs/part-iii-motion/chapter-8-locomotion/",
                "docId": "part-iii-motion/chapter-8-locomotion/index",
                "unlisted": false
              }
            ],
            "collapsed": true,
            "collapsible": true
          },
          {
            "type": "category",
            "label": "Chapter 9: Motion Planning and Navigation",
            "items": [
              {
                "type": "link",
                "label": "Chapter 9 - Motion Planning and Navigation",
                "href": "/docs/part-iii-motion/chapter-9-navigation/",
                "docId": "part-iii-motion/chapter-9-navigation/index",
                "unlisted": false
              }
            ],
            "collapsed": true,
            "collapsible": true
          }
        ],
        "collapsed": true,
        "collapsible": true
      },
      {
        "type": "category",
        "label": "Part IV: Intelligence and Learning",
        "items": [
          {
            "type": "category",
            "label": "Chapter 10: Reinforcement Learning for Robotics",
            "items": [
              {
                "type": "link",
                "label": "Chapter 10 - Reinforcement Learning for Robotics",
                "href": "/docs/part-iv-intelligence/chapter-10-reinforcement-learning/",
                "docId": "part-iv-intelligence/chapter-10-reinforcement-learning/index",
                "unlisted": false
              }
            ],
            "collapsed": true,
            "collapsible": true
          },
          {
            "type": "category",
            "label": "Chapter 11: Imitation Learning and VLA",
            "items": [
              {
                "type": "link",
                "label": "Chapter 11 - Imitation Learning and VLA",
                "href": "/docs/part-iv-intelligence/chapter-11-imitation-learning/",
                "docId": "part-iv-intelligence/chapter-11-imitation-learning/index",
                "unlisted": false
              }
            ],
            "collapsed": true,
            "collapsible": true
          },
          {
            "type": "category",
            "label": "Chapter 12: Human-Robot Interaction",
            "items": [
              {
                "type": "link",
                "label": "Chapter 12 - Human-Robot Interaction",
                "href": "/docs/part-iv-intelligence/chapter-12-hri/",
                "docId": "part-iv-intelligence/chapter-12-hri/index",
                "unlisted": false
              }
            ],
            "collapsed": true,
            "collapsible": true
          }
        ],
        "collapsed": true,
        "collapsible": true
      },
      {
        "type": "category",
        "label": "Part V: Integration and Applications",
        "items": [
          {
            "type": "category",
            "label": "Chapter 13: Multi-Robot Systems and Coordination",
            "items": [
              {
                "type": "link",
                "label": "Chapter 13 - Multi-Robot Systems",
                "href": "/docs/part-v-integration/chapter-13-multi-robot/",
                "docId": "part-v-integration/chapter-13-multi-robot/index",
                "unlisted": false
              }
            ],
            "collapsed": true,
            "collapsible": true
          },
          {
            "type": "category",
            "label": "Chapter 14: Real-World Deployment and Safety",
            "items": [
              {
                "type": "link",
                "label": "Chapter 14: Real-World Deployment and Safety",
                "href": "/docs/part-v-integration/chapter-14-deployment/",
                "docId": "part-v-integration/chapter-14-deployment/index",
                "unlisted": false
              }
            ],
            "collapsed": true,
            "collapsible": true
          },
          {
            "type": "category",
            "label": "Chapter 15: Advanced Topics and Future Directions",
            "items": [
              {
                "type": "link",
                "label": "Chapter 15: Advanced Topics and Future Directions",
                "href": "/docs/part-v-integration/chapter-15-future/",
                "docId": "part-v-integration/chapter-15-future/index",
                "unlisted": false
              }
            ],
            "collapsed": true,
            "collapsible": true
          }
        ],
        "collapsed": true,
        "collapsible": true
      },
      {
        "type": "category",
        "label": "VLA Book Series",
        "items": [
          {
            "type": "category",
            "label": "Introduction to Physical AI & Humanoid Robotics",
            "items": [
              {
                "type": "link",
                "label": "Introduction to Physical AI & Humanoid Robotics",
                "href": "/docs/intro",
                "docId": "intro",
                "unlisted": false
              }
            ],
            "collapsed": true,
            "collapsible": true
          },
          {
            "type": "category",
            "label": "ROS 2 Fundamentals",
            "items": [
              {
                "type": "link",
                "label": "ROS 2 Fundamentals",
                "href": "/docs/ros2",
                "docId": "ros2",
                "unlisted": false
              }
            ],
            "collapsed": true,
            "collapsible": true
          },
          {
            "type": "category",
            "label": "Robot Simulation (Gazebo & Unity)",
            "items": [
              {
                "type": "link",
                "label": "Robot Simulation (Gazebo & Unity)",
                "href": "/docs/gazebo",
                "docId": "gazebo",
                "unlisted": false
              }
            ],
            "collapsed": true,
            "collapsible": true
          },
          {
            "type": "category",
            "label": "NVIDIA Isaac AI Brain",
            "items": [
              {
                "type": "link",
                "label": "NVIDIA Isaac AI Brain",
                "href": "/docs/isaac",
                "docId": "isaac",
                "unlisted": false
              }
            ],
            "collapsed": true,
            "collapsible": true
          },
          {
            "type": "category",
            "label": "Vision-Language-Action (VLA)",
            "items": [
              {
                "type": "link",
                "label": "Vision-Language-Action (VLA)",
                "href": "/docs/vla",
                "docId": "vla",
                "unlisted": false
              }
            ],
            "collapsed": true,
            "collapsible": true
          },
          {
            "type": "category",
            "label": "Capstone Project: Autonomous Humanoid",
            "items": [
              {
                "type": "link",
                "label": "Capstone Project: Autonomous Humanoid",
                "href": "/docs/capstone",
                "docId": "capstone",
                "unlisted": false
              }
            ],
            "collapsed": true,
            "collapsible": true
          }
        ],
        "collapsed": true,
        "collapsible": true
      }
    ]
  },
  "docs": {
    "capstone": {
      "id": "capstone",
      "title": "Capstone Project: Autonomous Humanoid",
      "description": "Weekly Plan",
      "sidebar": "tutorialSidebar"
    },
    "gazebo": {
      "id": "gazebo",
      "title": "Robot Simulation (Gazebo & Unity)",
      "description": "Weekly Plan",
      "sidebar": "tutorialSidebar"
    },
    "index": {
      "id": "index",
      "title": "Physical AI & Humanoid Robotics",
      "description": "Welcome to the comprehensive guide on Physical AI and Humanoid Robotics. This book covers everything from fundamental concepts to advanced implementations using modern robotics technologies.",
      "sidebar": "tutorialSidebar"
    },
    "intro": {
      "id": "intro",
      "title": "Introduction to Physical AI & Humanoid Robotics",
      "description": "Weekly Plan",
      "sidebar": "tutorialSidebar"
    },
    "isaac": {
      "id": "isaac",
      "title": "NVIDIA Isaac AI Brain",
      "description": "Weekly Plan",
      "sidebar": "tutorialSidebar"
    },
    "part-i-foundations/chapter-1-introduction/index": {
      "id": "part-i-foundations/chapter-1-introduction/index",
      "title": "Chapter 1 - Introduction to Physical AI & Embodied Intelligence",
      "description": "Learning Goals",
      "sidebar": "tutorialSidebar"
    },
    "part-i-foundations/chapter-2-ros-fundamentals/index": {
      "id": "part-i-foundations/chapter-2-ros-fundamentals/index",
      "title": "Chapter 2 - Robot Operating System (ROS 2) Fundamentals",
      "description": "Learning Goals",
      "sidebar": "tutorialSidebar"
    },
    "part-i-foundations/chapter-3-robot-modeling/index": {
      "id": "part-i-foundations/chapter-3-robot-modeling/index",
      "title": "Chapter 3 - Robot Modeling and Simulation Fundamentals",
      "description": "Learning Goals",
      "sidebar": "tutorialSidebar"
    },
    "part-ii-perception/chapter-4-sensor-integration/index": {
      "id": "part-ii-perception/chapter-4-sensor-integration/index",
      "title": "Chapter 4 - Sensor Integration and Data Processing",
      "description": "Learning Goals",
      "sidebar": "tutorialSidebar"
    },
    "part-ii-perception/chapter-5-computer-vision/index": {
      "id": "part-ii-perception/chapter-5-computer-vision/index",
      "title": "Chapter 5 - Computer Vision for Robotics",
      "description": "Learning Goals",
      "sidebar": "tutorialSidebar"
    },
    "part-ii-perception/chapter-6-3d-perception/index": {
      "id": "part-ii-perception/chapter-6-3d-perception/index",
      "title": "Chapter 6 - 3D Perception and Scene Understanding",
      "description": "Learning Goals",
      "sidebar": "tutorialSidebar"
    },
    "part-iii-motion/chapter-7-kinematics/index": {
      "id": "part-iii-motion/chapter-7-kinematics/index",
      "title": "Chapter 7 - Kinematics and Dynamics",
      "description": "Learning Goals",
      "sidebar": "tutorialSidebar"
    },
    "part-iii-motion/chapter-8-locomotion/index": {
      "id": "part-iii-motion/chapter-8-locomotion/index",
      "title": "Chapter 8 - Locomotion and Balance Control",
      "description": "Learning Goals",
      "sidebar": "tutorialSidebar"
    },
    "part-iii-motion/chapter-9-navigation/index": {
      "id": "part-iii-motion/chapter-9-navigation/index",
      "title": "Chapter 9 - Motion Planning and Navigation",
      "description": "Learning Goals",
      "sidebar": "tutorialSidebar"
    },
    "part-iv-intelligence/chapter-10-reinforcement-learning/index": {
      "id": "part-iv-intelligence/chapter-10-reinforcement-learning/index",
      "title": "Chapter 10 - Reinforcement Learning for Robotics",
      "description": "Learning Goals",
      "sidebar": "tutorialSidebar"
    },
    "part-iv-intelligence/chapter-11-imitation-learning/index": {
      "id": "part-iv-intelligence/chapter-11-imitation-learning/index",
      "title": "Chapter 11 - Imitation Learning and VLA",
      "description": "Learning Goals",
      "sidebar": "tutorialSidebar"
    },
    "part-iv-intelligence/chapter-12-hri/index": {
      "id": "part-iv-intelligence/chapter-12-hri/index",
      "title": "Chapter 12 - Human-Robot Interaction",
      "description": "Learning Goals",
      "sidebar": "tutorialSidebar"
    },
    "part-v-integration/chapter-13-multi-robot/index": {
      "id": "part-v-integration/chapter-13-multi-robot/index",
      "title": "Chapter 13 - Multi-Robot Systems",
      "description": "Learning Goals",
      "sidebar": "tutorialSidebar"
    },
    "part-v-integration/chapter-14-deployment/index": {
      "id": "part-v-integration/chapter-14-deployment/index",
      "title": "Chapter 14: Real-World Deployment and Safety",
      "description": "Learning Goals",
      "sidebar": "tutorialSidebar"
    },
    "part-v-integration/chapter-15-future/index": {
      "id": "part-v-integration/chapter-15-future/index",
      "title": "Chapter 15: Advanced Topics and Future Directions",
      "description": "Learning Goals",
      "sidebar": "tutorialSidebar"
    },
    "ros2": {
      "id": "ros2",
      "title": "ROS 2 Fundamentals",
      "description": "Weekly Plan",
      "sidebar": "tutorialSidebar"
    },
    "vla": {
      "id": "vla",
      "title": "Vision-Language-Action (VLA)",
      "description": "Weekly Plan",
      "sidebar": "tutorialSidebar"
    }
  }
}

============================================================
FILE: book\.docusaurus\docusaurus-plugin-content-pages\default\plugin-route-context-module-100.json
============================================================
{
  "name": "docusaurus-plugin-content-pages",
  "id": "default"
}

============================================================
FILE: book\.docusaurus\docusaurus-plugin-content-pages\default\site-src-pages-index-md-f39.json
============================================================
{
  "type": "mdx",
  "permalink": "/",
  "source": "@site/src/pages/index.md",
  "title": "Physical AI & Humanoid Robotics",
  "description": "Welcome to the comprehensive guide to Physical AI and Humanoid Robotics. This book provides a hands-on, project-driven approach to understanding and implementing embodied intelligence in humanoid robotic systems.",
  "frontMatter": {
    "title": "Physical AI & Humanoid Robotics"
  },
  "unlisted": false
}

============================================================
FILE: book\.docusaurus\docusaurus-plugin-debug\default\docusaurus-debug-all-content-673.json
============================================================
{
  "docusaurus-plugin-content-docs": {
    "default": {
      "loadedVersions": [
        {
          "versionName": "current",
          "label": "Next",
          "banner": null,
          "badge": false,
          "noIndex": false,
          "className": "docs-version-current",
          "path": "/docs",
          "tagsPath": "/docs/tags",
          "editUrl": "https://github.com/your-username/physical-ai-humanoid-robotics/tree/main/docs",
          "editUrlLocalized": "https://github.com/your-username/physical-ai-humanoid-robotics/tree/main/i18n/en/docusaurus-plugin-content-docs/current",
          "isLast": true,
          "routePriority": -1,
          "sidebarFilePath": "C:\\Users\\Dell\\ai-book\\book\\sidebars.js",
          "contentPath": "C:\\Users\\Dell\\ai-book\\book\\docs",
          "contentPathLocalized": "C:\\Users\\Dell\\ai-book\\book\\i18n\\en\\docusaurus-plugin-content-docs\\current",
          "docs": [
            {
              "id": "capstone",
              "title": "Capstone Project: Autonomous Humanoid",
              "description": "Weekly Plan",
              "source": "@site/docs/capstone.md",
              "sourceDirName": ".",
              "slug": "/capstone",
              "permalink": "/docs/capstone",
              "draft": false,
              "unlisted": false,
              "editUrl": "https://github.com/your-username/physical-ai-humanoid-robotics/tree/main/docs/capstone.md",
              "tags": [],
              "version": "current",
              "sidebarPosition": 6,
              "frontMatter": {
                "sidebar_position": 6,
                "title": "Capstone Project: Autonomous Humanoid"
              },
              "sidebar": "tutorialSidebar",
              "previous": {
                "title": "Vision-Language-Action (VLA)",
                "permalink": "/docs/vla"
              }
            },
            {
              "id": "gazebo",
              "title": "Robot Simulation (Gazebo & Unity)",
              "description": "Weekly Plan",
              "source": "@site/docs/gazebo.md",
              "sourceDirName": ".",
              "slug": "/gazebo",
              "permalink": "/docs/gazebo",
              "draft": false,
              "unlisted": false,
              "editUrl": "https://github.com/your-username/physical-ai-humanoid-robotics/tree/main/docs/gazebo.md",
              "tags": [],
              "version": "current",
              "sidebarPosition": 3,
              "frontMatter": {
                "sidebar_position": 3,
                "title": "Robot Simulation (Gazebo & Unity)"
              },
              "sidebar": "tutorialSidebar",
              "previous": {
                "title": "ROS 2 Fundamentals",
                "permalink": "/docs/ros2"
              },
              "next": {
                "title": "NVIDIA Isaac AI Brain",
                "permalink": "/docs/isaac"
              }
            },
            {
              "id": "index",
              "title": "Physical AI & Humanoid Robotics",
              "description": "Welcome to the comprehensive guide on Physical AI and Humanoid Robotics. This book covers everything from fundamental concepts to advanced implementations using modern robotics technologies.",
              "source": "@site/docs/index.md",
              "sourceDirName": ".",
              "slug": "/",
              "permalink": "/docs/",
              "draft": false,
              "unlisted": false,
              "editUrl": "https://github.com/your-username/physical-ai-humanoid-robotics/tree/main/docs/index.md",
              "tags": [],
              "version": "current",
              "sidebarPosition": 0,
              "frontMatter": {
                "sidebar_position": 0,
                "title": "Physical AI & Humanoid Robotics"
              },
              "sidebar": "tutorialSidebar",
              "next": {
                "title": "Chapter 1 - Introduction to Physical AI & Embodied Intelligence",
                "permalink": "/docs/part-i-foundations/chapter-1-introduction/"
              }
            },
            {
              "id": "intro",
              "title": "Introduction to Physical AI & Humanoid Robotics",
              "description": "Weekly Plan",
              "source": "@site/docs/intro.md",
              "sourceDirName": ".",
              "slug": "/intro",
              "permalink": "/docs/intro",
              "draft": false,
              "unlisted": false,
              "editUrl": "https://github.com/your-username/physical-ai-humanoid-robotics/tree/main/docs/intro.md",
              "tags": [],
              "version": "current",
              "sidebarPosition": 1,
              "frontMatter": {
                "sidebar_position": 1,
                "title": "Introduction to Physical AI & Humanoid Robotics"
              },
              "sidebar": "tutorialSidebar",
              "previous": {
                "title": "Chapter 15: Advanced Topics and Future Directions",
                "permalink": "/docs/part-v-integration/chapter-15-future/"
              },
              "next": {
                "title": "ROS 2 Fundamentals",
                "permalink": "/docs/ros2"
              }
            },
            {
              "id": "isaac",
              "title": "NVIDIA Isaac AI Brain",
              "description": "Weekly Plan",
              "source": "@site/docs/isaac.md",
              "sourceDirName": ".",
              "slug": "/isaac",
              "permalink": "/docs/isaac",
              "draft": false,
              "unlisted": false,
              "editUrl": "https://github.com/your-username/physical-ai-humanoid-robotics/tree/main/docs/isaac.md",
              "tags": [],
              "version": "current",
              "sidebarPosition": 4,
              "frontMatter": {
                "sidebar_position": 4,
                "title": "NVIDIA Isaac AI Brain"
              },
              "sidebar": "tutorialSidebar",
              "previous": {
                "title": "Robot Simulation (Gazebo & Unity)",
                "permalink": "/docs/gazebo"
              },
              "next": {
                "title": "Vision-Language-Action (VLA)",
                "permalink": "/docs/vla"
              }
            },
            {
              "id": "part-i-foundations/chapter-1-introduction/index",
              "title": "Chapter 1 - Introduction to Physical AI & Embodied Intelligence",
              "description": "Learning Goals",
              "source": "@site/docs/part-i-foundations/chapter-1-introduction/index.md",
              "sourceDirName": "part-i-foundations/chapter-1-introduction",
              "slug": "/part-i-foundations/chapter-1-introduction/",
              "permalink": "/docs/part-i-foundations/chapter-1-introduction/",
              "draft": false,
              "unlisted": false,
              "editUrl": "https://github.com/your-username/physical-ai-humanoid-robotics/tree/main/docs/part-i-foundations/chapter-1-introduction/index.md",
              "tags": [],
              "version": "current",
              "sidebarPosition": 1,
              "frontMatter": {
                "title": "Chapter 1 - Introduction to Physical AI & Embodied Intelligence",
                "sidebar_position": 1
              },
              "sidebar": "tutorialSidebar",
              "previous": {
                "title": "Physical AI & Humanoid Robotics",
                "permalink": "/docs/"
              },
              "next": {
                "title": "Chapter 2 - Robot Operating System (ROS 2) Fundamentals",
                "permalink": "/docs/part-i-foundations/chapter-2-ros-fundamentals/"
              }
            },
            {
              "id": "part-i-foundations/chapter-2-ros-fundamentals/index",
              "title": "Chapter 2 - Robot Operating System (ROS 2) Fundamentals",
              "description": "Learning Goals",
              "source": "@site/docs/part-i-foundations/chapter-2-ros-fundamentals/index.md",
              "sourceDirName": "part-i-foundations/chapter-2-ros-fundamentals",
              "slug": "/part-i-foundations/chapter-2-ros-fundamentals/",
              "permalink": "/docs/part-i-foundations/chapter-2-ros-fundamentals/",
              "draft": false,
              "unlisted": false,
              "editUrl": "https://github.com/your-username/physical-ai-humanoid-robotics/tree/main/docs/part-i-foundations/chapter-2-ros-fundamentals/index.md",
              "tags": [],
              "version": "current",
              "sidebarPosition": 2,
              "frontMatter": {
                "title": "Chapter 2 - Robot Operating System (ROS 2) Fundamentals",
                "sidebar_position": 2
              },
              "sidebar": "tutorialSidebar",
              "previous": {
                "title": "Chapter 1 - Introduction to Physical AI & Embodied Intelligence",
                "permalink": "/docs/part-i-foundations/chapter-1-introduction/"
              },
              "next": {
                "title": "Chapter 3 - Robot Modeling and Simulation Fundamentals",
                "permalink": "/docs/part-i-foundations/chapter-3-robot-modeling/"
              }
            },
            {
              "id": "part-i-foundations/chapter-3-robot-modeling/index",
              "title": "Chapter 3 - Robot Modeling and Simulation Fundamentals",
              "description": "Learning Goals",
              "source": "@site/docs/part-i-foundations/chapter-3-robot-modeling/index.md",
              "sourceDirName": "part-i-foundations/chapter-3-robot-modeling",
              "slug": "/part-i-foundations/chapter-3-robot-modeling/",
              "permalink": "/docs/part-i-foundations/chapter-3-robot-modeling/",
              "draft": false,
              "unlisted": false,
              "editUrl": "https://github.com/your-username/physical-ai-humanoid-robotics/tree/main/docs/part-i-foundations/chapter-3-robot-modeling/index.md",
              "tags": [],
              "version": "current",
              "sidebarPosition": 3,
              "frontMatter": {
                "title": "Chapter 3 - Robot Modeling and Simulation Fundamentals",
                "sidebar_position": 3
              },
              "sidebar": "tutorialSidebar",
              "previous": {
                "title": "Chapter 2 - Robot Operating System (ROS 2) Fundamentals",
                "permalink": "/docs/part-i-foundations/chapter-2-ros-fundamentals/"
              },
              "next": {
                "title": "Chapter 4 - Sensor Integration and Data Processing",
                "permalink": "/docs/part-ii-perception/chapter-4-sensor-integration/"
              }
            },
            {
              "id": "part-ii-perception/chapter-4-sensor-integration/index",
              "title": "Chapter 4 - Sensor Integration and Data Processing",
              "description": "Learning Goals",
              "source": "@site/docs/part-ii-perception/chapter-4-sensor-integration/index.md",
              "sourceDirName": "part-ii-perception/chapter-4-sensor-integration",
              "slug": "/part-ii-perception/chapter-4-sensor-integration/",
              "permalink": "/docs/part-ii-perception/chapter-4-sensor-integration/",
              "draft": false,
              "unlisted": false,
              "editUrl": "https://github.com/your-username/physical-ai-humanoid-robotics/tree/main/docs/part-ii-perception/chapter-4-sensor-integration/index.md",
              "tags": [],
              "version": "current",
              "sidebarPosition": 1,
              "frontMatter": {
                "title": "Chapter 4 - Sensor Integration and Data Processing",
                "sidebar_position": 1
              },
              "sidebar": "tutorialSidebar",
              "previous": {
                "title": "Chapter 3 - Robot Modeling and Simulation Fundamentals",
                "permalink": "/docs/part-i-foundations/chapter-3-robot-modeling/"
              },
              "next": {
                "title": "Chapter 5 - Computer Vision for Robotics",
                "permalink": "/docs/part-ii-perception/chapter-5-computer-vision/"
              }
            },
            {
              "id": "part-ii-perception/chapter-5-computer-vision/index",
              "title": "Chapter 5 - Computer Vision for Robotics",
              "description": "Learning Goals",
              "source": "@site/docs/part-ii-perception/chapter-5-computer-vision/index.md",
              "sourceDirName": "part-ii-perception/chapter-5-computer-vision",
              "slug": "/part-ii-perception/chapter-5-computer-vision/",
              "permalink": "/docs/part-ii-perception/chapter-5-computer-vision/",
              "draft": false,
              "unlisted": false,
              "editUrl": "https://github.com/your-username/physical-ai-humanoid-robotics/tree/main/docs/part-ii-perception/chapter-5-computer-vision/index.md",
              "tags": [],
              "version": "current",
              "sidebarPosition": 2,
              "frontMatter": {
                "title": "Chapter 5 - Computer Vision for Robotics",
                "sidebar_position": 2
              },
              "sidebar": "tutorialSidebar",
              "previous": {
                "title": "Chapter 4 - Sensor Integration and Data Processing",
                "permalink": "/docs/part-ii-perception/chapter-4-sensor-integration/"
              },
              "next": {
                "title": "Chapter 6 - 3D Perception and Scene Understanding",
                "permalink": "/docs/part-ii-perception/chapter-6-3d-perception/"
              }
            },
            {
              "id": "part-ii-perception/chapter-6-3d-perception/index",
              "title": "Chapter 6 - 3D Perception and Scene Understanding",
              "description": "Learning Goals",
              "source": "@site/docs/part-ii-perception/chapter-6-3d-perception/index.md",
              "sourceDirName": "part-ii-perception/chapter-6-3d-perception",
              "slug": "/part-ii-perception/chapter-6-3d-perception/",
              "permalink": "/docs/part-ii-perception/chapter-6-3d-perception/",
              "draft": false,
              "unlisted": false,
              "editUrl": "https://github.com/your-username/physical-ai-humanoid-robotics/tree/main/docs/part-ii-perception/chapter-6-3d-perception/index.md",
              "tags": [],
              "version": "current",
              "sidebarPosition": 3,
              "frontMatter": {
                "title": "Chapter 6 - 3D Perception and Scene Understanding",
                "sidebar_position": 3
              },
              "sidebar": "tutorialSidebar",
              "previous": {
                "title": "Chapter 5 - Computer Vision for Robotics",
                "permalink": "/docs/part-ii-perception/chapter-5-computer-vision/"
              },
              "next": {
                "title": "Chapter 7 - Kinematics and Dynamics",
                "permalink": "/docs/part-iii-motion/chapter-7-kinematics/"
              }
            },
            {
              "id": "part-iii-motion/chapter-7-kinematics/index",
              "title": "Chapter 7 - Kinematics and Dynamics",
              "description": "Learning Goals",
              "source": "@site/docs/part-iii-motion/chapter-7-kinematics/index.md",
              "sourceDirName": "part-iii-motion/chapter-7-kinematics",
              "slug": "/part-iii-motion/chapter-7-kinematics/",
              "permalink": "/docs/part-iii-motion/chapter-7-kinematics/",
              "draft": false,
              "unlisted": false,
              "editUrl": "https://github.com/your-username/physical-ai-humanoid-robotics/tree/main/docs/part-iii-motion/chapter-7-kinematics/index.md",
              "tags": [],
              "version": "current",
              "sidebarPosition": 1,
              "frontMatter": {
                "title": "Chapter 7 - Kinematics and Dynamics",
                "sidebar_position": 1
              },
              "sidebar": "tutorialSidebar",
              "previous": {
                "title": "Chapter 6 - 3D Perception and Scene Understanding",
                "permalink": "/docs/part-ii-perception/chapter-6-3d-perception/"
              },
              "next": {
                "title": "Chapter 8 - Locomotion and Balance Control",
                "permalink": "/docs/part-iii-motion/chapter-8-locomotion/"
              }
            },
            {
              "id": "part-iii-motion/chapter-8-locomotion/index",
              "title": "Chapter 8 - Locomotion and Balance Control",
              "description": "Learning Goals",
              "source": "@site/docs/part-iii-motion/chapter-8-locomotion/index.md",
              "sourceDirName": "part-iii-motion/chapter-8-locomotion",
              "slug": "/part-iii-motion/chapter-8-locomotion/",
              "permalink": "/docs/part-iii-motion/chapter-8-locomotion/",
              "draft": false,
              "unlisted": false,
              "editUrl": "https://github.com/your-username/physical-ai-humanoid-robotics/tree/main/docs/part-iii-motion/chapter-8-locomotion/index.md",
              "tags": [],
              "version": "current",
              "sidebarPosition": 2,
              "frontMatter": {
                "title": "Chapter 8 - Locomotion and Balance Control",
                "sidebar_position": 2
              },
              "sidebar": "tutorialSidebar",
              "previous": {
                "title": "Chapter 7 - Kinematics and Dynamics",
                "permalink": "/docs/part-iii-motion/chapter-7-kinematics/"
              },
              "next": {
                "title": "Chapter 9 - Motion Planning and Navigation",
                "permalink": "/docs/part-iii-motion/chapter-9-navigation/"
              }
            },
            {
              "id": "part-iii-motion/chapter-9-navigation/index",
              "title": "Chapter 9 - Motion Planning and Navigation",
              "description": "Learning Goals",
              "source": "@site/docs/part-iii-motion/chapter-9-navigation/index.md",
              "sourceDirName": "part-iii-motion/chapter-9-navigation",
              "slug": "/part-iii-motion/chapter-9-navigation/",
              "permalink": "/docs/part-iii-motion/chapter-9-navigation/",
              "draft": false,
              "unlisted": false,
              "editUrl": "https://github.com/your-username/physical-ai-humanoid-robotics/tree/main/docs/part-iii-motion/chapter-9-navigation/index.md",
              "tags": [],
              "version": "current",
              "sidebarPosition": 3,
              "frontMatter": {
                "title": "Chapter 9 - Motion Planning and Navigation",
                "sidebar_position": 3
              },
              "sidebar": "tutorialSidebar",
              "previous": {
                "title": "Chapter 8 - Locomotion and Balance Control",
                "permalink": "/docs/part-iii-motion/chapter-8-locomotion/"
              },
              "next": {
                "title": "Chapter 10 - Reinforcement Learning for Robotics",
                "permalink": "/docs/part-iv-intelligence/chapter-10-reinforcement-learning/"
              }
            },
            {
              "id": "part-iv-intelligence/chapter-10-reinforcement-learning/index",
              "title": "Chapter 10 - Reinforcement Learning for Robotics",
              "description": "Learning Goals",
              "source": "@site/docs/part-iv-intelligence/chapter-10-reinforcement-learning/index.md",
              "sourceDirName": "part-iv-intelligence/chapter-10-reinforcement-learning",
              "slug": "/part-iv-intelligence/chapter-10-reinforcement-learning/",
              "permalink": "/docs/part-iv-intelligence/chapter-10-reinforcement-learning/",
              "draft": false,
              "unlisted": false,
              "editUrl": "https://github.com/your-username/physical-ai-humanoid-robotics/tree/main/docs/part-iv-intelligence/chapter-10-reinforcement-learning/index.md",
              "tags": [],
              "version": "current",
              "sidebarPosition": 1,
              "frontMatter": {
                "title": "Chapter 10 - Reinforcement Learning for Robotics",
                "sidebar_position": 1
              },
              "sidebar": "tutorialSidebar",
              "previous": {
                "title": "Chapter 9 - Motion Planning and Navigation",
                "permalink": "/docs/part-iii-motion/chapter-9-navigation/"
              },
              "next": {
                "title": "Chapter 11 - Imitation Learning and VLA",
                "permalink": "/docs/part-iv-intelligence/chapter-11-imitation-learning/"
              }
            },
            {
              "id": "part-iv-intelligence/chapter-11-imitation-learning/index",
              "title": "Chapter 11 - Imitation Learning and VLA",
              "description": "Learning Goals",
              "source": "@site/docs/part-iv-intelligence/chapter-11-imitation-learning/index.md",
              "sourceDirName": "part-iv-intelligence/chapter-11-imitation-learning",
              "slug": "/part-iv-intelligence/chapter-11-imitation-learning/",
              "permalink": "/docs/part-iv-intelligence/chapter-11-imitation-learning/",
              "draft": false,
              "unlisted": false,
              "editUrl": "https://github.com/your-username/physical-ai-humanoid-robotics/tree/main/docs/part-iv-intelligence/chapter-11-imitation-learning/index.md",
              "tags": [],
              "version": "current",
              "sidebarPosition": 2,
              "frontMatter": {
                "title": "Chapter 11 - Imitation Learning and VLA",
                "sidebar_position": 2
              },
              "sidebar": "tutorialSidebar",
              "previous": {
                "title": "Chapter 10 - Reinforcement Learning for Robotics",
                "permalink": "/docs/part-iv-intelligence/chapter-10-reinforcement-learning/"
              },
              "next": {
                "title": "Chapter 12 - Human-Robot Interaction",
                "permalink": "/docs/part-iv-intelligence/chapter-12-hri/"
              }
            },
            {
              "id": "part-iv-intelligence/chapter-12-hri/index",
              "title": "Chapter 12 - Human-Robot Interaction",
              "description": "Learning Goals",
              "source": "@site/docs/part-iv-intelligence/chapter-12-hri/index.md",
              "sourceDirName": "part-iv-intelligence/chapter-12-hri",
              "slug": "/part-iv-intelligence/chapter-12-hri/",
              "permalink": "/docs/part-iv-intelligence/chapter-12-hri/",
              "draft": false,
              "unlisted": false,
              "editUrl": "https://github.com/your-username/physical-ai-humanoid-robotics/tree/main/docs/part-iv-intelligence/chapter-12-hri/index.md",
              "tags": [],
              "version": "current",
              "sidebarPosition": 3,
              "frontMatter": {
                "title": "Chapter 12 - Human-Robot Interaction",
                "sidebar_position": 3
              },
              "sidebar": "tutorialSidebar",
              "previous": {
                "title": "Chapter 11 - Imitation Learning and VLA",
                "permalink": "/docs/part-iv-intelligence/chapter-11-imitation-learning/"
              },
              "next": {
                "title": "Chapter 13 - Multi-Robot Systems",
                "permalink": "/docs/part-v-integration/chapter-13-multi-robot/"
              }
            },
            {
              "id": "part-v-integration/chapter-13-multi-robot/index",
              "title": "Chapter 13 - Multi-Robot Systems",
              "description": "Learning Goals",
              "source": "@site/docs/part-v-integration/chapter-13-multi-robot/index.md",
              "sourceDirName": "part-v-integration/chapter-13-multi-robot",
              "slug": "/part-v-integration/chapter-13-multi-robot/",
              "permalink": "/docs/part-v-integration/chapter-13-multi-robot/",
              "draft": false,
              "unlisted": false,
              "editUrl": "https://github.com/your-username/physical-ai-humanoid-robotics/tree/main/docs/part-v-integration/chapter-13-multi-robot/index.md",
              "tags": [],
              "version": "current",
              "sidebarPosition": 1,
              "frontMatter": {
                "title": "Chapter 13 - Multi-Robot Systems",
                "sidebar_position": 1
              },
              "sidebar": "tutorialSidebar",
              "previous": {
                "title": "Chapter 12 - Human-Robot Interaction",
                "permalink": "/docs/part-iv-intelligence/chapter-12-hri/"
              },
              "next": {
                "title": "Chapter 14: Real-World Deployment and Safety",
                "permalink": "/docs/part-v-integration/chapter-14-deployment/"
              }
            },
            {
              "id": "part-v-integration/chapter-14-deployment/index",
              "title": "Chapter 14: Real-World Deployment and Safety",
              "description": "Learning Goals",
              "source": "@site/docs/part-v-integration/chapter-14-deployment/index.md",
              "sourceDirName": "part-v-integration/chapter-14-deployment",
              "slug": "/part-v-integration/chapter-14-deployment/",
              "permalink": "/docs/part-v-integration/chapter-14-deployment/",
              "draft": false,
              "unlisted": false,
              "editUrl": "https://github.com/your-username/physical-ai-humanoid-robotics/tree/main/docs/part-v-integration/chapter-14-deployment/index.md",
              "tags": [],
              "version": "current",
              "sidebarPosition": 14,
              "frontMatter": {
                "sidebar_position": 14,
                "title": "Chapter 14: Real-World Deployment and Safety"
              },
              "sidebar": "tutorialSidebar",
              "previous": {
                "title": "Chapter 13 - Multi-Robot Systems",
                "permalink": "/docs/part-v-integration/chapter-13-multi-robot/"
              },
              "next": {
                "title": "Chapter 15: Advanced Topics and Future Directions",
                "permalink": "/docs/part-v-integration/chapter-15-future/"
              }
            },
            {
              "id": "part-v-integration/chapter-15-future/index",
              "title": "Chapter 15: Advanced Topics and Future Directions",
              "description": "Learning Goals",
              "source": "@site/docs/part-v-integration/chapter-15-future/index.md",
              "sourceDirName": "part-v-integration/chapter-15-future",
              "slug": "/part-v-integration/chapter-15-future/",
              "permalink": "/docs/part-v-integration/chapter-15-future/",
              "draft": false,
              "unlisted": false,
              "editUrl": "https://github.com/your-username/physical-ai-humanoid-robotics/tree/main/docs/part-v-integration/chapter-15-future/index.md",
              "tags": [],
              "version": "current",
              "sidebarPosition": 15,
              "frontMatter": {
                "sidebar_position": 15,
                "title": "Chapter 15: Advanced Topics and Future Directions"
              },
              "sidebar": "tutorialSidebar",
              "previous": {
                "title": "Chapter 14: Real-World Deployment and Safety",
                "permalink": "/docs/part-v-integration/chapter-14-deployment/"
              },
              "next": {
                "title": "Introduction to Physical AI & Humanoid Robotics",
                "permalink": "/docs/intro"
              }
            },
            {
              "id": "ros2",
              "title": "ROS 2 Fundamentals",
              "description": "Weekly Plan",
              "source": "@site/docs/ros2.md",
              "sourceDirName": ".",
              "slug": "/ros2",
              "permalink": "/docs/ros2",
              "draft": false,
              "unlisted": false,
              "editUrl": "https://github.com/your-username/physical-ai-humanoid-robotics/tree/main/docs/ros2.md",
              "tags": [],
              "version": "current",
              "sidebarPosition": 2,
              "frontMatter": {
                "sidebar_position": 2,
                "title": "ROS 2 Fundamentals"
              },
              "sidebar": "tutorialSidebar",
              "previous": {
                "title": "Introduction to Physical AI & Humanoid Robotics",
                "permalink": "/docs/intro"
              },
              "next": {
                "title": "Robot Simulation (Gazebo & Unity)",
                "permalink": "/docs/gazebo"
              }
            },
            {
              "id": "vla",
              "title": "Vision-Language-Action (VLA)",
              "description": "Weekly Plan",
              "source": "@site/docs/vla.md",
              "sourceDirName": ".",
              "slug": "/vla",
              "permalink": "/docs/vla",
              "draft": false,
              "unlisted": false,
              "editUrl": "https://github.com/your-username/physical-ai-humanoid-robotics/tree/main/docs/vla.md",
              "tags": [],
              "version": "current",
              "sidebarPosition": 5,
              "frontMatter": {
                "sidebar_position": 5,
                "title": "Vision-Language-Action (VLA)"
              },
              "sidebar": "tutorialSidebar",
              "previous": {
                "title": "NVIDIA Isaac AI Brain",
                "permalink": "/docs/isaac"
              },
              "next": {
                "title": "Capstone Project: Autonomous Humanoid",
                "permalink": "/docs/capstone"
              }
            }
          ],
          "drafts": [],
          "sidebars": {
            "tutorialSidebar": [
              {
                "type": "doc",
                "id": "index"
              },
              {
                "type": "category",
                "label": "Part I: Foundations of Physical AI and Robotics",
                "items": [
                  {
                    "type": "category",
                    "label": "Chapter 1: Introduction to Physical AI & Embodied Intelligence",
                    "items": [
                      {
                        "type": "doc",
                        "id": "part-i-foundations/chapter-1-introduction/index"
                      }
                    ],
                    "collapsed": true,
                    "collapsible": true
                  },
                  {
                    "type": "category",
                    "label": "Chapter 2: Robot Operating System (ROS 2) Fundamentals",
                    "items": [
                      {
                        "type": "doc",
                        "id": "part-i-foundations/chapter-2-ros-fundamentals/index"
                      }
                    ],
                    "collapsed": true,
                    "collapsible": true
                  },
                  {
                    "type": "category",
                    "label": "Chapter 3: Robot Modeling and Simulation Fundamentals",
                    "items": [
                      {
                        "type": "doc",
                        "id": "part-i-foundations/chapter-3-robot-modeling/index"
                      }
                    ],
                    "collapsed": true,
                    "collapsible": true
                  }
                ],
                "collapsed": true,
                "collapsible": true
              },
              {
                "type": "category",
                "label": "Part II: Perception and Understanding",
                "items": [
                  {
                    "type": "category",
                    "label": "Chapter 4: Sensor Integration and Data Processing",
                    "items": [
                      {
                        "type": "doc",
                        "id": "part-ii-perception/chapter-4-sensor-integration/index"
                      }
                    ],
                    "collapsed": true,
                    "collapsible": true
                  },
                  {
                    "type": "category",
                    "label": "Chapter 5: Computer Vision for Robotics",
                    "items": [
                      {
                        "type": "doc",
                        "id": "part-ii-perception/chapter-5-computer-vision/index"
                      }
                    ],
                    "collapsed": true,
                    "collapsible": true
                  },
                  {
                    "type": "category",
                    "label": "Chapter 6: 3D Perception and Scene Understanding",
                    "items": [
                      {
                        "type": "doc",
                        "id": "part-ii-perception/chapter-6-3d-perception/index"
                      }
                    ],
                    "collapsed": true,
                    "collapsible": true
                  }
                ],
                "collapsed": true,
                "collapsible": true
              },
              {
                "type": "category",
                "label": "Part III: Motion and Control",
                "items": [
                  {
                    "type": "category",
                    "label": "Chapter 7: Kinematics and Dynamics",
                    "items": [
                      {
                        "type": "doc",
                        "id": "part-iii-motion/chapter-7-kinematics/index"
                      }
                    ],
                    "collapsed": true,
                    "collapsible": true
                  },
                  {
                    "type": "category",
                    "label": "Chapter 8: Locomotion and Balance Control",
                    "items": [
                      {
                        "type": "doc",
                        "id": "part-iii-motion/chapter-8-locomotion/index"
                      }
                    ],
                    "collapsed": true,
                    "collapsible": true
                  },
                  {
                    "type": "category",
                    "label": "Chapter 9: Motion Planning and Navigation",
                    "items": [
                      {
                        "type": "doc",
                        "id": "part-iii-motion/chapter-9-navigation/index"
                      }
                    ],
                    "collapsed": true,
                    "collapsible": true
                  }
                ],
                "collapsed": true,
                "collapsible": true
              },
              {
                "type": "category",
                "label": "Part IV: Intelligence and Learning",
                "items": [
                  {
                    "type": "category",
                    "label": "Chapter 10: Reinforcement Learning for Robotics",
                    "items": [
                      {
                        "type": "doc",
                        "id": "part-iv-intelligence/chapter-10-reinforcement-learning/index"
                      }
                    ],
                    "collapsed": true,
                    "collapsible": true
                  },
                  {
                    "type": "category",
                    "label": "Chapter 11: Imitation Learning and VLA",
                    "items": [
                      {
                        "type": "doc",
                        "id": "part-iv-intelligence/chapter-11-imitation-learning/index"
                      }
                    ],
                    "collapsed": true,
                    "collapsible": true
                  },
                  {
                    "type": "category",
                    "label": "Chapter 12: Human-Robot Interaction",
                    "items": [
                      {
                        "type": "doc",
                        "id": "part-iv-intelligence/chapter-12-hri/index"
                      }
                    ],
                    "collapsed": true,
                    "collapsible": true
                  }
                ],
                "collapsed": true,
                "collapsible": true
              },
              {
                "type": "category",
                "label": "Part V: Integration and Applications",
                "items": [
                  {
                    "type": "category",
                    "label": "Chapter 13: Multi-Robot Systems and Coordination",
                    "items": [
                      {
                        "type": "doc",
                        "id": "part-v-integration/chapter-13-multi-robot/index"
                      }
                    ],
                    "collapsed": true,
                    "collapsible": true
                  },
                  {
                    "type": "category",
                    "label": "Chapter 14: Real-World Deployment and Safety",
                    "items": [
                      {
                        "type": "doc",
                        "id": "part-v-integration/chapter-14-deployment/index"
                      }
                    ],
                    "collapsed": true,
                    "collapsible": true
                  },
                  {
                    "type": "category",
                    "label": "Chapter 15: Advanced Topics and Future Directions",
                    "items": [
                      {
                        "type": "doc",
                        "id": "part-v-integration/chapter-15-future/index"
                      }
                    ],
                    "collapsed": true,
                    "collapsible": true
                  }
                ],
                "collapsed": true,
                "collapsible": true
              },
              {
                "type": "category",
                "label": "VLA Book Series",
                "items": [
                  {
                    "type": "category",
                    "label": "Introduction to Physical AI & Humanoid Robotics",
                    "items": [
                      {
                        "type": "doc",
                        "id": "intro"
                      }
                    ],
                    "collapsed": true,
                    "collapsible": true
                  },
                  {
                    "type": "category",
                    "label": "ROS 2 Fundamentals",
                    "items": [
                      {
                        "type": "doc",
                        "id": "ros2"
                      }
                    ],
                    "collapsed": true,
                    "collapsible": true
                  },
                  {
                    "type": "category",
                    "label": "Robot Simulation (Gazebo & Unity)",
                    "items": [
                      {
                        "type": "doc",
                        "id": "gazebo"
                      }
                    ],
                    "collapsed": true,
                    "collapsible": true
                  },
                  {
                    "type": "category",
                    "label": "NVIDIA Isaac AI Brain",
                    "items": [
                      {
                        "type": "doc",
                        "id": "isaac"
                      }
                    ],
                    "collapsed": true,
                    "collapsible": true
                  },
                  {
                    "type": "category",
                    "label": "Vision-Language-Action (VLA)",
                    "items": [
                      {
                        "type": "doc",
                        "id": "vla"
                      }
                    ],
                    "collapsed": true,
                    "collapsible": true
                  },
                  {
                    "type": "category",
                    "label": "Capstone Project: Autonomous Humanoid",
                    "items": [
                      {
                        "type": "doc",
                        "id": "capstone"
                      }
                    ],
                    "collapsed": true,
                    "collapsible": true
                  }
                ],
                "collapsed": true,
                "collapsible": true
              }
            ]
          }
        }
      ]
    }
  },
  "docusaurus-plugin-content-pages": {
    "default": [
      {
        "type": "mdx",
        "permalink": "/",
        "source": "@site/src/pages/index.md",
        "title": "Physical AI & Humanoid Robotics",
        "description": "Welcome to the comprehensive guide to Physical AI and Humanoid Robotics. This book provides a hands-on, project-driven approach to understanding and implementing embodied intelligence in humanoid robotic systems.",
        "frontMatter": {
          "title": "Physical AI & Humanoid Robotics"
        },
        "unlisted": false
      },
      {
        "type": "jsx",
        "permalink": "/LayoutWrapper",
        "source": "@site/src/pages/LayoutWrapper.js"
      },
      {
        "type": "jsx",
        "permalink": "/mdxPage",
        "source": "@site/src/pages/mdxPage.js"
      }
    ]
  },
  "docusaurus-plugin-debug": {},
  "docusaurus-theme-classic": {},
  "docusaurus-theme-live-codeblock": {},
  "docusaurus-bootstrap-plugin": {},
  "docusaurus-mdx-fallback-plugin": {}
}

============================================================
FILE: book\.docusaurus\docusaurus-plugin-debug\default\plugin-route-context-module-100.json
============================================================
{
  "name": "docusaurus-plugin-debug",
  "id": "default"
}

============================================================
FILE: book\.github\workflows\deploy.yml
============================================================
name: Deploy to GitHub Pages

on:
  push:
    branches: [main]
  workflow_dispatch:

jobs:
  deploy:
    name: Deploy Docusaurus
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-node@v3
        with:
          node-version: 18
          cache: npm

      - name: Install dependencies
        run: npm ci
      - name: Build website
        run: npm run build

      - name: Deploy to GitHub Pages
        uses: peaceiris/actions-gh-pages@v3
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: ./build

============================================================
FILE: book\docs\capstone.md
============================================================
---
sidebar_position: 6
title: "Capstone Project: Autonomous Humanoid"
---

# Capstone Project: Autonomous Humanoid

## Weekly Plan
- Day 1-2: System architecture and component integration
- Day 3-4: Implementing perception-action loop
- Day 5-7: Creating complete autonomous behaviors and testing

## Learning Objectives
By the end of this chapter, you will:
- Integrate all previously learned technologies (ROS 2, Gazebo, Isaac, VLA)
- Design a complete autonomous humanoid robot system
- Implement perception-action loops for real-time operation
- Create complex autonomous behaviors combining multiple capabilities
- Test and validate the integrated system in simulation and potentially on real hardware

## System Architecture Overview

The autonomous humanoid system combines all the technologies learned in previous chapters into a cohesive architecture:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Autonomous Humanoid System                   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Perception Layer                                               ‚îÇ
‚îÇ  ‚îú‚îÄ Vision Processing (Isaac/CV)                               ‚îÇ
‚îÇ  ‚îú‚îÄ Audio Processing (Whisper)                                 ‚îÇ
‚îÇ  ‚îú‚îÄ Sensor Fusion                                              ‚îÇ
‚îÇ  ‚îî‚îÄ Environment Mapping                                        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Cognition Layer                                                ‚îÇ
‚îÇ  ‚îú‚îÄ Natural Language Understanding (LLM)                       ‚îÇ
‚îÇ  ‚îú‚îÄ Task Planning & Reasoning                                  ‚îÇ
‚îÇ  ‚îú‚îÄ Behavior Selection                                         ‚îÇ
‚îÇ  ‚îî‚îÄ Decision Making                                            ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Control Layer                                                  ‚îÇ
‚îÇ  ‚îú‚îÄ Motion Planning                                            ‚îÇ
‚îÇ  ‚îú‚îÄ Trajectory Generation                                      ‚îÇ
‚îÇ  ‚îú‚îÄ Motor Control                                              ‚îÇ
‚îÇ  ‚îî‚îÄ Balance & Locomotion                                       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Safety & Monitoring Layer                                      ‚îÇ
‚îÇ  ‚îú‚îÄ Collision Avoidance                                        ‚îÇ
‚îÇ  ‚îú‚îÄ Emergency Response                                         ‚îÇ
‚îÇ  ‚îú‚îÄ System Health Monitoring                                   ‚îÇ
‚îÇ  ‚îî‚îÄ Human Safety Protocols                                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## Complete System Integration

### Main Humanoid Controller Node
```python
import rclpy
from rclpy.node import Node
from std_msgs.msg import String, Bool
from sensor_msgs.msg import Image, LaserScan, JointState
from geometry_msgs.msg import Twist, PoseStamped
from nav_msgs.msg import Odometry
from builtin_interfaces.msg import Time
import threading
import time
import json
from collections import deque

class HumanoidController(Node):
    def __init__(self):
        super().__init__('humanoid_controller')

        # Publishers
        self.cmd_vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)
        self.joint_cmd_pub = self.create_publisher(JointState, '/joint_commands', 10)
        self.goal_pub = self.create_publisher(PoseStamped, '/move_base_simple/goal', 10)
        self.status_pub = self.create_publisher(String, '/humanoid_status', 10)

        # Subscribers
        self.odom_sub = self.create_subscription(
            Odometry, '/odom', self.odom_callback, 10
        )
        self.scan_sub = self.create_subscription(
            LaserScan, '/scan', self.scan_callback, 10
        )
        self.joint_state_sub = self.create_subscription(
            JointState, '/joint_states', self.joint_state_callback, 10
        )
        self.voice_cmd_sub = self.create_subscription(
            String, '/voice_commands', self.voice_command_callback, 10
        )
        self.vision_sub = self.create_subscription(
            Image, '/camera/color/image_raw', self.vision_callback, 10
        )

        # Internal state
        self.current_pose = None
        self.current_twist = None
        self.joint_positions = {}
        self.joint_velocities = {}
        self.joint_efforts = {}
        self.latest_scan = None
        self.current_image = None
        self.voice_commands = deque(maxlen=10)
        self.system_status = "IDLE"
        self.active_behavior = "STANDBY"

        # Behavior threads
        self.behavior_thread = None
        self.is_running = True

        # Initialize subsystems
        self.vision_system = VisionSystem()
        self.vla_planner = VLAPlanner()
        self.safety_monitor = VLASafetyMonitor()

        # Create timer for main control loop
        self.control_timer = self.create_timer(0.1, self.control_loop)

        self.get_logger().info('Humanoid Controller initialized')

    def odom_callback(self, msg):
        """Update robot pose and velocity from odometry"""
        self.current_pose = msg.pose.pose
        self.current_twist = msg.twist.twist

    def scan_callback(self, msg):
        """Update laser scan data"""
        self.latest_scan = msg

    def joint_state_callback(self, msg):
        """Update joint state information"""
        for i, name in enumerate(msg.name):
            if i < len(msg.position):
                self.joint_positions[name] = msg.position[i]
            if i < len(msg.velocity):
                self.joint_velocities[name] = msg.velocity[i]
            if i < len(msg.effort):
                self.joint_efforts[name] = msg.effort[i]

    def vision_callback(self, msg):
        """Process camera images"""
        try:
            self.current_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')
        except Exception as e:
            self.get_logger().error(f'Vision processing error: {e}')

    def voice_command_callback(self, msg):
        """Process voice commands"""
        self.voice_commands.append({
            'command': msg.data,
            'timestamp': self.get_clock().now()
        })

        # Process the command
        self.process_voice_command(msg.data)

    def process_voice_command(self, command):
        """Process a voice command through the full pipeline"""
        self.get_logger().info(f'Processing voice command: {command}')

        # Update system status
        self.system_status = "PROCESSING_COMMAND"
        self.publish_status()

        # Get current environment state
        env_state = self.get_environment_state()

        # Plan the task using VLA system
        plan = self.vla_planner.create_plan(command, env_state)

        if plan:
            self.get_logger().info(f'Generated plan with {len(plan)} steps')
            # Execute the plan in a separate thread to avoid blocking
            execution_thread = threading.Thread(target=self.execute_plan, args=(plan,))
            execution_thread.start()
        else:
            self.get_logger().warn('Could not generate plan for command')
            self.system_status = "IDLE"
            self.publish_status()

    def get_environment_state(self):
        """Get complete environment state for planning"""
        env_state = {
            "robot_pose": self.current_pose,
            "robot_twist": self.current_twist,
            "joint_states": {
                "positions": dict(self.joint_positions),
                "velocities": dict(self.joint_velocities),
                "efforts": dict(self.joint_efforts)
            },
            "objects": [],
            "obstacles": [],
            "navigation_map": None
        }

        # Add vision data
        if self.current_image is not None:
            try:
                detected_objects = self.vision_system.detect_objects(self.current_image)
                env_state["objects"] = detected_objects
            except Exception as e:
                self.get_logger().error(f'Vision processing error: {e}')

        # Add obstacle information from laser scan
        if self.latest_scan:
            obstacles = self.process_scan_for_obstacles(self.latest_scan)
            env_state["obstacles"] = obstacles

        return env_state

    def process_scan_for_obstacles(self, scan_msg):
        """Process laser scan to identify obstacles"""
        obstacles = []
        min_distance = min(scan_msg.ranges) if scan_msg.ranges else float('inf')

        # Simple obstacle detection based on scan
        for i, range_val in enumerate(scan_msg.ranges):
            if 0.1 < range_val < 2.0:  # Valid range
                angle = scan_msg.angle_min + i * scan_msg.angle_increment
                x = range_val * math.cos(angle)
                y = range_val * math.sin(angle)
                obstacles.append({
                    "x": x,
                    "y": y,
                    "distance": range_val,
                    "angle": angle
                })

        return obstacles

    def execute_plan(self, plan):
        """Execute a plan in a separate thread"""
        self.active_behavior = "EXECUTING_PLAN"
        self.system_status = "BUSY"

        for i, action in enumerate(plan):
            if not self.is_running:
                break

            self.get_logger().info(f'Executing action {i+1}/{len(plan)}: {action["action"]}')

            success = self.execute_action(action)

            if not success:
                self.get_logger().error(f'Action failed: {action}')
                break

            # Small delay between actions
            time.sleep(0.1)

        self.active_behavior = "STANDBY"
        self.system_status = "IDLE"
        self.publish_status()

    def execute_action(self, action):
        """Execute a single action from the plan"""
        action_type = action.get('action', '')
        params = action.get('parameters', {})

        self.get_logger().info(f'Executing action: {action_type}')

        if action_type == 'move_to_location':
            return self.execute_move_to_location(params)
        elif action_type == 'move_arm':
            return self.execute_move_arm(params)
        elif action_type == 'pick_object':
            return self.execute_pick_object(params)
        elif action_type == 'place_object':
            return self.execute_place_object(params)
        elif action_type == 'rotate_body':
            return self.execute_rotate_body(params)
        elif action_type == 'wait':
            duration = params.get('duration', 1.0)
            time.sleep(duration)
            return True
        else:
            self.get_logger().warn(f'Unknown action: {action_type}')
            return False

    def execute_move_to_location(self, params):
        """Execute move to location action"""
        target_x = params.get('x', 0.0)
        target_y = params.get('y', 0.0)
        target_z = params.get('z', 0.0)

        # Create navigation goal
        goal = PoseStamped()
        goal.header.stamp = self.get_clock().now().to_msg()
        goal.header.frame_id = 'map'
        goal.pose.position.x = target_x
        goal.pose.position.y = target_y
        goal.pose.position.z = target_z
        goal.pose.orientation.w = 1.0

        self.goal_pub.publish(goal)
        return True

    def execute_move_arm(self, params):
        """Execute arm movement action"""
        joint_positions = params.get('joint_positions', {})

        # Create joint command message
        joint_cmd = JointState()
        joint_cmd.header.stamp = self.get_clock().now().to_msg()
        joint_cmd.name = list(joint_positions.keys())
        joint_cmd.position = list(joint_positions.values())

        self.joint_cmd_pub.publish(joint_cmd)
        return True

    def execute_pick_object(self, params):
        """Execute pick object action"""
        # In a real system, this would control the gripper
        self.get_logger().info('Executing pick object action')

        # Example: close gripper
        joint_cmd = JointState()
        joint_cmd.header.stamp = self.get_clock().now().to_msg()
        joint_cmd.name = ['gripper_joint']
        joint_cmd.position = [0.0]  # Closed position

        self.joint_cmd_pub.publish(joint_cmd)
        return True

    def execute_place_object(self, params):
        """Execute place object action"""
        self.get_logger().info('Executing place object action')

        # Example: open gripper
        joint_cmd = JointState()
        joint_cmd.header.stamp = self.get_clock().now().to_msg()
        joint_cmd.name = ['gripper_joint']
        joint_cmd.position = [0.5]  # Open position

        self.joint_cmd_pub.publish(joint_cmd)
        return True

    def execute_rotate_body(self, params):
        """Execute body rotation action"""
        angle = params.get('angle', 0.0)

        cmd = Twist()
        cmd.angular.z = angle  # Simplified - in reality would be more complex
        self.cmd_vel_pub.publish(cmd)

        time.sleep(1.0)  # Wait for rotation
        cmd.angular.z = 0.0
        self.cmd_vel_pub.publish(cmd)

        return True

    def control_loop(self):
        """Main control loop for the humanoid"""
        if not self.is_running:
            return

        # Check for safety violations
        if self.safety_monitor.safety_violation:
            self.emergency_stop()
            return

        # Update system status
        self.publish_status()

        # Process any pending voice commands
        while self.voice_commands:
            cmd_data = self.voice_commands.popleft()
            # Process command if it's recent (less than 5 seconds old)
            if (self.get_clock().now() - cmd_data['timestamp']).nanoseconds < 5e9:
                self.process_voice_command(cmd_data['command'])

    def publish_status(self):
        """Publish system status"""
        status_msg = String()
        status_msg.data = f"Status: {self.system_status}, Behavior: {self.active_behavior}"
        self.status_pub.publish(status_msg)

    def emergency_stop(self):
        """Execute emergency stop procedures"""
        self.get_logger().error('EMERGENCY STOP ACTIVATED')

        # Stop all motion
        stop_cmd = Twist()
        self.cmd_vel_pub.publish(stop_cmd)

        # Stop all joints
        joint_stop = JointState()
        joint_stop.header.stamp = self.get_clock().now().to_msg()
        joint_stop.name = list(self.joint_positions.keys())
        joint_stop.position = [0.0] * len(joint_stop.name)
        self.joint_cmd_pub.publish(joint_stop)

        # Update status
        self.system_status = "EMERGENCY_STOP"
        self.publish_status()

    def destroy_node(self):
        """Clean up before node destruction"""
        self.is_running = False
        if self.behavior_thread:
            self.behavior_thread.join(timeout=1.0)
        super().destroy_node()

def main(args=None):
    rclpy.init(args=args)
    node = HumanoidController()

    try:
        rclpy.spin(node)
    except KeyboardInterrupt:
        node.get_logger().info('Shutting down Humanoid Controller')
    finally:
        node.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()
```

## Perception-Action Loop Implementation

### Real-time Perception System
```python
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image, LaserScan, PointCloud2
from std_msgs.msg import String
from cv_bridge import CvBridge
import numpy as np
import threading
import time
from queue import Queue, Empty

class RealTimePerception(Node):
    def __init__(self):
        super().__init__('realtime_perception')

        # Publishers
        self.perception_pub = self.create_publisher(String, '/perception_output', 10)

        # Subscribers
        self.image_sub = self.create_subscription(
            Image, '/camera/color/image_raw', self.image_callback, 10
        )
        self.scan_sub = self.create_subscription(
            LaserScan, '/scan', self.scan_callback, 10
        )
        self.pc_sub = self.create_subscription(
            PointCloud2, '/camera/depth/points', self.pointcloud_callback, 10
        )

        # Internal components
        self.bridge = CvBridge()
        self.vision_system = VisionSystem()

        # Data queues for processing
        self.image_queue = Queue(maxsize=5)
        self.scan_queue = Queue(maxsize=5)
        self.pc_queue = Queue(maxsize=5)

        # Processing thread
        self.processing_thread = threading.Thread(target=self.processing_loop)
        self.processing_thread.daemon = True
        self.processing_thread.start()

        # Performance monitoring
        self.processing_times = []
        self.frame_count = 0

        self.get_logger().info('Real-time Perception System initialized')

    def image_callback(self, msg):
        """Handle incoming image messages"""
        try:
            if not self.image_queue.full():
                cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')
                timestamp = msg.header.stamp.sec + msg.header.stamp.nanosec * 1e-9
                self.image_queue.put((cv_image, timestamp))
        except Exception as e:
            self.get_logger().error(f'Image callback error: {e}')

    def scan_callback(self, msg):
        """Handle incoming laser scan messages"""
        try:
            if not self.scan_queue.full():
                timestamp = msg.header.stamp.sec + msg.header.stamp.nanosec * 1e-9
                self.scan_queue.put((msg, timestamp))
        except Exception as e:
            self.get_logger().error(f'Scan callback error: {e}')

    def pointcloud_callback(self, msg):
        """Handle incoming point cloud messages"""
        try:
            if not self.pc_queue.full():
                timestamp = msg.header.stamp.sec + msg.header.stamp.nanosec * 1e-9
                self.pc_queue.put((msg, timestamp))
        except Exception as e:
            self.get_logger().error(f'Point cloud callback error: {e}')

    def processing_loop(self):
        """Main processing loop running in separate thread"""
        while rclpy.ok():
            start_time = time.time()

            # Process latest image
            try:
                image_data, timestamp = self.image_queue.get_nowait()
                processed_result = self.process_image_data(image_data, timestamp)
                self.publish_perception_result(processed_result)
            except Empty:
                pass  # No new image to process

            # Process latest scan
            try:
                scan_data, timestamp = self.scan_queue.get_nowait()
                processed_result = self.process_scan_data(scan_data, timestamp)
                self.publish_perception_result(processed_result)
            except Empty:
                pass  # No new scan to process

            # Process latest point cloud
            try:
                pc_data, timestamp = self.pc_queue.get_nowait()
                processed_result = self.process_pointcloud_data(pc_data, timestamp)
                self.publish_perception_result(processed_result)
            except Empty:
                pass  # No new point cloud to process

            # Calculate and store processing time
            processing_time = time.time() - start_time
            self.processing_times.append(processing_time)

            # Maintain reasonable processing rate
            time.sleep(0.01)  # ~100Hz processing rate

    def process_image_data(self, image, timestamp):
        """Process image data for perception"""
        try:
            # Detect objects in the image
            objects = self.vision_system.detect_objects(image)

            # Create perception result
            result = {
                'type': 'image',
                'timestamp': timestamp,
                'objects': objects,
                'frame_count': self.frame_count
            }

            self.frame_count += 1
            return result
        except Exception as e:
            self.get_logger().error(f'Image processing error: {e}')
            return None

    def process_scan_data(self, scan_msg, timestamp):
        """Process laser scan data for perception"""
        try:
            # Extract relevant information from scan
            ranges = np.array(scan_msg.ranges)
            valid_ranges = ranges[(ranges > scan_msg.range_min) & (ranges < scan_msg.range_max)]

            # Calculate statistics
            if len(valid_ranges) > 0:
                min_distance = np.min(valid_ranges)
                avg_distance = np.mean(valid_ranges)
            else:
                min_distance = float('inf')
                avg_distance = float('inf')

            result = {
                'type': 'laser_scan',
                'timestamp': timestamp,
                'min_distance': min_distance,
                'avg_distance': avg_distance,
                'valid_points': len(valid_ranges)
            }

            return result
        except Exception as e:
            self.get_logger().error(f'Scan processing error: {e}')
            return None

    def process_pointcloud_data(self, pc_msg, timestamp):
        """Process point cloud data for 3D perception"""
        try:
            # In a real implementation, convert PointCloud2 to numpy array
            # For this example, we'll just return basic info
            result = {
                'type': 'pointcloud',
                'timestamp': timestamp,
                'point_count': len(pc_msg.data) // 16  # Approximate (assuming 16 bytes per point)
            }

            return result
        except Exception as e:
            self.get_logger().error(f'Point cloud processing error: {e}')
            return None

    def publish_perception_result(self, result):
        """Publish perception results"""
        if result:
            result_msg = String()
            result_msg.data = json.dumps(result)
            self.perception_pub.publish(result_msg)

def main(args=None):
    rclpy.init(args=args)
    node = RealTimePerception()

    try:
        rclpy.spin(node)
    except KeyboardInterrupt:
        node.get_logger().info('Shutting down Real-time Perception System')
    finally:
        node.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()
```

## Autonomous Behaviors

### Behavior Manager for Complex Actions
```python
import rclpy
from rclpy.node import Node
from std_msgs.msg import String, Bool
from geometry_msgs.msg import Twist, PoseStamped
from sensor_msgs.msg import LaserScan
from nav_msgs.msg import Odometry
import threading
import time
import math

class BehaviorManager(Node):
    def __init__(self):
        super().__init__('behavior_manager')

        # Publishers
        self.cmd_vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)
        self.goal_pub = self.create_publisher(PoseStamped, '/move_base_simple/goal', 10)
        self.behavior_status_pub = self.create_publisher(String, '/behavior_status', 10)

        # Subscribers
        self.odom_sub = self.create_subscription(
            Odometry, '/odom', self.odom_callback, 10
        )
        self.scan_sub = self.create_subscription(
            LaserScan, '/scan', self.scan_callback, 10
        )
        self.command_sub = self.create_subscription(
            String, '/behavior_commands', self.command_callback, 10
        )

        # Internal state
        self.current_pose = None
        self.current_twist = None
        self.latest_scan = None
        self.current_behavior = "IDLE"
        self.behavior_active = False
        self.behavior_thread = None

        # Register behaviors
        self.behaviors = {
            'explore': self.explore_behavior,
            'follow_wall': self.follow_wall_behavior,
            'navigate_to_goal': self.navigate_to_goal_behavior,
            'avoid_obstacles': self.avoid_obstacles_behavior,
            'patrol': self.patrol_behavior,
            'greet_human': self.greet_human_behavior
        }

        self.get_logger().info('Behavior Manager initialized')

    def odom_callback(self, msg):
        """Update robot pose and velocity"""
        self.current_pose = msg.pose.pose
        self.current_twist = msg.twist.twist

    def scan_callback(self, msg):
        """Update laser scan data"""
        self.latest_scan = msg

    def command_callback(self, msg):
        """Handle behavior commands"""
        command = msg.data
        self.get_logger().info(f'Received behavior command: {command}')

        if command in self.behaviors:
            self.start_behavior(command)
        elif command == 'stop':
            self.stop_behavior()
        else:
            self.get_logger().warn(f'Unknown behavior command: {command}')

    def start_behavior(self, behavior_name):
        """Start a specific behavior"""
        if self.behavior_active:
            self.stop_behavior()

        self.current_behavior = behavior_name
        self.behavior_active = True

        # Start behavior in separate thread
        self.behavior_thread = threading.Thread(
            target=self.behaviors[behavior_name]
        )
        self.behavior_thread.start()

        self.update_status(f'Started behavior: {behavior_name}')

    def stop_behavior(self):
        """Stop the current behavior"""
        self.behavior_active = False

        if self.behavior_thread:
            self.behavior_thread.join(timeout=1.0)

        # Stop robot motion
        stop_cmd = Twist()
        self.cmd_vel_pub.publish(stop_cmd)

        self.current_behavior = "IDLE"
        self.update_status('Behavior stopped')

    def update_status(self, status):
        """Update behavior status"""
        status_msg = String()
        status_msg.data = f'{self.current_behavior}: {status}'
        self.behavior_status_pub.publish(status_msg)

    def explore_behavior(self):
        """Random exploration behavior"""
        self.get_logger().info('Starting exploration behavior')

        while self.behavior_active:
            if not self.latest_scan:
                time.sleep(0.1)
                continue

            # Find safe direction to move
            safe_direction = self.find_safe_direction()

            if safe_direction:
                cmd = Twist()
                cmd.linear.x = 0.3  # Move forward at 0.3 m/s
                cmd.angular.z = safe_direction * 0.2  # Gentle turn
                self.cmd_vel_pub.publish(cmd)
            else:
                # Turn in place to find a clear direction
                cmd = Twist()
                cmd.angular.z = 0.5  # Turn right
                self.cmd_vel_pub.publish(cmd)

            time.sleep(0.1)

        # Stop when behavior ends
        stop_cmd = Twist()
        self.cmd_vel_pub.publish(stop_cmd)

    def follow_wall_behavior(self):
        """Wall following behavior"""
        self.get_logger().info('Starting wall following behavior')

        target_distance = 0.5  # meters from wall
        kp = 1.0  # Proportional gain

        while self.behavior_active:
            if not self.latest_scan:
                time.sleep(0.1)
                continue

            # Get distance to wall on right side (simplified)
            right_distances = self.latest_scan.ranges[270:360]  # Right side
            if right_distances:
                avg_right_distance = sum(right_distances) / len(right_distances)
                error = avg_right_distance - target_distance
                angular_vel = kp * error

                cmd = Twist()
                cmd.linear.x = 0.2  # Forward speed
                cmd.angular.z = angular_vel  # Turn to maintain distance
                self.cmd_vel_pub.publish(cmd)

            time.sleep(0.1)

        # Stop when behavior ends
        stop_cmd = Twist()
        self.cmd_vel_pub.publish(stop_cmd)

    def navigate_to_goal_behavior(self):
        """Navigate to a specific goal"""
        self.get_logger().info('Starting navigation to goal behavior')

        # In a real system, this would use a navigation stack
        # For this example, we'll simulate navigation to a fixed goal
        goal_x, goal_y = 5.0, 5.0  # Fixed goal for demonstration

        while self.behavior_active:
            if not self.current_pose:
                time.sleep(0.1)
                continue

            # Calculate direction to goal
            dx = goal_x - self.current_pose.position.x
            dy = goal_y - self.current_pose.position.y
            distance = math.sqrt(dx*dx + dy*dy)

            if distance < 0.5:  # Close enough to goal
                self.get_logger().info('Reached goal')
                break

            # Calculate angle to goal
            target_angle = math.atan2(dy, dx)
            current_yaw = self.get_yaw_from_quaternion(self.current_pose.orientation)

            # Simple proportional controller
            angle_diff = target_angle - current_yaw
            while angle_diff > math.pi:
                angle_diff -= 2 * math.pi
            while angle_diff < -math.pi:
                angle_diff += 2 * math.pi

            cmd = Twist()
            cmd.linear.x = min(0.5, distance) * 0.5  # Scale speed with distance
            cmd.angular.z = angle_diff * 1.0  # Turn toward goal

            # Check for obstacles before moving forward
            if self.latest_scan and min(self.latest_scan.ranges) < 0.5:
                cmd.linear.x = 0.0  # Stop if obstacle too close

            self.cmd_vel_pub.publish(cmd)
            time.sleep(0.1)

        # Stop when behavior ends
        stop_cmd = Twist()
        self.cmd_vel_pub.publish(stop_cmd)

    def avoid_obstacles_behavior(self):
        """Obstacle avoidance behavior"""
        self.get_logger().info('Starting obstacle avoidance behavior')

        while self.behavior_active:
            if not self.latest_scan:
                time.sleep(0.1)
                continue

            # Check for obstacles in front
            front_scan = self.latest_scan.ranges[330:30] + self.latest_scan.ranges[330:360]  # Front 60 degrees
            min_front_distance = min(front_scan) if front_scan else float('inf')

            cmd = Twist()

            if min_front_distance < 0.5:  # Obstacle detected
                # Stop and turn away from obstacle
                cmd.linear.x = 0.0
                cmd.angular.z = 0.5 if self.get_clear_side() == 'right' else -0.5
            else:
                # Move forward if clear
                cmd.linear.x = 0.3
                cmd.angular.z = 0.0

            self.cmd_vel_pub.publish(cmd)
            time.sleep(0.1)

        # Stop when behavior ends
        stop_cmd = Twist()
        self.cmd_vel_pub.publish(stop_cmd)

    def patrol_behavior(self):
        """Patrol between predefined waypoints"""
        self.get_logger().info('Starting patrol behavior')

        # Define patrol waypoints
        waypoints = [
            (2.0, 0.0),
            (2.0, 2.0),
            (0.0, 2.0),
            (0.0, 0.0)
        ]

        current_waypoint = 0

        while self.behavior_active:
            if not self.current_pose:
                time.sleep(0.1)
                continue

            # Get current waypoint
            target_x, target_y = waypoints[current_waypoint]

            # Calculate distance to waypoint
            dx = target_x - self.current_pose.position.x
            dy = target_y - self.current_pose.position.y
            distance = math.sqrt(dx*dx + dy*dy)

            if distance < 0.5:  # Reached waypoint
                current_waypoint = (current_waypoint + 1) % len(waypoints)
                self.get_logger().info(f'Reached waypoint {current_waypoint}, moving to next')
                time.sleep(1.0)  # Brief pause at waypoint
                continue

            # Navigate to waypoint
            target_angle = math.atan2(dy, dx)
            current_yaw = self.get_yaw_from_quaternion(self.current_pose.orientation)

            angle_diff = target_angle - current_yaw
            while angle_diff > math.pi:
                angle_diff -= 2 * math.pi
            while angle_diff < -math.pi:
                angle_diff += 2 * math.pi

            cmd = Twist()
            cmd.linear.x = min(0.5, distance) * 0.5
            cmd.angular.z = angle_diff * 1.0

            # Avoid obstacles
            if self.latest_scan and min(self.latest_scan.ranges) < 0.5:
                cmd.linear.x = 0.0
                cmd.angular.z = 0.5  # Turn to avoid

            self.cmd_vel_pub.publish(cmd)
            time.sleep(0.1)

    def greet_human_behavior(self):
        """Human greeting behavior"""
        self.get_logger().info('Starting human greeting behavior')

        # This would involve detecting humans, moving toward them,
        # and performing greeting actions (waving, speaking, etc.)
        # For this example, we'll just turn to face forward

        for _ in range(50):  # Run for 5 seconds
            if not self.behavior_active:
                break

            cmd = Twist()
            cmd.angular.z = 0.0  # Face forward
            self.cmd_vel_pub.publish(cmd)
            time.sleep(0.1)

    def find_safe_direction(self):
        """Find a safe direction to move based on scan data"""
        if not self.latest_scan:
            return 0.0

        # Divide scan into sectors
        sector_size = len(self.latest_scan.ranges) // 8  # 8 sectors
        sectors = []

        for i in range(8):
            start_idx = i * sector_size
            end_idx = min((i + 1) * sector_size, len(self.latest_scan.ranges))
            sector_ranges = self.latest_scan.ranges[start_idx:end_idx]
            avg_distance = sum(sector_ranges) / len(sector_ranges) if sector_ranges else 0
            sectors.append(avg_distance)

        # Find the sector with maximum average distance (safest)
        max_idx = sectors.index(max(sectors))

        # Convert sector index to angle (-œÄ to œÄ)
        angle = (max_idx * 2 * math.pi / 8) - math.pi
        return angle

    def get_clear_side(self):
        """Determine which side is clearer for obstacle avoidance"""
        if not self.latest_scan:
            return 'right'

        left_distances = self.latest_scan.ranges[90:180]
        right_distances = self.latest_scan.ranges[180:270]

        left_avg = sum(left_distances) / len(left_distances) if left_distances else 0
        right_avg = sum(right_distances) / len(right_distances) if right_distances else 0

        return 'left' if left_avg > right_avg else 'right'

    def get_yaw_from_quaternion(self, q):
        """Convert quaternion to yaw angle"""
        siny_cosp = 2 * (q.w * q.z + q.x * q.y)
        cosy_cosp = 1 - 2 * (q.y * q.y + q.z * q.z)
        return math.atan2(siny_cosp, cosy_cosp)

def main(args=None):
    rclpy.init(args=args)
    node = BehaviorManager()

    try:
        rclpy.spin(node)
    except KeyboardInterrupt:
        node.get_logger().info('Shutting down Behavior Manager')
    finally:
        node.stop_behavior()
        node.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()
```

## System Testing and Validation

### Comprehensive Testing Framework
```python
import unittest
import rclpy
from rclpy.node import Node
from std_msgs.msg import String
from geometry_msgs.msg import Twist
from sensor_msgs.msg import LaserScan
import threading
import time

class HumanoidSystemTester(Node):
    def __init__(self):
        super().__init__('humanoid_system_tester')

        # Publishers for testing
        self.test_cmd_pub = self.create_publisher(String, '/test_commands', 10)
        self.test_vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)

        # Subscribers for monitoring
        self.status_sub = self.create_subscription(
            String, '/humanoid_status', self.status_callback, 10
        )
        self.test_result_pub = self.create_publisher(String, '/test_results', 10)

        # Internal state
        self.current_status = ""
        self.test_results = []
        self.test_active = False

        self.get_logger().info('Humanoid System Tester initialized')

    def status_callback(self, msg):
        """Update current status"""
        self.current_status = msg.data

    def run_comprehensive_tests(self):
        """Run all system tests"""
        self.get_logger().info('Starting comprehensive system tests')

        tests = [
            self.test_perception_system,
            self.test_navigation_system,
            self.test_vla_integration,
            self.test_safety_system,
            self.test_behavior_manager
        ]

        results = {}
        for test_func in tests:
            test_name = test_func.__name__
            self.get_logger().info(f'Running test: {test_name}')

            try:
                result = test_func()
                results[test_name] = result
                self.get_logger().info(f'Test {test_name}: {result}')
            except Exception as e:
                results[test_name] = f'FAILED: {str(e)}'
                self.get_logger().error(f'Test {test_name} failed: {str(e)}')

        # Publish summary
        summary = json.dumps(results, indent=2)
        result_msg = String()
        result_msg.data = f"Test Summary:\n{summary}"
        self.test_result_pub.publish(result_msg)

        return results

    def test_perception_system(self):
        """Test perception system functionality"""
        # This would involve checking if perception topics are publishing
        # For this example, we'll simulate the test
        time.sleep(1.0)  # Allow some time for system to stabilize

        # Check if we're receiving perception data
        # In a real test, you'd verify data is being published
        return "PASSED"  # Simulated result

    def test_navigation_system(self):
        """Test navigation system"""
        # Send a simple navigation command
        goal = PoseStamped()
        goal.header.frame_id = 'map'
        goal.pose.position.x = 1.0
        goal.pose.position.y = 1.0
        goal.pose.orientation.w = 1.0

        # In a real test, you'd verify the robot reaches the goal
        return "PASSED"  # Simulated result

    def test_vla_integration(self):
        """Test Vision-Language-Action integration"""
        # Send a voice command through the system
        cmd_msg = String()
        cmd_msg.data = "move forward 1 meter"

        # In a real test, you'd verify the command is processed correctly
        return "PASSED"  # Simulated result

    def test_safety_system(self):
        """Test safety system"""
        # This would involve triggering safety conditions
        # and verifying the system responds appropriately
        return "PASSED"  # Simulated result

    def test_behavior_manager(self):
        """Test behavior manager"""
        # Send behavior commands and verify they execute
        behavior_cmd = String()
        behavior_cmd.data = "explore"

        # In a real test, you'd verify the behavior executes
        return "PASSED"  # Simulated result

def main(args=None):
    rclpy.init(args=args)
    tester = HumanoidSystemTester()

    # Run tests in a separate thread to allow ROS spinning
    def run_tests():
        time.sleep(2.0)  # Wait for system to initialize
        results = tester.run_comprehensive_tests()
        tester.get_logger().info(f'All tests completed: {results}')

    test_thread = threading.Thread(target=run_tests)
    test_thread.start()

    try:
        rclpy.spin(tester)
    except KeyboardInterrupt:
        tester.get_logger().info('Shutting down Humanoid System Tester')
    finally:
        test_thread.join(timeout=2.0)
        tester.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()
```

## Launch File for Complete System

### Complete System Launch
```xml
<!-- launch/humanoid_system.launch.py -->
from launch import LaunchDescription
from launch.actions import DeclareLaunchArgument, IncludeLaunchDescription, TimerAction
from launch.launch_description_sources import PythonLaunchDescriptionSource
from launch.substitutions import PathJoinSubstitution, LaunchConfiguration
from launch_ros.actions import Node
from launch_ros.substitutions import FindPackageShare

def generate_launch_description():
    # Launch arguments
    use_sim_time = LaunchConfiguration('use_sim_time', default='true')

    # Include Gazebo launch (if using simulation)
    gazebo = IncludeLaunchDescription(
        PythonLaunchDescriptionSource([
            PathJoinSubstitution([
                FindPackageShare('gazebo_ros'),
                'launch',
                'gazebo.launch.py'
            ])
        ])
    )

    # Humanoid controller node
    humanoid_controller = Node(
        package='my_humanoid_package',
        executable='humanoid_controller',
        name='humanoid_controller',
        parameters=[{'use_sim_time': use_sim_time}],
        output='screen'
    )

    # Real-time perception node
    perception_node = Node(
        package='my_humanoid_package',
        executable='realtime_perception',
        name='realtime_perception',
        parameters=[{'use_sim_time': use_sim_time}],
        output='screen'
    )

    # Behavior manager node
    behavior_manager = Node(
        package='my_humanoid_package',
        executable='behavior_manager',
        name='behavior_manager',
        parameters=[{'use_sim_time': use_sim_time}],
        output='screen'
    )

    # VLA execution node
    vla_node = Node(
        package='my_humanoid_package',
        executable='vla_execution',
        name='vla_execution',
        parameters=[{'use_sim_time': use_sim_time}],
        output='screen'
    )

    # Safety monitor node
    safety_monitor = Node(
        package='my_humanoid_package',
        executable='safety_monitor',
        name='safety_monitor',
        parameters=[{'use_sim_time': use_sim_time}],
        output='screen'
    )

    # Voice recognition node (if using Whisper)
    voice_recognition = Node(
        package='my_humanoid_package',
        executable='voice_recognition',
        name='voice_recognition',
        parameters=[{'use_sim_time': use_sim_time}],
        output='screen'
    )

    # Navigation stack (Nav2)
    navigation = IncludeLaunchDescription(
        PythonLaunchDescriptionSource([
            PathJoinSubstitution([
                FindPackageShare('nav2_bringup'),
                'launch',
                'navigation_launch.py'
            ])
        ]),
        launch_arguments={
            'use_sim_time': use_sim_time
        }.items()
    )

    # Return launch description with all components
    return LaunchDescription([
        DeclareLaunchArgument(
            'use_sim_time',
            default_value='true',
            description='Use simulation (Gazebo) clock if true'
        ),
        TimerAction(
            period=1.0,
            actions=[gazebo]
        ),
        TimerAction(
            period=3.0,
            actions=[humanoid_controller]
        ),
        TimerAction(
            period=4.0,
            actions=[perception_node]
        ),
        TimerAction(
            period=5.0,
            actions=[behavior_manager]
        ),
        TimerAction(
            period=6.0,
            actions=[vla_node]
        ),
        TimerAction(
            period=7.0,
            actions=[safety_monitor]
        ),
        TimerAction(
            period=8.0,
            actions=[voice_recognition]
        ),
        TimerAction(
            period=10.0,
            actions=[navigation]
        )
    ])
```

## Weekly Exercises

### Exercise 1: System Integration
1. Integrate all components (ROS 2, Gazebo, Isaac, VLA) into a single system
2. Test communication between all subsystems
3. Verify that data flows correctly between components
4. Debug any integration issues

### Exercise 2: Perception-Action Loop
1. Implement a real-time perception system
2. Create feedback loops between perception and action
3. Test system response time and accuracy
4. Optimize for real-time performance

### Exercise 3: Autonomous Behaviors
1. Implement multiple autonomous behaviors
2. Create behavior switching logic
3. Test behaviors in various scenarios
4. Implement safety checks for each behavior

### Mini-Project: Complete Autonomous Humanoid
Create a complete autonomous humanoid system with:
- Integrated perception, planning, and control
- Multiple autonomous behaviors
- Voice and vision-based interaction
- Safety monitoring and emergency response
- Comprehensive testing framework

```python
# Complete Autonomous Humanoid System
import rclpy
from rclpy.node import Node
from std_msgs.msg import String, Bool
from sensor_msgs.msg import Image, LaserScan, JointState
from geometry_msgs.msg import Twist, PoseStamped
from nav_msgs.msg import Odometry
import threading
import time
import json
from collections import deque

class CompleteAutonomousHumanoid(Node):
    def __init__(self):
        super().__init__('complete_autonomous_humanoid')

        # Publishers
        self.cmd_vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)
        self.joint_cmd_pub = self.create_publisher(JointState, '/joint_commands', 10)
        self.goal_pub = self.create_publisher(PoseStamped, '/move_base_simple/goal', 10)
        self.status_pub = self.create_publisher(String, '/complete_system_status', 10)
        self.emergency_pub = self.create_publisher(Bool, '/emergency_stop', 10)

        # Subscribers
        self.odom_sub = self.create_subscription(
            Odometry, '/odom', self.odom_callback, 10
        )
        self.scan_sub = self.create_subscription(
            LaserScan, '/scan', self.scan_callback, 10
        )
        self.joint_state_sub = self.create_subscription(
            JointState, '/joint_states', self.joint_state_callback, 10
        )
        self.voice_cmd_sub = self.create_subscription(
            String, '/voice_commands', self.voice_command_callback, 10
        )
        self.vision_sub = self.create_subscription(
            Image, '/camera/color/image_raw', self.vision_callback, 10
        )

        # Internal state
        self.current_pose = None
        self.current_twist = None
        self.joint_positions = {}
        self.joint_velocities = {}
        self.joint_efforts = {}
        self.latest_scan = None
        self.current_image = None
        self.voice_commands = deque(maxlen=10)
        self.system_status = "BOOTING"
        self.active_behavior = "SYSTEM_CHECK"
        self.emergency_stop = False
        self.is_operational = False

        # Initialize all subsystems
        self.vision_system = VisionSystem()
        self.vla_planner = VLAPlanner()
        self.safety_monitor = VLASafetyMonitor()
        self.behavior_manager = BehaviorManager()

        # Control loop timer
        self.control_timer = self.create_timer(0.1, self.main_control_loop)

        # Start system initialization
        self.init_timer = self.create_timer(2.0, self.initialize_system)

        self.get_logger().info('Complete Autonomous Humanoid System initialized')

    def initialize_system(self):
        """Initialize the complete system"""
        self.get_logger().info('Initializing complete autonomous humanoid system...')

        # Initialize all components
        init_success = True

        if init_success:
            self.system_status = "READY"
            self.is_operational = True
            self.active_behavior = "IDLE"
            self.get_logger().info('System initialization complete')
        else:
            self.system_status = "INITIALIZATION_FAILED"
            self.get_logger().error('System initialization failed')

        self.publish_status()

    def main_control_loop(self):
        """Main control loop for the complete system"""
        if not self.is_operational or self.emergency_stop:
            return

        # Monitor system health
        self.monitor_system_health()

        # Process incoming commands
        self.process_incoming_commands()

        # Execute active behavior
        self.execute_active_behavior()

        # Publish system status
        self.publish_status()

    def monitor_system_health(self):
        """Monitor overall system health"""
        # Check for safety violations
        if hasattr(self.safety_monitor, 'safety_violation') and self.safety_monitor.safety_violation:
            self.trigger_emergency_stop("Safety violation detected")

        # Check component health
        # (In a real system, you'd check if all nodes are responding)

    def process_incoming_commands(self):
        """Process incoming voice and other commands"""
        # Process voice commands
        while self.voice_commands:
            cmd_data = self.voice_commands.popleft()
            # Process command if it's recent (less than 5 seconds old)
            if (self.get_clock().now() - cmd_data['timestamp']).nanoseconds < 5e9:
                self.process_voice_command(cmd_data['command'])

    def execute_active_behavior(self):
        """Execute the currently active behavior"""
        # In a real system, this would manage the active behavior
        pass

    def publish_status(self):
        """Publish comprehensive system status"""
        status_data = {
            'system_status': self.system_status,
            'active_behavior': self.active_behavior,
            'operational': self.is_operational,
            'emergency_stop': self.emergency_stop,
            'timestamp': self.get_clock().now().nanoseconds
        }

        status_msg = String()
        status_msg.data = json.dumps(status_data)
        self.status_pub.publish(status_msg)

    def trigger_emergency_stop(self, reason="Unknown"):
        """Trigger emergency stop procedure"""
        self.get_logger().error(f'EMERGENCY STOP: {reason}')

        self.emergency_stop = True
        self.system_status = "EMERGENCY_STOP"

        # Stop all motion
        stop_cmd = Twist()
        self.cmd_vel_pub.publish(stop_cmd)

        # Stop all joints
        joint_stop = JointState()
        joint_stop.header.stamp = self.get_clock().now().to_msg()
        joint_stop.name = list(self.joint_positions.keys())
        joint_stop.position = [0.0] * len(joint_stop.name)
        self.joint_cmd_pub.publish(joint_stop)

        # Publish emergency stop signal
        emergency_msg = Bool()
        emergency_msg.data = True
        self.emergency_pub.publish(emergency_msg)

        self.publish_status()

    def reset_emergency_stop(self):
        """Reset emergency stop state"""
        self.emergency_stop = False
        self.system_status = "READY"
        self.is_operational = True

        emergency_msg = Bool()
        emergency_msg.data = False
        self.emergency_pub.publish(emergency_msg)

        self.publish_status()

def main(args=None):
    rclpy.init(args=args)
    node = CompleteAutonomousHumanoid()

    try:
        rclpy.spin(node)
    except KeyboardInterrupt:
        node.get_logger().info('Shutting down Complete Autonomous Humanoid System')
    finally:
        node.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()
```

## Summary

This capstone project integrates all the technologies covered in the previous chapters to create a complete autonomous humanoid robot system. The system combines:

1. **ROS 2** for communication and coordination between components
2. **Gazebo** for simulation and testing
3. **NVIDIA Isaac** for AI-powered perception and navigation
4. **Vision-Language-Action (VLA)** systems for natural interaction

The complete system architecture includes:
- Real-time perception and environment understanding
- Natural language processing and task planning
- Autonomous behavior execution
- Safety monitoring and emergency response
- Comprehensive testing and validation

This project demonstrates the practical application of all the concepts learned throughout the book, showing how to combine individual technologies into a cohesive, functional autonomous robot system. The modular design allows for easy extension and modification, making it suitable for various robotic applications.

Students should now have a comprehensive understanding of modern robotics technologies and how to integrate them into complex autonomous systems.

============================================================
FILE: book\docs\gazebo.md
============================================================
---
sidebar_position: 3
title: "Robot Simulation (Gazebo & Unity)"
---

# Robot Simulation (Gazebo & Unity)

## Weekly Plan
- Day 1-2: Understanding Gazebo physics simulation and world creation
- Day 3-4: Creating robot models with URDF/SDF and sensors
- Day 5-7: Advanced simulation features and Unity integration

## Learning Objectives
By the end of this chapter, you will:
- Create and configure robot models in URDF/SDF formats
- Set up physics-based simulations with realistic dynamics
- Integrate sensors into simulated robots
- Use Unity for advanced visualization and simulation
- Connect simulated robots to ROS 2 systems

## Gazebo Simulation Overview

Gazebo is a physics-based simulation environment that provides realistic dynamics, sensor simulation, and rendering capabilities. It's essential for testing robotic systems before deployment on real hardware.

### Key Features
- Physics engine with collision detection
- Sensor simulation (cameras, LIDAR, IMU, etc.)
- Realistic rendering and visualization
- ROS 2 integration through gazebo_ros_pkgs
- Plugin architecture for custom behaviors

## Creating Robot Models with URDF

URDF (Unified Robot Description Format) defines the physical and kinematic properties of robots:

```xml
<!-- my_robot.urdf -->
<?xml version="1.0"?>
<robot name="my_robot">
  <!-- Base link -->
  <link name="base_link">
    <visual>
      <geometry>
        <box size="0.5 0.3 0.2"/>
      </geometry>
      <material name="blue">
        <color rgba="0 0 1 1"/>
      </material>
    </visual>
    <collision>
      <geometry>
        <box size="0.5 0.3 0.2"/>
      </geometry>
    </collision>
    <inertial>
      <mass value="1.0"/>
      <inertia ixx="0.1" ixy="0" ixz="0" iyy="0.1" iyz="0" izz="0.1"/>
    </inertial>
  </link>

  <!-- Wheel links -->
  <link name="wheel_left">
    <visual>
      <geometry>
        <cylinder radius="0.1" length="0.05"/>
      </geometry>
      <material name="black">
        <color rgba="0 0 0 1"/>
      </material>
    </visual>
    <collision>
      <geometry>
        <cylinder radius="0.1" length="0.05"/>
      </geometry>
    </collision>
    <inertial>
      <mass value="0.2"/>
      <inertia ixx="0.001" ixy="0" ixz="0" iyy="0.001" iyz="0" izz="0.002"/>
    </inertial>
  </link>

  <!-- Joints -->
  <joint name="wheel_left_joint" type="continuous">
    <parent link="base_link"/>
    <child link="wheel_left"/>
    <origin xyz="0.15 -0.2 0" rpy="0 0 0"/>
    <axis xyz="0 1 0"/>
  </joint>

  <!-- Differential drive controller -->
  <gazebo>
    <plugin name="diff_drive" filename="libgazebo_ros_diff_drive.so">
      <left_joint>wheel_left_joint</left_joint>
      <right_joint>wheel_right_joint</right_joint>
      <wheel_separation>0.4</wheel_separation>
      <wheel_diameter>0.2</wheel_diameter>
      <command_topic>cmd_vel</command_topic>
      <odometry_topic>odom</odometry_topic>
      <odometry_frame>odom</odometry_frame>
      <robot_base_frame>base_link</robot_base_frame>
    </plugin>
  </gazebo>
</robot>
```

## Creating World Files

World files define the environment for simulation:

```xml
<!-- simple_room.world -->
<?xml version="1.0" ?>
<sdf version="1.7">
  <world name="simple_room">
    <include>
      <uri>model://ground_plane</uri>
    </include>
    <include>
      <uri>model://sun</uri>
    </include>

    <!-- Walls -->
    <model name="wall_1">
      <pose>0 5 1 0 0 0</pose>
      <link name="link">
        <collision name="collision">
          <geometry>
            <box>
              <size>10 0.2 2</size>
            </box>
          </geometry>
        </collision>
        <visual name="visual">
          <geometry>
            <box>
              <size>10 0.2 2</size>
            </box>
          </geometry>
          <material>
            <ambient>0.8 0.8 0.8 1</ambient>
            <diffuse>0.8 0.8 0.8 1</diffuse>
          </material>
        </visual>
      </link>
    </model>

    <!-- Furniture -->
    <model name="table">
      <pose>2 0 0.5 0 0 0</pose>
      <link name="table_base">
        <collision name="collision">
          <geometry>
            <box>
              <size>1 0.8 0.8</size>
            </box>
          </geometry>
        </collision>
        <visual name="visual">
          <geometry>
            <box>
              <size>1 0.8 0.8</size>
            </box>
          </geometry>
          <material>
            <ambient>0.6 0.4 0.2 1</ambient>
            <diffuse>0.6 0.4 0.2 1</diffuse>
          </material>
        </visual>
      </link>
    </model>
  </world>
</sdf>
```

## Adding Sensors to Robots

### Camera Sensor
```xml
<gazebo reference="camera_link">
  <sensor type="camera" name="camera1">
    <update_rate>30.0</update_rate>
    <camera name="head">
      <horizontal_fov>1.3962634</horizontal_fov>
      <image>
        <width>800</width>
        <height>600</height>
        <format>R8G8B8</format>
      </image>
      <clip>
        <near>0.1</near>
        <far>100</far>
      </clip>
    </camera>
    <plugin name="camera_controller" filename="libgazebo_ros_camera.so">
      <frame_name>camera_link</frame_name>
      <topic_name>image_raw</topic_name>
    </plugin>
  </sensor>
</gazebo>
```

### LIDAR Sensor
```xml
<gazebo reference="lidar_link">
  <sensor type="ray" name="lidar_sensor">
    <pose>0 0 0 0 0 0</pose>
    <visualize>true</visualize>
    <update_rate>10</update_rate>
    <ray>
      <scan>
        <horizontal>
          <samples>360</samples>
          <resolution>1.0</resolution>
          <min_angle>-3.14159</min_angle>
          <max_angle>3.14159</max_angle>
        </horizontal>
      </scan>
      <range>
        <min>0.1</min>
        <max>30.0</max>
        <resolution>0.01</resolution>
      </range>
    </ray>
    <plugin name="lidar_controller" filename="libgazebo_ros_laser.so">
      <topic_name>scan</topic_name>
      <frame_name>lidar_link</frame_name>
    </plugin>
  </sensor>
</gazebo>
```

## Gazebo-ROS 2 Integration

### Launching Simulation with ROS 2
```python
# launch_simulation.py
from launch import LaunchDescription
from launch.actions import IncludeLaunchDescription
from launch.launch_description_sources import PythonLaunchDescriptionSource
from launch.substitutions import PathJoinSubstitution
from launch_ros.actions import Node
from launch_ros.substitutions import FindPackageShare

def generate_launch_description():
    # Launch Gazebo
    gazebo = IncludeLaunchDescription(
        PythonLaunchDescriptionSource([
            PathJoinSubstitution([
                FindPackageShare('gazebo_ros'),
                'launch',
                'gazebo.launch.py'
            ])
        ]),
        launch_arguments={
            'world': PathJoinSubstitution([
                FindPackageShare('my_robot_description'),
                'worlds',
                'simple_room.world'
            ])
        }.items()
    )

    # Spawn robot in Gazebo
    spawn_entity = Node(
        package='gazebo_ros',
        executable='spawn_entity.py',
        arguments=[
            '-topic', 'robot_description',
            '-entity', 'my_robot',
            '-x', '0',
            '-y', '0',
            '-z', '0.2'
        ],
        output='screen'
    )

    # Robot state publisher
    robot_state_publisher = Node(
        package='robot_state_publisher',
        executable='robot_state_publisher',
        output='screen',
        parameters=[{
            'robot_description': PathJoinSubstitution([
                FindPackageShare('my_robot_description'),
                'urdf',
                'my_robot.urdf'
            ])
        }]
    )

    return LaunchDescription([
        gazebo,
        spawn_entity,
        robot_state_publisher
    ])
```

## Physics Configuration

### Physics Parameters
```xml
<!-- In world file -->
<physics type="ode">
  <max_step_size>0.001</max_step_size>
  <real_time_factor>1.0</real_time_factor>
  <real_time_update_rate>1000.0</real_time_update_rate>
  <gravity>0 0 -9.8</gravity>
  <ode>
    <solver>
      <type>quick</type>
      <iters>10</iters>
      <sor>1.3</sor>
    </solver>
    <constraints>
      <cfm>0.0</cfm>
      <erp>0.2</erp>
      <contact_max_correcting_vel>100.0</contact_max_correcting_vel>
      <contact_surface_layer>0.001</contact_surface_layer>
    </constraints>
  </ode>
</physics>
```

## Unity Integration for Advanced Visualization

Unity can be used for more advanced visualization and simulation:

### Setting up Unity Robotics
```csharp
// UnityRobotController.cs
using UnityEngine;
using Unity.Robotics.ROSTCPConnector;
using RosMessageTypes.Geometry;

public class UnityRobotController : MonoBehaviour
{
    ROSConnection ros;
    string robotTopic = "unity_robot_pose";

    // Start is called before the first frame update
    void Start()
    {
        ros = ROSConnection.instance;
        InvokeRepeating("SendRobotPose", 0.1f, 0.1f);
    }

    void SendRobotPose()
    {
        // Create and publish robot pose message
        PoseMsg pose = new PoseMsg();
        pose.position.x = transform.position.x;
        pose.position.y = transform.position.y;
        pose.position.z = transform.position.z;

        pose.orientation.x = transform.rotation.x;
        pose.orientation.y = transform.rotation.y;
        pose.orientation.z = transform.rotation.z;
        pose.orientation.w = transform.rotation.w;

        ros.Publish(robotTopic, pose);
    }

    void OnMessageReceived(PoseMsg pose)
    {
        // Update robot position based on ROS message
        transform.position = new Vector3(
            (float)pose.position.x,
            (float)pose.position.y,
            (float)pose.position.z
        );

        transform.rotation = new Quaternion(
            (float)pose.orientation.x,
            (float)pose.orientation.y,
            (float)pose.orientation.z,
            (float)pose.orientation.w
        );
    }
}
```

## Simulation Best Practices

### Performance Optimization
```python
# Efficient simulation node
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import LaserScan
from geometry_msgs.msg import Twist

class SimulationOptimizedNode(Node):
    def __init__(self):
        super().__init__('simulation_optimized')

        # Use appropriate QoS for simulation
        from rclpy.qos import QoSProfile, ReliabilityPolicy
        qos = QoSProfile(
            depth=1,
            reliability=ReliabilityPolicy.BEST_EFFORT
        )

        self.scan_sub = self.create_subscription(
            LaserScan,
            'scan',
            self.scan_callback,
            qos
        )

        self.cmd_pub = self.create_publisher(
            Twist,
            'cmd_vel',
            10
        )

        # Throttle processing
        self.processing_rate = 10  # Hz
        self.timer = self.create_timer(
            1.0 / self.processing_rate,
            self.process_scan
        )

        self.latest_scan = None
        self.obstacle_detected = False

    def scan_callback(self, msg):
        self.latest_scan = msg

    def process_scan(self):
        if self.latest_scan is None:
            return

        # Simple obstacle detection
        min_range = min(self.latest_scan.ranges)
        self.obstacle_detected = min_range < 1.0  # 1 meter threshold

        # Send command based on obstacle detection
        cmd = Twist()
        if self.obstacle_detected:
            cmd.linear.x = 0.0
            cmd.angular.z = 0.5  # Turn
        else:
            cmd.linear.x = 0.5  # Forward
            cmd.angular.z = 0.0

        self.cmd_pub.publish(cmd)

def main(args=None):
    rclpy.init(args=args)
    node = SimulationOptimizedNode()
    rclpy.spin(node)
    node.destroy_node()
    rclpy.shutdown()

if __name__ == '__main__':
    main()
```

## Weekly Exercises

### Exercise 1: Robot Model Creation
1. Create a URDF model of a simple differential drive robot
2. Add visual and collision properties
3. Include a differential drive controller plugin
4. Test the model in Gazebo

### Exercise 2: Sensor Integration
1. Add a camera sensor to your robot model
2. Add a LIDAR sensor to your robot model
3. Verify that sensor data is published to ROS topics
4. Visualize the sensor data in RViz

### Exercise 3: World Creation
1. Create a custom world file with obstacles
2. Add furniture and navigation challenges
3. Test your robot's navigation in the custom world
4. Adjust physics parameters for realistic behavior

### Mini-Project: Autonomous Navigation Simulation
Create a complete simulation environment with:
- Custom robot model with sensors
- Complex world with obstacles
- Navigation stack integration
- Autonomous navigation demonstration

```python
# navigation_simulation.py
import rclpy
from rclpy.node import Node
from geometry_msgs.msg import Twist, PoseStamped
from sensor_msgs.msg import LaserScan
from nav_msgs.msg import Odometry
import math

class NavigationSimulator(Node):
    def __init__(self):
        super().__init__('navigation_simulator')

        # Publishers and subscribers
        self.cmd_pub = self.create_publisher(Twist, 'cmd_vel', 10)
        self.scan_sub = self.create_subscription(
            LaserScan, 'scan', self.scan_callback, 10
        )
        self.odom_sub = self.create_subscription(
            Odometry, 'odom', self.odom_callback, 10
        )

        # Navigation parameters
        self.target_x = 5.0
        self.target_y = 5.0
        self.current_x = 0.0
        self.current_y = 0.0
        self.current_yaw = 0.0

        # Control loop
        self.timer = self.create_timer(0.1, self.navigation_loop)

        self.get_logger().info('Navigation simulator started')

    def odom_callback(self, msg):
        self.current_x = msg.pose.pose.position.x
        self.current_y = msg.pose.pose.position.y

        # Convert quaternion to yaw
        q = msg.pose.pose.orientation
        self.current_yaw = math.atan2(2.0 * (q.w * q.z + q.x * q.y),
                                     1.0 - 2.0 * (q.y * q.y + q.z * q.z))

    def scan_callback(self, msg):
        # Store latest scan for navigation
        self.latest_scan = msg

    def navigation_loop(self):
        if not hasattr(self, 'latest_scan'):
            return

        # Calculate distance to target
        dx = self.target_x - self.current_x
        dy = self.target_y - self.current_y
        distance = math.sqrt(dx*dx + dy*dy)

        # Calculate target angle
        target_angle = math.atan2(dy, dx)
        angle_diff = target_angle - self.current_yaw

        # Normalize angle
        while angle_diff > math.pi:
            angle_diff -= 2 * math.pi
        while angle_diff < -math.pi:
            angle_diff += 2 * math.pi

        # Simple navigation controller
        cmd = Twist()

        # Check for obstacles
        min_range = min(self.latest_scan.ranges) if self.latest_scan.ranges else float('inf')

        if min_range < 0.5:  # Obstacle too close
            cmd.linear.x = 0.0
            cmd.angular.z = 0.5  # Turn to avoid
        elif abs(angle_diff) > 0.1:  # Need to turn
            cmd.angular.z = max(-0.5, min(0.5, angle_diff * 2.0))
        else:  # Move forward if not too close to target
            cmd.linear.x = 0.5 if distance > 0.5 else 0.0

        self.cmd_pub.publish(cmd)

def main(args=None):
    rclpy.init(args=args)
    node = NavigationSimulator()
    rclpy.spin(node)
    node.destroy_node()
    rclpy.shutdown()

if __name__ == '__main__':
    main()
```

## Summary
This chapter covered robot simulation using Gazebo and Unity, including URDF/SDF model creation, sensor integration, and physics configuration. You've learned how to create realistic simulation environments for testing robotic systems. The next chapter will explore the NVIDIA Isaac platform for AI-powered robotics.

============================================================
FILE: book\docs\index.md
============================================================
---
sidebar_position: 0
title: "Physical AI & Humanoid Robotics"
---

# Physical AI & Humanoid Robotics

Welcome to the comprehensive guide on Physical AI and Humanoid Robotics. This book covers everything from fundamental concepts to advanced implementations using modern robotics technologies.

## Book Structure

This book is organized into several comprehensive sections:

### Part I: Foundations of Physical AI and Robotics
- Introduction to Physical AI & Embodied Intelligence
- Robot Operating System (ROS 2) Fundamentals
- Robot Modeling and Simulation Fundamentals

### Part II: Perception and Understanding
- Sensor Integration and Data Processing
- Computer Vision for Robotics
- 3D Perception and Scene Understanding

### Part III: Motion and Control
- Kinematics and Dynamics
- Locomotion and Balance Control
- Motion Planning and Navigation

### Part IV: Intelligence and Learning
- Reinforcement Learning for Robotics
- Imitation Learning and VLA
- Human-Robot Interaction

### Part V: Integration and Applications
- Multi-Robot Systems and Coordination
- Real-World Deployment and Safety
- Advanced Topics and Future Directions

## VLA Book Series

Additionally, this repository contains a specialized series on Vision-Language-Action systems:

- Introduction to Physical AI & Humanoid Robotics
- ROS 2 Fundamentals
- Robot Simulation (Gazebo & Unity)
- NVIDIA Isaac AI Brain
- Vision-Language-Action (VLA)
- Capstone Project: Autonomous Humanoid

## Getting Started

To get started with this book:

1. Set up your development environment with ROS 2
2. Install required dependencies (Gazebo, Isaac Sim, etc.)
3. Work through the chapters sequentially
4. Practice with the provided code examples
5. Complete the exercises and mini-projects

Each chapter includes practical examples, code snippets, and exercises to reinforce your learning. The capstone project integrates all concepts into a complete autonomous humanoid robot system.

## Prerequisites

- Basic programming knowledge (Python)
- Understanding of linear algebra and calculus
- Familiarity with robotics concepts is helpful but not required

============================================================
FILE: book\docs\intro.md
============================================================
---
sidebar_position: 1
title: "Introduction to Physical AI & Humanoid Robotics"
---

# Introduction to Physical AI & Humanoid Robotics

## Weekly Plan
- Day 1-2: Understanding Physical AI concepts and humanoid robotics
- Day 3-4: Overview of key technologies (ROS 2, Gazebo, Isaac, VLA)
- Day 5-7: Setting up development environment and first simulation

## Learning Objectives
By the end of this chapter, you will:
- Understand the fundamentals of Physical AI and its applications
- Recognize the key challenges in humanoid robotics
- Identify the essential technologies used in modern robotics
- Set up your development environment for robotics programming

## What is Physical AI?

Physical AI represents the convergence of artificial intelligence with the physical world through robotic systems. Unlike traditional AI that operates primarily in digital spaces, Physical AI involves embodied intelligence that can perceive, reason, and act in real-world environments.

### Key Characteristics of Physical AI
- **Embodiment**: Intelligence is grounded in physical form and interaction
- **Real-time Processing**: Systems must respond to dynamic environments
- **Multi-sensory Integration**: Combining vision, touch, hearing, and other modalities
- **Physical Constraints**: Operating within laws of physics and mechanical limitations

### Humanoid Robotics
Humanoid robots are designed to resemble and interact with humans in human environments. They offer unique advantages:
- Natural human-robot interaction
- Compatibility with human-designed spaces
- Potential for human-like manipulation and locomotion

## Key Technologies Overview

### Robot Operating System (ROS 2)
ROS 2 is the middleware that enables communication between different components of a robotic system. It provides:
- Message passing between processes
- Hardware abstraction
- Device drivers
- Libraries for common robot functionality

### Simulation Platforms
Simulation is crucial for robotics development:
- **Gazebo**: Physics-based simulation environment
- **Unity**: Advanced visualization and simulation
- **Isaac Sim**: NVIDIA's high-fidelity simulation platform

### NVIDIA Isaac Platform
NVIDIA's robotics platform combines:
- GPU-accelerated computing
- AI and deep learning capabilities
- Simulation and real-world deployment tools

### Vision-Language-Action (VLA) Systems
VLA systems enable robots to understand and execute complex commands:
- Natural language processing
- Computer vision
- Action planning and execution

## Setting Up Your Environment

### Prerequisites
```bash
# Install ROS 2 (Humble Hawksbill recommended)
sudo apt update
sudo apt install software-properties-common
sudo add-apt-repository universe
sudo apt update
sudo apt install ros-humble-desktop
sudo apt install python3-rosdep python3-rosinstall python3-vcstool
```

### Environment Configuration
```bash
# Add to ~/.bashrc
source /opt/ros/humble/setup.bash
export ROS_DOMAIN_ID=1
```

## Basic ROS 2 Concepts

### Nodes
A node is a single executable that uses ROS 2 to communicate with other nodes.

```python
# example_node.py
import rclpy
from rclpy.node import Node

class MinimalNode(Node):
    def __init__(self):
        super().__init__('minimal_publisher')
        self.publisher = self.create_publisher(String, 'topic', 10)
        timer_period = 0.5  # seconds
        self.timer = self.create_timer(timer_period, self.timer_callback)

    def timer_callback(self):
        msg = String()
        msg.data = 'Hello World'
        self.publisher.publish(msg)
        self.get_logger().info('Publishing: "%s"' % msg.data)

def main(args=None):
    rclpy.init(args=args)
    minimal_publisher = MinimalNode()
    rclpy.spin(minimal_publisher)
    minimal_publisher.destroy_node()
    rclpy.shutdown()

if __name__ == '__main__':
    main()
```

### Topics and Messages
Topics enable asynchronous communication between nodes through publish/subscribe pattern.

## Weekly Exercises

### Exercise 1: Environment Setup
1. Install ROS 2 Humble Hawksbill
2. Verify installation with `ros2 topic list`
3. Create a simple workspace

### Exercise 2: Basic Node Creation
1. Create a new package: `ros2 pkg create --build-type ament_python my_robot_basics`
2. Implement a simple publisher node
3. Run the node and verify it publishes messages

### Mini-Project: Robot State Publisher
Create a node that publishes the state of a simple robot model:
```python
# robot_state_publisher.py
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import JointState
import math

class RobotStatePublisher(Node):
    def __init__(self):
        super().__init__('robot_state_publisher')
        self.publisher = self.create_publisher(JointState, 'joint_states', 10)
        self.timer = self.create_timer(0.1, self.publish_joint_states)
        self.time = 0.0

    def publish_joint_states(self):
        msg = JointState()
        msg.name = ['joint1', 'joint2', 'joint3']
        msg.position = [math.sin(self.time), math.cos(self.time), 0.0]
        msg.header.stamp = self.get_clock().now().to_msg()
        self.publisher.publish(msg)
        self.time += 0.1

def main(args=None):
    rclpy.init(args=args)
    node = RobotStatePublisher()
    rclpy.spin(node)
    node.destroy_node()
    rclpy.shutdown()

if __name__ == '__main__':
    main()
```

## Summary
This chapter introduced the fundamental concepts of Physical AI and humanoid robotics. You've learned about key technologies and set up your development environment. In the next chapter, we'll dive deeper into ROS 2 fundamentals and explore more complex communication patterns.

============================================================
FILE: book\docs\isaac.md
============================================================
---
sidebar_position: 4
title: "NVIDIA Isaac AI Brain"
---

# NVIDIA Isaac AI Brain

## Weekly Plan
- Day 1-2: Understanding Isaac Sim and Isaac ROS packages
- Day 3-4: Perception and navigation with Isaac
- Day 5-7: Sim-to-real transfer and advanced AI capabilities

## Learning Objectives
By the end of this chapter, you will:
- Understand the NVIDIA Isaac ecosystem and its components
- Use Isaac Sim for high-fidelity robotics simulation
- Implement perception and navigation using Isaac ROS packages
- Apply sim-to-real transfer techniques for real robot deployment
- Leverage GPU acceleration for AI-powered robotics

## NVIDIA Isaac Overview

NVIDIA Isaac is a comprehensive platform for developing, simulating, and deploying AI-powered robots. It combines:
- Isaac Sim: High-fidelity simulation environment
- Isaac ROS: ROS 2 packages for perception and navigation
- Isaac Apps: Pre-built applications for common robotics tasks
- Isaac Lab: Research framework for embodied AI

### Key Components
- GPU-accelerated physics simulation (PhysX engine)
- RTX rendering for photorealistic simulation
- AI training environments with domain randomization
- ROS 2 integration for real-world deployment

## Isaac Sim Setup and Usage

### Installing Isaac Sim
```bash
# Download and install Isaac Sim from NVIDIA Developer website
# Or use the containerized version
docker pull nvcr.io/nvidia/isaac-sim:latest
```

### Basic Isaac Sim Concepts
```python
# Example Python API usage
import omni
from pxr import Gf, UsdGeom
import carb

# Get the USD stage
stage = omni.usd.get_context().get_stage()

# Create a simple cube
cube = UsdGeom.Cube.Define(stage, "/World/Cube")
cube.GetSizeAttr().Set(1.0)

# Set position
cube.AddTranslateOp().Set(Gf.Vec3d(0, 0, 1))
```

### Creating Robot Models in Isaac Sim
```python
# Robot creation in Isaac Sim
import omni
from omni.isaac.core import World
from omni.isaac.core.utils.nucleus import get_assets_root_path
from omni.isaac.core.utils.stage import add_reference_to_stage

# Create world instance
world = World(stage_units_in_meters=1.0)

# Add robot to the stage
assets_root_path = get_assets_root_path()
if assets_root_path is None:
    carb.log_error("Could not find Isaac Sim assets folder")

# Add a simple robot (using existing assets)
add_reference_to_stage(
    usd_path=assets_root_path + "/Isaac/Robots/Franka/franka.usd",
    prim_path="/World/Robot"
)

# Reset the world to initialize the robot
world.reset()
```

## Isaac ROS Packages

Isaac ROS provides optimized ROS 2 packages for perception and navigation:

### Image Pipeline
```python
# Isaac ROS Image Pipeline Example
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image
from cv_bridge import CvBridge
import cv2
import numpy as np

class IsaacImageProcessor(Node):
    def __init__(self):
        super().__init__('isaac_image_processor')

        # Create subscription to camera topic
        self.subscription = self.create_subscription(
            Image,
            '/camera/color/image_raw',
            self.image_callback,
            10
        )

        # Create publisher for processed image
        self.publisher = self.create_publisher(
            Image,
            '/camera/color/image_processed',
            10
        )

        self.bridge = CvBridge()
        self.get_logger().info('Isaac Image Processor initialized')

    def image_callback(self, msg):
        # Convert ROS Image message to OpenCV image
        cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')

        # Process image using optimized Isaac functions
        processed_image = self.process_image(cv_image)

        # Convert back to ROS Image message
        processed_msg = self.bridge.cv2_to_imgmsg(processed_image, encoding='bgr8')
        processed_msg.header = msg.header

        # Publish processed image
        self.publisher.publish(processed_msg)

    def process_image(self, image):
        # Example: Apply Gaussian blur and edge detection
        blurred = cv2.GaussianBlur(image, (5, 5), 0)
        gray = cv2.cvtColor(blurred, cv2.COLOR_BGR2GRAY)
        edges = cv2.Canny(gray, 50, 150)

        # Convert back to 3-channel for display
        result = cv2.cvtColor(edges, cv2.COLOR_GRAY2BGR)
        return result

def main(args=None):
    rclpy.init(args=args)
    processor = IsaacImageProcessor()
    rclpy.spin(processor)
    processor.destroy_node()
    rclpy.shutdown()

if __name__ == '__main__':
    main()
```

### Perception Packages
```python
# Isaac ROS Perception Example
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import PointCloud2
from geometry_msgs.msg import PoseStamped
from visualization_msgs.msg import Marker
import numpy as np

class IsaacPerceptionNode(Node):
    def __init__(self):
        super().__init__('isaac_perception')

        # Point cloud subscriber
        self.pc_sub = self.create_subscription(
            PointCloud2,
            '/camera/depth/points',
            self.pointcloud_callback,
            10
        )

        # Object detection publisher
        self.obj_pub = self.create_publisher(
            PoseStamped,
            '/detected_object',
            10
        )

        # Visualization publisher
        self.vis_pub = self.create_publisher(
            Marker,
            '/object_marker',
            10
        )

        self.get_logger().info('Isaac Perception Node initialized')

    def pointcloud_callback(self, msg):
        # Process point cloud data
        points = self.pointcloud2_to_array(msg)

        # Detect objects (simplified example)
        detected_objects = self.detect_objects(points)

        if detected_objects:
            # Publish first detected object
            obj = detected_objects[0]
            pose_msg = PoseStamped()
            pose_msg.header.stamp = self.get_clock().now().to_msg()
            pose_msg.header.frame_id = 'camera_depth_optical_frame'
            pose_msg.pose.position.x = obj[0]
            pose_msg.pose.position.y = obj[1]
            pose_msg.pose.position.z = obj[2]

            self.obj_pub.publish(pose_msg)

            # Publish visualization marker
            marker = Marker()
            marker.header = pose_msg.header
            marker.ns = "objects"
            marker.id = 0
            marker.type = Marker.SPHERE
            marker.action = Marker.ADD
            marker.pose = pose_msg.pose
            marker.scale.x = 0.1
            marker.scale.y = 0.1
            marker.scale.z = 0.1
            marker.color.r = 1.0
            marker.color.g = 0.0
            marker.color.b = 0.0
            marker.color.a = 1.0

            self.vis_pub.publish(marker)

    def pointcloud2_to_array(self, cloud_msg):
        # Convert PointCloud2 message to numpy array
        import sensor_msgs.point_cloud2 as pc2
        points = pc2.read_points_numpy(
            cloud_msg,
            field_names=("x", "y", "z"),
            skip_nans=True
        )
        return points

    def detect_objects(self, points):
        # Simplified object detection: find clusters of points
        # In practice, use Isaac's optimized perception algorithms
        if len(points) < 100:
            return []

        # Find points within a certain range (simplified)
        center = np.mean(points, axis=0)
        distances = np.linalg.norm(points - center, axis=1)
        mask = distances < 0.5  # 50cm radius

        if np.sum(mask) > 50:  # At least 50 points
            return [center]

        return []

def main(args=None):
    rclpy.init(args=args)
    node = IsaacPerceptionNode()
    rclpy.spin(node)
    node.destroy_node()
    rclpy.shutdown()

if __name__ == '__main__':
    main()
```

## Navigation with Isaac and Nav2

### Isaac Navigation Stack Integration
```python
# Isaac Navigation Example
import rclpy
from rclpy.node import Node
from geometry_msgs.msg import PoseStamped, Twist
from nav_msgs.msg import Odometry
from sensor_msgs.msg import LaserScan
from rclpy.qos import QoSProfile, ReliabilityPolicy
import math

class IsaacNavigationNode(Node):
    def __init__(self):
        super().__init__('isaac_navigation')

        # QoS profile for simulation
        qos = QoSProfile(depth=10, reliability=ReliabilityPolicy.BEST_EFFORT)

        # Subscriptions
        self.odom_sub = self.create_subscription(
            Odometry,
            '/odom',
            self.odom_callback,
            qos
        )

        self.scan_sub = self.create_subscription(
            LaserScan,
            '/scan',
            self.scan_callback,
            qos
        )

        # Publishers
        self.cmd_pub = self.create_publisher(Twist, '/cmd_vel', 10)
        self.goal_pub = self.create_publisher(PoseStamped, '/goal_pose', 10)

        # Navigation parameters
        self.current_pose = None
        self.latest_scan = None
        self.target_pose = None
        self.navigation_active = False

        # Control timer
        self.timer = self.create_timer(0.1, self.navigation_callback)

        self.get_logger().info('Isaac Navigation Node initialized')

    def odom_callback(self, msg):
        self.current_pose = msg.pose.pose

    def scan_callback(self, msg):
        self.latest_scan = msg

    def navigation_callback(self):
        if not self.current_pose or not self.latest_scan:
            return

        if not self.navigation_active:
            return

        # Simple navigation algorithm
        cmd = Twist()

        # Calculate distance to target
        if self.target_pose:
            dx = self.target_pose.pose.position.x - self.current_pose.position.x
            dy = self.target_pose.pose.position.y - self.current_pose.position.y
            distance = math.sqrt(dx*dx + dy*dy)

            # Calculate angle to target
            target_angle = math.atan2(dy, dx)

            # Get current orientation (simplified)
            current_yaw = self.quaternion_to_yaw(self.current_pose.orientation)

            # Calculate angle difference
            angle_diff = target_angle - current_yaw
            while angle_diff > math.pi:
                angle_diff -= 2 * math.pi
            while angle_diff < -math.pi:
                angle_diff += 2 * math.pi

            # Obstacle avoidance
            min_range = min(self.latest_scan.ranges) if self.latest_scan.ranges else float('inf')

            if min_range < 0.5:  # Obstacle detected
                cmd.linear.x = 0.0
                cmd.angular.z = 0.8  # Turn to avoid
            elif abs(angle_diff) > 0.1:  # Need to turn
                cmd.angular.z = max(-0.8, min(0.8, angle_diff * 2.0))
            else:  # Move forward
                cmd.linear.x = 0.8 if distance > 0.5 else 0.0

        self.cmd_pub.publish(cmd)

    def quaternion_to_yaw(self, q):
        # Convert quaternion to yaw angle
        siny_cosp = 2 * (q.w * q.z + q.x * q.y)
        cosy_cosp = 1 - 2 * (q.y * q.y + q.z * q.z)
        return math.atan2(siny_cosp, cosy_cosp)

    def set_navigation_goal(self, x, y):
        goal_msg = PoseStamped()
        goal_msg.header.stamp = self.get_clock().now().to_msg()
        goal_msg.header.frame_id = 'map'
        goal_msg.pose.position.x = x
        goal_msg.pose.position.y = y
        goal_msg.pose.position.z = 0.0
        goal_msg.pose.orientation.w = 1.0

        self.target_pose = goal_msg
        self.navigation_active = True

def main(args=None):
    rclpy.init(args=args)
    node = IsaacNavigationNode()

    # Set a test goal
    node.set_navigation_goal(5.0, 5.0)

    rclpy.spin(node)
    node.destroy_node()
    rclpy.shutdown()

if __name__ == '__main__':
    main()
```

## Sim-to-Real Transfer

### Domain Randomization for Robust Training
```python
# Domain Randomization Example
import numpy as np
import random

class DomainRandomizer:
    def __init__(self):
        self.randomization_params = {
            'lighting': {
                'intensity_range': (0.5, 2.0),
                'color_temperature_range': (3000, 8000)
            },
            'textures': {
                'roughness_range': (0.0, 1.0),
                'metallic_range': (0.0, 1.0)
            },
            'physics': {
                'friction_range': (0.1, 1.0),
                'restitution_range': (0.0, 0.5)
            }
        }

    def randomize_environment(self, stage):
        """Apply domain randomization to the simulation environment"""
        # Randomize lighting
        self._randomize_lighting(stage)

        # Randomize textures
        self._randomize_textures(stage)

        # Randomize physics properties
        self._randomize_physics(stage)

    def _randomize_lighting(self, stage):
        """Randomize lighting conditions"""
        intensity = np.random.uniform(
            self.randomization_params['lighting']['intensity_range'][0],
            self.randomization_params['lighting']['intensity_range'][1]
        )

        color_temp = np.random.uniform(
            self.randomization_params['lighting']['color_temperature_range'][0],
            self.randomization_params['lighting']['color_temperature_range'][1]
        )

        # Apply lighting changes (simplified)
        print(f"Randomized lighting: intensity={intensity:.2f}, color_temp={color_temp:.0f}")

    def _randomize_textures(self, stage):
        """Randomize surface textures"""
        roughness = np.random.uniform(
            self.randomization_params['textures']['roughness_range'][0],
            self.randomization_params['textures']['roughness_range'][1]
        )

        metallic = np.random.uniform(
            self.randomization_params['textures']['metallic_range'][0],
            self.randomization_params['textures']['metallic_range'][1]
        )

        print(f"Randomized textures: roughness={roughness:.2f}, metallic={metallic:.2f}")

    def _randomize_physics(self, stage):
        """Randomize physics properties"""
        friction = np.random.uniform(
            self.randomization_params['physics']['friction_range'][0],
            self.randomization_params['physics']['friction_range'][1]
        )

        restitution = np.random.uniform(
            self.randomization_params['physics']['restitution_range'][0],
            self.randomization_params['physics']['restitution_range'][1]
        )

        print(f"Randomized physics: friction={friction:.2f}, restitution={restitution:.2f}")

# Usage in training loop
def training_loop():
    randomizer = DomainRandomizer()

    for episode in range(1000):
        # Randomize environment at start of each episode
        if episode % 10 == 0:  # Randomize every 10 episodes
            randomizer.randomize_environment(None)  # Pass stage reference in real implementation

        # Run training episode
        print(f"Running episode {episode}")

        # ... training code here ...
```

## Isaac Lab for Advanced AI

### Reinforcement Learning with Isaac Lab
```python
# Isaac Lab RL Example (Conceptual)
import torch
import numpy as np

class IsaacRLAgent:
    def __init__(self, observation_dim, action_dim):
        self.observation_dim = observation_dim
        self.action_dim = action_dim

        # Simple neural network for policy
        self.policy_network = torch.nn.Sequential(
            torch.nn.Linear(observation_dim, 256),
            torch.nn.ReLU(),
            torch.nn.Linear(256, 256),
            torch.nn.ReLU(),
            torch.nn.Linear(256, action_dim)
        )

        self.optimizer = torch.optim.Adam(self.policy_network.parameters(), lr=0.001)

    def get_action(self, observation):
        """Get action from policy network"""
        obs_tensor = torch.FloatTensor(observation).unsqueeze(0)
        action = self.policy_network(obs_tensor)
        return action.detach().numpy()[0]

    def update_policy(self, observations, actions, rewards):
        """Update policy based on collected experiences"""
        obs_tensor = torch.FloatTensor(observations)
        actions_tensor = torch.FloatTensor(actions)
        rewards_tensor = torch.FloatTensor(rewards)

        # Compute loss (simplified policy gradient)
        predicted_actions = self.policy_network(obs_tensor)
        loss = torch.nn.functional.mse_loss(predicted_actions, actions_tensor)

        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        return loss.item()

# Example usage in simulation loop
def run_isaac_rl_training():
    agent = IsaacRLAgent(observation_dim=24, action_dim=6)  # Example dimensions

    for episode in range(1000):
        observations = []
        actions = []
        rewards = []

        # Simulate episode
        for step in range(100):  # 100 steps per episode
            # Get observation from Isaac Sim
            obs = get_observation_from_sim()  # This would come from Isaac Sim

            # Get action from agent
            action = agent.get_action(obs)

            # Apply action in simulation
            reward = apply_action_and_get_reward(action)  # This would interact with Isaac Sim

            # Store experience
            observations.append(obs)
            actions.append(action)
            rewards.append(reward)

        # Update policy after episode
        loss = agent.update_policy(observations, actions, rewards)

        if episode % 100 == 0:
            print(f"Episode {episode}, Loss: {loss:.4f}")

def get_observation_from_sim():
    """Mock function - in reality this would get data from Isaac Sim"""
    return np.random.random(24)  # Example observation

def apply_action_and_get_reward(action):
    """Mock function - in reality this would interact with Isaac Sim"""
    return np.random.random()  # Example reward
```

## GPU Acceleration in Isaac

### Optimized Perception Pipeline
```python
# GPU-accelerated perception using Isaac ROS
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image
from cv_bridge import CvBridge
import numpy as np
import cupy as cp  # CUDA-accelerated NumPy

class GpuPerceptionNode(Node):
    def __init__(self):
        super().__init__('gpu_perception')

        self.subscription = self.create_subscription(
            Image,
            '/camera/color/image_raw',
            self.image_callback,
            10
        )

        self.publisher = self.create_publisher(
            Image,
            '/camera/color/image_processed',
            10
        )

        self.bridge = CvBridge()
        self.get_logger().info('GPU Perception Node initialized')

    def image_callback(self, msg):
        # Convert ROS Image to OpenCV
        cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')

        # Move to GPU for processing
        gpu_image = cp.asarray(cv_image)

        # GPU-accelerated processing
        processed_gpu = self.gpu_process_image(gpu_image)

        # Move back to CPU
        processed_image = cp.asnumpy(processed_gpu)

        # Convert back to ROS Image
        processed_msg = self.bridge.cv2_to_imgmsg(processed_image, encoding='bgr8')
        processed_msg.header = msg.header

        self.publisher.publish(processed_msg)

    def gpu_process_image(self, image):
        # Example: GPU-accelerated edge detection
        # Convert to grayscale
        gray = 0.299 * image[:, :, 0] + 0.587 * image[:, :, 1] + 0.114 * image[:, :, 2]

        # Simple gradient computation on GPU
        grad_x = cp.zeros_like(gray)
        grad_y = cp.zeros_like(gray)

        # Compute gradients (simplified)
        grad_x[1:-1, 1:-1] = gray[1:-1, 2:] - gray[1:-1, :-2]
        grad_y[1:-1, 1:-1] = gray[2:, 1:-1] - gray[:-2, 1:-1]

        # Compute magnitude
        magnitude = cp.sqrt(grad_x**2 + grad_y**2)

        # Normalize to 0-255 range
        magnitude = (magnitude / cp.max(magnitude)) * 255
        magnitude = cp.clip(magnitude, 0, 255).astype(cp.uint8)

        # Convert back to 3-channel
        result = cp.stack([magnitude, magnitude, magnitude], axis=2)

        return result

def main(args=None):
    rclpy.init(args=args)
    node = GpuPerceptionNode()
    rclpy.spin(node)
    node.destroy_node()
    rclpy.shutdown()

if __name__ == '__main__':
    main()
```

## Weekly Exercises

### Exercise 1: Isaac Sim Setup
1. Install Isaac Sim on your system
2. Load a sample robot model in Isaac Sim
3. Create a simple scene with objects
4. Control the robot using Isaac's Python API

### Exercise 2: Isaac ROS Perception
1. Set up Isaac ROS packages
2. Create a perception pipeline using Isaac's optimized packages
3. Process camera and LiDAR data
4. Visualize the processed data in RViz

### Exercise 3: Navigation with Isaac
1. Configure Nav2 with Isaac-specific optimizations
2. Set up costmaps and planners
3. Test navigation in Isaac Sim
4. Compare performance with standard ROS 2 navigation

### Mini-Project: AI-Powered Object Manipulation
Create a complete AI-powered manipulation system:
- Use Isaac Sim for training data generation
- Implement perception pipeline to detect objects
- Create navigation system to approach objects
- Implement grasping with reinforcement learning

```python
# Complete Isaac Manipulation System
import rclpy
from rclpy.node import Node
from geometry_msgs.msg import Pose, Point
from sensor_msgs.msg import Image, PointCloud2
from std_msgs.msg import String
from cv_bridge import CvBridge
import numpy as np
import cv2

class IsaacManipulationSystem(Node):
    def __init__(self):
        super().__init__('isaac_manipulation_system')

        # Publishers and subscribers
        self.image_sub = self.create_subscription(
            Image, '/camera/color/image_raw', self.image_callback, 10
        )
        self.pc_sub = self.create_subscription(
            PointCloud2, '/camera/depth/points', self.pointcloud_callback, 10
        )
        self.object_pub = self.create_publisher(
            Pose, '/detected_object_pose', 10
        )
        self.status_pub = self.create_publisher(
            String, '/manipulation_status', 10
        )

        self.bridge = CvBridge()
        self.latest_image = None
        self.latest_pointcloud = None
        self.detected_objects = []

        # Processing timer
        self.timer = self.create_timer(0.1, self.process_callback)

        self.get_logger().info('Isaac Manipulation System initialized')

    def image_callback(self, msg):
        self.latest_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')

    def pointcloud_callback(self, msg):
        self.latest_pointcloud = msg

    def process_callback(self):
        if self.latest_image is not None:
            # Detect objects in image
            self.detected_objects = self.detect_objects_in_image(self.latest_image)

            # If objects detected, find 3D positions
            if self.detected_objects and self.latest_pointcloud:
                for obj_2d in self.detected_objects:
                    obj_3d = self.convert_2d_to_3d(obj_2d, self.latest_pointcloud)
                    if obj_3d is not None:
                        # Publish object pose
                        pose_msg = Pose()
                        pose_msg.position = obj_3d
                        self.object_pub.publish(pose_msg)

                        # Publish status
                        status_msg = String()
                        status_msg.data = f"Object detected at ({obj_3d.x:.2f}, {obj_3d.y:.2f}, {obj_3d.z:.2f})"
                        self.status_pub.publish(status_msg)

    def detect_objects_in_image(self, image):
        """Simple object detection using color thresholding"""
        # Convert BGR to HSV
        hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)

        # Define range for red color (adjust as needed)
        lower_red = np.array([0, 50, 50])
        upper_red = np.array([10, 255, 255])

        # Threshold the HSV image to get only red colors
        mask1 = cv2.inRange(hsv, lower_red, upper_red)

        # Red color in HSV (for wrap-around)
        lower_red = np.array([170, 50, 50])
        upper_red = np.array([180, 255, 255])
        mask2 = cv2.inRange(hsv, lower_red, upper_red)

        mask = mask1 + mask2

        # Find contours
        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

        detected_objects = []
        for contour in contours:
            if cv2.contourArea(contour) > 100:  # Minimum area threshold
                # Get center of contour
                M = cv2.moments(contour)
                if M["m00"] != 0:
                    cx = int(M["m10"] / M["m00"])
                    cy = int(M["m01"] / M["m00"])
                    detected_objects.append((cx, cy))

        return detected_objects

    def convert_2d_to_3d(self, point_2d, pointcloud):
        """Convert 2D image coordinates to 3D world coordinates"""
        # This is a simplified version - in practice, you'd use camera info
        # and point cloud data to get accurate 3D positions
        x, y = point_2d

        # For demonstration, return a fixed offset from camera
        # In real implementation, use actual point cloud data
        return Point(x=x*0.001, y=y*0.001, z=1.0)  # Simplified conversion

def main(args=None):
    rclpy.init(args=args)
    node = IsaacManipulationSystem()
    rclpy.spin(node)
    node.destroy_node()
    rclpy.shutdown()

if __name__ == '__main__':
    main()
```

## Summary
This chapter covered the NVIDIA Isaac platform for AI-powered robotics, including Isaac Sim, Isaac ROS packages, navigation, and sim-to-real transfer techniques. You've learned how to leverage GPU acceleration for perception and navigation tasks. The next chapter will explore Vision-Language-Action (VLA) systems for natural human-robot interaction.

============================================================
FILE: book\docs\ros2.md
============================================================
---
sidebar_position: 2
title: "ROS 2 Fundamentals"
---

# ROS 2 Fundamentals

## Weekly Plan
- Day 1-2: Understanding ROS 2 architecture and concepts
- Day 3-4: Creating nodes, topics, and services
- Day 5-7: Advanced concepts - actions, parameters, and launch files

## Learning Objectives
By the end of this chapter, you will:
- Understand the ROS 2 architecture and communication patterns
- Create nodes that communicate via topics, services, and actions
- Use parameters for configuration management
- Create launch files for complex system startup

## ROS 2 Architecture

ROS 2 uses a distributed architecture where multiple processes (nodes) communicate through a publish/subscribe model. The key components are:

- **Nodes**: Individual processes that perform specific functions
- **Topics**: Channels for asynchronous message passing
- **Services**: Synchronous request/response communication
- **Actions**: Goal-based communication with feedback
- **Parameters**: Configuration values that can be changed at runtime

### The DDS Middleware
ROS 2 uses Data Distribution Service (DDS) as its middleware, providing:
- Real-time performance
- Quality of Service (QoS) policies
- Reliable message delivery
- Language independence

## Creating Nodes

### Basic Node Structure
```python
import rclpy
from rclpy.node import Node

class MyRobotNode(Node):
    def __init__(self):
        super().__init__('my_robot_node')
        # Initialize components here
        self.get_logger().info('My Robot Node initialized')

def main(args=None):
    rclpy.init(args=args)
    node = MyRobotNode()
    rclpy.spin(node)
    node.destroy_node()
    rclpy.shutdown()

if __name__ == '__main__':
    main()
```

### Publishers and Subscribers
```python
import rclpy
from rclpy.node import Node
from std_msgs.msg import String, Int32

class TalkerListenerNode(Node):
    def __init__(self):
        super().__init__('talker_listener')

        # Create publisher
        self.publisher = self.create_publisher(String, 'chatter', 10)

        # Create subscriber
        self.subscription = self.create_subscription(
            String,
            'chatter',
            self.listener_callback,
            10)

        # Timer to publish messages
        self.timer = self.create_timer(0.5, self.timer_callback)
        self.i = 0

    def timer_callback(self):
        msg = String()
        msg.data = f'Hello World: {self.i}'
        self.publisher.publish(msg)
        self.get_logger().info(f'Publishing: "{msg.data}"')
        self.i += 1

    def listener_callback(self, msg):
        self.get_logger().info(f'I heard: "{msg.data}"')

def main(args=None):
    rclpy.init(args=args)
    node = TalkerListenerNode()
    rclpy.spin(node)
    node.destroy_node()
    rclpy.shutdown()

if __name__ == '__main__':
    main()
```

## Services

Services provide synchronous request/response communication:

```python
# Service server
import rclpy
from rclpy.node import Node
from example_interfaces.srv import AddTwoInts

class MinimalService(Node):
    def __init__(self):
        super().__init__('minimal_service')
        self.srv = self.create_service(AddTwoInts, 'add_two_ints', self.add_two_ints_callback)

    def add_two_ints_callback(self, request, response):
        response.sum = request.a + request.b
        self.get_logger().info(f'Incoming request\na: {request.a}, b: {request.b}\n')
        return response

def main(args=None):
    rclpy.init(args=args)
    minimal_service = MinimalService()
    rclpy.spin(minimal_service)
    rclpy.shutdown()

if __name__ == '__main__':
    main()
```

```python
# Service client
import rclpy
from rclpy.node import Node
from example_interfaces.srv import AddTwoInts

class MinimalClient(Node):
    def __init__(self):
        super().__init__('minimal_client')
        self.cli = self.create_client(AddTwoInts, 'add_two_ints')
        while not self.cli.wait_for_service(timeout_sec=1.0):
            self.get_logger().info('Service not available, waiting again...')
        self.req = AddTwoInts.Request()

    def send_request(self, a, b):
        self.req.a = a
        self.req.b = b
        self.future = self.cli.call_async(self.req)
        rclpy.spin_until_future_complete(self, self.future)
        return self.future.result()

def main(args=None):
    rclpy.init(args=args)
    minimal_client = MinimalClient()
    response = minimal_client.send_request(1, 2)
    minimal_client.get_logger().info(f'Result of add_two_ints: {response.sum}')
    minimal_client.destroy_node()
    rclpy.shutdown()

if __name__ == '__main__':
    main()
```

## Actions

Actions are used for long-running tasks with feedback:

```python
# Action server
import rclpy
from rclpy.action import ActionServer, CancelResponse, GoalResponse
from rclpy.node import Node
from example_interfaces.action import Fibonacci

class FibonacciActionServer(Node):
    def __init__(self):
        super().__init__('fibonacci_action_server')
        self._action_server = ActionServer(
            self,
            Fibonacci,
            'fibonacci',
            execute_callback=self.execute_callback,
            goal_callback=self.goal_callback,
            cancel_callback=self.cancel_callback)

    def goal_callback(self, goal_request):
        self.get_logger().info('Received goal request')
        return GoalResponse.ACCEPT

    def cancel_callback(self, goal_handle):
        self.get_logger().info('Received cancel request')
        return CancelResponse.ACCEPT

    def execute_callback(self, goal_handle):
        self.get_logger().info('Executing goal...')

        feedback_msg = Fibonacci.Feedback()
        feedback_msg.sequence = [0, 1]

        for i in range(1, goal_handle.request.order):
            if goal_handle.is_cancel_requested:
                goal_handle.canceled()
                self.get_logger().info('Goal canceled')
                return Fibonacci.Result()

            feedback_msg.sequence.append(
                feedback_msg.sequence[i] + feedback_msg.sequence[i-1])

            goal_handle.publish_feedback(feedback_msg)
            self.get_logger().info(f'Publishing feedback: {feedback_msg.sequence}')

        goal_handle.succeed()
        result = Fibonacci.Result()
        result.sequence = feedback_msg.sequence
        self.get_logger().info(f'Returning result: {result.sequence}')
        return result

def main(args=None):
    rclpy.init(args=args)
    fibonacci_action_server = FibonacciActionServer()
    rclpy.spin(fibonacci_action_server)
    fibonacci_action_server.destroy_node()
    rclpy.shutdown()

if __name__ == '__main__':
    main()
```

## Parameters

Parameters allow runtime configuration of nodes:

```python
import rclpy
from rclpy.node import Node

class ParameterNode(Node):
    def __init__(self):
        super().__init__('parameter_node')

        # Declare parameters with default values
        self.declare_parameter('robot_name', 'my_robot')
        self.declare_parameter('max_velocity', 1.0)
        self.declare_parameter('safety_factor', 0.8)

        # Get parameter values
        self.robot_name = self.get_parameter('robot_name').value
        self.max_velocity = self.get_parameter('max_velocity').value
        self.safety_factor = self.get_parameter('safety_factor').value

        self.get_logger().info(f'Robot name: {self.robot_name}')
        self.get_logger().info(f'Max velocity: {self.max_velocity}')
        self.get_logger().info(f'Safety factor: {self.safety_factor}')

def main(args=None):
    rclpy.init(args=args)
    node = ParameterNode()
    rclpy.spin(node)
    node.destroy_node()
    rclpy.shutdown()

if __name__ == '__main__':
    main()
```

## Launch Files

Launch files allow starting multiple nodes with specific configurations:

```xml
<!-- robot_system.launch.py -->
from launch import LaunchDescription
from launch_ros.actions import Node

def generate_launch_description():
    return LaunchDescription([
        Node(
            package='my_robot_package',
            executable='robot_controller',
            name='robot_controller',
            parameters=[
                {'max_velocity': 2.0},
                {'safety_factor': 0.9}
            ],
            remappings=[
                ('/cmd_vel', '/my_cmd_vel')
            ]
        ),
        Node(
            package='my_robot_package',
            executable='sensor_processor',
            name='sensor_processor',
            parameters=[
                {'sensor_range': 10.0}
            ]
        ),
        Node(
            package='rviz2',
            executable='rviz2',
            name='rviz',
            arguments=['-d', 'config/my_robot.rviz']
        )
    ])
```

## Quality of Service (QoS) Settings

QoS settings allow fine-tuning communication behavior:

```python
from rclpy.qos import QoSProfile, ReliabilityPolicy, DurabilityPolicy

class QoSDemoNode(Node):
    def __init__(self):
        super().__init__('qos_demo')

        # Create a QoS profile for reliable communication
        qos_profile = QoSProfile(
            depth=10,
            reliability=ReliabilityPolicy.RELIABLE,
            durability=DurabilityPolicy.VOLATILE
        )

        self.publisher = self.create_publisher(
            String,
            'qos_chatter',
            qos_profile
        )

        self.subscription = self.create_subscription(
            String,
            'qos_chatter',
            self.listener_callback,
            qos_profile
        )
```

## Weekly Exercises

### Exercise 1: Publisher and Subscriber
1. Create a publisher that sends sensor data (temperature, distance, etc.)
2. Create a subscriber that processes and logs the sensor data
3. Test the communication between nodes

### Exercise 2: Service Implementation
1. Create a service that calculates the distance between two points
2. Implement a client that calls the service with different coordinates
3. Test the service with various inputs

### Exercise 3: Action Server
1. Create an action server that moves a robot to a specified position
2. Implement feedback to show progress
3. Create a client that sends goals and monitors progress

### Mini-Project: Robot Arm Controller
Create a complete robot arm controller with:
- Joint position publisher
- Gripper control service
- Trajectory execution action
- Parameter-based configuration

```python
# robot_arm_controller.py
import rclpy
from rclpy.node import Node
from std_msgs.msg import Float64MultiArray
from example_interfaces.srv import SetBool
from example_interfaces.action import FollowJointTrajectory
from rclpy.action import ActionServer
import time

class RobotArmController(Node):
    def __init__(self):
        super().__init__('robot_arm_controller')

        # Declare parameters
        self.declare_parameter('joint_names', ['joint1', 'joint2', 'joint3'])
        self.declare_parameter('max_velocity', 1.0)

        # Joint position publisher
        self.joint_pub = self.create_publisher(Float64MultiArray, '/joint_positions', 10)

        # Gripper service
        self.gripper_service = self.create_service(
            SetBool,
            'control_gripper',
            self.gripper_callback
        )

        # Trajectory action server
        self.trajectory_server = ActionServer(
            self,
            FollowJointTrajectory,
            'follow_joint_trajectory',
            self.execute_trajectory
        )

        self.current_joints = [0.0, 0.0, 0.0]
        self.get_logger().info('Robot Arm Controller initialized')

    def gripper_callback(self, request, response):
        if request.data:
            self.get_logger().info('Gripper closing')
        else:
            self.get_logger().info('Gripper opening')
        response.success = True
        response.message = 'Gripper command executed'
        return response

    def execute_trajectory(self, goal_handle):
        self.get_logger().info('Executing trajectory...')

        for point in goal_handle.request.trajectory.points:
            # Move to joint positions
            joint_msg = Float64MultiArray()
            joint_msg.data = point.positions
            self.joint_pub.publish(joint_msg)
            self.current_joints = list(point.positions)

            # Wait for movement
            time.sleep(0.5)

            # Publish feedback
            # (In a real implementation, you'd check actual joint positions)

        goal_handle.succeed()
        result = FollowJointTrajectory.Result()
        result.error_code = 0
        return result

def main(args=None):
    rclpy.init(args=args)
    controller = RobotArmController()
    rclpy.spin(controller)
    controller.destroy_node()
    rclpy.shutdown()

if __name__ == '__main__':
    main()
```

## Summary
This chapter covered the fundamental concepts of ROS 2, including nodes, topics, services, actions, parameters, and launch files. You've learned how to create and configure these components to build complex robotic systems. The next chapter will focus on robot simulation using Gazebo and Unity.

============================================================
FILE: book\docs\vla.md
============================================================
---
sidebar_position: 5
title: "Vision-Language-Action (VLA)"
---

# Vision-Language-Action (VLA)

## Weekly Plan
- Day 1-2: Understanding VLA systems and multimodal AI
- Day 3-4: Implementing speech recognition and natural language processing
- Day 5-7: Creating action planning and execution systems with safety

## Learning Objectives
By the end of this chapter, you will:
- Understand Vision-Language-Action (VLA) systems for robotics
- Implement speech recognition using Whisper for voice-to-action conversion
- Create natural language understanding and task planning systems
- Develop safe action execution with multimodal feedback
- Integrate VLA systems with ROS 2 for robot control

## Vision-Language-Action Overview

Vision-Language-Action (VLA) systems represent a paradigm shift in robotics, enabling robots to understand and execute complex commands expressed in natural language. These systems combine:
- Computer vision for scene understanding
- Natural language processing for command interpretation
- Action planning and execution for physical task completion
- Safety protocols to ensure reliable operation

### Key Components of VLA Systems
- **Perception Module**: Processes visual input to understand the environment
- **Language Module**: Interprets natural language commands
- **Planning Module**: Decomposes high-level commands into executable actions
- **Execution Module**: Executes actions with safety monitoring
- **Feedback Module**: Provides multimodal feedback to users

## Speech Recognition with Whisper

### Setting up Whisper for Voice Commands
```python
# Install required packages
# pip install openai-whisper torch

import whisper
import torch
import numpy as np
import pyaudio
import wave
import threading
import queue

class WhisperVoiceProcessor:
    def __init__(self, model_size="base"):
        # Load Whisper model
        self.model = whisper.load_model(model_size)
        self.audio_queue = queue.Queue()

        # Audio parameters
        self.chunk = 1024
        self.format = pyaudio.paInt16
        self.channels = 1
        self.rate = 16000
        self.record_seconds = 3

        self.audio = pyaudio.PyAudio()
        self.is_listening = False

    def start_listening(self):
        """Start voice recognition in a separate thread"""
        self.is_listening = True
        listening_thread = threading.Thread(target=self._listen_loop)
        listening_thread.start()

    def _listen_loop(self):
        """Continuous listening loop"""
        while self.is_listening:
            # Record audio
            frames = []

            stream = self.audio.open(
                format=self.format,
                channels=self.channels,
                rate=self.rate,
                input=True,
                frames_per_buffer=self.chunk
            )

            for i in range(0, int(self.rate / self.chunk * self.record_seconds)):
                data = stream.read(self.chunk)
                frames.append(data)

            stream.stop_stream()
            stream.close()

            # Save to temporary WAV file
            temp_filename = "temp_recording.wav"
            wf = wave.open(temp_filename, 'wb')
            wf.setnchannels(self.channels)
            wf.setsampwidth(self.audio.get_sample_size(self.format))
            wf.setframerate(self.rate)
            wf.writeframes(b''.join(frames))
            wf.close()

            # Transcribe audio
            result = self.model.transcribe(temp_filename)
            if result["text"].strip():  # Only process non-empty results
                self.process_command(result["text"])

    def process_command(self, text):
        """Process the transcribed command"""
        print(f"Recognized: {text}")
        # Send to command processor
        self.command_callback(text)

    def command_callback(self, command_text):
        """Override this method to handle commands"""
        pass

    def stop_listening(self):
        """Stop the listening process"""
        self.is_listening = False

# Example usage
class RobotVoiceController(WhisperVoiceProcessor):
    def __init__(self):
        super().__init__()
        self.robot_commands = {
            "move forward": self.move_forward,
            "move backward": self.move_backward,
            "turn left": self.turn_left,
            "turn right": self.turn_right,
            "stop": self.stop_robot,
            "pick up object": self.pick_object,
            "place object": self.place_object
        }

    def command_callback(self, command_text):
        """Process robot commands"""
        command_text = command_text.lower().strip()

        # Simple command matching (in practice, use more sophisticated NLP)
        for cmd, action in self.robot_commands.items():
            if cmd in command_text:
                print(f"Executing command: {cmd}")
                action()
                return

        print(f"Unknown command: {command_text}")

    def move_forward(self):
        print("Moving robot forward")
        # Publish ROS message to move robot forward
        self.publish_robot_command("move_forward")

    def move_backward(self):
        print("Moving robot backward")
        self.publish_robot_command("move_backward")

    def turn_left(self):
        print("Turning robot left")
        self.publish_robot_command("turn_left")

    def turn_right(self):
        print("Turning robot right")
        self.publish_robot_command("turn_right")

    def stop_robot(self):
        print("Stopping robot")
        self.publish_robot_command("stop")

    def pick_object(self):
        print("Picking up object")
        self.publish_robot_command("pick_object")

    def place_object(self):
        print("Placing object")
        self.publish_robot_command("place_object")

    def publish_robot_command(self, command):
        """Publish command to ROS system (placeholder)"""
        print(f"Publishing to ROS: {command}")

# Example usage
if __name__ == "__main__":
    controller = RobotVoiceController()
    print("Starting voice recognition...")
    controller.start_listening()

    # Keep the program running
    try:
        while True:
            pass
    except KeyboardInterrupt:
        controller.stop_listening()
        controller.audio.terminate()
```

## Natural Language Understanding and Planning

### LLM-Based Task Planning
```python
import openai
from typing import List, Dict, Any
import json

class LLMPlanner:
    def __init__(self, api_key: str = None):
        if api_key:
            openai.api_key = api_key
        self.action_space = [
            "move_to_location",
            "pick_object",
            "place_object",
            "open_gripper",
            "close_gripper",
            "rotate_gripper",
            "wait",
            "check_condition"
        ]

    def plan_task(self, natural_language_command: str, environment_state: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        Convert natural language command to a sequence of executable actions
        """
        prompt = f"""
        Given the following natural language command and environment state,
        decompose the command into a sequence of executable robotic actions.

        Environment state: {json.dumps(environment_state)}

        Natural language command: "{natural_language_command}"

        Available actions: {', '.join(self.action_space)}

        Return the action sequence as a JSON list of objects with 'action' and 'parameters' keys.
        Example format:
        [
            {{"action": "move_to_location", "parameters": {{"x": 1.0, "y": 2.0, "z": 0.5}}}},
            {{"action": "pick_object", "parameters": {{"object_id": "red_block"}}}},
            {{"action": "move_to_location", "parameters": {{"x": 3.0, "y": 1.0, "z": 0.5}}}},
            {{"action": "place_object", "parameters": {{"object_id": "red_block"}}}}
        ]

        Action sequence:
        """

        try:
            # Using OpenAI API (replace with your preferred LLM)
            response = openai.ChatCompletion.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.1
            )

            action_sequence = json.loads(response.choices[0].message.content)
            return action_sequence

        except Exception as e:
            print(f"Error in LLM planning: {e}")
            return self.fallback_plan(natural_language_command)

    def fallback_plan(self, command: str) -> List[Dict[str, Any]]:
        """
        Simple fallback planner for when LLM is unavailable
        """
        command_lower = command.lower()

        if "pick" in command_lower or "grasp" in command_lower:
            return [
                {"action": "move_to_location", "parameters": {"x": 1.0, "y": 0.0, "z": 0.0}},
                {"action": "pick_object", "parameters": {"object_id": "unknown"}}
            ]
        elif "place" in command_lower or "put" in command_lower:
            return [
                {"action": "move_to_location", "parameters": {"x": 2.0, "y": 0.0, "z": 0.0}},
                {"action": "place_object", "parameters": {"object_id": "unknown"}}
            ]
        elif "move" in command_lower or "go" in command_lower:
            return [
                {"action": "move_to_location", "parameters": {"x": 1.0, "y": 1.0, "z": 0.0}}
            ]
        else:
            return [
                {"action": "wait", "parameters": {"duration": 1.0}}
            ]

# Example usage
class VLAPlanner:
    def __init__(self):
        self.llm_planner = LLMPlanner()  # Initialize with your API key if needed

    def create_plan(self, command: str, env_state: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Create a plan from natural language command"""
        plan = self.llm_planner.plan_task(command, env_state)
        return self.validate_plan(plan)

    def validate_plan(self, plan: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Validate the plan for safety and feasibility"""
        validated_plan = []

        for action in plan:
            if self.is_action_safe(action) and self.is_action_feasible(action):
                validated_plan.append(action)
            else:
                print(f"Skipping unsafe or infeasible action: {action}")

        return validated_plan

    def is_action_safe(self, action: Dict[str, Any]) -> bool:
        """Check if action is safe to execute"""
        # Implement safety checks
        action_type = action.get("action", "")

        # Example safety checks
        if action_type == "move_to_location":
            params = action.get("parameters", {})
            x, y, z = params.get("x", 0), params.get("y", 0), params.get("z", 0)

            # Check if coordinates are within safe bounds
            if abs(x) > 10 or abs(y) > 10 or z < 0 or z > 2:
                return False

        return True

    def is_action_feasible(self, action: Dict[str, Any]) -> bool:
        """Check if action is feasible given robot capabilities"""
        # Implement feasibility checks
        return True

# Example usage
if __name__ == "__main__":
    planner = VLAPlanner()

    env_state = {
        "objects": [
            {"id": "red_block", "type": "block", "position": [0.5, 0.5, 0.1]},
            {"id": "blue_block", "type": "block", "position": [1.0, 1.0, 0.1]}
        ],
        "robot_position": [0.0, 0.0, 0.0],
        "gripper_status": "open"
    }

    command = "Pick up the red block and place it on the table at position 2, 2"
    plan = planner.create_plan(command, env_state)

    print("Generated plan:")
    for i, action in enumerate(plan):
        print(f"{i+1}. {action['action']} with params {action['parameters']}")
```

## Multi-Modal Interaction System

### Vision-Based Object Recognition
```python
import cv2
import numpy as np
import torch
import torchvision.transforms as T
from PIL import Image
import requests
from transformers import DetrImageProcessor, DetrForObjectDetection

class VisionSystem:
    def __init__(self):
        # Initialize DETR object detection model
        self.processor = DetrImageProcessor.from_pretrained("facebook/detr-resnet-50")
        self.model = DetrForObjectDetection.from_pretrained("facebook/detr-resnet-50")
        self.model.eval()

        # COCO dataset labels for object detection
        self.coco_labels = [
            "N/A", "person", "bicycle", "car", "motorcycle", "airplane", "bus", "train",
            "truck", "boat", "traffic light", "fire hydrant", "N/A", "stop sign",
            "parking meter", "bench", "bird", "cat", "dog", "horse", "sheep", "cow",
            "elephant", "bear", "zebra", "giraffe", "N/A", "backpack", "umbrella",
            "N/A", "N/A", "handbag", "tie", "suitcase", "frisbee", "skis", "snowboard",
            "sports ball", "kite", "baseball bat", "baseball glove", "skateboard",
            "surfboard", "tennis racket", "bottle", "N/A", "wine glass", "cup",
            "fork", "knife", "spoon", "bowl", "banana", "apple", "sandwich", "orange",
            "broccoli", "carrot", "hot dog", "pizza", "donut", "cake", "chair", "couch",
            "potted plant", "bed", "N/A", "dining table", "N/A", "N/A", "toilet", "N/A",
            "tv", "laptop", "mouse", "remote", "keyboard", "cell phone", "microwave",
            "oven", "toaster", "sink", "refrigerator", "N/A", "book", "clock", "vase",
            "scissors", "teddy bear", "hair drier", "toothbrush"
        ]

    def detect_objects(self, image_path_or_array):
        """
        Detect objects in an image using DETR
        """
        if isinstance(image_path_or_array, str):
            image = Image.open(image_path_or_array)
        else:
            # Convert numpy array to PIL Image
            image = Image.fromarray(cv2.cvtColor(image_path_or_array, cv2.COLOR_BGR2RGB))

        inputs = self.processor(images=image, return_tensors="pt")

        with torch.no_grad():
            outputs = self.model(**inputs)

        # Convert outputs to COCO API
        target_sizes = torch.tensor([image.size[::-1]])
        results = self.processor.post_process_object_detection(
            outputs, target_sizes=target_sizes, threshold=0.9
        )[0]

        detected_objects = []
        for score, label, box in zip(results["scores"], results["labels"], results["boxes"]):
            detected_objects.append({
                "label": self.coco_labels[label.item()],
                "score": score.item(),
                "bbox": box.tolist()  # [x_min, y_min, x_max, y_max]
            })

        return detected_objects

    def find_object_by_description(self, image, description):
        """
        Find objects matching a textual description
        """
        detected_objects = self.detect_objects(image)

        # Simple matching based on description keywords
        description_lower = description.lower()
        matching_objects = []

        for obj in detected_objects:
            if obj["label"] in description_lower or description_lower in obj["label"]:
                matching_objects.append(obj)

        return matching_objects

# Example usage
vision_system = VisionSystem()

# Example: Detect objects in an image
# objects = vision_system.detect_objects("path/to/image.jpg")
# print("Detected objects:", objects)
```

## ROS 2 Integration for VLA Systems

### VLA Command Execution Node
```python
import rclpy
from rclpy.node import Node
from std_msgs.msg import String
from geometry_msgs.msg import Pose, Point
from sensor_msgs.msg import Image
from cv_bridge import CvBridge
from std_srvs.srv import Trigger
import json
import threading

class VLAExecutionNode(Node):
    def __init__(self):
        super().__init__('vla_execution_node')

        # Publishers
        self.cmd_pub = self.create_publisher(String, '/vla_commands', 10)
        self.pose_pub = self.create_publisher(Pose, '/robot_target_pose', 10)
        self.status_pub = self.create_publisher(String, '/vla_status', 10)

        # Subscribers
        self.voice_sub = self.create_subscription(
            String, '/voice_commands', self.voice_command_callback, 10
        )
        self.vision_sub = self.create_subscription(
            Image, '/camera/color/image_raw', self.vision_callback, 10
        )

        # Services
        self.execute_srv = self.create_service(
            Trigger, '/execute_vla_plan', self.execute_plan_callback
        )

        # Internal state
        self.bridge = CvBridge()
        self.current_plan = []
        self.is_executing = False
        self.current_image = None

        # Vision system
        self.vision_system = VisionSystem()

        self.get_logger().info('VLA Execution Node initialized')

    def voice_command_callback(self, msg):
        """Process voice commands from speech recognition"""
        command_text = msg.data
        self.get_logger().info(f'Received voice command: {command_text}')

        # Process the command through the VLA pipeline
        self.process_vla_command(command_text)

    def vision_callback(self, msg):
        """Process camera images for vision system"""
        try:
            self.current_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')
        except Exception as e:
            self.get_logger().error(f'Error processing image: {e}')

    def process_vla_command(self, command_text):
        """Process a VLA command through the full pipeline"""
        self.get_logger().info(f'Processing VLA command: {command_text}')

        # Update status
        status_msg = String()
        status_msg.data = f'Processing command: {command_text}'
        self.status_pub.publish(status_msg)

        # Get current environment state (simplified)
        env_state = self.get_environment_state()

        # Create plan using VLA planner
        planner = VLAPlanner()
        plan = planner.create_plan(command_text, env_state)

        if plan:
            self.current_plan = plan
            self.get_logger().info(f'Generated plan with {len(plan)} steps')

            # Execute the plan
            self.execute_plan()
        else:
            self.get_logger().warn('Could not generate plan for command')

    def get_environment_state(self):
        """Get current environment state for planning"""
        # In a real system, this would gather state from multiple sensors
        env_state = {
            "objects": [],
            "robot_position": [0.0, 0.0, 0.0],
            "gripper_status": "open"
        }

        # If we have a current image, detect objects
        if self.current_image is not None:
            try:
                # Convert OpenCV image to format expected by vision system
                detected_objects = self.vision_system.detect_objects(self.current_image)
                env_state["objects"] = detected_objects
            except Exception as e:
                self.get_logger().error(f'Error detecting objects: {e}')

        return env_state

    def execute_plan(self):
        """Execute the current plan step by step"""
        if not self.current_plan or self.is_executing:
            return

        self.is_executing = True

        # Execute each action in the plan
        for i, action in enumerate(self.current_plan):
            self.get_logger().info(f'Executing action {i+1}/{len(self.current_plan)}: {action}')

            success = self.execute_single_action(action)

            if not success:
                self.get_logger().error(f'Action failed: {action}')
                break

        self.is_executing = False
        self.current_plan = []

        # Update status
        status_msg = String()
        status_msg.data = 'Plan execution completed'
        self.status_pub.publish(status_msg)

    def execute_single_action(self, action):
        """Execute a single action from the plan"""
        action_type = action.get('action', '')
        parameters = action.get('parameters', {})

        self.get_logger().info(f'Executing action: {action_type} with params: {parameters}')

        if action_type == 'move_to_location':
            return self.execute_move_to_location(parameters)
        elif action_type == 'pick_object':
            return self.execute_pick_object(parameters)
        elif action_type == 'place_object':
            return self.execute_place_object(parameters)
        elif action_type == 'open_gripper':
            return self.execute_open_gripper()
        elif action_type == 'close_gripper':
            return self.execute_close_gripper()
        elif action_type == 'wait':
            return self.execute_wait(parameters)
        else:
            self.get_logger().warn(f'Unknown action type: {action_type}')
            return False

    def execute_move_to_location(self, params):
        """Execute move to location action"""
        x = params.get('x', 0.0)
        y = params.get('y', 0.0)
        z = params.get('z', 0.0)

        # Create and publish target pose
        pose_msg = Pose()
        pose_msg.position.x = x
        pose_msg.position.y = y
        pose_msg.position.z = z
        # Set orientation to face forward (simplified)
        pose_msg.orientation.z = 0.0
        pose_msg.orientation.w = 1.0

        self.pose_pub.publish(pose_msg)
        self.get_logger().info(f'Moved to location: ({x}, {y}, {z})')

        return True

    def execute_pick_object(self, params):
        """Execute pick object action"""
        object_id = params.get('object_id', 'unknown')
        self.get_logger().info(f'Picking object: {object_id}')

        # Publish gripper command
        cmd_msg = String()
        cmd_msg.data = f'pick_object_{object_id}'
        self.cmd_pub.publish(cmd_msg)

        return True

    def execute_place_object(self, params):
        """Execute place object action"""
        object_id = params.get('object_id', 'unknown')
        self.get_logger().info(f'Placing object: {object_id}')

        # Publish gripper command
        cmd_msg = String()
        cmd_msg.data = f'place_object_{object_id}'
        self.cmd_pub.publish(cmd_msg)

        return True

    def execute_open_gripper(self):
        """Execute open gripper action"""
        self.get_logger().info('Opening gripper')

        cmd_msg = String()
        cmd_msg.data = 'open_gripper'
        self.cmd_pub.publish(cmd_msg)

        return True

    def execute_close_gripper(self):
        """Execute close gripper action"""
        self.get_logger().info('Closing gripper')

        cmd_msg = String()
        cmd_msg.data = 'close_gripper'
        self.cmd_pub.publish(cmd_msg)

        return True

    def execute_wait(self, params):
        """Execute wait action"""
        duration = params.get('duration', 1.0)
        self.get_logger().info(f'Waiting for {duration} seconds')

        # In a real system, you'd use a timer or action client
        # For simulation, just return immediately
        return True

    def execute_plan_callback(self, request, response):
        """Service callback to execute current plan"""
        if self.current_plan:
            self.execute_plan()
            response.success = True
            response.message = 'Plan executed successfully'
        else:
            response.success = False
            response.message = 'No plan available to execute'

        return response

def main(args=None):
    rclpy.init(args=args)
    node = VLAExecutionNode()

    try:
        rclpy.spin(node)
    except KeyboardInterrupt:
        node.get_logger().info('Shutting down VLA Execution Node')
    finally:
        node.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()
```

## Safety Protocols in VLA Systems

### Safety Monitor for VLA Execution
```python
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import LaserScan, Image
from geometry_msgs.msg import Twist, PoseStamped
from std_msgs.msg import Bool, String
from builtin_interfaces.msg import Duration
import threading
import time

class VLASafetyMonitor(Node):
    def __init__(self):
        super().__init__('vla_safety_monitor')

        # Publishers
        self.emergency_stop_pub = self.create_publisher(Bool, '/emergency_stop', 10)
        self.safety_status_pub = self.create_publisher(String, '/safety_status', 10)

        # Subscribers
        self.laser_sub = self.create_subscription(
            LaserScan, '/scan', self.laser_callback, 10
        )
        self.cmd_vel_sub = self.create_subscription(
            Twist, '/cmd_vel', self.cmd_vel_callback, 10
        )
        self.vla_status_sub = self.create_subscription(
            String, '/vla_status', self.vla_status_callback, 10
        )

        # Parameters
        self.declare_parameter('safe_distance', 0.5)  # meters
        self.declare_parameter('max_linear_velocity', 0.5)  # m/s
        self.declare_parameter('max_angular_velocity', 0.5)  # rad/s

        self.safe_distance = self.get_parameter('safe_distance').value
        self.max_linear_vel = self.get_parameter('max_linear_velocity').value
        self.max_angular_vel = self.get_parameter('max_angular_velocity').value

        # Internal state
        self.latest_scan = None
        self.latest_cmd_vel = None
        self.vla_active = False
        self.emergency_stop_active = False
        self.safety_violation = False

        # Safety timer
        self.safety_timer = self.create_timer(0.1, self.safety_check_callback)

        self.get_logger().info('VLA Safety Monitor initialized')

    def laser_callback(self, msg):
        """Process laser scan data"""
        self.latest_scan = msg

    def cmd_vel_callback(self, msg):
        """Monitor velocity commands"""
        self.latest_cmd_vel = msg

    def vla_status_callback(self, msg):
        """Monitor VLA system status"""
        self.vla_active = 'executing' in msg.data.lower()

    def safety_check_callback(self):
        """Main safety monitoring loop"""
        if self.emergency_stop_active:
            return  # Already in emergency stop state

        # Check for safety violations
        violations = []

        # Check proximity to obstacles
        if self.latest_scan:
            min_distance = min(self.latest_scan.ranges) if self.latest_scan.ranges else float('inf')
            if min_distance < self.safe_distance:
                violations.append(f'Obstacle too close: {min_distance:.2f}m < {self.safe_distance}m')

        # Check velocity limits
        if self.latest_cmd_vel:
            if abs(self.latest_cmd_vel.linear.x) > self.max_linear_vel:
                violations.append(f'Linear velocity too high: {self.latest_cmd_vel.linear.x:.2f} > {self.max_linear_vel}')
            if abs(self.latest_cmd_vel.angular.z) > self.max_angular_vel:
                violations.append(f'Angular velocity too high: {self.latest_cmd_vel.angular.z:.2f} > {self.max_angular_vel}')

        # If violations detected, trigger emergency stop
        if violations:
            self.safety_violation = True
            self.trigger_emergency_stop(violations)
        else:
            self.safety_violation = False

    def trigger_emergency_stop(self, violations):
        """Trigger emergency stop and log violations"""
        self.get_logger().error(f'Safety violation detected: {", ".join(violations)}')

        # Publish emergency stop
        stop_msg = Bool()
        stop_msg.data = True
        self.emergency_stop_pub.publish(stop_msg)

        self.emergency_stop_active = True

        # Update safety status
        status_msg = String()
        status_msg.data = f'EMERGENCY_STOP: {", ".join(violations)}'
        self.safety_status_pub.publish(status_msg)

    def reset_safety(self):
        """Reset emergency stop state"""
        self.emergency_stop_active = False
        self.safety_violation = False

        # Publish reset
        stop_msg = Bool()
        stop_msg.data = False
        self.emergency_stop_pub.publish(stop_msg)

        status_msg = String()
        status_msg.data = 'SAFETY_NORMAL'
        self.safety_status_pub.publish(status_msg)

def main(args=None):
    rclpy.init(args=args)
    node = VLASafetyMonitor()

    try:
        rclpy.spin(node)
    except KeyboardInterrupt:
        node.get_logger().info('Shutting down VLA Safety Monitor')
    finally:
        node.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()
```

## Weekly Exercises

### Exercise 1: Voice Command Recognition
1. Set up Whisper for speech recognition
2. Create a vocabulary of robot commands
3. Test voice recognition accuracy
4. Integrate with a simple robot simulator

### Exercise 2: Natural Language Processing
1. Implement an LLM-based planner using OpenAI API or open-source alternative
2. Test with various natural language commands
3. Validate the generated action sequences
4. Handle ambiguous or unclear commands

### Exercise 3: Multi-Modal Integration
1. Combine vision and language processing
2. Create a system that can identify objects mentioned in commands
3. Test with different object types and environments
4. Implement safety checks for the integrated system

### Mini-Project: Complete VLA System
Create a complete Vision-Language-Action system:
- Voice recognition for command input
- Vision system for environment understanding
- LLM-based planning for task decomposition
- Safe execution with monitoring
- ROS 2 integration for robot control

```python
# Complete VLA System Integration
import rclpy
from rclpy.node import Node
from std_msgs.msg import String
from sensor_msgs.msg import Image, LaserScan
from geometry_msgs.msg import Twist
from cv_bridge import CvBridge
import threading
import time

class CompleteVLASystem(Node):
    def __init__(self):
        super().__init__('complete_vla_system')

        # Publishers
        self.cmd_pub = self.create_publisher(Twist, '/cmd_vel', 10)
        self.status_pub = self.create_publisher(String, '/vla_system_status', 10)

        # Subscribers
        self.voice_sub = self.create_subscription(
            String, '/voice_commands', self.voice_callback, 10
        )
        self.vision_sub = self.create_subscription(
            Image, '/camera/color/image_raw', self.vision_callback, 10
        )
        self.scan_sub = self.create_subscription(
            LaserScan, '/scan', self.scan_callback, 10
        )

        # Internal components
        self.bridge = CvBridge()
        self.voice_processor = RobotVoiceController()
        self.planner = VLAPlanner()
        self.vision_system = VisionSystem()
        self.safety_monitor = VLASafetyMonitor()

        # State
        self.current_image = None
        self.latest_scan = None
        self.is_executing = False
        self.active_plan = []

        # Start voice processor
        self.voice_processor.start_listening()

        self.get_logger().info('Complete VLA System initialized')

    def voice_callback(self, msg):
        """Handle voice commands"""
        command = msg.data
        self.get_logger().info(f'Processing voice command: {command}')

        # Update status
        status_msg = String()
        status_msg.data = f'Processing: {command}'
        self.status_pub.publish(status_msg)

        # Get environment state
        env_state = self.get_environment_state()

        # Plan and execute
        plan = self.planner.create_plan(command, env_state)
        if plan:
            self.execute_plan(plan)

    def vision_callback(self, msg):
        """Handle vision data"""
        try:
            self.current_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')
        except Exception as e:
            self.get_logger().error(f'Vision error: {e}')

    def scan_callback(self, msg):
        """Handle laser scan data"""
        self.latest_scan = msg

    def get_environment_state(self):
        """Get current environment state"""
        env_state = {
            "objects": [],
            "robot_position": [0.0, 0.0, 0.0],
            "gripper_status": "open"
        }

        # Add vision data if available
        if self.current_image is not None:
            try:
                detected_objects = self.vision_system.detect_objects(self.current_image)
                env_state["objects"] = detected_objects
            except Exception as e:
                self.get_logger().error(f'Vision processing error: {e}')

        # Add proximity data
        if self.latest_scan:
            min_distance = min(self.latest_scan.ranges) if self.latest_scan.ranges else float('inf')
            env_state["min_obstacle_distance"] = min_distance

        return env_state

    def execute_plan(self, plan):
        """Execute a planned sequence of actions"""
        if self.is_executing:
            self.get_logger().warn('Plan execution already in progress')
            return

        self.is_executing = True
        self.active_plan = plan

        self.get_logger().info(f'Executing plan with {len(plan)} actions')

        for i, action in enumerate(plan):
            self.get_logger().info(f'Executing action {i+1}/{len(plan)}: {action["action"]}')

            success = self.execute_action(action)
            if not success:
                self.get_logger().error(f'Action failed: {action}')
                break

            # Small delay between actions
            time.sleep(0.1)

        self.is_executing = False
        self.active_plan = []

        # Update status
        status_msg = String()
        status_msg.data = 'Plan completed'
        self.status_pub.publish(status_msg)

    def execute_action(self, action):
        """Execute a single action"""
        action_type = action.get('action', '')
        params = action.get('parameters', {})

        if action_type == 'move_to_location':
            return self.move_to_location(params)
        elif action_type == 'wait':
            duration = params.get('duration', 1.0)
            time.sleep(duration)
            return True
        else:
            # For other actions, publish as command
            cmd_msg = String()
            cmd_msg.data = f'{action_type}_{json.dumps(params)}'
            self.cmd_pub.publish(cmd_msg)
            return True

    def move_to_location(self, params):
        """Move robot to specified location"""
        target_x = params.get('x', 0.0)
        target_y = params.get('y', 0.0)

        # Simple proportional controller
        current_x, current_y = 0.0, 0.0  # In real system, get from odometry

        dx = target_x - current_x
        dy = target_y - current_y
        distance = (dx**2 + dy**2)**0.5

        if distance > 0.1:  # If not close enough
            cmd = Twist()
            cmd.linear.x = min(0.5, distance) * (1 if dx > 0 else -1)  # Simplified
            cmd.angular.z = min(0.5, abs(dy)) * (1 if dy > 0 else -1)  # Simplified

            self.cmd_pub.publish(cmd)
            time.sleep(0.5)  # Move for half a second
            cmd.linear.x = 0.0
            cmd.angular.z = 0.0
            self.cmd_pub.publish(cmd)

        return True

def main(args=None):
    rclpy.init(args=args)
    node = CompleteVLASystem()

    try:
        rclpy.spin(node)
    except KeyboardInterrupt:
        node.get_logger().info('Shutting down Complete VLA System')
    finally:
        node.voice_processor.stop_listening()
        node.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()
```

## Summary
This chapter covered Vision-Language-Action (VLA) systems for robotics, including speech recognition with Whisper, natural language processing with LLMs, multimodal interaction, and safety protocols. You've learned how to create systems that understand natural language commands, perceive their environment visually, and execute complex tasks safely. The next chapter will integrate all these concepts in a comprehensive capstone project.

============================================================
FILE: book\docs\part-i-foundations\chapter-1-introduction\index.md
============================================================
---
title: Chapter 1 - Introduction to Physical AI & Embodied Intelligence
sidebar_position: 1
---

# Chapter 1: Introduction to Physical AI & Embodied Intelligence

## Learning Goals

- Understand the scope and applications of humanoid robotics
- Distinguish between physical AI and traditional AI
- Identify key challenges in humanoid robotics
- Set up ROS 2 development environment
- Create first ROS 2 package and nodes
- Launch basic simulation environment

## What is Physical AI?

Physical AI, also known as embodied AI, represents a paradigm shift from traditional artificial intelligence that operates purely in digital spaces to AI systems that interact with and operate within the physical world. Unlike conventional AI that processes data and makes decisions in virtual environments, physical AI systems must navigate the complexities of real-world physics, sensorimotor integration, and dynamic environments.

### Traditional AI vs. Physical AI

Traditional AI systems typically operate on well-structured data in controlled environments. They might process text, images, or numerical data with predictable inputs and outputs. In contrast, physical AI systems must handle:

- **Real-time constraints**: Decisions must be made within strict timing requirements
- **Sensorimotor integration**: Coordinating multiple sensors and actuators simultaneously
- **Uncertainty and noise**: Real-world sensors provide imperfect, noisy data
- **Physics constraints**: Systems must respect laws of physics and dynamics
- **Embodiment effects**: The physical form influences perception and action

### Key Characteristics of Physical AI

1. **Embodiment**: The system has a physical form that interacts with the environment
2. **Real-time processing**: Continuous interaction with the environment in real-time
3. **Sensorimotor coupling**: Perception and action are tightly integrated
4. **Adaptation**: Ability to adapt to changing environmental conditions
5. **Autonomy**: Capacity for independent operation within defined parameters

## Applications of Humanoid Robotics

Humanoid robots, with their human-like form factor, offer unique advantages in human environments:

### Service Robotics
- Assistive care for elderly and disabled individuals
- Customer service in retail and hospitality
- Domestic assistance and household tasks
- Educational companions and tutors

### Industrial Applications
- Collaborative robots (cobots) working alongside humans
- Inspection and maintenance in hazardous environments
- Quality control and assembly assistance
- Logistics and material handling

### Research and Development
- Human-robot interaction studies
- Cognitive science and psychology research
- Rehabilitation and therapy applications
- Social robotics research

### Entertainment and Social Interaction
- Theme park attractions and guides
- Entertainment and performance robots
- Social companions for emotional support
- Cultural and educational exhibits

## Key Challenges in Humanoid Robotics

### Balance and Locomotion
Maintaining balance while walking, running, or performing complex movements requires sophisticated control algorithms that can handle the dynamic nature of bipedal locomotion. Humanoid robots must manage their center of mass, adapt to terrain variations, and recover from disturbances.

### Perception in Dynamic Environments
Humanoid robots must perceive and understand their environment while moving. This includes:
- Real-time object recognition and tracking
- Scene understanding and spatial reasoning
- Human pose and gesture recognition
- Environmental mapping and navigation

### Human-Robot Interaction
Creating natural, intuitive interactions between humans and humanoid robots involves:
- Natural language understanding and generation
- Social cues recognition and response
- Emotional intelligence and empathy
- Cultural sensitivity and adaptation

### Integration Complexity
Humanoid robots integrate multiple complex systems:
- Perception systems (vision, audio, touch)
- Planning and reasoning systems
- Control systems for multiple degrees of freedom
- Communication and coordination mechanisms

## Setting Up Your Development Environment

To work with humanoid robotics, you'll need to set up several key tools and frameworks. We'll focus on the Robot Operating System 2 (ROS 2), which provides the middleware infrastructure for robotics applications.

### Installing ROS 2 Humble Hawksbill

ROS 2 is the primary framework we'll use throughout this book. Follow these steps to install ROS 2 Humble Hawksbill (the LTS version):

```bash
# Add the ROS 2 apt repository
sudo apt update && sudo apt install -y software-properties-common
sudo add-apt-repository universe

# Add the ROS 2 GPG key
sudo apt update && sudo apt install -y curl gnupg lsb-release
curl -sSL https://raw.githubusercontent.com/ros/rosdistro/master/ros.key | sudo gpg --dearmor -o /usr/share/keyrings/ros-archive-keyring.gpg

# Add the repository to your sources list
echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/ros-archive-keyring.gpg] http://packages.ros.org/ros2/ubuntu $(source /etc/os-release && echo $UBUNTU_CODENAME) main" | sudo tee /etc/apt/sources.list.d/ros2.list > /dev/null

# Install ROS 2 packages
sudo apt update
sudo apt install -y ros-humble-desktop
sudo apt install -y python3-rosdep2
sudo apt install -y python3-colcon-common-extensions

# Initialize rosdep
sudo rosdep init
rosdep update

# Source the ROS 2 setup script
echo "source /opt/ros/humble/setup.bash" >> ~/.bashrc
source ~/.bashrc
```

### Installing Gazebo Simulation

Gazebo provides the physics simulation environment for testing our robots:

```bash
# Install Gazebo Garden (recommended version for ROS 2 Humble)
sudo apt install -y gazebo libgazebo-dev
# Or install the ROS 2 specific version
sudo apt install -y ros-humble-gazebo-*
```

### Creating Your First ROS 2 Package

Let's create your first ROS 2 package to get familiar with the development workflow:

```bash
# Create a workspace directory
mkdir -p ~/robotics_ws/src
cd ~/robotics_ws

# Create a new package
ros2 pkg create --build-type ament_python my_first_robot --dependencies rclpy std_msgs

# Navigate to the package
cd src/my_first_robot
```

The package structure will look like this:
```
my_first_robot/
‚îú‚îÄ‚îÄ my_first_robot/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ my_first_robot.py
‚îú‚îÄ‚îÄ test/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ test_copyright.py
‚îÇ   ‚îî‚îÄ‚îÄ test_flake8.py
‚îÇ   ‚îî‚îÄ‚îÄ test_pep257.py
‚îú‚îÄ‚îÄ package.xml
‚îú‚îÄ‚îÄ setup.cfg
‚îú‚îÄ‚îÄ setup.py
‚îî‚îÄ‚îÄ README.md
```

### Creating Your First ROS 2 Node

Let's create a simple publisher node that publishes messages to a topic:

```python
# In my_first_robot/my_first_robot/simple_publisher.py
import rclpy
from rclpy.node import Node
from std_msgs.msg import String


class SimplePublisher(Node):
    def __init__(self):
        super().__init__('simple_publisher')
        self.publisher_ = self.create_publisher(String, 'robot_messages', 10)
        timer_period = 0.5  # seconds
        self.timer = self.create_timer(timer_period, self.timer_callback)
        self.i = 0

    def timer_callback(self):
        msg = String()
        msg.data = f'Hello Robot World: {self.i}'
        self.publisher_.publish(msg)
        self.get_logger().info(f'Publishing: "{msg.data}"')
        self.i += 1


def main(args=None):
    rclpy.init(args=args)
    simple_publisher = SimplePublisher()
    rclpy.spin(simple_publisher)
    simple_publisher.destroy_node()
    rclpy.shutdown()


if __name__ == '__main__':
    main()
```

### Creating a Subscriber Node

Now let's create a subscriber node that listens to messages:

```python
# In my_first_robot/my_first_robot/simple_subscriber.py
import rclpy
from rclpy.node import Node
from std_msgs.msg import String


class SimpleSubscriber(Node):
    def __init__(self):
        super().__init__('simple_subscriber')
        self.subscription = self.create_subscription(
            String,
            'robot_messages',
            self.listener_callback,
            10)
        self.subscription  # prevent unused variable warning

    def listener_callback(self, msg):
        self.get_logger().info(f'I heard: "{msg.data}"')


def main(args=None):
    rclpy.init(args=args)
    simple_subscriber = SimpleSubscriber()
    rclpy.spin(simple_subscriber)
    simple_subscriber.destroy_node()
    rclpy.shutdown()


if __name__ == '__main__':
    main()
```

### Running Your Nodes

To run your nodes, you'll need to build your package first:

```bash
# Go back to the workspace root
cd ~/robotics_ws

# Build the package
colcon build --packages-select my_first_robot

# Source the workspace
source install/setup.bash

# Run the publisher in one terminal
ros2 run my_first_robot simple_publisher

# Run the subscriber in another terminal
ros2 run my_first_robot simple_subscriber
```

## Introduction to Robot Simulation

Simulation is a crucial component of robotics development, allowing you to test algorithms and behaviors in a safe, controlled environment before deploying to real hardware.

### Basic Gazebo Simulation

Let's create a simple launch file to start a basic simulation:

```python
# In my_first_robot/my_first_robot/launch/simple_sim.launch.py
from launch import LaunchDescription
from launch.actions import ExecuteProcess
from launch_ros.actions import Node


def generate_launch_description():
    return LaunchDescription([
        # Start Gazebo
        ExecuteProcess(
            cmd=['gazebo', '--verbose', '-s', 'libgazebo_ros_factory.so'],
            output='screen'
        ),

        # Start our publisher node
        Node(
            package='my_first_robot',
            executable='simple_publisher',
            output='screen'
        ),
    ])
```

### Adding the Launch File to Setup

Update your setup.py to include the launch files:

```python
# In setup.py, add to the data_files section:
data_files=[
    # ... existing entries ...
    ('share/' + package_name + '/launch', glob.glob('launch/*.launch.py')),
],
```

## Hands-On Lab: Environment Setup and First Robot

### Objective
Set up your development environment and create your first ROS 2 package with publisher and subscriber nodes.

### Prerequisites
- Ubuntu 22.04 (or equivalent Linux distribution)
- Administrative access to install packages
- Basic Python knowledge

### Steps

1. **Install ROS 2 Humble Hawksbill** following the installation instructions above

2. **Create your workspace**:
   ```bash
   mkdir -p ~/robotics_ws/src
   cd ~/robotics_ws/src
   ```

3. **Create the package**:
   ```bash
   ros2 pkg create --build-type ament_python my_first_robot --dependencies rclpy std_msgs
   ```

4. **Add the publisher code** to `my_first_robot/my_first_robot/simple_publisher.py`

5. **Add the subscriber code** to `my_first_robot/my_first_robot/simple_subscriber.py`

6. **Update the setup.py file** to include executables:
   ```python
   entry_points={
       'console_scripts': [
           'simple_publisher = my_first_robot.simple_publisher:main',
           'simple_subscriber = my_first_robot.simple_subscriber:main',
       ],
   },
   ```

7. **Build and run**:
   ```bash
   cd ~/robotics_ws
   colcon build --packages-select my_first_robot
   source install/setup.bash
   ros2 run my_first_robot simple_publisher
   ```

8. **In a separate terminal**, run the subscriber:
   ```bash
   cd ~/robotics_ws
   source install/setup.bash
   ros2 run my_first_robot simple_subscriber
   ```

### Expected Results
- The publisher should output messages to the terminal every 0.5 seconds
- The subscriber should receive and display these messages
- Both nodes should communicate successfully through ROS 2 topics

### Troubleshooting Tips
- Ensure ROS 2 environment is sourced in each terminal
- Check that package names and executable names match exactly
- Verify that the package was built successfully with no errors

## Summary

In this chapter, we've introduced the fundamental concepts of Physical AI and embodied intelligence, explored the applications and challenges of humanoid robotics, and set up our development environment. You've created your first ROS 2 package with publisher and subscriber nodes, establishing the foundation for more complex robotics applications.

The next chapter will dive deeper into ROS 2 fundamentals, exploring topics, services, actions, and more advanced communication patterns that form the backbone of robotics software architecture.

============================================================
FILE: book\docs\part-i-foundations\chapter-2-ros-fundamentals\index.md
============================================================
---
title: Chapter 2 - Robot Operating System (ROS 2) Fundamentals
sidebar_position: 2
---

# Chapter 2: Robot Operating System (ROS 2) Fundamentals

## Learning Goals

- Master ROS 2 architecture and communication patterns
- Understand nodes, topics, services, and actions
- Learn about parameter management and launch files
- Build a multi-node system for robot control
- Implement custom message types and services
- Create launch files for complex robot systems

## Introduction to ROS 2

The Robot Operating System 2 (ROS 2) is not an actual operating system, but rather a flexible framework for writing robot software. It provides services designed for a heterogeneous computer cluster, including hardware abstraction, device drivers, libraries, visualizers, message-passing, package management, and more.

ROS 2 is the second generation of the Robot Operating System, designed to address the limitations of ROS 1 and to provide features needed for production robotics applications, including improved security, real-time support, and better cross-platform compatibility.

### Evolution from ROS 1 to ROS 2

ROS 2 was developed to address several key limitations of ROS 1:

- **Real-time support**: ROS 2 provides better support for real-time systems
- **Multi-robot systems**: Improved support for multiple robots and distributed systems
- **Production deployment**: Better tools and practices for deploying ROS in production
- **Cross-platform support**: Expanded platform support including Windows and macOS
- **Security**: Built-in security features for protected communication
- **DDS Integration**: Uses Data Distribution Service (DDS) as the underlying communication middleware

## ROS 2 Architecture

### Nodes

A node is an executable that uses ROS 2 to communicate with other nodes. Nodes are the fundamental building blocks of a ROS 2 system. Each node is designed to perform specific computations and can communicate with other nodes through various mechanisms.

In ROS 2, nodes are more robust than in ROS 1. They can be created and destroyed more easily, and they provide better introspection capabilities.

```python
# Example of a basic ROS 2 node
import rclpy
from rclpy.node import Node


class MinimalNode(Node):
    def __init__(self):
        super().__init__('minimal_node')
        self.get_logger().info('Hello from minimal node!')


def main(args=None):
    rclpy.init(args=args)
    node = MinimalNode()

    # Keep the node running
    try:
        rclpy.spin(node)
    except KeyboardInterrupt:
        pass
    finally:
        node.destroy_node()
        rclpy.shutdown()


if __name__ == '__main__':
    main()
```

### Topics and Message Passing

Topics are named buses over which nodes exchange messages. They implement a publish/subscribe communication pattern where publishers send messages to a topic and subscribers receive messages from a topic.

The communication is asynchronous - publishers don't wait for subscribers and vice versa. Multiple publishers can publish to the same topic, and multiple subscribers can subscribe to the same topic.

```python
# Publisher example
import rclpy
from rclpy.node import Node
from std_msgs.msg import String


class MinimalPublisher(Node):
    def __init__(self):
        super().__init__('minimal_publisher')
        self.publisher_ = self.create_publisher(String, 'topic', 10)
        timer_period = 0.5  # seconds
        self.timer = self.create_timer(timer_period, self.timer_callback)
        self.i = 0

    def timer_callback(self):
        msg = String()
        msg.data = f'Hello World: {self.i}'
        self.publisher_.publish(msg)
        self.get_logger().info(f'Publishing: "{msg.data}"')
        self.i += 1


def main(args=None):
    rclpy.init(args=args)
    minimal_publisher = MinimalPublisher()
    rclpy.spin(minimal_publisher)
    minimal_publisher.destroy_node()
    rclpy.shutdown()


if __name__ == '__main__':
    main()
```

```python
# Subscriber example
import rclpy
from rclpy.node import Node
from std_msgs.msg import String


class MinimalSubscriber(Node):
    def __init__(self):
        super().__init__('minimal_subscriber')
        self.subscription = self.create_subscription(
            String,
            'topic',
            self.listener_callback,
            10)
        self.subscription  # prevent unused variable warning

    def listener_callback(self, msg):
        self.get_logger().info(f'I heard: "{msg.data}"')


def main(args=None):
    rclpy.init(args=args)
    minimal_subscriber = MinimalSubscriber()
    rclpy.spin(minimal_subscriber)
    minimal_subscriber.destroy_node()
    rclpy.shutdown()


if __name__ == '__main__':
    main()
```

### Services

Services implement a request/reply communication pattern. A service client sends a request to a service server, which processes the request and sends back a response. This is synchronous communication - the client waits for the response.

Services are defined by `.srv` files that specify the request and response message types.

```python
# Service definition example (in srv/AddTwoInts.srv):
# int64 a
# int64 b
# ---
# int64 sum
```

```python
# Service server example
import rclpy
from rclpy.node import Node
from example_interfaces.srv import AddTwoInts


class MinimalService(Node):
    def __init__(self):
        super().__init__('minimal_service')
        self.srv = self.create_service(AddTwoInts, 'add_two_ints', self.add_two_ints_callback)

    def add_two_ints_callback(self, request, response):
        response.sum = request.a + request.b
        self.get_logger().info(f'Returning {request.a} + {request.b} = {response.sum}')
        return response


def main(args=None):
    rclpy.init(args=args)
    minimal_service = MinimalService()
    rclpy.spin(minimal_service)
    rclpy.shutdown()


if __name__ == '__main__':
    main()
```

```python
# Service client example
import sys
import rclpy
from rclpy.node import Node
from example_interfaces.srv import AddTwoInts


class MinimalClientAsync(Node):
    def __init__(self):
        super().__init__('minimal_client_async')
        self.cli = self.create_client(AddTwoInts, 'add_two_ints')
        while not self.cli.wait_for_service(timeout_sec=1.0):
            self.get_logger().info('Service not available, waiting again...')
        self.req = AddTwoInts.Request()

    def send_request(self, a, b):
        self.req.a = a
        self.req.b = b
        self.future = self.cli.call_async(self.req)
        rclpy.spin_until_future_complete(self, self.future)
        return self.future.result()


def main(args=None):
    rclpy.init(args=args)
    minimal_client = MinimalClientAsync()
    response = minimal_client.send_request(int(sys.argv[1]), int(sys.argv[2]))
    minimal_client.get_logger().info(f'Result of add_two_ints: {response.sum}')
    minimal_client.destroy_node()
    rclpy.shutdown()


if __name__ == '__main__':
    main()
```

### Actions

Actions are a more advanced communication pattern that allows for long-running tasks with feedback and goal management. They're ideal for tasks like navigation, where you want to track progress and potentially cancel the operation.

Actions consist of three message types:
- Goal: What the action should do
- Result: What happened when the action completed
- Feedback: Periodic updates on progress

```python
# Action example for navigation
import rclpy
from rclpy.action import ActionServer
from rclpy.node import Node
from nav2_msgs.action import NavigateToPose


class NavigateToPoseActionServer(Node):
    def __init__(self):
        super().__init__('navigate_to_pose_action_server')
        self._action_server = ActionServer(
            self,
            NavigateToPose,
            'navigate_to_pose',
            self.execute_callback)

    def execute_callback(self, goal_handle):
        self.get_logger().info('Executing goal...')

        # Simulate navigation progress
        feedback_msg = NavigateToPose.Feedback()
        feedback_msg.current_pose = goal_handle.request.pose

        # Simulate navigation
        for i in range(0, 101, 10):
            if goal_handle.is_cancel_requested:
                goal_handle.canceled()
                self.get_logger().info('Goal canceled')
                return NavigateToPose.Result()

            feedback_msg.feedback = f'Navigating: {i}% complete'
            goal_handle.publish_feedback(feedback_msg)

            time.sleep(0.5)  # Simulate work

        goal_handle.succeed()
        result = NavigateToPose.Result()
        result.result = True
        self.get_logger().info('Goal succeeded')
        return result


def main(args=None):
    rclpy.init(args=args)
    action_server = NavigateToPoseActionServer()
    rclpy.spin(action_server)
    rclpy.shutdown()


if __name__ == '__main__':
    main()
```

## Parameters

Parameters in ROS 2 are named values that can be set at runtime and changed dynamically. They provide a way to configure nodes without recompiling. Parameters can be set through launch files, command line, or programmatically.

```python
# Parameter usage example
import rclpy
from rclpy.node import Node


class ParameterNode(Node):
    def __init__(self):
        super().__init__('parameter_node')

        # Declare parameters with default values
        self.declare_parameter('robot_name', 'default_robot')
        self.declare_parameter('max_velocity', 1.0)
        self.declare_parameter('safety_enabled', True)

        # Get parameter values
        self.robot_name = self.get_parameter('robot_name').value
        self.max_velocity = self.get_parameter('max_velocity').value
        self.safety_enabled = self.get_parameter('safety_enabled').value

        self.get_logger().info(f'Robot name: {self.robot_name}')
        self.get_logger().info(f'Max velocity: {self.max_velocity}')
        self.get_logger().info(f'Safety enabled: {self.safety_enabled}')

        # Set up parameter callback
        self.add_on_set_parameters_callback(self.parameter_callback)

    def parameter_callback(self, params):
        for param in params:
            if param.name == 'max_velocity' and param.type_ == Parameter.Type.PARAMETER_DOUBLE:
                if param.value > 5.0:
                    return SetParametersResult(successful=False, reason='Max velocity too high')
        return SetParametersResult(successful=True)


def main(args=None):
    rclpy.init(args=args)
    node = ParameterNode()
    rclpy.spin(node)
    rclpy.shutdown()


if __name__ == '__main__':
    main()
```

## Launch Files

Launch files in ROS 2 allow you to start multiple nodes with a single command. They provide a way to configure and start complex systems with many interconnected nodes. Launch files are written in Python and offer powerful features like conditional launching, parameter setting, and node remapping.

```python
# Example launch file (launch/robot_system.launch.py)
from launch import LaunchDescription
from launch.actions import DeclareLaunchArgument
from launch.substitutions import LaunchConfiguration
from launch_ros.actions import Node


def generate_launch_description():
    # Declare launch arguments
    use_sim_time = LaunchConfiguration('use_sim_time')

    # Declare launch argument
    declare_use_sim_time_cmd = DeclareLaunchArgument(
        'use_sim_time',
        default_value='false',
        description='Use simulation (Gazebo) clock if true')

    # Create nodes
    minimal_publisher = Node(
        package='demo_nodes_py',
        executable='talker',
        name='publisher_node',
        parameters=[{'use_sim_time': use_sim_time}],
        remappings=[('chatter', 'robot_messages')]
    )

    minimal_subscriber = Node(
        package='demo_nodes_py',
        executable='listener',
        name='subscriber_node',
        parameters=[{'use_sim_time': use_sim_time}]
    )

    # Create launch description
    ld = LaunchDescription()

    # Add launch arguments
    ld.add_action(declare_use_sim_time_cmd)

    # Add nodes
    ld.add_action(minimal_publisher)
    ld.add_action(minimal_subscriber)

    return ld
```

## Creating Custom Message Types

ROS 2 allows you to define custom message types for your specific applications. Messages are defined using the Interface Definition Language (IDL) and are stored in `.msg` files.

### Defining a Custom Message

Create a file named `RobotStatus.msg` in your package's `msg/` directory:

```
# RobotStatus.msg
string robot_name
float64 battery_level
bool is_charging
int32[] joint_positions
geometry_msgs/Pose current_pose
```

### Using Custom Messages

```python
# Publisher using custom message
import rclpy
from rclpy.node import Node
from your_package_name.msg import RobotStatus  # Import your custom message
from geometry_msgs.msg import Pose


class RobotStatusPublisher(Node):
    def __init__(self):
        super().__init__('robot_status_publisher')
        self.publisher_ = self.create_publisher(RobotStatus, 'robot_status', 10)
        timer_period = 1.0  # seconds
        self.timer = self.create_timer(timer_period, self.timer_callback)

    def timer_callback(self):
        msg = RobotStatus()
        msg.robot_name = "MyRobot"
        msg.battery_level = 85.5
        msg.is_charging = False
        msg.joint_positions = [0, 45, 90, -45, 0, 30]  # Example joint angles

        # Set pose
        msg.current_pose.position.x = 1.0
        msg.current_pose.position.y = 2.0
        msg.current_pose.position.z = 0.0
        msg.current_pose.orientation.w = 1.0

        self.publisher_.publish(msg)
        self.get_logger().info(f'Published robot status for {msg.robot_name}')


def main(args=None):
    rclpy.init(args=args)
    robot_status_publisher = RobotStatusPublisher()
    rclpy.spin(robot_status_publisher)
    robot_status_publisher.destroy_node()
    rclpy.shutdown()


if __name__ == '__main__':
    main()
```

## Advanced Communication Patterns

### Quality of Service (QoS) Settings

QoS settings allow you to control the reliability and durability of message transmission:

```python
from rclpy.qos import QoSProfile, ReliabilityPolicy, DurabilityPolicy

# Create a QoS profile for reliable communication
reliable_qos = QoSProfile(
    depth=10,
    reliability=ReliabilityPolicy.RELIABLE,
    durability=DurabilityPolicy.VOLATILE
)

# Create a QoS profile for best-effort communication
best_effort_qos = QoSProfile(
    depth=10,
    reliability=ReliabilityPolicy.BEST_EFFORT,
    durability=DurabilityPolicy.VOLATILE
)

# Use in publisher
publisher = self.create_publisher(String, 'topic', reliable_qos)
```

### Lifecycle Nodes

Lifecycle nodes provide a way to manage the state of nodes through a well-defined state machine:

```python
from rclpy.lifecycle import LifecycleNode, TransitionCallbackReturn
from rclpy.lifecycle import LifecycleState


class LifecycleDemoNode(LifecycleNode):
    def __init__(self):
        super().__init__('lifecycle_demo_node')
        self.get_logger().info('Constructor called.')

    def on_configure(self, state: LifecycleState):
        self.get_logger().info('on_configure() is called.')
        return TransitionCallbackReturn.SUCCESS

    def on_activate(self, state: LifecycleState):
        self.get_logger().info('on_activate() is called.')
        return TransitionCallbackReturn.SUCCESS

    def on_deactivate(self, state: LifecycleState):
        self.get_logger().info('on_deactivate() is called.')
        return TransitionCallbackReturn.SUCCESS

    def on_cleanup(self, state: LifecycleState):
        self.get_logger().info('on_cleanup() is called.')
        return TransitionCallbackReturn.SUCCESS
```

## Hands-On Lab: Multi-Node Robot Control System

### Objective
Create a multi-node system that simulates robot control with status monitoring and command processing.

### Prerequisites
- Completed Chapter 1 setup
- ROS 2 Humble installed

### Steps

1. **Create a new package** for the lab:
   ```bash
   cd ~/robotics_ws/src
   ros2 pkg create --build-type ament_python robot_control_lab --dependencies rclpy std_msgs geometry_msgs
   ```

2. **Create the robot controller node** (`robot_control_lab/robot_control_lab/robot_controller.py`):
   ```python
   import rclpy
   from rclpy.node import Node
   from std_msgs.msg import String
   from geometry_msgs.msg import Twist
   import time


   class RobotController(Node):
       def __init__(self):
           super().__init__('robot_controller')

           # Create publisher for velocity commands
           self.cmd_vel_pub = self.create_publisher(Twist, 'cmd_vel', 10)

           # Create subscriber for movement commands
           self.cmd_sub = self.create_subscription(
               String,
               'movement_commands',
               self.command_callback,
               10
           )

           # Create timer for status updates
           self.status_timer = self.create_timer(1.0, self.publish_status)

           self.get_logger().info('Robot Controller initialized')

       def command_callback(self, msg):
           command = msg.data.lower()
           twist = Twist()

           if command == 'forward':
               twist.linear.x = 1.0
           elif command == 'backward':
               twist.linear.x = -1.0
           elif command == 'left':
               twist.angular.z = 1.0
           elif command == 'right':
               twist.angular.z = -1.0
           elif command == 'stop':
               twist.linear.x = 0.0
               twist.angular.z = 0.0
           else:
               self.get_logger().warn(f'Unknown command: {command}')
               return

           self.cmd_vel_pub.publish(twist)
           self.get_logger().info(f'Executing command: {command}')

       def publish_status(self):
           status_msg = String()
           status_msg.data = f'Robot status: OK - {time.time()}'
           status_pub = self.create_publisher(String, 'robot_status', 10)
           status_pub.publish(status_msg)


   def main(args=None):
       rclpy.init(args=args)
       controller = RobotController()
       rclpy.spin(controller)
       controller.destroy_node()
       rclpy.shutdown()


   if __name__ == '__main__':
       main()
   ```

3. **Create the command interface node** (`robot_control_lab/robot_control_lab/command_interface.py`):
   ```python
   import rclpy
   from rclpy.node import Node
   from std_msgs.msg import String
   import sys


   class CommandInterface(Node):
       def __init__(self):
           super().__init__('command_interface')
           self.publisher = self.create_publisher(String, 'movement_commands', 10)
           self.get_logger().info('Command Interface ready. Send commands: forward, backward, left, right, stop')

       def send_command(self, command):
           msg = String()
           msg.data = command
           self.publisher.publish(msg)
           self.get_logger().info(f'Sent command: {command}')


   def main(args=None):
       rclpy.init(args=args)
       interface = CommandInterface()

       if len(sys.argv) > 1:
           command = sys.argv[1]
           interface.send_command(command)
       else:
           print("Usage: ros2 run robot_control_lab command_interface <command>")
           print("Commands: forward, backward, left, right, stop")

       # Keep node alive briefly to send message
       interface.send_command('stop')  # Ensure robot stops
       interface.destroy_node()
       rclpy.shutdown()


   if __name__ == '__main__':
       main()
   ```

4. **Update setup.py** to include executables:
   ```python
   import os
   from glob import glob
   from setuptools import setup
   from setuptools import find_packages

   package_name = 'robot_control_lab'

   setup(
       name=package_name,
       version='0.0.0',
       packages=find_packages(exclude=['test']),
       data_files=[
           ('share/ament_index/resource_index/packages',
               ['resource/' + package_name]),
           ('share/' + package_name, ['package.xml']),
       ],
       install_requires=['setuptools'],
       zip_safe=True,
       maintainer='Your Name',
       maintainer_email='your.email@example.com',
       description='Robot Control Lab for ROS 2',
       license='Apache License 2.0',
       tests_require=['pytest'],
       entry_points={
           'console_scripts': [
               'robot_controller = robot_control_lab.robot_controller:main',
               'command_interface = robot_control_lab.command_interface:main',
           ],
       },
   )
   ```

5. **Build the package**:
   ```bash
   cd ~/robotics_ws
   colcon build --packages-select robot_control_lab
   source install/setup.bash
   ```

6. **Run the multi-node system**:
   ```bash
   # Terminal 1: Start the robot controller
   ros2 run robot_control_lab robot_controller

   # Terminal 2: Send commands
   ros2 run robot_control_lab command_interface forward
   ros2 run robot_control_lab command_interface left
   ros2 run robot_control_lab command_interface stop
   ```

### Expected Results
- The robot controller node should respond to movement commands
- Velocity commands should be published to the `/cmd_vel` topic
- Status messages should be published periodically
- The system should demonstrate proper node communication

### Troubleshooting Tips
- Ensure all packages are built and sourced
- Check topic names match between publisher and subscriber
- Verify node names don't conflict
- Use `ros2 topic list` and `ros2 node list` to verify connections

## Summary

In this chapter, we've explored the fundamental concepts of ROS 2, including nodes, topics, services, actions, parameters, and launch files. You've learned how to create multi-node systems and implement custom message types. The hands-on lab provided practical experience with a robot control system that demonstrates these concepts in action.

These fundamentals form the backbone of any ROS 2-based robotics application. Understanding these concepts is crucial for developing more complex robotic systems in the subsequent chapters, where we'll apply these principles to perception, control, and intelligence systems.

============================================================
FILE: book\docs\part-i-foundations\chapter-3-robot-modeling\index.md
============================================================
---
title: Chapter 3 - Robot Modeling and Simulation Fundamentals
sidebar_position: 3
---

# Chapter 3: Robot Modeling and Simulation Fundamentals

## Learning Goals

- Understand URDF and SDF robot description formats
- Learn kinematic and dynamic modeling concepts
- Master simulation environment setup
- Create URDF model of a simple humanoid robot
- Simulate robot in Gazebo environment
- Visualize robot in RViz

## Introduction to Robot Modeling

Robot modeling is the process of creating digital representations of physical robots that can be used for simulation, visualization, and control development. In robotics, we use standardized formats to describe robot geometry, kinematics, dynamics, and sensors. The two primary formats in ROS are URDF (Unified Robot Description Format) for ROS-based systems and SDF (Simulation Description Format) for Gazebo simulation.

### Why Robot Modeling Matters

Robot modeling is crucial for several reasons:

1. **Simulation**: Test algorithms without physical hardware
2. **Visualization**: Understand robot kinematics and motion
3. **Control Development**: Develop and test controllers in a safe environment
4. **Collision Detection**: Prevent self-collision and environment collision
5. **Sensor Simulation**: Test perception algorithms with realistic sensor data

## Unified Robot Description Format (URDF)

URDF is an XML-based format that describes robot models in ROS. It defines the robot's physical properties including links (rigid bodies), joints (connections between links), and other elements like sensors and actuators.

### URDF Structure

A URDF file contains several key elements:

- **Links**: Rigid bodies that make up the robot
- **Joints**: Connections between links with specific degrees of freedom
- **Materials**: Visual properties like color and texture
- **Gazebo plugins**: Simulation-specific extensions

### Basic URDF Example

```xml
<?xml version="1.0"?>
<robot name="simple_robot">
  <!-- Base link -->
  <link name="base_link">
    <visual>
      <geometry>
        <cylinder length="0.6" radius="0.2"/>
      </geometry>
      <material name="blue">
        <color rgba="0 0 0.8 1"/>
      </material>
    </visual>
    <collision>
      <geometry>
        <cylinder length="0.6" radius="0.2"/>
      </geometry>
    </collision>
    <inertial>
      <mass value="10"/>
      <inertia ixx="1.0" ixy="0.0" ixz="0.0" iyy="1.0" iyz="0.0" izz="1.0"/>
    </inertial>
  </link>

  <!-- Arm link -->
  <link name="arm_link">
    <visual>
      <geometry>
        <box size="0.1 0.1 0.5"/>
      </geometry>
      <material name="red">
        <color rgba="0.8 0 0 1"/>
      </material>
    </visual>
    <collision>
      <geometry>
        <box size="0.1 0.1 0.5"/>
      </geometry>
    </collision>
    <inertial>
      <mass value="2"/>
      <inertia ixx="0.1" ixy="0.0" ixz="0.0" iyy="0.1" iyz="0.0" izz="0.1"/>
    </inertial>
  </link>

  <!-- Joint connecting base and arm -->
  <joint name="arm_joint" type="revolute">
    <parent link="base_link"/>
    <child link="arm_link"/>
    <origin xyz="0 0 0.4" rpy="0 0 0"/>
    <axis xyz="0 1 0"/>
    <limit lower="-1.57" upper="1.57" effort="100" velocity="1"/>
  </joint>
</robot>
```

### URDF Links

Links represent rigid bodies in the robot. Each link can have:

- **Visual**: How the link appears in visualization tools
- **Collision**: How the link interacts in collision detection
- **Inertial**: Physical properties for dynamics simulation

```xml
<link name="example_link">
  <!-- Visual properties -->
  <visual>
    <origin xyz="0 0 0" rpy="0 0 0"/>
    <geometry>
      <!-- Can be box, cylinder, sphere, or mesh -->
      <box size="0.1 0.1 0.1"/>
    </geometry>
    <material name="green">
      <color rgba="0 0.8 0 1"/>
    </material>
  </visual>

  <!-- Collision properties -->
  <collision>
    <origin xyz="0 0 0" rpy="0 0 0"/>
    <geometry>
      <box size="0.1 0.1 0.1"/>
    </geometry>
  </collision>

  <!-- Inertial properties -->
  <inertial>
    <mass value="0.5"/>
    <origin xyz="0 0 0" rpy="0 0 0"/>
    <inertia ixx="0.01" ixy="0.0" ixz="0.0" iyy="0.01" iyz="0.0" izz="0.01"/>
  </inertial>
</link>
```

### URDF Joints

Joints define how links connect and move relative to each other. Common joint types include:

- **Revolute**: Rotational joint with limited range
- **Continuous**: Rotational joint without limits
- **Prismatic**: Linear sliding joint
- **Fixed**: No movement (welded connection)
- **Floating**: 6 DOF (used for base of mobile robots)

```xml
<!-- Revolute joint example -->
<joint name="joint_name" type="revolute">
  <parent link="parent_link"/>
  <child link="child_link"/>
  <origin xyz="0.1 0 0" rpy="0 0 0"/>
  <axis xyz="0 0 1"/>
  <limit lower="-1.57" upper="1.57" effort="10" velocity="1"/>
  <dynamics damping="0.1" friction="0.0"/>
</joint>

<!-- Fixed joint example -->
<joint name="fixed_joint" type="fixed">
  <parent link="parent_link"/>
  <child link="child_link"/>
  <origin xyz="0 0 0.1" rpy="0 0 0"/>
</joint>
```

## Xacro: XML Macros for URDF

Xacro is a macro language that extends URDF, allowing you to create more maintainable and reusable robot descriptions through variables, properties, and includes.

### Basic Xacro Example

```xml
<?xml version="1.0"?>
<robot xmlns:xacro="http://www.ros.org/wiki/xacro" name="xacro_robot">

  <!-- Define properties -->
  <xacro:property name="M_PI" value="3.1415926535897931" />
  <xacro:property name="base_radius" value="0.2" />
  <xacro:property name="base_length" value="0.6" />

  <!-- Define a macro for a wheel -->
  <xacro:macro name="wheel" params="prefix parent xyz">
    <link name="${prefix}_wheel">
      <visual>
        <origin xyz="0 0 0" rpy="${M_PI/2} 0 0"/>
        <geometry>
          <cylinder radius="0.1" length="0.05"/>
        </geometry>
        <material name="black">
          <color rgba="0 0 0 1"/>
        </material>
      </visual>
      <collision>
        <origin xyz="0 0 0" rpy="${M_PI/2} 0 0"/>
        <geometry>
          <cylinder radius="0.1" length="0.05"/>
        </geometry>
      </collision>
      <inertial>
        <mass value="1"/>
        <inertia ixx="0.01" ixy="0.0" ixz="0.0" iyy="0.01" iyz="0.0" izz="0.02"/>
      </inertial>
    </link>

    <joint name="${prefix}_wheel_joint" type="continuous">
      <parent link="${parent}"/>
      <child link="${prefix}_wheel"/>
      <origin xyz="${xyz}" rpy="0 0 0"/>
      <axis xyz="0 1 0"/>
    </joint>
  </xacro:macro>

  <!-- Use the base link -->
  <link name="base_link">
    <visual>
      <geometry>
        <cylinder length="${base_length}" radius="${base_radius}"/>
      </geometry>
      <material name="blue">
        <color rgba="0 0 0.8 1"/>
      </material>
    </visual>
    <collision>
      <geometry>
        <cylinder length="${base_length}" radius="${base_radius}"/>
      </geometry>
    </collision>
    <inertial>
      <mass value="10"/>
      <inertia ixx="1.0" ixy="0.0" ixz="0.0" iyy="1.0" iyz="0.0" izz="2.0"/>
    </inertial>
  </link>

  <!-- Use the wheel macro -->
  <xacro:wheel prefix="front_left" parent="base_link" xyz="0.15 0.15 0"/>
  <xacro:wheel prefix="front_right" parent="base_link" xyz="0.15 -0.15 0"/>
  <xacro:wheel prefix="rear_left" parent="base_link" xyz="-0.15 0.15 0"/>
  <xacro:wheel prefix="rear_right" parent="base_link" xyz="-0.15 -0.15 0"/>

</robot>
```

## Simulation with Gazebo

Gazebo is a 3D simulation environment that provides high-fidelity physics simulation, realistic rendering, and convenient programmatic interfaces. It's commonly used with ROS for robot simulation.

### Gazebo Integration with URDF

To use your URDF model in Gazebo, you need to add Gazebo-specific extensions:

```xml
<!-- Add Gazebo-specific properties -->
<gazebo reference="base_link">
  <material>Gazebo/Blue</material>
  <turnGravityOff>false</turnGravityOff>
</gazebo>

<!-- Add transmission for joint control -->
<transmission name="arm_trans">
  <type>transmission_interface/SimpleTransmission</type>
  <joint name="arm_joint">
    <hardwareInterface>hardware_interface/PositionJointInterface</hardwareInterface>
  </joint>
  <actuator name="arm_motor">
    <hardwareInterface>hardware_interface/PositionJointInterface</hardwareInterface>
    <mechanicalReduction>1</mechanicalReduction>
  </actuator>
</transmission>

<!-- Add gazebo plugin for ROS control -->
<gazebo>
  <plugin name="gazebo_ros_control" filename="libgazebo_ros_control.so">
    <robotNamespace>/simple_robot</robotNamespace>
  </plugin>
</gazebo>
```

### Launching Gazebo with Your Robot

Create a launch file to spawn your robot in Gazebo:

```python
# launch/robot_spawn.launch.py
from launch import LaunchDescription
from launch.substitutions import LaunchConfiguration, PathJoinSubstitution
from launch_ros.actions import Node
from launch.actions import DeclareLaunchArgument, IncludeLaunchDescription
from launch.launch_description_sources import PythonLaunchDescriptionSource
from ament_index_python.packages import get_package_share_directory


def generate_launch_description():
    # Declare launch arguments
    model_arg = DeclareLaunchArgument(
        'model',
        default_value='simple_robot',
        description='Model name for the robot'
    )

    # Get URDF file path
    robot_description_path = PathJoinSubstitution([
        get_package_share_directory('your_robot_package'),
        'urdf',
        LaunchConfiguration('model') + '.urdf.xacro'
    ])

    # Robot State Publisher node
    robot_state_publisher = Node(
        package='robot_state_publisher',
        executable='robot_state_publisher',
        parameters=[{'robot_description': robot_description_path}]
    )

    # Gazebo server
    gzserver = IncludeLaunchDescription(
        PythonLaunchDescriptionSource([
            get_package_share_directory('gazebo_ros'),
            '/launch/gzserver.launch.py'
        ])
    )

    # Gazebo client (GUI)
    gzclient = IncludeLaunchDescription(
        PythonLaunchDescriptionSource([
            get_package_share_directory('gazebo_ros'),
            '/launch/gzclient.launch.py'
        ])
    )

    # Spawn entity in Gazebo
    spawn_entity = Node(
        package='gazebo_ros',
        executable='spawn_entity.py',
        arguments=[
            '-topic', 'robot_description',
            '-entity', LaunchConfiguration('model')
        ],
        output='screen'
    )

    return LaunchDescription([
        model_arg,
        robot_state_publisher,
        gzserver,
        gzclient,
        spawn_entity
    ])
```

## RViz Visualization

RViz is ROS's 3D visualization tool that allows you to visualize robot models, sensor data, paths, and other ROS messages in a 3D environment.

### Basic RViz Configuration

Create an RViz configuration file to properly visualize your robot:

```yaml
# config/robot_view.rviz
Panels:
  - Class: rviz_common/Displays
    Help Height: 78
    Name: Displays
    Property Tree Widget:
      Expanded:
        - /Global Options1
        - /Status1
        - /RobotModel1
      Splitter Ratio: 0.5
    Tree Height: 617
Visualization Manager:
  Class: ""
  Displays:
    - Alpha: 0.5
      Cell Size: 1
      Class: rviz_default_plugins/Grid
      Color: 160; 160; 164
      Enabled: true
      Line Style:
        Line Width: 0.029999999329447746
        Value: Lines
      Name: Grid
      Normal Cell Count: 0
      Offset:
        X: 0
        Y: 0
        Z: 0
      Plane: XY
      Plane Cell Count: 10
      Reference Frame: <Fixed Frame>
      Value: true
    - Alpha: 1
      Class: rviz_default_plugins/RobotModel
      Collision Enabled: false
      Enabled: true
      Links:
        All Links Enabled: true
        Expand Joint Details: false
        Expand Link Details: false
        Expand Tree: false
        Link Tree Style: Links in Alphabetic Order
      Name: RobotModel
      Robot Description: robot_description
      TF Prefix: ""
      Update Interval: 0
      Value: true
      Visual Enabled: true
  Enabled: true
  Global Options:
    Background Color: 48; 48; 48
    Fixed Frame: base_link
    Frame Rate: 30
  Name: root
  Tools:
    - Class: rviz_default_plugins/Interact
      Hide Inactive Objects: true
    - Class: rviz_default_plugins/MoveCamera
  Transformation:
    Current:
      Class: rviz_default_plugins/TF
  Value: true
  Views:
    Current:
      Class: rviz_default_plugins/Orbit
      Name: Current View
      Target Frame: <Fixed Frame>
      Value: Orbit (rviz)
    Saved: ~
Window Geometry:
  Displays:
    collapsed: false
  Height: 846
  Width: 1200
```

## Creating a Simple Humanoid Robot Model

Let's create a simplified humanoid robot model using URDF and Xacro:

```xml
<?xml version="1.0"?>
<robot xmlns:xacro="http://www.ros.org/wiki/xacro" name="simple_humanoid">

  <!-- Properties -->
  <xacro:property name="M_PI" value="3.1415926535897931" />
  <xacro:property name="body_mass" value="10.0" />
  <xacro:property name="limb_mass" value="2.0" />
  <xacro:property name="head_mass" value="1.0" />

  <!-- Materials -->
  <material name="white">
    <color rgba="1 1 1 1"/>
  </material>
  <material name="black">
    <color rgba="0 0 0 1"/>
  </material>
  <material name="red">
    <color rgba="1 0 0 1"/>
  </material>
  <material name="blue">
    <color rgba="0 0 1 1"/>
  </material>

  <!-- Base Footprint (for navigation) -->
  <link name="base_footprint">
    <visual>
      <geometry>
        <sphere radius="0.01"/>
      </geometry>
      <material name="black"/>
    </visual>
    <collision>
      <geometry>
        <sphere radius="0.01"/>
      </geometry>
    </collision>
    <inertial>
      <mass value="0.0001"/>
      <inertia ixx="0.0001" ixy="0.0" ixz="0.0" iyy="0.0001" iyz="0.0" izz="0.0001"/>
    </inertial>
  </link>

  <!-- Base Link (torso) -->
  <joint name="base_joint" type="fixed">
    <parent link="base_footprint"/>
    <child link="base_link"/>
    <origin xyz="0 0 0.7" rpy="0 0 0"/>
  </joint>

  <link name="base_link">
    <visual>
      <origin xyz="0 0 0.4" rpy="0 0 0"/>
      <geometry>
        <box size="0.3 0.2 0.8"/>
      </geometry>
      <material name="white"/>
    </visual>
    <collision>
      <origin xyz="0 0 0.4" rpy="0 0 0"/>
      <geometry>
        <box size="0.3 0.2 0.8"/>
      </geometry>
    </collision>
    <inertial>
      <mass value="${body_mass}"/>
      <origin xyz="0 0 0.4" rpy="0 0 0"/>
      <inertia ixx="0.2" ixy="0.0" ixz="0.0" iyy="0.3" iyz="0.0" izz="0.4"/>
    </inertial>
  </link>

  <!-- Head -->
  <joint name="head_joint" type="fixed">
    <parent link="base_link"/>
    <child link="head_link"/>
    <origin xyz="0 0 0.85" rpy="0 0 0"/>
  </joint>

  <link name="head_link">
    <visual>
      <geometry>
        <sphere radius="0.1"/>
      </geometry>
      <material name="white"/>
    </visual>
    <collision>
      <geometry>
        <sphere radius="0.1"/>
      </geometry>
    </collision>
    <inertial>
      <mass value="${head_mass}"/>
      <inertia ixx="0.004" ixy="0.0" ixz="0.0" iyy="0.004" iyz="0.0" izz="0.004"/>
    </inertial>
  </link>

  <!-- Left Arm -->
  <joint name="left_shoulder_joint" type="revolute">
    <parent link="base_link"/>
    <child link="left_upper_arm"/>
    <origin xyz="0.15 0 0.5" rpy="0 0 0"/>
    <axis xyz="0 1 0"/>
    <limit lower="${-M_PI/2}" upper="${M_PI/2}" effort="100" velocity="1"/>
  </joint>

  <link name="left_upper_arm">
    <visual>
      <origin xyz="0 0 -0.15" rpy="0 0 0"/>
      <geometry>
        <cylinder length="0.3" radius="0.05"/>
      </geometry>
      <material name="blue"/>
    </visual>
    <collision>
      <origin xyz="0 0 -0.15" rpy="0 0 0"/>
      <geometry>
        <cylinder length="0.3" radius="0.05"/>
      </geometry>
    </collision>
    <inertial>
      <mass value="${limb_mass}"/>
      <origin xyz="0 0 -0.15" rpy="0 0 0"/>
      <inertia ixx="0.02" ixy="0.0" ixz="0.0" iyy="0.02" iyz="0.0" izz="0.001"/>
    </inertial>
  </link>

  <joint name="left_elbow_joint" type="revolute">
    <parent link="left_upper_arm"/>
    <child link="left_lower_arm"/>
    <origin xyz="0 0 -0.3" rpy="0 0 0"/>
    <axis xyz="0 1 0"/>
    <limit lower="${-M_PI/2}" upper="${M_PI/2}" effort="100" velocity="1"/>
  </joint>

  <link name="left_lower_arm">
    <visual>
      <origin xyz="0 0 -0.15" rpy="0 0 0"/>
      <geometry>
        <cylinder length="0.3" radius="0.04"/>
      </geometry>
      <material name="blue"/>
    </visual>
    <collision>
      <origin xyz="0 0 -0.15" rpy="0 0 0"/>
      <geometry>
        <cylinder length="0.3" radius="0.04"/>
      </geometry>
    </collision>
    <inertial>
      <mass value="${limb_mass}"/>
      <origin xyz="0 0 -0.15" rpy="0 0 0"/>
      <inertia ixx="0.02" ixy="0.0" ixz="0.0" iyy="0.02" iyz="0.0" izz="0.001"/>
    </inertial>
  </link>

  <!-- Right Arm (mirror of left) -->
  <joint name="right_shoulder_joint" type="revolute">
    <parent link="base_link"/>
    <child link="right_upper_arm"/>
    <origin xyz="-0.15 0 0.5" rpy="0 0 0"/>
    <axis xyz="0 1 0"/>
    <limit lower="${-M_PI/2}" upper="${M_PI/2}" effort="100" velocity="1"/>
  </joint>

  <link name="right_upper_arm">
    <visual>
      <origin xyz="0 0 -0.15" rpy="0 0 0"/>
      <geometry>
        <cylinder length="0.3" radius="0.05"/>
      </geometry>
      <material name="red"/>
    </visual>
    <collision>
      <origin xyz="0 0 -0.15" rpy="0 0 0"/>
      <geometry>
        <cylinder length="0.3" radius="0.05"/>
      </geometry>
    </collision>
    <inertial>
      <mass value="${limb_mass}"/>
      <origin xyz="0 0 -0.15" rpy="0 0 0"/>
      <inertia ixx="0.02" ixy="0.0" ixz="0.0" iyy="0.02" iyz="0.0" izz="0.001"/>
    </inertial>
  </link>

  <joint name="right_elbow_joint" type="revolute">
    <parent link="right_upper_arm"/>
    <child link="right_lower_arm"/>
    <origin xyz="0 0 -0.3" rpy="0 0 0"/>
    <axis xyz="0 1 0"/>
    <limit lower="${-M_PI/2}" upper="${M_PI/2}" effort="100" velocity="1"/>
  </joint>

  <link name="right_lower_arm">
    <visual>
      <origin xyz="0 0 -0.15" rpy="0 0 0"/>
      <geometry>
        <cylinder length="0.3" radius="0.04"/>
      </geometry>
      <material name="red"/>
    </visual>
    <collision>
      <origin xyz="0 0 -0.15" rpy="0 0 0"/>
      <geometry>
        <cylinder length="0.3" radius="0.04"/>
      </geometry>
    </collision>
    <inertial>
      <mass value="${limb_mass}"/>
      <origin xyz="0 0 -0.15" rpy="0 0 0"/>
      <inertia ixx="0.02" ixy="0.0" ixz="0.0" iyy="0.02" iyz="0.0" izz="0.001"/>
    </inertial>
  </link>

  <!-- Left Leg -->
  <joint name="left_hip_joint" type="revolute">
    <parent link="base_link"/>
    <child link="left_upper_leg"/>
    <origin xyz="0.07 0 0" rpy="0 0 0"/>
    <axis xyz="1 0 0"/>
    <limit lower="${-M_PI/2}" upper="${M_PI/2}" effort="100" velocity="1"/>
  </joint>

  <link name="left_upper_leg">
    <visual>
      <origin xyz="0 0 -0.2" rpy="0 0 0"/>
      <geometry>
        <cylinder length="0.4" radius="0.06"/>
      </geometry>
      <material name="blue"/>
    </visual>
    <collision>
      <origin xyz="0 0 -0.2" rpy="0 0 0"/>
      <geometry>
        <cylinder length="0.4" radius="0.06"/>
      </geometry>
    </collision>
    <inertial>
      <mass value="${limb_mass}"/>
      <origin xyz="0 0 -0.2" rpy="0 0 0"/>
      <inertia ixx="0.04" ixy="0.0" ixz="0.0" iyy="0.04" iyz="0.0" izz="0.002"/>
    </inertial>
  </link>

  <joint name="left_knee_joint" type="revolute">
    <parent link="left_upper_leg"/>
    <child link="left_lower_leg"/>
    <origin xyz="0 0 -0.4" rpy="0 0 0"/>
    <axis xyz="1 0 0"/>
    <limit lower="${-M_PI/2}" upper="0" effort="100" velocity="1"/>
  </joint>

  <link name="left_lower_leg">
    <visual>
      <origin xyz="0 0 -0.2" rpy="0 0 0"/>
      <geometry>
        <cylinder length="0.4" radius="0.05"/>
      </geometry>
      <material name="blue"/>
    </visual>
    <collision>
      <origin xyz="0 0 -0.2" rpy="0 0 0"/>
      <geometry>
        <cylinder length="0.4" radius="0.05"/>
      </geometry>
    </collision>
    <inertial>
      <mass value="${limb_mass}"/>
      <origin xyz="0 0 -0.2" rpy="0 0 0"/>
      <inertia ixx="0.04" ixy="0.0" ixz="0.0" iyy="0.04" iyz="0.0" izz="0.002"/>
    </inertial>
  </link>

  <!-- Right Leg (mirror of left) -->
  <joint name="right_hip_joint" type="revolute">
    <parent link="base_link"/>
    <child link="right_upper_leg"/>
    <origin xyz="-0.07 0 0" rpy="0 0 0"/>
    <axis xyz="1 0 0"/>
    <limit lower="${-M_PI/2}" upper="${M_PI/2}" effort="100" velocity="1"/>
  </joint>

  <link name="right_upper_leg">
    <visual>
      <origin xyz="0 0 -0.2" rpy="0 0 0"/>
      <geometry>
        <cylinder length="0.4" radius="0.06"/>
      </geometry>
      <material name="red"/>
    </visual>
    <collision>
      <origin xyz="0 0 -0.2" rpy="0 0 0"/>
      <geometry>
        <cylinder length="0.4" radius="0.06"/>
      </geometry>
    </collision>
    <inertial>
      <mass value="${limb_mass}"/>
      <origin xyz="0 0 -0.2" rpy="0 0 0"/>
      <inertia ixx="0.04" ixy="0.0" ixz="0.0" iyy="0.04" iyz="0.0" izz="0.002"/>
    </inertial>
  </link>

  <joint name="right_knee_joint" type="revolute">
    <parent link="right_upper_leg"/>
    <child link="right_lower_leg"/>
    <origin xyz="0 0 -0.4" rpy="0 0 0"/>
    <axis xyz="1 0 0"/>
    <limit lower="${-M_PI/2}" upper="0" effort="100" velocity="1"/>
  </joint>

  <link name="right_lower_leg">
    <visual>
      <origin xyz="0 0 -0.2" rpy="0 0 0"/>
      <geometry>
        <cylinder length="0.4" radius="0.05"/>
      </geometry>
      <material name="red"/>
    </visual>
    <collision>
      <origin xyz="0 0 -0.2" rpy="0 0 0"/>
      <geometry>
        <cylinder length="0.4" radius="0.05"/>
      </geometry>
    </collision>
    <inertial>
      <mass value="${limb_mass}"/>
      <origin xyz="0 0 -0.2" rpy="0 0 0"/>
      <inertia ixx="0.04" ixy="0.0" ixz="0.0" iyy="0.04" iyz="0.0" izz="0.002"/>
    </inertial>
  </link>

  <!-- Gazebo plugins -->
  <gazebo>
    <plugin name="gazebo_ros_control" filename="libgazebo_ros_control.so">
      <robotNamespace>/simple_humanoid</robotNamespace>
    </plugin>
  </gazebo>

  <!-- Joint state publisher for visualization -->
  <gazebo>
    <plugin name="joint_state_publisher" filename="libgazebo_ros_joint_state_publisher.so">
      <robotNamespace>/simple_humanoid</robotNamespace>
      <jointName>left_shoulder_joint</jointName>
      <jointName>left_elbow_joint</jointName>
      <jointName>right_shoulder_joint</jointName>
      <jointName>right_elbow_joint</jointName>
      <jointName>left_hip_joint</jointName>
      <jointName>left_knee_joint</jointName>
      <jointName>right_hip_joint</jointName>
      <jointName>right_knee_joint</jointName>
    </plugin>
  </gazebo>

</robot>
```

## Working with Robot State Publisher

The robot_state_publisher node is crucial for visualizing your robot in RViz. It reads the robot description parameter and joint states to publish the TF tree.

```python
# Example of setting up robot state publisher in a launch file
from launch import LaunchDescription
from launch_ros.actions import Node
from ament_index_python.packages import get_package_share_directory
import os


def generate_launch_description():
    # Get the URDF file path
    urdf_file = os.path.join(
        get_package_share_directory('your_robot_package'),
        'urdf',
        'simple_humanoid.urdf.xacro'
    )

    # Read the URDF file
    with open(urdf_file, 'r') as infp:
        robot_desc = infp.read()

    # Robot State Publisher node
    robot_state_publisher = Node(
        package='robot_state_publisher',
        executable='robot_state_publisher',
        parameters=[{
            'robot_description': robot_desc,
            'publish_frequency': 50.0
        }]
    )

    # Joint State Publisher (for visualization)
    joint_state_publisher = Node(
        package='joint_state_publisher',
        executable='joint_state_publisher',
        parameters=[{
            'source_list': ['joint_states'],
            'rate': 50.0
        }]
    )

    return LaunchDescription([
        robot_state_publisher,
        joint_state_publisher
    ])
```

## Hands-On Lab: Create and Visualize Your Robot Model

### Objective
Create a complete robot model using URDF/Xacro, visualize it in RViz, and simulate it in Gazebo.

### Prerequisites
- Completed Chapter 1 and 2
- ROS 2 Humble with Gazebo installed

### Steps

1. **Create a robot description package**:
   ```bash
   cd ~/robotics_ws/src
   ros2 pkg create --build-type ament_cmake robot_description --dependencies urdf xacro
   ```

2. **Create the URDF directory structure**:
   ```bash
   mkdir -p robot_description/urdf
   mkdir -p robot_description/meshes
   mkdir -p robot_description/config
   mkdir -p robot_description/launch
   ```

3. **Create the robot model file** (`robot_description/urdf/simple_humanoid.urdf.xacro`):
   Copy the humanoid robot URDF code from the previous section into this file.

4. **Create a launch file for visualization** (`robot_description/launch/robot_visualize.launch.py`):
   ```python
   from launch import LaunchDescription
   from launch.actions import DeclareLaunchArgument
   from launch.substitutions import LaunchConfiguration, PathJoinSubstitution
   from launch_ros.actions import Node
   from ament_index_python.packages import get_package_share_directory


   def generate_launch_description():
       # Declare launch arguments
       model_arg = DeclareLaunchArgument(
           'model',
           default_value='simple_humanoid.urdf.xacro',
           description='Robot description file'
       )

       # Get URDF file path
       urdf_file = PathJoinSubstitution([
           get_package_share_directory('robot_description'),
           'urdf',
           LaunchConfiguration('model')
       ])

       # Robot State Publisher node
       robot_state_publisher = Node(
           package='robot_state_publisher',
           executable='robot_state_publisher',
           parameters=[{
               'robot_description': urdf_file,
               'publish_frequency': 50.0
           }]
       )

       # Joint State Publisher (GUI for testing joint movement)
       joint_state_publisher_gui = Node(
           package='joint_state_publisher_gui',
           executable='joint_state_publisher_gui',
           name='joint_state_publisher_gui'
       )

       # RViz2 node
       rviz = Node(
           package='rviz2',
           executable='rviz2',
           name='rviz2',
           arguments=['-d', PathJoinSubstitution([
               get_package_share_directory('robot_description'),
               'config',
               'robot_view.rviz'
           ])]
       )

       return LaunchDescription([
           model_arg,
           robot_state_publisher,
           joint_state_publisher_gui,
           rviz
       ])
   ```

5. **Create the RViz configuration file** (`robot_description/config/robot_view.rviz`):
   Copy the RViz configuration from the previous section into this file.

6. **Create a launch file for Gazebo simulation** (`robot_description/launch/robot_gazebo.launch.py`):
   ```python
   from launch import LaunchDescription
   from launch.actions import DeclareLaunchArgument, IncludeLaunchDescription
   from launch.substitutions import LaunchConfiguration, PathJoinSubstitution
   from launch.launch_description_sources import PythonLaunchDescriptionSource
   from launch_ros.actions import Node
   from ament_index_python.packages import get_package_share_directory


   def generate_launch_description():
       # Declare launch arguments
       model_arg = DeclareLaunchArgument(
           'model',
           default_value='simple_humanoid.urdf.xacro',
           description='Robot description file'
       )

       # Get URDF file path
       urdf_file = PathJoinSubstitution([
           get_package_share_directory('robot_description'),
           'urdf',
           LaunchConfiguration('model')
       ])

       # Robot State Publisher node
       robot_state_publisher = Node(
           package='robot_state_publisher',
           executable='robot_state_publisher',
           parameters=[{
               'robot_description': urdf_file,
               'publish_frequency': 50.0
           }]
       )

       # Gazebo server
       gzserver = IncludeLaunchDescription(
           PythonLaunchDescriptionSource([
               get_package_share_directory('gazebo_ros'),
               '/launch/gzserver.launch.py'
           ])
       )

       # Gazebo client (GUI)
       gzclient = IncludeLaunchDescription(
           PythonLaunchDescriptionSource([
               get_package_share_directory('gazebo_ros'),
               '/launch/gzclient.launch.py'
           ])
       )

       # Spawn entity in Gazebo
       spawn_entity = Node(
           package='gazebo_ros',
           executable='spawn_entity.py',
           arguments=[
               '-file', urdf_file,
               '-entity', 'simple_humanoid'
           ],
           output='screen'
       )

       return LaunchDescription([
           model_arg,
           robot_state_publisher,
           gzserver,
           gzclient,
           spawn_entity
       ])
   ```

7. **Update the package.xml file** (`robot_description/package.xml`):
   ```xml
   <?xml version="1.0"?>
   <?xml-model href="http://download.ros.org/schema/package_format3.xsd" schematypens="http://www.w3.org/2001/XMLSchema"?>
   <package format="3">
     <name>robot_description</name>
     <version>0.0.0</version>
     <description>Robot description package for simple humanoid</description>
     <maintainer email="your.email@example.com">Your Name</maintainer>
     <license>Apache License 2.0</license>

     <buildtool_depend>ament_cmake</buildtool_depend>

     <depend>urdf</depend>
     <depend>xacro</depend>

     <test_depend>ament_lint_auto</test_depend>
     <test_depend>ament_lint_common</test_depend>

     <export>
       <build_type>ament_cmake</build_type>
     </export>
   </package>
   ```

8. **Build the package**:
   ```bash
   cd ~/robotics_ws
   colcon build --packages-select robot_description
   source install/setup.bash
   ```

9. **Visualize the robot in RViz**:
   ```bash
   ros2 launch robot_description robot_visualize.launch.py
   ```

10. **Simulate the robot in Gazebo**:
    ```bash
    ros2 launch robot_description robot_gazebo.launch.py
    ```

### Expected Results
- The robot model should appear in RViz with all links properly connected
- Joint state publisher GUI should allow you to move the joints and see the robot move
- The robot should spawn correctly in Gazebo simulation
- TF frames should be published correctly for all robot parts

### Troubleshooting Tips
- Ensure the URDF file is valid XML and properly formatted
- Check that joint limits are reasonable and not causing issues
- Verify that the robot model doesn't have self-collisions in its default pose
- Make sure all required packages (urdf, xacro, robot_state_publisher) are installed

## Summary

In this chapter, we've covered the fundamentals of robot modeling using URDF and Xacro, including how to create complex robot models with multiple links and joints. We've also explored simulation with Gazebo and visualization with RViz, which are essential tools for robotics development.

The hands-on lab provided practical experience in creating a complete humanoid robot model and visualizing it in both RViz and Gazebo. These skills are fundamental for any robotics application, as they allow you to test and develop algorithms in a safe, simulated environment before deploying to real hardware.

In the next chapters, we'll build upon these foundations to explore perception systems, control algorithms, and intelligent behaviors for humanoid robots.

============================================================
FILE: book\docs\part-ii-perception\chapter-4-sensor-integration\index.md
============================================================
---
title: Chapter 4 - Sensor Integration and Data Processing
sidebar_position: 1
---

# Chapter 4: Sensor Integration and Data Processing

## Learning Goals

- Understand various robot sensors and their applications
- Learn to process sensor data streams
- Master sensor fusion techniques
- Integrate cameras, LIDAR, IMU, and other sensors
- Process and visualize sensor data streams
- Implement basic sensor fusion

## Introduction to Robot Sensors

Robots operate in complex, dynamic environments that require them to perceive and understand their surroundings. This perception is achieved through various sensors that provide information about the robot's internal state and external environment. Understanding how to integrate and process sensor data is fundamental to creating intelligent robotic systems.

### Sensor Categories

Robot sensors can be broadly categorized into:

1. **Proprioceptive Sensors**: Measure the robot's internal state (joint angles, motor currents, etc.)
2. **Exteroceptive Sensors**: Measure the external environment (cameras, LIDAR, etc.)
3. **Interoceptive Sensors**: Measure internal systems (temperature, battery level, etc.)

### Sensor Characteristics

When working with sensors, it's important to understand their key characteristics:

- **Resolution**: The smallest change a sensor can detect
- **Accuracy**: How close the measurement is to the true value
- **Precision**: How repeatable the measurements are
- **Range**: The minimum and maximum values the sensor can measure
- **Bandwidth**: The frequency range over which the sensor operates
- **Latency**: The delay between measurement and output
- **Noise**: Random variations in the measurement

## Common Robot Sensors

### Cameras

Cameras are among the most important sensors for robots, providing rich visual information about the environment. They can be categorized as:

- **Monocular Cameras**: Single camera providing 2D images
- **Stereo Cameras**: Two cameras providing depth information
- **RGB-D Cameras**: Provide both color and depth data
- **Fish-eye Cameras**: Provide wide-angle views

```python
# Example of camera data processing with ROS 2
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image
from cv_bridge import CvBridge
import cv2
import numpy as np


class CameraProcessor(Node):
    def __init__(self):
        super().__init__('camera_processor')
        self.subscription = self.create_subscription(
            Image,
            '/camera/image_raw',
            self.image_callback,
            10)
        self.subscription  # prevent unused variable warning
        self.bridge = CvBridge()

    def image_callback(self, msg):
        # Convert ROS Image message to OpenCV image
        cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')

        # Process the image (example: edge detection)
        gray = cv2.cvtColor(cv_image, cv2.COLOR_BGR2GRAY)
        edges = cv2.Canny(gray, 50, 150)

        # Display the processed image
        cv2.imshow('Original', cv_image)
        cv2.imshow('Edges', edges)
        cv2.waitKey(1)


def main(args=None):
    rclpy.init(args=args)
    camera_processor = CameraProcessor()
    rclpy.spin(camera_processor)
    cv2.destroyAllWindows()
    camera_processor.destroy_node()
    rclpy.shutdown()


if __name__ == '__main__':
    main()
```

### LIDAR (Light Detection and Ranging)

LIDAR sensors provide accurate distance measurements by emitting laser pulses and measuring the time it takes for them to return. They are essential for mapping and navigation.

```python
# Example of LIDAR data processing
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import LaserScan
import numpy as np
import matplotlib.pyplot as plt


class LidarProcessor(Node):
    def __init__(self):
        super().__init__('lidar_processor')
        self.subscription = self.create_subscription(
            LaserScan,
            '/scan',
            self.scan_callback,
            10)
        self.subscription  # prevent unused variable warning
        self.scan_data = None

    def scan_callback(self, msg):
        # Extract ranges from the scan message
        ranges = np.array(msg.ranges)

        # Filter out invalid ranges (inf or nan)
        valid_ranges = ranges[np.isfinite(ranges)]

        # Calculate statistics
        if len(valid_ranges) > 0:
            min_distance = np.min(valid_ranges)
            max_distance = np.max(valid_ranges)
            avg_distance = np.mean(valid_ranges)

            self.get_logger().info(
                f'LIDAR: Min={min_distance:.2f}, Max={max_distance:.2f}, Avg={avg_distance:.2f}'
            )

        # Store for visualization
        self.scan_data = {
            'ranges': ranges,
            'angle_min': msg.angle_min,
            'angle_max': msg.angle_max,
            'angle_increment': msg.angle_increment
        }


def main(args=None):
    rclpy.init(args=args)
    lidar_processor = LidarProcessor()
    rclpy.spin(lidar_processor)
    lidar_processor.destroy_node()
    rclpy.shutdown()


if __name__ == '__main__':
    main()
```

### Inertial Measurement Units (IMU)

IMUs measure linear acceleration and angular velocity, providing information about the robot's motion and orientation. They typically contain accelerometers, gyroscopes, and sometimes magnetometers.

```python
# Example of IMU data processing
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Imu
from geometry_msgs.msg import Vector3
import numpy as np


class ImuProcessor(Node):
    def __init__(self):
        super().__init__('imu_processor')
        self.subscription = self.create_subscription(
            Imu,
            '/imu/data',
            self.imu_callback,
            10)
        self.subscription  # prevent unused variable warning

        # Initialize variables for orientation estimation
        self.orientation = np.array([0.0, 0.0, 0.0, 1.0])  # [x, y, z, w]
        self.angular_velocity = np.array([0.0, 0.0, 0.0])
        self.linear_acceleration = np.array([0.0, 0.0, 0.0])

    def imu_callback(self, msg):
        # Extract orientation (if available)
        if msg.orientation_covariance[0] >= 0:  # Check if orientation is valid
            self.orientation = np.array([
                msg.orientation.x,
                msg.orientation.y,
                msg.orientation.z,
                msg.orientation.w
            ])

        # Extract angular velocity
        self.angular_velocity = np.array([
            msg.angular_velocity.x,
            msg.angular_velocity.y,
            msg.angular_velocity.z
        ])

        # Extract linear acceleration
        self.linear_acceleration = np.array([
            msg.linear_acceleration.x,
            msg.linear_acceleration.y,
            msg.linear_acceleration.z
        ])

        # Log the data
        self.get_logger().info(
            f'IMU: AngVel=({self.angular_velocity[0]:.3f}, {self.angular_velocity[1]:.3f}, {self.angular_velocity[2]:.3f}), '
            f'LinAcc=({self.linear_acceleration[0]:.3f}, {self.linear_acceleration[1]:.3f}, {self.linear_acceleration[2]:.3f})'
        )


def main(args=None):
    rclpy.init(args=args)
    imu_processor = ImuProcessor()
    rclpy.spin(imu_processor)
    imu_processor.destroy_node()
    rclpy.shutdown()


if __name__ == '__main__':
    main()
```

### Other Sensors

#### GPS (Global Positioning System)
- Provides absolute position information
- Limited accuracy in indoor environments
- Often used for outdoor navigation

#### Force/Torque Sensors
- Measure forces and torques applied to the robot
- Critical for manipulation tasks
- Used for compliant control

#### Temperature Sensors
- Monitor internal and external temperatures
- Important for system safety
- Used for thermal management

## Sensor Data Processing

### Time Synchronization

When working with multiple sensors, it's crucial to synchronize their data in time. ROS 2 provides message filters for this purpose:

```python
# Example of sensor synchronization
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image, LaserScan
from message_filters import ApproximateTimeSynchronizer, Subscriber
from cv_bridge import CvBridge
import numpy as np


class SensorSynchronizer(Node):
    def __init__(self):
        super().__init__('sensor_synchronizer')

        # Create subscribers for different sensor topics
        image_sub = Subscriber(self, Image, '/camera/image_raw')
        scan_sub = Subscriber(self, LaserScan, '/scan')

        # Synchronize messages based on timestamps
        self.ts = ApproximateTimeSynchronizer(
            [image_sub, scan_sub],
            queue_size=10,
            slop=0.1  # 100ms tolerance
        )
        self.ts.registerCallback(self.sync_callback)

        self.bridge = CvBridge()

    def sync_callback(self, image_msg, scan_msg):
        # Process synchronized sensor data
        cv_image = self.bridge.imgmsg_to_cv2(image_msg, desired_encoding='bgr8')
        ranges = np.array(scan_msg.ranges)

        self.get_logger().info(f'Synchronized: Image at {image_msg.header.stamp.sec}, Scan at {scan_msg.header.stamp.sec}')


def main(args=None):
    rclpy.init(args=args)
    synchronizer = SensorSynchronizer()
    rclpy.spin(synchronizer)
    synchronizer.destroy_node()
    rclpy.shutdown()


if __name__ == '__main__':
    main()
```

### Data Filtering

Sensor data often contains noise that needs to be filtered for reliable operation:

```python
# Example of sensor data filtering
import numpy as np
from scipy import signal
import matplotlib.pyplot as plt


class SensorFilter:
    def __init__(self, cutoff_freq=10.0, sampling_freq=100.0):
        # Design a low-pass filter
        nyquist_freq = sampling_freq / 2.0
        normalized_cutoff = cutoff_freq / nyquist_freq

        # Create Butterworth filter
        self.b, self.a = signal.butter(4, normalized_cutoff, btype='low', analog=False)

        # Initialize filter state
        self.z = signal.lfilter_zi(self.b, self.a)

    def filter_data(self, new_sample):
        # Apply the filter to new data
        filtered, self.z = signal.lfilter(self.b, self.a, [new_sample], zi=self.z)
        return filtered[0]


# Example usage
def main():
    # Simulate noisy sensor data
    t = np.linspace(0, 1, 1000)
    true_signal = np.sin(2 * np.pi * 1 * t)  # 1 Hz signal
    noise = np.random.normal(0, 0.1, len(t))  # Add noise
    noisy_signal = true_signal + noise

    # Apply filtering
    sensor_filter = SensorFilter(cutoff_freq=5.0, sampling_freq=100.0)
    filtered_signal = np.zeros_like(noisy_signal)

    for i in range(len(noisy_signal)):
        filtered_signal[i] = sensor_filter.filter_data(noisy_signal[i])

    # Plot results
    plt.figure(figsize=(12, 4))
    plt.plot(t, true_signal, label='True Signal', linewidth=2)
    plt.plot(t, noisy_signal, label='Noisy Signal', alpha=0.7)
    plt.plot(t, filtered_signal, label='Filtered Signal', linewidth=2)
    plt.legend()
    plt.xlabel('Time (s)')
    plt.ylabel('Value')
    plt.title('Sensor Data Filtering Example')
    plt.grid(True)
    plt.show()


if __name__ == '__main__':
    main()
```

## Sensor Fusion

Sensor fusion combines data from multiple sensors to improve the accuracy and reliability of the robot's perception. Common fusion techniques include:

### Kalman Filtering

The Kalman filter is a mathematical method that uses a series of measurements observed over time to estimate unknown variables.

```python
# Simple Kalman Filter implementation for position estimation
import numpy as np


class KalmanFilter:
    def __init__(self, dt=0.1, process_noise=1.0, measurement_noise=1.0):
        # Time step
        self.dt = dt

        # State transition matrix (position and velocity)
        self.F = np.array([[1, dt],
                          [0, 1]])

        # Control matrix (not used in this example)
        self.B = np.array([[0.5 * dt**2],
                          [dt]])

        # Measurement matrix
        self.H = np.array([[1, 0]])

        # Process noise covariance
        self.Q = np.array([[process_noise**2 * dt**4 / 4, process_noise**2 * dt**3 / 2],
                          [process_noise**2 * dt**3 / 2, process_noise**2 * dt**2]])

        # Measurement noise covariance
        self.R = np.array([[measurement_noise**2]])

        # Error covariance matrix
        self.P = np.array([[1000, 0],
                          [0, 1000]])

        # State vector [position, velocity]
        self.x = np.array([[0],
                          [0]])

    def predict(self):
        # Predict state
        self.x = np.dot(self.F, self.x)

        # Predict error covariance
        self.P = np.dot(np.dot(self.F, self.P), self.F.T) + self.Q

    def update(self, measurement):
        # Calculate Kalman gain
        S = np.dot(np.dot(self.H, self.P), self.H.T) + self.R
        K = np.dot(np.dot(self.P, self.H.T), np.linalg.inv(S))

        # Update state
        y = measurement - np.dot(self.H, self.x)  # Innovation
        self.x = self.x + np.dot(K, y)

        # Update error covariance
        I = np.eye(len(self.x))
        self.P = np.dot((I - np.dot(K, self.H)), self.P)


# Example usage for sensor fusion
def main():
    # Create Kalman filter
    kf = KalmanFilter(dt=0.1, process_noise=0.1, measurement_noise=0.5)

    # Simulate true trajectory and measurements
    dt = 0.1
    t = np.arange(0, 10, dt)
    true_position = 10 * np.sin(0.5 * t)  # True position
    measurements = true_position + np.random.normal(0, 0.5, len(t))  # Noisy measurements

    # Store results
    estimated_positions = []

    for i, measurement in enumerate(measurements):
        # Predict step
        kf.predict()

        # Update step
        kf.update(measurement)

        # Store estimated position
        estimated_positions.append(kf.x[0, 0])

    # Plot results
    estimated_positions = np.array(estimated_positions)

    plt.figure(figsize=(12, 6))
    plt.plot(t, true_position, label='True Position', linewidth=2)
    plt.plot(t, measurements, label='Noisy Measurements', alpha=0.7)
    plt.plot(t, estimated_positions, label='Kalman Filter Estimate', linewidth=2)
    plt.legend()
    plt.xlabel('Time (s)')
    plt.ylabel('Position')
    plt.title('Kalman Filter for Sensor Fusion')
    plt.grid(True)
    plt.show()


if __name__ == '__main__':
    main()
```

### Particle Filtering

Particle filters are useful for non-linear, non-Gaussian systems:

```python
# Simple particle filter for robot localization
import numpy as np
import matplotlib.pyplot as plt


class ParticleFilter:
    def __init__(self, num_particles=1000, state_dim=2):
        self.num_particles = num_particles
        self.state_dim = state_dim

        # Initialize particles randomly
        self.particles = np.random.uniform(-10, 10, (num_particles, state_dim))
        self.weights = np.ones(num_particles) / num_particles

    def predict(self, control_input, noise_std=0.1):
        # Move particles according to motion model
        self.particles += control_input + np.random.normal(0, noise_std, self.particles.shape)

    def update(self, measurement, measurement_std=0.5):
        # Calculate likelihood of each particle given the measurement
        distances = np.linalg.norm(self.particles - measurement, axis=1)
        likelihood = np.exp(-0.5 * (distances / measurement_std)**2)

        # Update weights
        self.weights *= likelihood
        self.weights += 1e-300  # Avoid zero weights
        self.weights /= np.sum(self.weights)  # Normalize

    def resample(self):
        # Resample particles based on weights
        indices = np.random.choice(
            self.num_particles,
            size=self.num_particles,
            p=self.weights
        )
        self.particles = self.particles[indices]
        self.weights.fill(1.0 / self.num_particles)

    def estimate(self):
        # Calculate weighted mean of particles
        return np.average(self.particles, axis=0, weights=self.weights)


# Example usage
def main():
    # Create particle filter
    pf = ParticleFilter(num_particles=1000, state_dim=2)

    # Simulate true trajectory
    dt = 0.1
    t = np.arange(0, 10, dt)
    true_trajectory = np.column_stack([2 * np.sin(0.5 * t), 2 * np.cos(0.5 * t)])

    # Store estimates
    estimates = []

    for i, true_pos in enumerate(true_trajectory):
        # Add noise to true position to simulate measurement
        measurement = true_pos + np.random.normal(0, 0.3, 2)

        # Predict and update
        control_input = np.array([0.1, 0.0]) if i > 0 else np.array([0.0, 0.0])
        pf.predict(control_input, noise_std=0.1)
        pf.update(measurement, measurement_std=0.5)

        # Resample if effective sample size is too low
        effective_samples = 1.0 / np.sum(pf.weights**2)
        if effective_samples < 0.5 * pf.num_particles:
            pf.resample()

        # Estimate position
        estimate = pf.estimate()
        estimates.append(estimate)

    # Convert to arrays
    estimates = np.array(estimates)

    # Plot results
    plt.figure(figsize=(10, 8))
    plt.plot(true_trajectory[:, 0], true_trajectory[:, 1], 'g-', label='True Trajectory', linewidth=2)
    plt.plot(estimates[:, 0], estimates[:, 1], 'r-', label='Particle Filter Estimate', linewidth=2)
    plt.scatter(true_trajectory[::10, 0], true_trajectory[::10, 1], c='g', s=50, label='True Positions', alpha=0.7)
    plt.scatter(estimates[::10, 0], estimates[::10, 1], c='r', s=50, label='Estimates', alpha=0.7)
    plt.legend()
    plt.xlabel('X Position')
    plt.ylabel('Y Position')
    plt.title('Particle Filter for Robot Localization')
    plt.grid(True)
    plt.axis('equal')
    plt.show()


if __name__ == '__main__':
    main()
```

## ROS 2 Sensor Integration

### Sensor Message Types

ROS 2 provides standardized message types for common sensors:

- `sensor_msgs/Image`: Camera images
- `sensor_msgs/LaserScan`: LIDAR scans
- `sensor_msgs/PointCloud2`: 3D point cloud data
- `sensor_msgs/Imu`: Inertial measurement unit data
- `sensor_msgs/JointState`: Joint positions, velocities, efforts

### Creating a Sensor Integration Node

```python
# Complete sensor integration example
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image, LaserScan, Imu, JointState
from geometry_msgs.msg import Twist
from cv_bridge import CvBridge
import numpy as np
import threading
import time


class SensorIntegrator(Node):
    def __init__(self):
        super().__init__('sensor_integrator')

        # Initialize data storage
        self.camera_data = None
        self.lidar_data = None
        self.imu_data = None
        self.joint_data = None

        # Create subscribers for all sensor types
        self.camera_sub = self.create_subscription(
            Image, '/camera/image_raw', self.camera_callback, 10)
        self.lidar_sub = self.create_subscription(
            LaserScan, '/scan', self.lidar_callback, 10)
        self.imu_sub = self.create_subscription(
            Imu, '/imu/data', self.imu_callback, 10)
        self.joint_sub = self.create_subscription(
            JointState, '/joint_states', self.joint_callback, 10)

        # Create publisher for robot commands
        self.cmd_vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)

        # Create timer for sensor processing
        self.timer = self.create_timer(0.1, self.process_sensors)

        # Initialize CV bridge
        self.bridge = CvBridge()

        # Lock for thread safety
        self.data_lock = threading.Lock()

        self.get_logger().info('Sensor integrator initialized')

    def camera_callback(self, msg):
        with self.data_lock:
            self.camera_data = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')

    def lidar_callback(self, msg):
        with self.data_lock:
            self.lidar_data = {
                'ranges': np.array(msg.ranges),
                'intensities': np.array(msg.intensities),
                'angle_min': msg.angle_min,
                'angle_max': msg.angle_max,
                'angle_increment': msg.angle_increment
            }

    def imu_callback(self, msg):
        with self.data_lock:
            self.imu_data = {
                'orientation': [msg.orientation.x, msg.orientation.y, msg.orientation.z, msg.orientation.w],
                'angular_velocity': [msg.angular_velocity.x, msg.angular_velocity.y, msg.angular_velocity.z],
                'linear_acceleration': [msg.linear_acceleration.x, msg.linear_acceleration.y, msg.linear_acceleration.z]
            }

    def joint_callback(self, msg):
        with self.data_lock:
            self.joint_data = {
                'names': msg.name,
                'positions': np.array(msg.position),
                'velocities': np.array(msg.velocity),
                'efforts': np.array(msg.effort)
            }

    def process_sensors(self):
        with self.data_lock:
            # Process sensor data based on available information
            cmd_vel = Twist()

            # Example: Stop if obstacle detected in front (LIDAR)
            if self.lidar_data is not None:
                ranges = self.lidar_data['ranges']
                # Get front-facing ranges (forward 30 degrees)
                front_ranges = ranges[0:15]  # Approximate front ranges
                front_ranges = front_ranges[np.isfinite(front_ranges)]  # Remove invalid readings

                if len(front_ranges) > 0 and np.min(front_ranges) < 1.0:  # Obstacle within 1m
                    cmd_vel.linear.x = 0.0  # Stop
                    self.get_logger().warn('Obstacle detected! Stopping.')
                else:
                    cmd_vel.linear.x = 0.5  # Move forward slowly
            else:
                cmd_vel.linear.x = 0.0  # Stop if no LIDAR data

            # Use IMU for stability (example: adjust based on tilt)
            if self.imu_data is not None:
                # Extract roll from quaternion (simplified)
                orientation = self.imu_data['orientation']
                # Simple check for significant tilt
                if abs(orientation[1]) > 0.2:  # Significant pitch
                    cmd_vel.angular.z = -orientation[1] * 2.0  # Counteract tilt
                    self.get_logger().info('Correcting for robot tilt')

        # Publish command
        self.cmd_vel_pub.publish(cmd_vel)


def main(args=None):
    rclpy.init(args=args)
    sensor_integrator = SensorIntegrator()

    try:
        rclpy.spin(sensor_integrator)
    except KeyboardInterrupt:
        pass
    finally:
        sensor_integrator.destroy_node()
        rclpy.shutdown()


if __name__ == '__main__':
    main()
```

## Hands-On Lab: Multi-Sensor Integration System

### Objective
Create a complete sensor integration system that combines camera, LIDAR, and IMU data to navigate a robot safely.

### Prerequisites
- Completed Chapter 1-3
- ROS 2 Humble with Gazebo installed
- Basic Python and OpenCV knowledge

### Steps

1. **Create a sensor fusion package**:
   ```bash
   cd ~/robotics_ws/src
   ros2 pkg create --build-type ament_python sensor_fusion_lab --dependencies rclpy sensor_msgs cv_bridge opencv-python numpy matplotlib
   ```

2. **Create the main sensor fusion node** (`sensor_fusion_lab/sensor_fusion_lab/sensor_fusion_node.py`):
   ```python
   #!/usr/bin/env python3
   import rclpy
   from rclpy.node import Node
   from sensor_msgs.msg import Image, LaserScan, Imu
   from geometry_msgs.msg import Twist, Vector3
   from cv_bridge import CvBridge
   import numpy as np
   import threading
   import math
   from collections import deque


   class SensorFusionNode(Node):
       def __init__(self):
           super().__init__('sensor_fusion_node')

           # Initialize data storage with history for filtering
           self.camera_data = None
           self.lidar_data = None
           self.imu_data = None

           # Data history for filtering
           self.lidar_history = deque(maxlen=5)
           self.imu_history = deque(maxlen=10)

           # Create subscribers
           self.camera_sub = self.create_subscription(
               Image, '/camera/image_raw', self.camera_callback, 10)
           self.lidar_sub = self.create_subscription(
               LaserScan, '/scan', self.lidar_callback, 10)
           self.imu_sub = self.create_subscription(
               Imu, '/imu/data', self.imu_callback, 10)

           # Create publisher for velocity commands
           self.cmd_vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)

           # Create publisher for fused sensor status
           self.status_pub = self.create_publisher(
               Vector3, '/sensor_fusion/status', 10)

           # Timer for processing loop
           self.timer = self.create_timer(0.05, self.process_sensors)  # 20 Hz

           # CV bridge for image processing
           self.bridge = CvBridge()

           # Robot state
           self.linear_velocity = 0.0
           self.angular_velocity = 0.0

           # Lock for thread safety
           self.data_lock = threading.Lock()

           self.get_logger().info('Sensor fusion node initialized')

       def camera_callback(self, msg):
           with self.data_lock:
               try:
                   self.camera_data = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')
               except Exception as e:
                   self.get_logger().error(f'Error processing camera image: {e}')

       def lidar_callback(self, msg):
           with self.data_lock:
               try:
                   ranges = np.array(msg.ranges)
                   # Filter out invalid ranges
                   ranges = np.where((ranges >= msg.range_min) & (ranges <= msg.range_max), ranges, np.inf)

                   lidar_info = {
                       'ranges': ranges,
                       'angle_min': msg.angle_min,
                       'angle_max': msg.angle_max,
                       'angle_increment': msg.angle_increment
                   }

                   # Add to history for filtering
                   self.lidar_history.append(lidar_info)

                   self.lidar_data = lidar_info
               except Exception as e:
                   self.get_logger().error(f'Error processing LIDAR data: {e}')

       def imu_callback(self, msg):
           with self.data_lock:
               try:
                   imu_info = {
                       'orientation': [msg.orientation.x, msg.orientation.y, msg.orientation.z, msg.orientation.w],
                       'angular_velocity': [msg.angular_velocity.x, msg.angular_velocity.y, msg.angular_velocity.z],
                       'linear_acceleration': [msg.linear_acceleration.x, msg.linear_acceleration.y, msg.linear_acceleration.z]
                   }

                   # Add to history for filtering
                   self.imu_history.append(imu_info)

                   self.imu_data = imu_info
               except Exception as e:
                   self.get_logger().error(f'Error processing IMU data: {e}')

       def process_sensors(self):
           with self.data_lock:
               # Initialize command
               cmd_vel = Twist()

               # Process LIDAR data for obstacle avoidance
               if self.lidar_data is not None:
                   cmd_vel.linear.x, cmd_vel.angular.z = self.process_lidar_navigation()

               # Process IMU data for stability
               if self.imu_data is not None:
                   cmd_vel = self.process_imu_stability(cmd_vel)

               # Process camera data for visual features (simplified)
               if self.camera_data is not None:
                   visual_cmd = self.process_camera_navigation()
                   # Combine with other commands (simple weighted average)
                   cmd_vel.linear.x = 0.7 * cmd_vel.linear.x + 0.3 * visual_cmd.linear.x
                   cmd_vel.angular.z = 0.7 * cmd_vel.angular.z + 0.3 * visual_cmd.angular.z

               # Apply velocity limits
               cmd_vel.linear.x = max(-1.0, min(1.0, cmd_vel.linear.x))
               cmd_vel.angular.z = max(-1.0, min(1.0, cmd_vel.angular.z))

               # Publish command
               self.cmd_vel_pub.publish(cmd_vel)

               # Publish status
               status = Vector3()
               status.x = cmd_vel.linear.x
               status.y = cmd_vel.angular.z
               status.z = 1.0 if self.lidar_data is not None else 0.0
               self.status_pub.publish(status)

       def process_lidar_navigation(self):
           """Process LIDAR data for obstacle avoidance and navigation"""
           ranges = self.lidar_data['ranges']
           angle_min = self.lidar_data['angle_min']
           angle_increment = self.lidar_data['angle_increment']

           # Define sectors: front, front-left, front-right, left, right
           total_angles = len(ranges)
           front_idx = total_angles // 2
           sector_size = total_angles // 8  # 45-degree sectors

           front_range = np.min(ranges[front_idx - sector_size//2 : front_idx + sector_size//2])
           front_left_range = np.min(ranges[front_idx - sector_size : front_idx - sector_size//2])
           front_right_range = np.min(ranges[front_idx + sector_size//2 : front_idx + sector_size])
           left_range = np.min(ranges[0 : sector_size])
           right_range = np.min(ranges[-sector_size :])

           # Obstacle avoidance logic
           min_distance = 0.8  # meters
           target_speed = 0.5  # m/s

           linear_vel = target_speed
           angular_vel = 0.0

           # If obstacle is very close, stop and turn
           if front_range < min_distance * 0.7:
               linear_vel = 0.0
               # Turn away from the closest obstacle
               if front_left_range < front_right_range:
                   angular_vel = -0.8  # Turn right
               else:
                   angular_vel = 0.8   # Turn left
           # If obstacle is close in front, slow down and turn
           elif front_range < min_distance:
               linear_vel = target_speed * 0.3
               if front_left_range < front_right_range:
                   angular_vel = -0.4
               else:
                   angular_vel = 0.4
           # If obstacle is to the side, gently turn away
           elif left_range < min_distance * 1.2:
               angular_vel = 0.3
           elif right_range < min_distance * 1.2:
               angular_vel = -0.3

           return linear_vel, angular_vel

       def process_imu_stability(self, cmd_vel):
           """Process IMU data for robot stability"""
           # Extract orientation from quaternion (simplified approach)
           orientation = self.imu_data['orientation']

           # Convert quaternion to roll/pitch (simplified)
           # This is a basic approximation - in practice, use proper quaternion math
           sinr_cosp = 2 * (orientation[3] * orientation[0] + orientation[1] * orientation[2])
           cosr_cosp = 1 - 2 * (orientation[0]**2 + orientation[1]**2)
           roll = math.atan2(sinr_cosp, cosr_cosp)

           sinp = 2 * (orientation[3] * orientation[1] - orientation[2] * orientation[0])
           # Check for gimbal lock
           if abs(sinp) >= 1:
               pitch = math.copysign(math.pi / 2, sinp)  # Use 90 degrees if out of range
           else:
               pitch = math.asin(sinp)

           # If robot is tilted too much, reduce speed
           tilt_threshold = 0.3  # radians
           if abs(pitch) > tilt_threshold or abs(roll) > tilt_threshold:
               cmd_vel.linear.x *= 0.3  # Reduce speed significantly
               cmd_vel.angular.z *= 0.7  # Reduce turning
               self.get_logger().warn('Robot tilt detected, reducing speed')

           return cmd_vel

       def process_camera_navigation(self):
           """Process camera data for visual navigation (simplified)"""
           # This is a simplified example - in practice, this would involve
           # complex computer vision algorithms

           import cv2

           # Convert to grayscale
           gray = cv2.cvtColor(self.camera_data, cv2.COLOR_BGR2GRAY)

           # Simple edge detection to find obstacles or features
           edges = cv2.Canny(gray, 50, 150)

           # Calculate the center of mass of edges to estimate where to go
           # (simplified - in practice, use more sophisticated methods)
           height, width = edges.shape
           x_coords, y_coords = np.where(edges > 0)

           cmd_vel = Twist()

           if len(x_coords) > 0:
               # Calculate centroid of detected edges
               centroid_x = np.mean(y_coords)  # x in image coordinates
               center_x = width / 2

               # If edges are mostly on the left, turn right; if on right, turn left
               if centroid_x < center_x - 20:  # Left side has more edges
                   cmd_vel.angular.z = 0.3
               elif centroid_x > center_x + 20:  # Right side has more edges
                   cmd_vel.angular.z = -0.3

               # Reduce speed if many edges detected (potential obstacle)
               if len(x_coords) > height * width * 0.1:  # If more than 10% of pixels are edges
                   cmd_vel.linear.x = 0.2
               else:
                   cmd_vel.linear.x = 0.5
           else:
               # No significant features detected, go forward
               cmd_vel.linear.x = 0.5

           return cmd_vel


   def main(args=None):
       rclpy.init(args=args)
       sensor_fusion_node = SensorFusionNode()

       try:
           rclpy.spin(sensor_fusion_node)
       except KeyboardInterrupt:
           pass
       finally:
           sensor_fusion_node.destroy_node()
           rclpy.shutdown()


   if __name__ == '__main__':
       main()
   ```

3. **Create a launch file** (`sensor_fusion_lab/launch/sensor_fusion.launch.py`):
   ```python
   from launch import LaunchDescription
   from launch.actions import DeclareLaunchArgument, IncludeLaunchDescription
   from launch.substitutions import LaunchConfiguration, PathJoinSubstitution
   from launch.launch_description_sources import PythonLaunchDescriptionSource
   from launch_ros.actions import Node
   from ament_index_python.packages import get_package_share_directory


   def generate_launch_description():
       # Declare launch arguments
       use_sim_time = DeclareLaunchArgument(
           'use_sim_time',
           default_value='true',
           description='Use simulation (Gazebo) clock if true'
       )

       # Include Gazebo launch (if needed)
       # gazebo = IncludeLaunchDescription(
       #     PythonLaunchDescriptionSource([
       #         get_package_share_directory('gazebo_ros'),
       #         '/launch/gzserver.launch.py'
       #     ])
       # )

       # Sensor fusion node
       sensor_fusion_node = Node(
           package='sensor_fusion_lab',
           executable='sensor_fusion_node',
           name='sensor_fusion_node',
           parameters=[{'use_sim_time': LaunchConfiguration('use_sim_time')}],
           output='screen'
       )

       return LaunchDescription([
           use_sim_time,
           # gazebo,
           sensor_fusion_node
       ])
   ```

4. **Update setup.py** to include the executable:
   ```python
   import os
   from glob import glob
   from setuptools import setup
   from setuptools import find_packages

   package_name = 'sensor_fusion_lab'

   setup(
       name=package_name,
       version='0.0.0',
       packages=find_packages(exclude=['test']),
       data_files=[
           ('share/ament_index/resource_index/packages',
               ['resource/' + package_name]),
           ('share/' + package_name, ['package.xml']),
           (os.path.join('share', package_name, 'launch'), glob('launch/*.launch.py')),
       ],
       install_requires=['setuptools'],
       zip_safe=True,
       maintainer='Your Name',
       maintainer_email='your.email@example.com',
       description='Sensor fusion lab for robotics',
       license='Apache License 2.0',
       tests_require=['pytest'],
       entry_points={
           'console_scripts': [
               'sensor_fusion_node = sensor_fusion_lab.sensor_fusion_node:main',
           ],
       },
   )
   ```

5. **Build the package**:
   ```bash
   cd ~/robotics_ws
   colcon build --packages-select sensor_fusion_lab
   source install/setup.bash
   ```

6. **Run the sensor fusion system**:
   ```bash
   ros2 launch sensor_fusion_lab sensor_fusion.launch.py
   ```

### Expected Results
- The robot should navigate using a combination of sensor inputs
- LIDAR data should be used for obstacle avoidance
- IMU data should be used for stability
- Camera data should provide visual guidance
- The system should demonstrate basic sensor fusion principles

### Troubleshooting Tips
- Ensure all sensor topics are being published
- Check that the robot model has all required sensors
- Verify that the sensor fusion node has the correct topic names
- Monitor the robot's behavior to ensure safe operation

## Summary

In this chapter, we've explored the fundamental concepts of sensor integration and data processing for robotics. We covered various types of sensors commonly used in robotics, techniques for processing sensor data, and methods for fusing information from multiple sensors.

The hands-on lab provided practical experience in creating a complete sensor fusion system that combines camera, LIDAR, and IMU data to enable safe navigation. This foundation is essential for more advanced perception systems that we'll explore in the next chapters, including computer vision for robotics and 3D perception.

============================================================
FILE: book\docs\part-ii-perception\chapter-5-computer-vision\index.md
============================================================
---
title: Chapter 5 - Computer Vision for Robotics
sidebar_position: 2
---

# Chapter 5: Computer Vision for Robotics

## Learning Goals

- Apply computer vision techniques to robotic perception
- Understand visual SLAM and object recognition
- Learn real-time image processing
- Implement object detection and tracking
- Create visual SLAM pipeline
- Integrate vision with robot control

## Introduction to Computer Vision in Robotics

Computer vision is a critical component of robotic perception, enabling robots to interpret and understand visual information from their environment. Unlike traditional computer vision applications that process images in isolation, robotics applications require real-time processing, robustness to changing conditions, and integration with other sensors and control systems.

### Key Challenges in Robotic Vision

1. **Real-time Processing**: Robots must process visual information quickly to make timely decisions
2. **Dynamic Environments**: Lighting, viewpoints, and scenes constantly change
3. **Motion Blur**: Robot movement can cause image blur
4. **Computational Constraints**: Limited processing power on mobile robots
5. **Integration**: Vision must work seamlessly with other sensors and control systems

### ROS 2 Vision Ecosystem

ROS 2 provides several packages for computer vision:

- **vision_opencv**: Bridges between ROS 2 and OpenCV
- **image_transport**: Efficient image message transport
- **cv_bridge**: Conversions between ROS 2 and OpenCV formats
- **image_pipeline**: Collection of image processing tools
- **vision_msgs**: Standard message types for vision results

## Image Processing Fundamentals

### Basic Image Operations

```python
# Basic image processing with OpenCV in ROS 2
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image
from cv_bridge import CvBridge
import cv2
import numpy as np


class ImageProcessor(Node):
    def __init__(self):
        super().__init__('image_processor')
        self.subscription = self.create_subscription(
            Image,
            '/camera/image_raw',
            self.image_callback,
            10)
        self.subscription  # prevent unused variable warning
        self.bridge = CvBridge()

    def image_callback(self, msg):
        # Convert ROS Image to OpenCV image
        cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')

        # Basic image operations
        processed_image = self.process_image(cv_image)

        # Display results
        cv2.imshow('Original', cv_image)
        cv2.imshow('Processed', processed_image)
        cv2.waitKey(1)

    def process_image(self, image):
        # Convert to grayscale
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

        # Apply Gaussian blur to reduce noise
        blurred = cv2.GaussianBlur(image, (5, 5), 0)

        # Apply edge detection
        edges = cv2.Canny(gray, 50, 150)

        # Combine results (for visualization)
        result = image.copy()
        result[edges > 0] = [0, 255, 0]  # Mark edges in green

        return result


def main(args=None):
    rclpy.init(args=args)
    image_processor = ImageProcessor()
    rclpy.spin(image_processor)
    cv2.destroyAllWindows()
    image_processor.destroy_node()
    rclpy.shutdown()


if __name__ == '__main__':
    main()
```

### Image Filtering and Enhancement

```python
import cv2
import numpy as np


class ImageEnhancer:
    def __init__(self):
        pass

    def enhance_brightness_contrast(self, image, brightness=0, contrast=1.0):
        """Enhance brightness and contrast of an image"""
        return cv2.convertScaleAbs(image, alpha=contrast, beta=brightness)

    def apply_morphological_operations(self, image, kernel_size=5):
        """Apply morphological operations for noise reduction"""
        kernel = np.ones((kernel_size, kernel_size), np.uint8)

        # Erosion and dilation for noise removal
        erosion = cv2.erode(image, kernel, iterations=1)
        dilation = cv2.dilate(erosion, kernel, iterations=1)

        # Opening and closing
        opening = cv2.morphologyEx(image, cv2.MORPH_OPEN, kernel)
        closing = cv2.morphologyEx(opening, cv2.MORPH_CLOSE, kernel)

        return closing

    def adaptive_threshold(self, image):
        """Apply adaptive thresholding for varying lighting conditions"""
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) if len(image.shape) == 3 else image
        return cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)

    def histogram_equalization(self, image):
        """Apply histogram equalization for better contrast"""
        if len(image.shape) == 3:
            # Convert to YUV for better histogram equalization
            yuv = cv2.cvtColor(image, cv2.COLOR_BGR2YUV)
            yuv[:,:,0] = cv2.equalizeHist(yuv[:,:,0])
            return cv2.cvtColor(yuv, cv2.COLOR_YUV2BGR)
        else:
            return cv2.equalizeHist(image)


# Example usage
def main():
    enhancer = ImageEnhancer()

    # Load an image (in practice, this would come from a camera)
    # image = cv2.imread('example.jpg')

    # For this example, create a synthetic image
    image = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)

    # Apply enhancements
    enhanced = enhancer.enhance_brightness_contrast(image, brightness=30, contrast=1.2)
    morphed = enhancer.apply_morphological_operations(enhanced)
    adaptive_thresh = enhancer.adaptive_threshold(morphed)
    hist_eq = enhancer.histogram_equalization(image)

    # Display results (in practice, you'd publish these as ROS messages)
    cv2.imshow('Original', image)
    cv2.imshow('Enhanced', enhanced)
    cv2.imshow('Morphed', morphed)
    cv2.imshow('Adaptive Threshold', adaptive_thresh)
    cv2.imshow('Histogram Equalized', hist_eq)
    cv2.waitKey(0)
    cv2.destroyAllWindows()


if __name__ == '__main__':
    main()
```

## Feature Detection and Matching

### Corner Detection

```python
import cv2
import numpy as np


class FeatureDetector:
    def __init__(self):
        pass

    def detect_corners_harris(self, image, block_size=2, ksize=3, k=0.04):
        """Detect corners using Harris corner detector"""
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) if len(image.shape) == 3 else image
        gray = np.float32(gray)

        dst = cv2.cornerHarris(gray, block_size, ksize, k)

        # Result is dilated for marking the corners
        dst = cv2.dilate(dst, None)

        # Threshold for an optimal value
        corner_image = image.copy()
        corner_image[dst > 0.01 * dst.max()] = [0, 0, 255]  # Mark corners in red

        return corner_image, dst

    def detect_corners_shi_tomasi(self, image, max_corners=100, quality_level=0.01, min_distance=10):
        """Detect corners using Shi-Tomasi corner detector"""
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) if len(image.shape) == 3 else image

        corners = cv2.goodFeaturesToTrack(gray, max_corners, quality_level, min_distance)

        corner_image = image.copy()
        if corners is not None:
            corners = np.int0(corners)
            for corner in corners:
                x, y = corner.ravel()
                cv2.circle(corner_image, (x, y), 3, [0, 0, 255], -1)

        return corner_image, corners

    def detect_sift_features(self, image):
        """Detect SIFT features (if available)"""
        try:
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) if len(image.shape) == 3 else image

            # Create SIFT detector
            sift = cv2.SIFT_create()

            # Detect keypoints and compute descriptors
            keypoints, descriptors = sift.detectAndCompute(gray, None)

            # Draw keypoints
            feature_image = cv2.drawKeypoints(image, keypoints, None)

            return feature_image, keypoints, descriptors
        except AttributeError:
            # SIFT may not be available in some OpenCV builds
            print("SIFT not available, using ORB instead")
            return self.detect_orb_features(image)

    def detect_orb_features(self, image):
        """Detect ORB features (free alternative to SIFT)"""
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) if len(image.shape) == 3 else image

        # Create ORB detector
        orb = cv2.ORB_create()

        # Detect keypoints and compute descriptors
        keypoints, descriptors = orb.detectAndCompute(gray, None)

        # Draw keypoints
        feature_image = cv2.drawKeypoints(image, keypoints, None, color=(0, 255, 0))

        return feature_image, keypoints, descriptors


# Example usage
def main():
    detector = FeatureDetector()

    # Load an image (in practice, this would come from a camera)
    # image = cv2.imread('example.jpg')

    # For this example, create a synthetic image with some features
    image = np.zeros((480, 640, 3), dtype=np.uint8)
    cv2.rectangle(image, (100, 100), (200, 200), (255, 0, 0), -1)
    cv2.circle(image, (300, 300), 50, (0, 255, 0), -1)
    cv2.line(image, (400, 100), (500, 200), (0, 0, 255), 5)

    # Detect features
    harris_result, _ = detector.detect_corners_harris(image)
    shi_tomasi_result, _ = detector.detect_corners_shi_tomasi(image)
    orb_result, _, _ = detector.detect_orb_features(image)

    # Display results
    cv2.imshow('Original', image)
    cv2.imshow('Harris Corners', harris_result)
    cv2.imshow('Shi-Tomasi Corners', shi_tomasi_result)
    cv2.imshow('ORB Features', orb_result)
    cv2.waitKey(0)
    cv2.destroyAllWindows()


if __name__ == '__main__':
    main()
```

## Object Detection

### Traditional Object Detection Methods

```python
import cv2
import numpy as np


class ObjectDetector:
    def __init__(self):
        # Load Haar cascade for face detection (as an example)
        self.face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')

    def detect_faces_haar(self, image):
        """Detect faces using Haar cascades"""
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

        faces = self.face_cascade.detectMultiScale(gray, 1.1, 4)

        result_image = image.copy()
        for (x, y, w, h) in faces:
            cv2.rectangle(result_image, (x, y), (x+w, y+h), (255, 0, 0), 2)

        return result_image, faces

    def detect_colors(self, image, lower_color, upper_color):
        """Detect objects of specific color"""
        hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)

        # Create mask for the color range
        mask = cv2.inRange(hsv, lower_color, upper_color)

        # Find contours in the mask
        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

        result_image = image.copy()
        for contour in contours:
            if cv2.contourArea(contour) > 100:  # Filter small contours
                x, y, w, h = cv2.boundingRect(contour)
                cv2.rectangle(result_image, (x, y), (x+w, y+h), (0, 255, 0), 2)

        return result_image, contours

    def template_matching(self, image, template, threshold=0.8):
        """Find template in image using template matching"""
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        template_gray = cv2.cvtColor(template, cv2.COLOR_BGR2GRAY)

        # Perform template matching
        result = cv2.matchTemplate(gray, template_gray, cv2.TM_CCOEFF_NORMED)

        # Find locations where the matching result is above threshold
        locations = np.where(result >= threshold)

        h, w = template_gray.shape
        result_image = image.copy()

        # Draw rectangles around matches
        for pt in zip(*locations[::-1]):
            cv2.rectangle(result_image, pt, (pt[0] + w, pt[1] + h), (0, 255, 255), 2)

        return result_image, locations, result


# Example usage
def main():
    detector = ObjectDetector()

    # Create a test image
    image = np.zeros((480, 640, 3), dtype=np.uint8)
    cv2.rectangle(image, (100, 100), (200, 200), (255, 0, 0), -1)  # Blue rectangle
    cv2.circle(image, (300, 300), 50, (0, 255, 0), -1)  # Green circle
    cv2.rectangle(image, (400, 100), (500, 200), (0, 0, 255), 2)  # Red rectangle outline

    # Detect blue objects (BGR format)
    lower_blue = np.array([200, 0, 0])
    upper_blue = np.array([255, 50, 50])
    color_result, contours = detector.detect_colors(image, lower_blue, upper_blue)

    # Display results
    cv2.imshow('Original', image)
    cv2.imshow('Color Detection', color_result)
    cv2.waitKey(0)
    cv2.destroyAllWindows()


if __name__ == '__main__':
    main()
```

### Deep Learning-based Object Detection

```python
# This is a conceptual example - actual implementation would require
# additional dependencies like PyTorch or TensorFlow
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image
from vision_msgs.msg import Detection2D, Detection2DArray, ObjectHypothesisWithPose
from cv_bridge import CvBridge
import cv2
import numpy as np


class DeepObjectDetector(Node):
    def __init__(self):
        super().__init__('deep_object_detector')
        self.subscription = self.create_subscription(
            Image,
            '/camera/image_raw',
            self.image_callback,
            10)
        self.subscription  # prevent unused variable warning

        # Publisher for detection results
        self.detection_pub = self.create_publisher(Detection2DArray, '/detections', 10)

        self.bridge = CvBridge()

        # Load a pre-trained model (conceptual - in practice, this would load an actual model)
        self.load_model()

        self.get_logger().info('Deep object detector initialized')

    def load_model(self):
        """Load pre-trained object detection model"""
        # In practice, this would load a model like YOLO, SSD, or Faster R-CNN
        # For this example, we'll use a placeholder
        self.model = None
        self.get_logger().info('Model loaded')

    def preprocess_image(self, image):
        """Preprocess image for the neural network"""
        # Resize image to model input size
        input_size = (416, 416)  # Example size for YOLO
        resized = cv2.resize(image, input_size)

        # Normalize pixel values
        normalized = resized.astype(np.float32) / 255.0

        # Convert to NCHW format (batch, channels, height, width)
        # This would be done with actual deep learning frameworks
        return normalized

    def postprocess_detections(self, raw_detections, original_shape):
        """Convert raw model outputs to detection messages"""
        # This is a simplified example
        # In practice, this would decode bounding boxes, class probabilities, etc.

        detections = Detection2DArray()
        detections.header.stamp = self.get_clock().now().to_msg()
        detections.header.frame_id = 'camera_frame'  # Should come from image header

        # Example: create mock detections
        for i in range(2):  # Simulate 2 detections
            detection = Detection2D()

            # Set bounding box (in practice, these would come from the model)
            detection.bbox.center.x = original_shape[1] // 2
            detection.bbox.center.y = original_shape[0] // 2
            detection.bbox.size_x = 100
            detection.bbox.size_y = 100

            # Set detection results
            hypothesis = ObjectHypothesisWithPose()
            hypothesis.hypothesis.class_id = 'object'
            hypothesis.hypothesis.score = 0.95
            detection.results.append(hypothesis)

            detections.detections.append(detection)

        return detections

    def image_callback(self, msg):
        """Process incoming image and detect objects"""
        try:
            # Convert ROS Image to OpenCV
            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')

            # Preprocess image
            preprocessed = self.preprocess_image(cv_image)

            # Run inference (conceptual)
            # raw_detections = self.model(preprocessed)

            # For this example, we'll simulate detection results
            raw_detections = np.random.random((1, 100, 6))  # Simulated detection format

            # Postprocess detections
            detection_array = self.postprocess_detections(raw_detections, cv_image.shape)

            # Publish detections
            self.detection_pub.publish(detection_array)

            self.get_logger().info(f'Published {len(detection_array.detections)} detections')

        except Exception as e:
            self.get_logger().error(f'Error in image callback: {e}')


def main(args=None):
    rclpy.init(args=args)
    deep_object_detector = DeepObjectDetector()

    try:
        rclpy.spin(deep_object_detector)
    except KeyboardInterrupt:
        pass
    finally:
        deep_object_detector.destroy_node()
        rclpy.shutdown()


if __name__ == '__main__':
    main()
```

## Visual SLAM (Simultaneous Localization and Mapping)

### Feature-based SLAM Concepts

```python
import numpy as np
import cv2
from collections import deque
import matplotlib.pyplot as plt
from scipy.spatial.transform import Rotation as R


class SimpleVisualSLAM:
    def __init__(self):
        # ORB detector for feature detection
        self.detector = cv2.ORB_create(nfeatures=1000)
        self.bf_matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)

        # Store keyframes and their features
        self.keyframes = deque(maxlen=100)
        self.current_pose = np.eye(4)  # 4x4 identity matrix
        self.poses = []  # Store trajectory

        # Feature tracking
        self.prev_keypoints = None
        self.prev_descriptors = None

        # Camera parameters (example values)
        self.fx = 525.0  # Focal length x
        self.fy = 525.0  # Focal length y
        self.cx = 319.5  # Principal point x
        self.cy = 239.5  # Principal point y

        self.camera_matrix = np.array([
            [self.fx, 0, self.cx],
            [0, self.fy, self.cy],
            [0, 0, 1]
        ])

    def detect_features(self, image):
        """Detect features in the current image"""
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) if len(image.shape) == 3 else image
        keypoints, descriptors = self.detector.detectAndCompute(gray, None)
        return keypoints, descriptors

    def match_features(self, kp1, desc1, kp2, desc2):
        """Match features between two images"""
        if desc1 is None or desc2 is None:
            return [], []

        matches = self.bf_matcher.match(desc1, desc2)
        matches = sorted(matches, key=lambda x: x.distance)

        # Keep only good matches
        good_matches = [m for m in matches if m.distance < 50]

        # Extract corresponding points
        pts1 = np.float32([kp1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)
        pts2 = np.float32([kp2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)

        return pts1, pts2, good_matches

    def estimate_motion(self, pts1, pts2):
        """Estimate camera motion between two frames"""
        if len(pts1) >= 8:
            # Compute fundamental matrix
            F, mask = cv2.findFundamentalMat(pts1, pts2, cv2.RANSAC, 4, 0.999)
            F = F/F[2,2]  # Normalize

            # Essential matrix
            E = self.camera_matrix.T @ F @ self.camera_matrix

            # Decompose essential matrix
            _, R, t, _ = cv2.recoverPose(E, pts1, pts2, self.camera_matrix)

            # Create transformation matrix
            transformation = np.eye(4)
            transformation[:3, :3] = R
            transformation[:3, 3] = t.ravel()

            return transformation
        else:
            return np.eye(4)  # No motion if not enough points

    def process_frame(self, image):
        """Process a single frame for SLAM"""
        # Detect features in current frame
        current_kp, current_desc = self.detect_features(image)

        # If we have previous frame, compute motion
        if self.prev_keypoints is not None and self.prev_descriptors is not None:
            # Match features
            pts1, pts2, matches = self.match_features(
                self.prev_keypoints, self.prev_descriptors,
                current_kp, current_desc
            )

            if len(pts1) > 0:
                # Estimate motion
                motion = self.estimate_motion(pts1, pts2)

                # Update current pose
                self.current_pose = self.current_pose @ motion

                # Store the pose
                self.poses.append(self.current_pose.copy())

        # Update previous frame data
        self.prev_keypoints = current_kp
        self.prev_descriptors = current_desc

        return self.current_pose.copy()

    def visualize_trajectory(self):
        """Visualize the estimated trajectory"""
        if len(self.poses) == 0:
            return

        # Extract x, y positions from poses
        positions = []
        for pose in self.poses:
            positions.append([pose[0, 3], pose[1, 3]])  # x, y coordinates

        positions = np.array(positions)

        plt.figure(figsize=(10, 8))
        plt.plot(positions[:, 0], positions[:, 1], 'b-', label='Trajectory', linewidth=2)
        plt.scatter(positions[0, 0], positions[0, 1], color='green', s=100, label='Start', zorder=5)
        plt.scatter(positions[-1, 0], positions[-1, 1], color='red', s=100, label='End', zorder=5)
        plt.xlabel('X Position')
        plt.ylabel('Y Position')
        plt.title('Estimated Trajectory from Visual SLAM')
        plt.legend()
        plt.grid(True)
        plt.axis('equal')
        plt.show()


# Example usage
def main():
    slam = SimpleVisualSLAM()

    # Simulate processing a sequence of images
    # In practice, these would come from a camera
    for i in range(50):
        # Create a synthetic "image" with some features
        image = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)

        # Add some synthetic features (squares)
        cv2.rectangle(image, (100 + i*2, 100 + i*1), (150 + i*2, 150 + i*1), (255, 255, 255), -1)
        cv2.rectangle(image, (300 - i*1, 200 + i*2), (350 - i*1, 250 + i*2), (255, 0, 0), -1)

        # Process the frame
        current_pose = slam.process_frame(image)

        print(f"Frame {i+1}: Position = ({current_pose[0, 3]:.2f}, {current_pose[1, 3]:.2f}, {current_pose[2, 3]:.2f})")

    # Visualize the trajectory
    slam.visualize_trajectory()


if __name__ == '__main__':
    main()
```

## Object Tracking

### Single Object Tracking

```python
import cv2
import numpy as np


class ObjectTracker:
    def __init__(self):
        # Different tracker types available in OpenCV
        self.tracker_types = [
            'BOOSTING', 'MIL', 'KCF', 'TLD', 'MEDIANFLOW', 'GOTURN', 'MOSSE', 'CSRT'
        ]
        self.tracker_type = 'CSRT'  # CSRT is generally the most accurate

    def create_tracker(self):
        """Create tracker based on selected type"""
        if self.tracker_type == 'BOOSTING':
            tracker = cv2.TrackerBoosting_create()
        elif self.tracker_type == 'MIL':
            tracker = cv2.TrackerMIL_create()
        elif self.tracker_type == 'KCF':
            tracker = cv2.TrackerKCF_create()
        elif self.tracker_type == 'TLD':
            tracker = cv2.TrackerTLD_create()
        elif self.tracker_type == 'MEDIANFLOW':
            tracker = cv2.TrackerMedianFlow_create()
        elif self.tracker_type == 'GOTURN':
            tracker = cv2.TrackerGOTURN_create()
        elif self.tracker_type == 'MOSSE':
            tracker = cv2.TrackerMOSSE_create()
        elif self.tracker_type == 'CSRT':
            tracker = cv2.TrackerCSRT_create()

        return tracker

    def track_object(self, video_source=0):
        """Track object in video stream"""
        cap = cv2.VideoCapture(video_source)

        # Read first frame
        ret, frame = cap.read()
        if not ret:
            print("Cannot read video stream")
            return

        # Select region to track (manually or automatically)
        bbox = cv2.selectROI("Tracking", frame, fromCenter=False, showCrosshair=True)
        cv2.destroyWindow("Tracking")

        # Create tracker
        tracker = self.create_tracker()

        # Initialize tracker with first frame and bounding box
        ret = tracker.init(frame, bbox)

        while True:
            # Read a new frame
            ret, frame = cap.read()
            if not ret:
                break

            # Update tracker
            success, bbox = tracker.update(frame)

            # Draw bounding box
            if success:
                p1 = (int(bbox[0]), int(bbox[1]))
                p2 = (int(bbox[0] + bbox[2]), int(bbox[1] + bbox[3]))
                cv2.rectangle(frame, p1, p2, (255, 0, 0), 2, 1)
            else:
                cv2.putText(frame, "Tracking failure detected", (100, 80),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 0, 255), 2)

            # Display frame
            cv2.imshow("Tracking", frame)

            # Exit if ESC pressed
            k = cv2.waitKey(1) & 0xff
            if k == 27:  # ESC key
                break

        cap.release()
        cv2.destroyAllWindows()


# Example usage for tracking
def main():
    tracker = ObjectTracker()

    # For this example, we'll create a simple tracking function
    # that processes a single frame with a known object
    image = np.zeros((480, 640, 3), dtype=np.uint8)
    cv2.rectangle(image, (200, 150), (300, 250), (255, 255, 255), -1)  # White square

    # Initialize tracker
    tracker_instance = tracker.create_tracker()
    bbox = (200, 150, 100, 100)  # x, y, width, height

    # Initialize tracker with the bounding box
    tracker_instance.init(image, bbox)

    print("Tracker initialized with bounding box:", bbox)


if __name__ == '__main__':
    main()
```

## Vision-Based Control

### Image-Based Visual Servoing

```python
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image
from geometry_msgs.msg import Twist
from cv_bridge import CvBridge
import cv2
import numpy as np


class VisualServoingController(Node):
    def __init__(self):
        super().__init__('visual_servoing_controller')

        # Subscribe to camera image
        self.subscription = self.create_subscription(
            Image,
            '/camera/image_raw',
            self.image_callback,
            10)

        # Publish velocity commands
        self.cmd_vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)

        self.bridge = CvBridge()

        # Target object parameters
        self.target_x = None
        self.target_y = None
        self.target_area = None

        # Control parameters
        self.kp_linear = 0.005  # Proportional gain for linear velocity
        self.kp_angular = 0.01  # Proportional gain for angular velocity
        self.target_area_desired = 5000  # Desired area of target in pixels

        # Object detection parameters
        self.lower_color = np.array([20, 100, 100])  # Lower HSV for color detection
        self.upper_color = np.array([30, 255, 255])  # Upper HSV for color detection

        self.get_logger().info('Visual servoing controller initialized')

    def detect_target(self, image):
        """Detect target object in image"""
        # Convert to HSV for color-based detection
        hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)

        # Create mask for target color
        mask = cv2.inRange(hsv, self.lower_color, self.upper_color)

        # Apply morphological operations to clean up the mask
        kernel = np.ones((5, 5), np.uint8)
        mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)
        mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)

        # Find contours in the mask
        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

        if contours:
            # Find the largest contour (assuming it's our target)
            largest_contour = max(contours, key=cv2.contourArea)
            area = cv2.contourArea(largest_contour)

            if area > 100:  # Filter out small contours
                # Get the center of the contour
                M = cv2.moments(largest_contour)
                if M["m00"] != 0:
                    cx = int(M["m10"] / M["m00"])
                    cy = int(M["m01"] / M["m00"])

                    return cx, cy, area, largest_contour

        return None, None, None, None

    def compute_control(self, image_width, image_height, target_x, target_y, target_area):
        """Compute control commands based on target position"""
        cmd_vel = Twist()

        if target_x is not None and target_y is not None:
            # Calculate errors
            error_x = target_x - image_width / 2  # Horizontal error (for rotation)
            error_y = target_y - image_height / 2  # Vertical error (ignored in this example)
            area_error = target_area - self.target_area_desired  # Area error (for forward/backward)

            # Compute control commands
            cmd_vel.linear.x = -self.kp_linear * area_error  # Move forward/backward based on size
            cmd_vel.angular.z = -self.kp_angular * error_x   # Rotate based on horizontal position

            # Apply velocity limits
            cmd_vel.linear.x = max(-0.5, min(0.5, cmd_vel.linear.x))
            cmd_vel.angular.z = max(-1.0, min(1.0, cmd_vel.angular.z))

        return cmd_vel

    def image_callback(self, msg):
        """Process incoming image and compute control commands"""
        try:
            # Convert ROS Image to OpenCV
            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')

            # Detect target in image
            self.target_x, self.target_y, self.target_area, contour = self.detect_target(cv_image)

            # Draw target if found
            if contour is not None:
                cv2.drawContours(cv_image, [contour], -1, (0, 255, 0), 3)
                cv2.circle(cv_image, (int(self.target_x), int(self.target_y)), 5, (0, 0, 255), -1)

            # Compute control commands
            cmd_vel = self.compute_control(
                cv_image.shape[1], cv_image.shape[0],  # image width, height
                self.target_x, self.target_y, self.target_area
            )

            # Publish control commands
            self.cmd_vel_pub.publish(cmd_vel)

            # Log control information
            if self.target_x is not None:
                self.get_logger().info(
                    f'Target: pos=({self.target_x:.1f}, {self.target_y:.1f}), '
                    f'area={self.target_area:.1f}, '
                    f'cmd_vel=({cmd_vel.linear.x:.3f}, {cmd_vel.angular.z:.3f})'
                )

            # Display image with target
            cv2.imshow('Visual Servoing', cv_image)
            cv2.waitKey(1)

        except Exception as e:
            self.get_logger().error(f'Error in image callback: {e}')


def main(args=None):
    rclpy.init(args=args)
    visual_servoing_controller = VisualServoingController()

    try:
        rclpy.spin(visual_servoing_controller)
    except KeyboardInterrupt:
        pass
    finally:
        cv2.destroyAllWindows()
        visual_servoing_controller.destroy_node()
        rclpy.shutdown()


if __name__ == '__main__':
    main()
```

## Hands-On Lab: Vision-Based Navigation System

### Objective
Create a complete vision-based navigation system that detects colored objects and navigates toward them using visual servoing.

### Prerequisites
- Completed Chapter 1-4
- ROS 2 Humble with Gazebo installed
- Basic understanding of robot control

### Steps

1. **Create a vision navigation package**:
   ```bash
   cd ~/robotics_ws/src
   ros2 pkg create --build-type ament_python vision_navigation_lab --dependencies rclpy sensor_msgs geometry_msgs cv_bridge opencv-python numpy
   ```

2. **Create the vision navigation node** (`vision_navigation_lab/vision_navigation_lab/vision_navigation_node.py`):
   ```python
   #!/usr/bin/env python3
   import rclpy
   from rclpy.node import Node
   from sensor_msgs.msg import Image
   from geometry_msgs.msg import Twist, Point
   from cv_bridge import CvBridge
   import cv2
   import numpy as np
   from std_msgs.msg import Bool
   import time


   class VisionNavigationNode(Node):
       def __init__(self):
           super().__init__('vision_navigation_node')

           # Subscribe to camera image
           self.image_sub = self.create_subscription(
               Image,
               '/camera/image_raw',
               self.image_callback,
               10)

           # Publish velocity commands
           self.cmd_vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)

           # Publish navigation status
           self.nav_status_pub = self.create_publisher(Bool, '/navigation_active', 10)

           # CV bridge for image processing
           self.bridge = CvBridge()

           # Navigation state
           self.target_detected = False
           self.navigation_active = False
           self.last_detection_time = time.time()
           self.detection_timeout = 3.0  # Stop if no detection for 3 seconds

           # Control parameters
           self.kp_linear = 0.003
           self.kp_angular = 0.01
           self.target_area_desired = 8000
           self.min_target_area = 500
           self.max_target_area = 50000

           # Target color in HSV (orange example)
           self.lower_color = np.array([10, 100, 100])
           self.upper_color = np.array([30, 255, 255])

           # Timer for control loop
           self.control_timer = self.create_timer(0.1, self.control_loop)

           self.get_logger().info('Vision navigation node initialized')

       def detect_target(self, image):
           """Detect colored target in image"""
           # Convert to HSV
           hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)

           # Create mask for target color
           mask = cv2.inRange(hsv, self.lower_color, self.upper_color)

           # Apply morphological operations
           kernel = np.ones((7, 7), np.uint8)
           mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)
           mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)

           # Find contours
           contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

           if contours:
               # Find largest contour
               largest_contour = max(contours, key=cv2.contourArea)
               area = cv2.contourArea(largest_contour)

               if area > self.min_target_area and area < self.max_target_area:
                   # Calculate centroid
                   M = cv2.moments(largest_contour)
                   if M["m00"] != 0:
                       cx = int(M["m10"] / M["m00"])
                       cy = int(M["m01"] / M["m00"])
                       return cx, cy, area, largest_contour

           return None, None, None, None

       def compute_navigation_command(self, image_width, image_height, target_x, target_y, target_area):
           """Compute navigation commands based on target position"""
           cmd_vel = Twist()

           if target_x is not None and target_y is not None:
               # Calculate errors
               error_x = target_x - image_width / 2
               area_error = target_area - self.target_area_desired

               # Compute velocities
               cmd_vel.linear.x = -self.kp_linear * area_error
               cmd_vel.angular.z = -self.kp_angular * error_x

               # Apply limits
               cmd_vel.linear.x = max(-0.4, min(0.4, cmd_vel.linear.x))
               cmd_vel.angular.z = max(-1.0, min(1.0, cmd_vel.angular.z))

               # Stop if close enough to target
               if abs(area_error) < 1000:  # If area is close to desired
                   cmd_vel.linear.x = 0.0
                   cmd_vel.angular.z = 0.0
                   self.get_logger().info('Target reached!')

           return cmd_vel

       def image_callback(self, msg):
           """Process incoming image"""
           try:
               # Convert to OpenCV
               cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')

               # Detect target
               target_x, target_y, target_area, contour = self.detect_target(cv_image)

               # Update detection status
               if target_x is not None:
                   self.target_detected = True
                   self.last_detection_time = time.time()

                   # Draw target on image
                   if contour is not None:
                       cv2.drawContours(cv_image, [contour], -1, (0, 255, 0), 3)
                       cv2.circle(cv_image, (int(target_x), int(target_y)), 5, (0, 0, 255), -1)
                       cv2.putText(cv_image, f'Area: {target_area:.0f}',
                                 (int(target_x), int(target_y) - 10),
                                 cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
               else:
                   self.target_detected = False

               # Show image
               cv2.imshow('Vision Navigation', cv_image)
               cv2.waitKey(1)

           except Exception as e:
               self.get_logger().error(f'Error processing image: {e}')

       def control_loop(self):
           """Main control loop"""
           # Check for detection timeout
           if time.time() - self.last_detection_time > self.detection_timeout:
               self.target_detected = False

           # Determine if navigation should be active
           self.navigation_active = self.target_detected

           cmd_vel = Twist()

           if self.navigation_active:
               # Get current image dimensions (in a real system, you'd store this from the last image)
               # For this example, we'll use a fixed size
               img_width, img_height = 640, 480

               # We need to access the last detected target info
               # In a real system, you'd store this from the image callback
               # For this example, we'll simulate the detection
               cmd_vel = self.compute_navigation_command(
                   img_width, img_height,
                   320, 240, 4000  # Simulated target position and area
               )
           else:
               # Stop if no target detected recently
               cmd_vel.linear.x = 0.0
               cmd_vel.angular.z = 0.0
               if self.target_detected:  # Only log once when target is lost
                   self.get_logger().info('Target lost, stopping')

           # Publish command and status
           self.cmd_vel_pub.publish(cmd_vel)

           status_msg = Bool()
           status_msg.data = self.navigation_active
           self.nav_status_pub.publish(status_msg)


   def main(args=None):
       rclpy.init(args=args)
       vision_navigation_node = VisionNavigationNode()

       try:
           rclpy.spin(vision_navigation_node)
       except KeyboardInterrupt:
           pass
       finally:
           cv2.destroyAllWindows()
           vision_navigation_node.destroy_node()
           rclpy.shutdown()


   if __name__ == '__main__':
       main()
   ```

3. **Create a launch file** (`vision_navigation_lab/launch/vision_navigation.launch.py`):
   ```python
   from launch import LaunchDescription
   from launch.actions import DeclareLaunchArgument, IncludeLaunchDescription
   from launch.substitutions import LaunchConfiguration, PathJoinSubstitution
   from launch.launch_description_sources import PythonLaunchDescriptionSource
   from launch_ros.actions import Node
   from ament_index_python.packages import get_package_share_directory


   def generate_launch_description():
       # Declare launch arguments
       use_sim_time = DeclareLaunchArgument(
           'use_sim_time',
           default_value='true',
           description='Use simulation (Gazebo) clock if true'
       )

       # Vision navigation node
       vision_navigation_node = Node(
           package='vision_navigation_lab',
           executable='vision_navigation_node',
           name='vision_navigation_node',
           parameters=[{'use_sim_time': LaunchConfiguration('use_sim_time')}],
           output='screen'
       )

       return LaunchDescription([
           use_sim_time,
           vision_navigation_node
       ])
   ```

4. **Update setup.py**:
   ```python
   import os
   from glob import glob
   from setuptools import setup
   from setuptools import find_packages

   package_name = 'vision_navigation_lab'

   setup(
       name=package_name,
       version='0.0.0',
       packages=find_packages(exclude=['test']),
       data_files=[
           ('share/ament_index/resource_index/packages',
               ['resource/' + package_name]),
           ('share/' + package_name, ['package.xml']),
           (os.path.join('share', package_name, 'launch'), glob('launch/*.launch.py')),
       ],
       install_requires=['setuptools'],
       zip_safe=True,
       maintainer='Your Name',
       maintainer_email='your.email@example.com',
       description='Vision navigation lab for robotics',
       license='Apache License 2.0',
       tests_require=['pytest'],
       entry_points={
           'console_scripts': [
               'vision_navigation_node = vision_navigation_lab.vision_navigation_node:main',
           ],
       },
   )
   ```

5. **Build the package**:
   ```bash
   cd ~/robotics_ws
   colcon build --packages-select vision_navigation_lab
   source install/setup.bash
   ```

6. **Run the vision navigation system**:
   ```bash
   ros2 launch vision_navigation_lab vision_navigation.launch.py
   ```

### Expected Results
- The system should detect colored objects in the camera feed
- The robot should navigate toward the detected object
- Visual feedback should be displayed showing the detected target
- The system should stop when it gets close to the target

### Troubleshooting Tips
- Adjust color thresholds based on your target object
- Verify camera topic name matches your robot's camera
- Check that the robot's base controller is properly configured
- Ensure proper lighting conditions for color detection

## Summary

In this chapter, we've explored the fundamental concepts of computer vision for robotics, including image processing, feature detection, object detection, visual SLAM, and vision-based control. We've implemented practical examples of each concept and created a complete vision-based navigation system.

The hands-on lab provided experience with creating a vision-based navigation system that detects objects and controls robot motion based on visual feedback. This foundation is essential for more advanced robotic perception and control systems that we'll explore in the upcoming chapters.

============================================================
FILE: book\docs\part-ii-perception\chapter-6-3d-perception\index.md
============================================================
---
title: Chapter 6 - 3D Perception and Scene Understanding
sidebar_position: 3
---

# Chapter 6: 3D Perception and Scene Understanding

## Learning Goals

- Process 3D point cloud data
- Understand spatial reasoning and mapping
- Learn scene segmentation and understanding
- Process and visualize 3D point clouds
- Create 3D maps of environments
- Implement scene segmentation

## Introduction to 3D Perception

3D perception is a critical capability for robots operating in real-world environments. Unlike 2D computer vision, 3D perception provides geometric information about the environment, enabling robots to understand spatial relationships, navigate safely, and interact with objects in three-dimensional space.

### Point Clouds

Point clouds are collections of 3D points that represent the surface geometry of objects and environments. They are typically acquired using:

- **LIDAR sensors**: Provide accurate 3D measurements using laser ranging
- **RGB-D cameras**: Provide both color and depth information
- **Stereo cameras**: Generate depth through triangulation
- **Structured light sensors**: Project patterns to measure depth

### ROS 2 3D Perception Ecosystem

ROS 2 provides several packages for 3D perception:

- **sensor_msgs/PointCloud2**: Standard message type for point cloud data
- **PCL (Point Cloud Library)**: Extensive library for point cloud processing
- **rviz**: Visualization tool for 3D data
- **octomap**: 3D occupancy mapping
- **moveit**: Motion planning with 3D collision checking

## Point Cloud Processing Fundamentals

### Point Cloud Data Structure

Point clouds in ROS 2 use the `sensor_msgs/PointCloud2` message format, which is a flexible binary format that can store various types of point data (XYZ, XYZRGB, XYZI, etc.).

```python
# Basic point cloud processing in ROS 2
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import PointCloud2
from sensor_msgs_py import point_cloud2
from std_msgs.msg import Header
import numpy as np


class PointCloudProcessor(Node):
    def __init__(self):
        super().__init__('point_cloud_processor')
        self.subscription = self.create_subscription(
            PointCloud2,
            '/points_raw',  # Common topic name for point cloud data
            self.pointcloud_callback,
            10)
        self.subscription  # prevent unused variable warning

        self.get_logger().info('Point cloud processor initialized')

    def pointcloud_callback(self, msg):
        """Process incoming point cloud data"""
        try:
            # Convert PointCloud2 message to list of points
            points = list(point_cloud2.read_points(
                msg,
                field_names=("x", "y", "z"),
                skip_nans=True
            ))

            # Convert to numpy array for processing
            points_array = np.array(points)

            if len(points_array) > 0:
                # Calculate basic statistics
                mean_point = np.mean(points_array, axis=0)
                std_point = np.std(points_array, axis=0)

                self.get_logger().info(
                    f'Point cloud: {len(points_array)} points, '
                    f'Mean: ({mean_point[0]:.2f}, {mean_point[1]:.2f}, {mean_point[2]:.2f}), '
                    f'Std: ({std_point[0]:.2f}, {std_point[1]:.2f}, {std_point[2]:.2f})'
                )

                # Example processing: filter points by distance
                filtered_points = self.filter_by_distance(points_array, max_distance=5.0)
                self.get_logger().info(f'Filtered points: {len(filtered_points)}')

        except Exception as e:
            self.get_logger().error(f'Error processing point cloud: {e}')

    def filter_by_distance(self, points, max_distance=5.0):
        """Filter points based on distance from origin"""
        distances = np.linalg.norm(points[:, :3], axis=1)
        filtered_indices = distances <= max_distance
        return points[filtered_indices]


def main(args=None):
    rclpy.init(args=args)
    point_cloud_processor = PointCloudProcessor()
    rclpy.spin(point_cloud_processor)
    point_cloud_processor.destroy_node()
    rclpy.shutdown()


if __name__ == '__main__':
    main()
```

### Point Cloud Filtering

Point clouds often contain noise and outliers that need to be filtered before processing:

```python
import numpy as np
from scipy.spatial import cKDTree


class PointCloudFilter:
    def __init__(self):
        pass

    def statistical_outlier_removal(self, points, k=20, std_dev_thresh=2.0):
        """Remove statistical outliers from point cloud"""
        if len(points) < k:
            return points

        # Build k-d tree for neighbor search
        tree = cKDTree(points[:, :3])  # Use only x, y, z coordinates

        # Calculate distances to k nearest neighbors
        distances, _ = tree.query(points[:, :3], k=k+1)  # Include self in query

        # Calculate mean distance for each point (excluding self)
        mean_distances = np.mean(distances[:, 1:], axis=1)  # Exclude first distance (to self)

        # Calculate global statistics
        global_mean = np.mean(mean_distances)
        global_std = np.std(mean_distances)

        # Filter points based on distance threshold
        valid_indices = mean_distances < (global_mean + std_dev_thresh * global_std)
        return points[valid_indices]

    def voxel_grid_filter(self, points, voxel_size=0.1):
        """Downsample point cloud using voxel grid filter"""
        # Calculate voxel coordinates
        voxel_coords = np.floor(points[:, :3] / voxel_size).astype(int)

        # Create unique voxel keys
        unique_voxels, indices = np.unique(voxel_coords, axis=0, return_index=True)

        # Return one point per voxel
        return points[indices]

    def radius_outlier_removal(self, points, radius=0.1, min_neighbors=2):
        """Remove points with few neighbors within radius"""
        if len(points) == 0:
            return points

        tree = cKDTree(points[:, :3])

        # Count neighbors within radius for each point
        neighbor_counts = tree.query_ball_point(points[:, :3], radius, return_length=True)

        # Keep points with sufficient neighbors
        valid_indices = neighbor_counts >= min_neighbors
        return points[valid_indices]

    def passthrough_filter(self, points, axis='z', min_val=-1.0, max_val=1.0):
        """Filter points based on axis limits"""
        axis_idx = {'x': 0, 'y': 1, 'z': 2}[axis.lower()]
        valid_indices = (points[:, axis_idx] >= min_val) & (points[:, axis_idx] <= max_val)
        return points[valid_indices]


# Example usage
def main():
    # Create synthetic point cloud data
    np.random.seed(42)

    # Generate some points in a plane
    x = np.random.uniform(-5, 5, 1000)
    y = np.random.uniform(-5, 5, 1000)
    z = np.random.normal(0, 0.1, 1000)  # Plane around z=0
    points = np.column_stack([x, y, z])

    # Add some noise/outliers
    outliers = np.random.uniform(-10, 10, (100, 3))
    points = np.vstack([points, outliers])

    print(f'Original points: {len(points)}')

    # Create filter instance
    filter_obj = PointCloudFilter()

    # Apply filters
    filtered_points = filter_obj.statistical_outlier_removal(points)
    print(f'After statistical outlier removal: {len(filtered_points)}')

    filtered_points = filter_obj.voxel_grid_filter(filtered_points, voxel_size=0.2)
    print(f'After voxel grid filtering: {len(filtered_points)}')

    filtered_points = filter_obj.passthrough_filter(filtered_points, 'z', -2.0, 2.0)
    print(f'After passthrough filtering: {len(filtered_points)}')


if __name__ == '__main__':
    main()
```

## Point Cloud Segmentation

### Ground Plane Segmentation

Ground plane segmentation is crucial for mobile robotics applications:

```python
import numpy as np
from sklearn.linear_model import RANSACRegressor


class GroundPlaneSegmenter:
    def __init__(self, distance_threshold=0.1, max_iterations=1000):
        self.distance_threshold = distance_threshold
        self.max_iterations = max_iterations

    def segment_ground_plane(self, points):
        """Segment ground plane using RANSAC"""
        if len(points) < 10:
            return np.array([]), np.array([])  # Not enough points

        # Prepare data for RANSAC (x, y, z -> fit z = ax + by + c)
        X = points[:, [0, 1]]  # x, y coordinates
        y = points[:, 2]       # z coordinates

        # Create and fit RANSAC model
        ransac = RANSACRegressor(
            min_samples=3,
            residual_threshold=self.distance_threshold,
            max_trials=self.max_iterations
        )
        ransac.fit(X, y)

        # Predict z values for all points
        predicted_z = ransac.predict(X)

        # Calculate distances from points to plane
        distances = np.abs(y - predicted_z)

        # Classify points as ground or obstacle
        ground_mask = distances < self.distance_threshold
        ground_points = points[ground_mask]
        obstacle_points = points[~ground_mask]

        # Extract plane parameters (ax + by + cz + d = 0)
        a, b = ransac.estimator_.coef_
        c = -1
        d = ransac.estimator_.intercept_

        plane_params = np.array([a, b, c, d])

        return ground_points, obstacle_points, plane_params

    def segment_ground_plane_manual(self, points, max_slope=0.1, max_height=0.2):
        """Simple ground segmentation based on height and slope"""
        # Project points to 2D grid and find ground level
        x, y, z = points[:, 0], points[:, 1], points[:, 2]

        # Simple approach: find minimum z in grid cells
        grid_size = 0.5
        x_grid = np.floor(x / grid_size).astype(int)
        y_grid = np.floor(y / grid_size).astype(int)

        # Group points by grid cell and find minimum z
        grid_dict = {}
        for i in range(len(points)):
            grid_key = (x_grid[i], y_grid[i])
            if grid_key not in grid_dict:
                grid_dict[grid_key] = []
            grid_dict[grid_key].append(z[i])

        # Calculate ground level for each cell
        ground_levels = {}
        for grid_key, z_values in grid_dict.items():
            if len(z_values) > 3:  # Need enough points
                ground_levels[grid_key] = np.percentile(z_values, 10)  # Use 10th percentile

        # Classify points based on ground level
        ground_points = []
        obstacle_points = []

        for i in range(len(points)):
            grid_key = (x_grid[i], y_grid[i])
            if grid_key in ground_levels:
                ground_z = ground_levels[grid_key]
                if z[i] < ground_z + max_height:
                    ground_points.append(points[i])
                else:
                    obstacle_points.append(points[i])
            else:
                obstacle_points.append(points[i])

        return np.array(ground_points), np.array(obstacle_points)


# Example usage
def main():
    # Create synthetic data with ground plane and obstacles
    np.random.seed(42)

    # Ground plane points
    x_ground = np.random.uniform(-10, 10, 2000)
    y_ground = np.random.uniform(-10, 10, 2000)
    z_ground = np.random.normal(0, 0.05, 2000)  # Ground at z=0 with noise
    ground_points = np.column_stack([x_ground, y_ground, z_ground])

    # Add some obstacles (boxes, poles, etc.)
    # Box
    x_box = np.random.uniform(-2, -1, 100)
    y_box = np.random.uniform(1, 2, 100)
    z_box = np.random.uniform(0.1, 0.5, 100)
    box_points = np.column_stack([x_box, y_box, z_box])

    # Pole
    x_pole = np.random.uniform(3, 3.1, 50)
    y_pole = np.random.uniform(-1, -0.9, 50)
    z_pole = np.random.uniform(0.1, 1.0, 50)
    pole_points = np.column_stack([x_pole, y_pole, z_pole])

    # Combine all points
    all_points = np.vstack([ground_points, box_points, pole_points])

    print(f'Total points: {len(all_points)}')

    # Segment ground plane
    segmenter = GroundPlaneSegmenter(distance_threshold=0.1)
    ground, obstacles, plane_params = segmenter.segment_ground_plane(all_points)

    print(f'Ground points: {len(ground)}, Obstacle points: {len(obstacles)}')
    print(f'Ground plane parameters: {plane_params}')


if __name__ == '__main__':
    main()
```

### Euclidean Clustering

Euclidean clustering groups nearby points into objects:

```python
import numpy as np
from sklearn.cluster import DBSCAN


class EuclideanClusterer:
    def __init__(self, eps=0.3, min_points=10):
        self.eps = eps  # Maximum distance between points in same cluster
        self.min_points = min_points  # Minimum points to form a cluster

    def cluster_points(self, points):
        """Cluster points using DBSCAN"""
        if len(points) < self.min_points:
            return np.array([])  # Not enough points to cluster

        # Use only x, y, z coordinates for clustering
        coordinates = points[:, :3]

        # Apply DBSCAN clustering
        clustering = DBSCAN(eps=self.eps, min_samples=self.min_points)
        labels = clustering.fit_predict(coordinates)

        # Create clusters dictionary
        clusters = {}
        for i, label in enumerate(labels):
            if label not in clusters:
                clusters[label] = []
            clusters[label].append(i)

        # Convert to list of point indices for each cluster
        cluster_indices = [clusters[label] for label in clusters if label != -1]  # Exclude noise points

        return cluster_indices

    def extract_cluster_properties(self, points, cluster_indices):
        """Extract properties for each cluster"""
        cluster_properties = []

        for cluster_idx_list in cluster_indices:
            cluster_points = points[cluster_idx_list]

            # Calculate cluster properties
            centroid = np.mean(cluster_points, axis=0)
            size = np.max(cluster_points, axis=0) - np.min(cluster_points, axis=0)
            num_points = len(cluster_points)

            properties = {
                'centroid': centroid,
                'size': size,
                'num_points': num_points,
                'points': cluster_points,
                'indices': cluster_idx_list
            }

            cluster_properties.append(properties)

        return cluster_properties


# Example usage
def main():
    # Create synthetic data with multiple objects
    np.random.seed(42)

    # Object 1: Box
    x1 = np.random.uniform(0, 1, 100)
    y1 = np.random.uniform(0, 1, 100)
    z1 = np.random.uniform(0, 0.5, 100)
    object1 = np.column_stack([x1, y1, z1])

    # Object 2: Cylinder
    theta = np.random.uniform(0, 2*np.pi, 80)
    radius = np.random.uniform(0, 0.3, 80)
    x2 = 3 + radius * np.cos(theta)
    y2 = 2 + radius * np.sin(theta)
    z2 = np.random.uniform(0, 0.8, 80)
    object2 = np.column_stack([x2, y2, z2])

    # Object 3: Another box
    x3 = np.random.uniform(-2, -1, 120)
    y3 = np.random.uniform(-1, 0, 120)
    z3 = np.random.uniform(0, 0.6, 120)
    object3 = np.column_stack([x3, y3, z3])

    # Combine all objects
    all_points = np.vstack([object1, object2, object3])

    print(f'Total points: {len(all_points)}')

    # Cluster the points
    clusterer = EuclideanClusterer(eps=0.3, min_points=20)
    clusters = clusterer.cluster_points(all_points)

    print(f'Found {len(clusters)} clusters')

    # Extract cluster properties
    properties = clusterer.extract_cluster_properties(all_points, clusters)

    for i, prop in enumerate(properties):
        print(f'Cluster {i}: {prop["num_points"]} points, centroid at ({prop["centroid"][0]:.2f}, {prop["centroid"][1]:.2f}, {prop["centroid"][2]:.2f})')


if __name__ == '__main__':
    main()
```

## 3D Mapping and Reconstruction

### Occupancy Grid Mapping

Occupancy grid mapping represents the environment as a discrete grid of occupied/free/unknown states:

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.ndimage import binary_dilation


class OccupancyGridMapper:
    def __init__(self, resolution=0.1, width=100, height=100):
        self.resolution = resolution  # meters per cell
        self.width = width  # number of cells
        self.height = height
        self.grid = np.full((height, width), 0.5, dtype=np.float32)  # 0.5 = unknown
        self.origin_x = -width * resolution / 2
        self.origin_y = -height * resolution / 2

    def update_with_laser_scan(self, ranges, angles, robot_x, robot_y, robot_yaw):
        """Update occupancy grid with laser scan data"""
        # Convert scan to world coordinates
        for i, (range_val, angle) in enumerate(zip(ranges, angles)):
            if not (np.isfinite(range_val) and range_val > 0):
                continue

            # Calculate world coordinates of obstacle
            world_x = robot_x + range_val * np.cos(robot_yaw + angle)
            world_y = robot_y + range_val * np.sin(robot_yaw + angle)

            # Calculate cell coordinates
            cell_x = int((world_x - self.origin_x) / self.resolution)
            cell_y = int((world_y - self.origin_y) / self.resolution)

            # Check bounds
            if 0 <= cell_x < self.width and 0 <= cell_y < self.height:
                # Update cell as occupied
                self.grid[cell_y, cell_x] = 0.9  # highly occupied

                # Update free space along the ray
                num_steps = int(range_val / self.resolution)
                for step in range(num_steps):
                    ray_x = robot_x + (step * self.resolution) * np.cos(robot_yaw + angle)
                    ray_y = robot_y + (step * self.resolution) * np.sin(robot_yaw + angle)

                    ray_cell_x = int((ray_x - self.origin_x) / self.resolution)
                    ray_cell_y = int((ray_y - self.origin_y) / self.resolution)

                    if 0 <= ray_cell_x < self.width and 0 <= ray_cell_y < self.height:
                        # Update as free space (but don't override occupied areas)
                        if self.grid[ray_cell_y, ray_cell_x] > 0.3:
                            self.grid[ray_cell_y, ray_cell_x] = 0.2  # free space

    def get_grid_coordinates(self, world_x, world_y):
        """Convert world coordinates to grid coordinates"""
        grid_x = int((world_x - self.origin_x) / self.resolution)
        grid_y = int((world_y - self.origin_y) / self.resolution)
        return grid_x, grid_y

    def get_world_coordinates(self, grid_x, grid_y):
        """Convert grid coordinates to world coordinates"""
        world_x = self.origin_x + grid_x * self.resolution
        world_y = self.origin_y + grid_y * self.resolution
        return world_x, world_y

    def inflate_obstacles(self, inflation_radius=0.3):
        """Inflate obstacles by a certain radius"""
        inflation_cells = int(inflation_radius / self.resolution)

        # Create binary mask of occupied cells
        occupied_mask = self.grid > 0.7

        # Dilate the mask
        for _ in range(inflation_cells):
            occupied_mask = binary_dilation(occupied_mask)

        # Update grid with inflated obstacles
        self.grid[occupied_mask] = 0.9

    def visualize(self):
        """Visualize the occupancy grid"""
        plt.figure(figsize=(10, 10))
        plt.imshow(self.grid, cmap='gray', origin='lower',
                  extent=[self.origin_x, self.origin_x + self.width * self.resolution,
                         self.origin_y, self.origin_y + self.height * self.resolution])
        plt.colorbar(label='Occupancy Probability')
        plt.title('Occupancy Grid Map')
        plt.xlabel('X (m)')
        plt.ylabel('Y (m)')
        plt.grid(True, alpha=0.3)
        plt.show()


# Example usage
def main():
    mapper = OccupancyGridMapper(resolution=0.1, width=200, height=200)

    # Simulate robot moving in a square path and taking laser scans
    robot_path = []
    for i in range(10):
        # Robot position
        robot_x = 2 * np.cos(i * 0.5)
        robot_y = 2 * np.sin(i * 0.5)
        robot_yaw = i * 0.5  # Robot orientation

        # Simulate laser scan (270 degree scan with 1 degree resolution)
        angles = np.deg2rad(np.arange(-135, 136, 1))
        ranges = np.full_like(angles, 5.0)  # Default range

        # Add some obstacles
        for j, angle in enumerate(angles):
            # Simulate an obstacle at (3, 0) relative to robot
            obs_x = 3.0
            obs_y = 0.0
            dist_to_obs = np.sqrt((obs_x - robot_x)**2 + (obs_y - robot_y)**2)
            angle_to_obs = np.arctan2(obs_y - robot_y, obs_x - robot_x) - robot_yaw

            # If laser ray points toward obstacle and is within range
            if abs(angle - angle_to_obs) < 0.2 and dist_to_obs < 5.0:
                ranges[j] = dist_to_obs

        # Update map with this scan
        mapper.update_with_laser_scan(ranges, angles, robot_x, robot_y, robot_yaw)
        robot_path.append((robot_x, robot_y))

    # Inflate obstacles
    mapper.inflate_obstacles(inflation_radius=0.3)

    # Visualize the map
    mapper.visualize()

    print(f'Map size: {mapper.width * mapper.resolution}m x {mapper.height * mapper.resolution}m')
    print(f'Resolution: {mapper.resolution}m per cell')


if __name__ == '__main__':
    main()
```

### 3D Reconstruction

3D reconstruction from multiple views or depth sensors:

```python
import numpy as np
import open3d as o3d  # This would be used in practice
from scipy.spatial import cKDTree


class Simple3DReconstructor:
    def __init__(self):
        pass

    def integrate_depth_images(self, depth_images, poses, camera_intrinsics):
        """Simple integration of depth images into a 3D model"""
        # This is a simplified version - in practice, you'd use TSDF integration
        # or other advanced techniques

        # Convert depth images to point clouds and transform to global frame
        all_points = []

        for depth_img, pose in zip(depth_images, poses):
            # Convert depth image to point cloud
            points = self.depth_to_pointcloud(depth_img, camera_intrinsics)

            # Transform points to global frame
            transformed_points = self.transform_points(points, pose)

            all_points.extend(transformed_points)

        return np.array(all_points)

    def depth_to_pointcloud(self, depth_img, camera_intrinsics):
        """Convert depth image to point cloud"""
        height, width = depth_img.shape

        # Create coordinate grids
        u_coords, v_coords = np.meshgrid(np.arange(width), np.arange(height))

        # Convert pixel coordinates to camera coordinates
        x = (u_coords - camera_intrinsics['cx']) * depth_img / camera_intrinsics['fx']
        y = (v_coords - camera_intrinsics['cy']) * depth_img / camera_intrinsics['fy']
        z = depth_img

        # Stack into points
        points = np.stack([x, y, z], axis=-1).reshape(-1, 3)

        # Remove invalid points (where depth is 0 or invalid)
        valid_mask = np.isfinite(points[:, 2]) & (points[:, 2] > 0)
        return points[valid_mask]

    def transform_points(self, points, pose):
        """Transform points using 4x4 pose matrix"""
        # Add homogeneous coordinate
        points_h = np.hstack([points, np.ones((len(points), 1))])

        # Apply transformation
        transformed_h = (pose @ points_h.T).T

        # Remove homogeneous coordinate
        return transformed_h[:, :3]

    def voxel_grid_downsample(self, points, voxel_size=0.01):
        """Downsample point cloud using voxel grid"""
        # Calculate voxel coordinates
        voxel_coords = np.floor(points / voxel_size).astype(int)

        # Create unique voxel keys
        unique_voxels, indices = np.unique(voxel_coords, axis=0, return_index=True)

        # Return one point per voxel
        return points[indices]

    def estimate_normals(self, points, k=20):
        """Estimate surface normals for point cloud"""
        if len(points) < k:
            return np.array([])

        # Build k-d tree for neighbor search
        tree = cKDTree(points)

        normals = []
        for point in points:
            # Find k nearest neighbors
            _, indices = tree.query(point, k=min(k, len(points)))
            neighbor_points = points[indices]

            # Calculate covariance matrix
            cov_matrix = np.cov(neighbor_points.T)

            # Get eigenvectors and eigenvalues
            eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)

            # Normal is the eigenvector corresponding to smallest eigenvalue
            normal = eigenvectors[:, 0]
            normals.append(normal)

        return np.array(normals)


# Example usage
def main():
    reconstructor = Simple3DReconstructor()

    # Simulate depth images from different viewpoints
    # In practice, these would come from an RGB-D camera

    # Create synthetic depth images (simplified)
    depth_images = []
    poses = []

    # Camera intrinsics (example values)
    camera_intrinsics = {
        'fx': 525.0,
        'fy': 525.0,
        'cx': 319.5,
        'cy': 239.5
    }

    # Simulate a few poses around an object
    for i in range(8):
        # Create a simple depth image (a plane at z=2)
        depth_img = np.full((480, 640), 2.0, dtype=np.float32)

        # Add some "objects" to the depth image
        depth_img[200:280, 300:340] = 1.5  # A box
        depth_img[150:170, 400:450] = 1.8  # A pole

        depth_images.append(depth_img)

        # Create a pose (rotation and translation)
        yaw = i * (2 * np.pi / 8)
        x = 3 * np.cos(yaw)
        y = 3 * np.sin(yaw)
        z = 1.0  # Height

        # Simple pose matrix (looking at origin)
        pose = np.eye(4)
        pose[0, 3] = x
        pose[1, 3] = y
        pose[2, 3] = z

        # Add rotation to look at origin
        pose[:3, :3] = np.array([
            [np.cos(yaw), -np.sin(yaw), 0],
            [np.sin(yaw), np.cos(yaw), 0],
            [0, 0, 1]
        ])

        poses.append(pose)

    # Reconstruct 3D model
    print(f'Processing {len(depth_images)} depth images...')

    # Convert to point clouds and integrate
    all_points = reconstructor.integrate_depth_images(depth_images, poses, camera_intrinsics)

    print(f'Reconstructed {len(all_points)} points')

    # Downsample for visualization
    downsampled = reconstructor.voxel_grid_downsample(all_points, voxel_size=0.05)
    print(f'Downsampled to {len(downsampled)} points')


if __name__ == '__main__':
    main()
```

## Scene Understanding

### Object Recognition in 3D

```python
import numpy as np
from sklearn.cluster import DBSCAN
from sklearn.linear_model import RANSACRegressor


class SceneUnderstanding:
    def __init__(self):
        self.object_database = {}  # Store known object models

    def register_object_model(self, name, point_cloud, features=None):
        """Register a known object model"""
        self.object_database[name] = {
            'model': point_cloud,
            'features': features or self.extract_features(point_cloud)
        }

    def extract_features(self, point_cloud):
        """Extract geometric features from point cloud"""
        features = {}

        # Basic statistics
        features['centroid'] = np.mean(point_cloud, axis=0)
        features['size'] = np.max(point_cloud, axis=0) - np.min(point_cloud, axis=0)
        features['volume'] = np.prod(features['size'])
        features['num_points'] = len(point_cloud)

        # Principal component analysis
        centered = point_cloud - features['centroid']
        cov_matrix = np.cov(centered.T)
        eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)

        # Sort eigenvalues and eigenvectors
        idx = np.argsort(eigenvalues)[::-1]
        eigenvalues = eigenvalues[idx]
        eigenvectors = eigenvectors[:, idx]

        features['pca_axes'] = eigenvectors
        features['pca_values'] = eigenvalues

        # Calculate shape descriptors
        features['linearity'] = (eigenvalues[0] - eigenvalues[1]) / eigenvalues[0]
        features['planarity'] = (eigenvalues[1] - eigenvalues[2]) / eigenvalues[0]
        features['scattering'] = eigenvalues[2] / eigenvalues[0]

        return features

    def segment_objects(self, scene_points, distance_threshold=0.3, min_points=20):
        """Segment objects in scene using clustering"""
        # Use DBSCAN for clustering
        clustering = DBSCAN(eps=distance_threshold, min_samples=min_points)
        labels = clustering.fit_predict(scene_points[:, :3])

        objects = []
        for label in set(labels):
            if label == -1:  # Noise points
                continue

            # Get points belonging to this cluster
            object_points = scene_points[labels == label]

            # Extract features for this object
            features = self.extract_features(object_points)

            objects.append({
                'points': object_points,
                'features': features,
                'label': label
            })

        return objects

    def match_object(self, object_features, threshold=0.1):
        """Match object features to known models"""
        best_match = None
        best_score = float('inf')

        for name, model in self.object_database.items():
            # Calculate similarity score (simplified - in practice, use more sophisticated methods)
            score = self.calculate_feature_similarity(object_features, model['features'])

            if score < best_score and score < threshold:
                best_score = score
                best_match = name

        return best_match, best_score

    def calculate_feature_similarity(self, features1, features2):
        """Calculate similarity between two sets of features"""
        # Simplified similarity calculation
        # In practice, you'd use more sophisticated methods like ICP, feature matching, etc.

        # Compare centroids
        centroid_diff = np.linalg.norm(features1['centroid'] - features2['centroid'])

        # Compare sizes
        size_diff = np.linalg.norm(features1['size'] - features2['size'])

        # Compare PCA values
        pca_diff = np.linalg.norm(features1['pca_values'] - features2['pca_values'])

        # Combine differences (weighted)
        similarity = 0.5 * centroid_diff + 0.3 * size_diff + 0.2 * pca_diff

        return similarity

    def understand_scene(self, scene_points):
        """Perform complete scene understanding"""
        # 1. Segment objects
        objects = self.segment_objects(scene_points)

        # 2. Classify each object
        scene_description = []
        for obj in objects:
            # Match to known objects
            match_name, confidence = self.match_object(obj['features'])

            scene_element = {
                'type': match_name if match_name else 'unknown_object',
                'confidence': confidence,
                'features': obj['features'],
                'points': obj['points']
            }

            scene_description.append(scene_element)

        return scene_description


# Example usage
def main():
    # Create scene understanding instance
    scene_understanding = SceneUnderstanding()

    # Register some known object models
    # Cube model (simplified)
    cube_points = []
    for x in np.linspace(-0.1, 0.1, 10):
        for y in np.linspace(-0.1, 0.1, 10):
            for z in [0.1, -0.1]:  # Top and bottom
                cube_points.append([x, y, z])
            for z in np.linspace(-0.1, 0.1, 10):
                for side_x, side_y in [(-0.1, y), (0.1, y), (x, -0.1), (x, 0.1)]:
                    cube_points.append([side_x, side_y, z])

    scene_understanding.register_object_model('cube', np.array(cube_points))

    # Cylinder model (simplified)
    cylinder_points = []
    for height in np.linspace(-0.1, 0.1, 10):
        for angle in np.linspace(0, 2*np.pi, 20):
            x = 0.1 * np.cos(angle)
            y = 0.1 * np.sin(angle)
            z = height
            cylinder_points.append([x, y, z])

    scene_understanding.register_object_model('cylinder', np.array(cylinder_points))

    # Create a scene with objects
    scene_points = []

    # Add a cube at (1, 1, 0)
    cube_offset = np.array([1, 1, 0])
    for point in cube_points:
        scene_points.append(np.array(point) + cube_offset)

    # Add a cylinder at (2, 0, 0)
    cylinder_offset = np.array([2, 0, 0])
    for point in cylinder_points:
        scene_points.append(np.array(point) + cylinder_offset)

    # Add some random points (clutter)
    random_points = np.random.uniform(-3, 3, (100, 3))
    random_points[:, 2] = np.abs(random_points[:, 2]) * 0.1  # Keep near ground
    for point in random_points:
        scene_points.append(point)

    scene_points = np.array(scene_points)

    print(f'Created scene with {len(scene_points)} points')

    # Perform scene understanding
    scene_description = scene_understanding.understand_scene(scene_points)

    print(f'\nScene understanding results:')
    for i, element in enumerate(scene_description):
        print(f'Object {i+1}: {element["type"]} (confidence: {element["confidence"]:.3f})')
        print(f'  Position: ({element["features"]["centroid"][0]:.2f}, {element["features"]["centroid"][1]:.2f}, {element["features"]["centroid"][2]:.2f})')
        print(f'  Size: ({element["features"]["size"][0]:.2f}, {element["features"]["size"][1]:.2f}, {element["features"]["size"][2]:.2f})')


if __name__ == '__main__':
    main()
```

## ROS 2 3D Perception Pipeline

### Complete 3D Perception Node

```python
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import PointCloud2, LaserScan
from sensor_msgs_py import point_cloud2
from visualization_msgs.msg import MarkerArray, Marker
from geometry_msgs.msg import Point
from std_msgs.msg import ColorRGBA
import numpy as np
from sklearn.cluster import DBSCAN
from sklearn.linear_model import RANSACRegressor


class Perception3DNode(Node):
    def __init__(self):
        super().__init__('perception_3d_node')

        # Subscribe to point cloud data
        self.pc_sub = self.create_subscription(
            PointCloud2,
            '/points_raw',
            self.pointcloud_callback,
            10)

        # Subscribe to laser scan (for occupancy grid)
        self.scan_sub = self.create_subscription(
            LaserScan,
            '/scan',
            self.scan_callback,
            10)

        # Publishers
        self.cluster_pub = self.create_publisher(MarkerArray, '/object_clusters', 10)
        self.ground_pub = self.create_publisher(MarkerArray, '/ground_plane', 10)
        self.map_pub = self.create_publisher(MarkerArray, '/occupancy_map', 10)

        # Parameters
        self.voxel_size = 0.1
        self.cluster_eps = 0.3
        self.cluster_min_points = 20
        self.ground_threshold = 0.1

        # State
        self.occupancy_grid = {}
        self.robot_pose = (0.0, 0.0, 0.0)  # x, y, theta

        self.get_logger().info('3D Perception node initialized')

    def pointcloud_callback(self, msg):
        """Process incoming point cloud"""
        try:
            # Convert to numpy array
            points = list(point_cloud2.read_points(
                msg,
                field_names=("x", "y", "z"),
                skip_nans=True
            ))
            points = np.array(points)

            if len(points) == 0:
                return

            # Filter ground plane
            ground_points, obstacle_points = self.segment_ground_plane(points)

            # Cluster obstacles
            clusters = self.cluster_points(obstacle_points)

            # Publish results
            self.publish_clusters(clusters, obstacle_points)
            self.publish_ground_plane(ground_points)

        except Exception as e:
            self.get_logger().error(f'Error in point cloud callback: {e}')

    def scan_callback(self, msg):
        """Process laser scan for occupancy mapping"""
        try:
            # Convert scan to points
            angles = np.linspace(msg.angle_min, msg.angle_max, len(msg.ranges))
            valid_ranges = np.array(msg.ranges)
            valid_angles = angles[np.isfinite(valid_ranges)]
            valid_ranges = valid_ranges[np.isfinite(valid_ranges)]

            # Convert to Cartesian coordinates
            x_points = valid_ranges * np.cos(valid_angles)
            y_points = valid_ranges * np.sin(valid_angles)
            scan_points = np.column_stack([x_points, y_points, np.zeros_like(x_points)])

            # Update occupancy grid
            self.update_occupancy_grid(scan_points, self.robot_pose)

            # Publish map
            self.publish_occupancy_map()

        except Exception as e:
            self.get_logger().error(f'Error in scan callback: {e}')

    def segment_ground_plane(self, points):
        """Segment ground plane using RANSAC"""
        if len(points) < 10:
            return np.array([]), points

        # Use only x, y, z coordinates
        coords = points[:, :3]

        # Apply RANSAC to find ground plane
        ransac = RANSACRegressor(
            min_samples=3,
            residual_threshold=self.ground_threshold,
            max_trials=1000
        )

        # Try to fit z = ax + by + c (or z = constant for flat ground)
        X = coords[:, [0, 1]]  # x, y
        y = coords[:, 2]       # z

        try:
            ransac.fit(X, y)
            ground_mask = np.abs(ransac.predict(X) - y) < self.ground_threshold
            ground_points = points[ground_mask]
            obstacle_points = points[~ground_mask]
        except:
            # If RANSAC fails, use simple height thresholding
            z_median = np.median(coords[:, 2])
            ground_mask = np.abs(coords[:, 2] - z_median) < self.ground_threshold
            ground_points = points[ground_mask]
            obstacle_points = points[~ground_mask]

        return ground_points, obstacle_points

    def cluster_points(self, points):
        """Cluster points using DBSCAN"""
        if len(points) < self.cluster_min_points:
            return []

        # Use x, y coordinates for clustering (ignore z)
        coords = points[:, :2]

        clustering = DBSCAN(eps=self.cluster_eps, min_samples=self.cluster_min_points)
        labels = clustering.fit_predict(coords)

        clusters = []
        for label in set(labels):
            if label == -1:  # Noise
                continue

            cluster_points = points[labels == label]
            clusters.append(cluster_points)

        return clusters

    def publish_clusters(self, clusters, all_points):
        """Publish clusters as visualization markers"""
        marker_array = MarkerArray()

        for i, cluster in enumerate(clusters):
            marker = Marker()
            marker.header.frame_id = "base_link"
            marker.header.stamp = self.get_clock().now().to_msg()
            marker.ns = "clusters"
            marker.id = i
            marker.type = Marker.SPHERE_LIST
            marker.action = Marker.ADD

            # Set scale
            marker.scale.x = 0.1
            marker.scale.y = 0.1
            marker.scale.z = 0.1

            # Set color
            marker.color.r = float(i % 3) / 2.0
            marker.color.g = float((i + 1) % 3) / 2.0
            marker.color.b = float((i + 2) % 3) / 2.0
            marker.color.a = 0.8

            # Add points
            for point in cluster:
                p = Point()
                p.x = point[0]
                p.y = point[1]
                p.z = point[2]
                marker.points.append(p)

            marker_array.markers.append(marker)

        self.cluster_pub.publish(marker_array)

    def publish_ground_plane(self, ground_points):
        """Publish ground plane as visualization marker"""
        if len(ground_points) == 0:
            return

        marker = Marker()
        marker.header.frame_id = "base_link"
        marker.header.stamp = self.get_clock().now().to_msg()
        marker.ns = "ground"
        marker.id = 0
        marker.type = Marker.POINTS
        marker.action = Marker.ADD

        # Set scale
        marker.scale.x = 0.05
        marker.scale.y = 0.05
        marker.scale.z = 0.05

        # Set color
        marker.color.r = 0.0
        marker.color.g = 1.0
        marker.color.b = 0.0
        marker.color.a = 0.6

        # Add points
        for point in ground_points[:500]:  # Limit number of points for performance
            p = Point()
            p.x = point[0]
            p.y = point[1]
            p.z = point[2]
            marker.points.append(p)

        ground_marker = MarkerArray()
        ground_marker.markers.append(marker)
        self.ground_pub.publish(ground_marker)

    def update_occupancy_grid(self, scan_points, robot_pose):
        """Update occupancy grid with scan data"""
        # Simplified occupancy grid update
        resolution = 0.2
        grid_size = 200  # 200x200 grid, 40mx40m area

        # Convert to grid coordinates
        grid_x = np.floor((scan_points[:, 0] - robot_pose[0]) / resolution).astype(int)
        grid_y = np.floor((scan_points[:, 1] - robot_pose[1]) / resolution).astype(int)

        # Update grid (simplified)
        valid_mask = (grid_x >= -grid_size//2) & (grid_x < grid_size//2) & \
                     (grid_y >= -grid_size//2) & (grid_y < grid_size//2)

        for x, y in zip(grid_x[valid_mask], grid_y[valid_mask]):
            grid_key = (int(x), int(y))
            self.occupancy_grid[grid_key] = 0.9  # Occupied

    def publish_occupancy_map(self):
        """Publish occupancy map as markers"""
        marker_array = MarkerArray()

        resolution = 0.2
        for i, ((grid_x, grid_y), occupancy) in enumerate(list(self.occupancy_grid.items())[:1000]):  # Limit for performance
            marker = Marker()
            marker.header.frame_id = "map"
            marker.header.stamp = self.get_clock().now().to_msg()
            marker.ns = "map"
            marker.id = i
            marker.type = Marker.CUBE
            marker.action = Marker.ADD

            # Set position
            marker.pose.position.x = grid_x * resolution
            marker.pose.position.y = grid_y * resolution
            marker.pose.position.z = 0.0
            marker.pose.orientation.w = 1.0

            # Set scale
            marker.scale.x = resolution
            marker.scale.y = resolution
            marker.scale.z = 0.1

            # Set color based on occupancy
            marker.color.r = occupancy
            marker.color.g = 1.0 - occupancy
            marker.color.b = 0.0
            marker.color.a = 0.8

            marker_array.markers.append(marker)

        self.map_pub.publish(marker_array)


def main(args=None):
    rclpy.init(args=args)
    perception_3d_node = Perception3DNode()

    try:
        rclpy.spin(perception_3d_node)
    except KeyboardInterrupt:
        pass
    finally:
        perception_3d_node.destroy_node()
        rclpy.shutdown()


if __name__ == '__main__':
    main()
```

## Hands-On Lab: 3D Scene Understanding System

### Objective
Create a complete 3D scene understanding system that processes point cloud data, segments objects, and builds an occupancy map.

### Prerequisites
- Completed Chapter 1-5
- ROS 2 Humble with Gazebo installed
- Basic understanding of 3D perception concepts

### Steps

1. **Create a 3D perception package**:
   ```bash
   cd ~/robotics_ws/src
   ros2 pkg create --build-type ament_python perception_3d_lab --dependencies rclpy sensor_msgs visualization_msgs geometry_msgs cv_bridge opencv-python numpy sklearn scipy
   ```

2. **Create the main perception node** (`perception_3d_lab/perception_3d_lab/perception_3d_node.py`):
   ```python
   #!/usr/bin/env python3
   import rclpy
   from rclpy.node import Node
   from sensor_msgs.msg import PointCloud2, LaserScan
   from sensor_msgs_py import point_cloud2
   from visualization_msgs.msg import MarkerArray, Marker
   from geometry_msgs.msg import Point
   from std_msgs.msg import ColorRGBA, Bool
   from nav_msgs.msg import OccupancyGrid
   import numpy as np
   from sklearn.cluster import DBSCAN
   from sklearn.linear_model import RANSACRegressor
   import time
   import math


   class Perception3DLabNode(Node):
       def __init__(self):
           super().__init__('perception_3d_lab_node')

           # Subscribe to sensor data
           self.pc_sub = self.create_subscription(
               PointCloud2,
               '/camera/depth/color/points',  # Typical topic for RGB-D point cloud
               self.pointcloud_callback,
               10)
           self.scan_sub = self.create_subscription(
               LaserScan,
               '/scan',
               self.scan_callback,
               10)

           # Publishers
           self.cluster_pub = self.create_publisher(MarkerArray, '/object_clusters', 10)
           self.map_pub = self.create_publisher(OccupancyGrid, '/local_map', 10)
           self.status_pub = self.create_publisher(Bool, '/perception_ready', 10)

           # Parameters
           self.voxel_size = 0.05
           self.cluster_eps = 0.2
           self.cluster_min_points = 10
           self.ground_threshold = 0.1
           self.map_resolution = 0.1
           self.map_width = 400  # 40m x 40m map
           self.map_height = 400

           # State
           self.occupancy_map = np.full((self.map_height, self.map_width), 0.5, dtype=np.float32)  # 0.5 = unknown
           self.map_origin_x = -self.map_width * self.map_resolution / 2
           self.map_origin_y = -self.map_height * self.map_resolution / 2
           self.robot_x = 0.0
           self.robot_y = 0.0
           self.last_process_time = time.time()

           # Timer for periodic processing
           self.process_timer = self.create_timer(0.5, self.process_timer_callback)

           self.get_logger().info('3D Perception Lab node initialized')

       def pointcloud_callback(self, msg):
           """Process incoming point cloud"""
           try:
               # Convert to numpy array
               points = list(point_cloud2.read_points(
                   msg,
                   field_names=("x", "y", "z"),
                   skip_nans=True
               ))
               points = np.array(points)

               if len(points) == 0:
                   return

               # Downsample point cloud
               downsampled = self.voxel_grid_filter(points, self.voxel_size)

               # Segment ground plane
               ground_points, obstacle_points = self.segment_ground_plane(downsampled)

               # Cluster obstacles
               clusters = self.cluster_points(obstacle_points)

               # Update occupancy map with obstacle information
               self.update_occupancy_map(clusters)

               # Publish results
               self.publish_clusters(clusters)
               self.publish_occupancy_grid()

           except Exception as e:
               self.get_logger().error(f'Error processing point cloud: {e}')

       def scan_callback(self, msg):
           """Process laser scan to update map"""
           try:
               # Convert scan to obstacle points
               angles = np.linspace(msg.angle_min, msg.angle_max, len(msg.ranges))
               valid_ranges = np.array(msg.ranges)
               valid_mask = np.isfinite(valid_ranges) & (valid_ranges > 0) & (valid_ranges < msg.range_max)

               if not np.any(valid_mask):
                   return

               ranges = valid_ranges[valid_mask]
               angles = angles[valid_mask]

               # Convert to Cartesian coordinates
               x_scan = ranges * np.cos(angles)
               y_scan = ranges * np.sin(angles)
               scan_points = np.column_stack([x_scan, y_scan, np.zeros_like(x_scan)])

               # Update occupancy map
               self.update_map_with_scan(scan_points)

           except Exception as e:
               self.get_logger().error(f'Error processing scan: {e}')

       def voxel_grid_filter(self, points, voxel_size):
           """Downsample point cloud using voxel grid"""
           if len(points) == 0:
               return points

           # Calculate voxel coordinates
           voxel_coords = np.floor(points[:, :3] / voxel_size).astype(int)

           # Create unique voxel keys
           unique_voxels, indices = np.unique(voxel_coords, axis=0, return_index=True)

           return points[indices]

       def segment_ground_plane(self, points):
           """Segment ground plane using RANSAC"""
           if len(points) < 10:
               return np.array([]), points

           # Use x, y, z coordinates
           coords = points[:, :3]

           # Prepare data for RANSAC (z = ax + by + c)
           X = coords[:, [0, 1]]  # x, y
           y = coords[:, 2]       # z

           ransac = RANSACRegressor(
               min_samples=3,
               residual_threshold=self.ground_threshold,
               max_trials=1000
           )

           try:
               ransac.fit(X, y)
               ground_mask = np.abs(ransac.predict(X) - y) < self.ground_threshold
               ground_points = points[ground_mask]
               obstacle_points = points[~ground_mask]
           except:
               # Fallback to simple height thresholding
               z_median = np.median(coords[:, 2])
               ground_mask = np.abs(coords[:, 2] - z_median) < self.ground_threshold
               ground_points = points[ground_mask]
               obstacle_points = points[~ground_mask]

           return ground_points, obstacle_points

       def cluster_points(self, points):
           """Cluster points using DBSCAN"""
           if len(points) < self.cluster_min_points:
               return []

           # Use x, y coordinates for clustering
           coords = points[:, :2]

           clustering = DBSCAN(eps=self.cluster_eps, min_samples=self.cluster_min_points)
           labels = clustering.fit_predict(coords)

           clusters = []
           for label in set(labels):
               if label == -1:  # Noise
                   continue

               cluster_points = points[labels == label]
               clusters.append(cluster_points)

           return clusters

       def update_occupancy_map(self, clusters):
           """Update occupancy map with clustered objects"""
           for cluster in clusters:
               if len(cluster) == 0:
                   continue

               # Calculate cluster centroid
               centroid = np.mean(cluster[:, :2], axis=0)

               # Convert to map coordinates
               map_x = int((centroid[0] - self.map_origin_x) / self.map_resolution)
               map_y = int((centroid[1] - self.map_origin_y) / self.map_resolution)

               # Check bounds
               if 0 <= map_x < self.map_width and 0 <= map_y < self.map_height:
                   # Mark as occupied
                   self.occupancy_map[map_y, map_x] = 0.9

       def update_map_with_scan(self, scan_points):
           """Update map with laser scan data"""
           for point in scan_points:
               # Convert to map coordinates
               map_x = int((point[0] - self.map_origin_x) / self.map_resolution)
               map_y = int((point[1] - self.map_origin_y) / self.map_resolution)

               # Check bounds
               if 0 <= map_x < self.map_width and 0 <= map_y < self.map_height:
                   # Mark as occupied (simplified - in practice, use proper ray tracing)
                   self.occupancy_map[map_y, map_x] = 0.8

       def publish_clusters(self, clusters):
           """Publish clusters as visualization markers"""
           marker_array = MarkerArray()

           for i, cluster in enumerate(clusters):
               if len(cluster) == 0:
                   continue

               marker = Marker()
               marker.header.frame_id = "base_link"
               marker.header.stamp = self.get_clock().now().to_msg()
               marker.ns = "objects"
               marker.id = i
               marker.type = Marker.SPHERE
               marker.action = Marker.ADD

               # Calculate bounding box for visualization
               min_pt = np.min(cluster[:, :3], axis=0)
               max_pt = np.max(cluster[:, :3], axis=0)
               center = (min_pt + max_pt) / 2

               # Set position
               marker.pose.position.x = center[0]
               marker.pose.position.y = center[1]
               marker.pose.position.z = center[2]
               marker.pose.orientation.w = 1.0

               # Set scale (bounding box size)
               marker.scale.x = max_pt[0] - min_pt[0]
               marker.scale.y = max_pt[1] - min_pt[1]
               marker.scale.z = max_pt[2] - min_pt[2]

               # Set color
               marker.color.r = 1.0
               marker.color.g = 0.0
               marker.color.b = 0.0
               marker.color.a = 0.7

               marker_array.markers.append(marker)

           self.cluster_pub.publish(marker_array)

       def publish_occupancy_grid(self):
           """Publish occupancy grid"""
           msg = OccupancyGrid()
           msg.header.frame_id = "map"
           msg.header.stamp = self.get_clock().now().to_msg()

           # Set metadata
           msg.info.resolution = self.map_resolution
           msg.info.width = self.map_width
           msg.info.height = self.map_height
           msg.info.origin.position.x = self.map_origin_x
           msg.info.origin.position.y = self.map_origin_y
           msg.info.origin.position.z = 0.0
           msg.info.origin.orientation.w = 1.0

           # Convert probabilities to int8 format (-1: unknown, 0-100: occupied percentage)
           grid_data = (self.occupancy_map * 100).astype(np.int8)
           grid_data = np.clip(grid_data, 0, 100)  # Ensure values are in range
           grid_data = np.where(self.occupancy_map < 0.2, 0, grid_data)  # Free space
           grid_data = np.where(self.occupancy_map > 0.8, 100, grid_data)  # Occupied space
           grid_data = np.where((self.occupancy_map >= 0.2) & (self.occupancy_map <= 0.8), -1, grid_data)  # Unknown

           # Flatten and convert to list
           msg.data = grid_data.flatten().tolist()

           self.map_pub.publish(msg)

       def process_timer_callback(self):
           """Periodic processing"""
           # Publish status
           status_msg = Bool()
           status_msg.data = True
           self.status_pub.publish(status_msg)

           # Log map statistics periodically
           if time.time() - self.last_process_time > 5.0:  # Every 5 seconds
               occupied_cells = np.sum(self.occupancy_map > 0.7)
               free_cells = np.sum(self.occupancy_map < 0.3)
               total_cells = self.occupancy_map.size

               self.get_logger().info(
                   f'Map: {occupied_cells} occupied, {free_cells} free, '
                   f'{total_cells - occupied_cells - free_cells} unknown cells'
               )
               self.last_process_time = time.time()


   def main(args=None):
       rclpy.init(args=args)
       perception_3d_lab_node = Perception3DLabNode()

       try:
           rclpy.spin(perception_3d_lab_node)
       except KeyboardInterrupt:
           pass
       finally:
           perception_3d_lab_node.destroy_node()
           rclpy.shutdown()


   if __name__ == '__main__':
       main()
   ```

3. **Create a launch file** (`perception_3d_lab/launch/perception_3d_lab.launch.py`):
   ```python
   from launch import LaunchDescription
   from launch.actions import DeclareLaunchArgument, IncludeLaunchDescription
   from launch.substitutions import LaunchConfiguration, PathJoinSubstitution
   from launch.launch_description_sources import PythonLaunchDescriptionSource
   from launch_ros.actions import Node
   from ament_index_python.packages import get_package_share_directory


   def generate_launch_description():
       # Declare launch arguments
       use_sim_time = DeclareLaunchArgument(
           'use_sim_time',
           default_value='true',
           description='Use simulation (Gazebo) clock if true'
       )

       # Perception 3D node
       perception_3d_node = Node(
           package='perception_3d_lab',
           executable='perception_3d_node',
           name='perception_3d_lab_node',
           parameters=[{'use_sim_time': LaunchConfiguration('use_sim_time')}],
           output='screen'
       )

       return LaunchDescription([
           use_sim_time,
           perception_3d_node
       ])
   ```

4. **Update setup.py**:
   ```python
   import os
   from glob import glob
   from setuptools import setup
   from setuptools import find_packages

   package_name = 'perception_3d_lab'

   setup(
       name=package_name,
       version='0.0.0',
       packages=find_packages(exclude=['test']),
       data_files=[
           ('share/ament_index/resource_index/packages',
               ['resource/' + package_name]),
           ('share/' + package_name, ['package.xml']),
           (os.path.join('share', package_name, 'launch'), glob('launch/*.launch.py')),
       ],
       install_requires=['setuptools'],
       zip_safe=True,
       maintainer='Your Name',
       maintainer_email='your.email@example.com',
       description='3D Perception lab for robotics',
       license='Apache License 2.0',
       tests_require=['pytest'],
       entry_points={
           'console_scripts': [
               'perception_3d_node = perception_3d_lab.perception_3d_node:main',
           ],
       },
   )
   ```

5. **Build the package**:
   ```bash
   cd ~/robotics_ws
   colcon build --packages-select perception_3d_lab
   source install/setup.bash
   ```

6. **Run the 3D perception system**:
   ```bash
   ros2 launch perception_3d_lab perception_3d_lab.launch.py
   ```

### Expected Results
- The system should process point cloud data and identify objects
- Clusters should be published as visualization markers
- An occupancy map should be generated and published
- The system should integrate both 3D point cloud and 2D laser scan data

### Troubleshooting Tips
- Ensure your robot has both 3D and 2D sensors publishing data
- Check topic names match your robot's sensor configuration
- Adjust clustering parameters based on your environment
- Verify TF frames are properly configured

## Summary

In this chapter, we've explored the fundamental concepts of 3D perception and scene understanding, including point cloud processing, segmentation, mapping, and object recognition. We've implemented practical examples of each concept and created a complete 3D scene understanding system.

The hands-on lab provided experience with creating a system that processes both 3D point clouds and 2D laser scans to build a comprehensive understanding of the environment. This foundation is essential for more advanced robotic capabilities like navigation, manipulation, and interaction with the environment, which we'll explore in the upcoming chapters.

============================================================
FILE: book\docs\part-iii-motion\chapter-7-kinematics\index.md
============================================================
---
title: Chapter 7 - Kinematics and Dynamics
sidebar_position: 1
---

# Chapter 7: Kinematics and Dynamics

## Learning Goals

- Master forward and inverse kinematics
- Understand robot dynamics and motion planning
- Learn trajectory generation techniques
- Implement kinematic solvers for robotic arms
- Generate smooth trajectories for robot motion
- Control robot joints with precise positioning

## Introduction to Robot Kinematics

Robot kinematics is the study of motion in robotic systems without considering the forces that cause the motion. It's divided into two main areas:

1. **Forward Kinematics**: Given joint angles, calculate the position and orientation of the end-effector
2. **Inverse Kinematics**: Given a desired end-effector position and orientation, calculate the required joint angles

Kinematics is fundamental to robot control, enabling precise positioning and motion planning.

### Coordinate Systems and Transformations

Robots operate in 3D space using various coordinate systems:

- **World Frame**: Fixed reference frame for the entire workspace
- **Base Frame**: Attached to the robot's base
- **Joint Frames**: Attached to each joint
- **End-Effector Frame**: Attached to the robot's tool or gripper

Homogeneous transformation matrices are used to represent position and orientation in a single 4√ó4 matrix:

```python
import numpy as np


def create_rotation_matrix(roll, pitch, yaw):
    """Create rotation matrix from roll, pitch, yaw angles"""
    cr, cp, cy = np.cos(roll), np.cos(pitch), np.cos(yaw)
    sr, sp, sy = np.sin(roll), np.sin(pitch), np.sin(yaw)

    R = np.array([
        [cy*cp, cy*sp*sr - sy*cr, cy*sp*cr + sy*sr],
        [sy*cp, sy*sp*sr + cy*cr, sy*sp*cr - cy*sr],
        [-sp, cp*sr, cp*cr]
    ])
    return R


def create_homogeneous_transform(translation, rotation_matrix):
    """Create 4x4 homogeneous transformation matrix"""
    T = np.eye(4)
    T[0:3, 0:3] = rotation_matrix
    T[0:3, 3] = translation
    return T


def transform_point(point, transform_matrix):
    """Transform a 3D point using a 4x4 transformation matrix"""
    # Convert to homogeneous coordinates
    homogeneous_point = np.append(point, 1)
    # Apply transformation
    transformed_point = transform_matrix @ homogeneous_point
    # Convert back to 3D
    return transformed_point[:3]


# Example usage
def main():
    # Create a transformation: translate by (1, 2, 3) and rotate by 45 degrees around Z-axis
    translation = np.array([1.0, 2.0, 3.0])
    rotation_matrix = create_rotation_matrix(0, 0, np.pi/4)  # 45 degrees around Z
    T = create_homogeneous_transform(translation, rotation_matrix)

    # Transform a point
    original_point = np.array([1.0, 0.0, 0.0])
    transformed_point = transform_point(original_point, T)

    print(f"Original point: {original_point}")
    print(f"Transformed point: {transformed_point}")


if __name__ == '__main__':
    main()
```

## Forward Kinematics

Forward kinematics calculates the end-effector position and orientation given joint angles. For serial manipulators, this is computed using the Denavit-Hartenberg (DH) parameters.

### Denavit-Hartenberg Parameters

The DH convention provides a systematic way to define coordinate frames on a robotic manipulator:

```python
import numpy as np


class DHParameter:
    def __init__(self, a, alpha, d, theta):
        """
        Denavit-Hartenberg parameters:
        a: link length
        alpha: link twist
        d: link offset
        theta: joint angle
        """
        self.a = a
        self.alpha = alpha
        self.d = d
        self.theta = theta

    def get_transformation_matrix(self):
        """Calculate the transformation matrix for this joint"""
        # Calculate transformation matrix based on DH parameters
        sa = np.sin(self.alpha)
        ca = np.cos(self.alpha)
        st = np.sin(self.theta)
        ct = np.cos(self.theta)

        T = np.array([
            [ct, -st*ca, st*sa, self.a*ct],
            [st, ct*ca, -ct*sa, self.a*st],
            [0, sa, ca, self.d],
            [0, 0, 0, 1]
        ])
        return T


class ForwardKinematics:
    def __init__(self, dh_parameters):
        """
        Initialize with DH parameters for each joint
        dh_parameters: list of DHParameter objects
        """
        self.dh_params = dh_parameters

    def calculate_transform(self, joint_angles):
        """
        Calculate the transformation from base to end-effector
        joint_angles: list of joint angles (for revolute joints)
        """
        if len(joint_angles) != len(self.dh_params):
            raise ValueError(f"Number of joint angles ({len(joint_angles)}) must match number of joints ({len(self.dh_params)})")

        # Update theta values in DH parameters
        dh_params = []
        for i, (param, angle) in enumerate(zip(self.dh_params, joint_angles)):
            new_param = DHParameter(param.a, param.alpha, param.d, angle)
            dh_params.append(new_param)

        # Calculate cumulative transformation
        T_total = np.eye(4)
        for param in dh_params:
            T = param.get_transformation_matrix()
            T_total = T_total @ T

        return T_total

    def get_end_effector_position(self, joint_angles):
        """Get only the position of the end-effector"""
        T = self.calculate_transform(joint_angles)
        return T[0:3, 3]

    def get_end_effector_pose(self, joint_angles):
        """Get both position and orientation of the end-effector"""
        T = self.calculate_transform(joint_angles)
        position = T[0:3, 3]
        orientation = T[0:3, 0:3]
        return position, orientation


# Example: 3-DOF planar manipulator
def main():
    # Define DH parameters for a simple 3-DOF planar manipulator
    dh_params = [
        DHParameter(a=1.0, alpha=0, d=0, theta=0),  # Joint 1
        DHParameter(a=1.0, alpha=0, d=0, theta=0),  # Joint 2
        DHParameter(a=0.5, alpha=0, d=0, theta=0)   # Joint 3 (end-effector)
    ]

    fk = ForwardKinematics(dh_params)

    # Calculate pose for a specific joint configuration
    joint_angles = [np.pi/4, np.pi/6, -np.pi/3]  # 45¬∞, 30¬∞, -60¬∞
    position, orientation = fk.get_end_effector_pose(joint_angles)

    print(f"Joint angles: {np.degrees(joint_angles)} degrees")
    print(f"End-effector position: {position}")
    print(f"End-effector orientation:\n{orientation}")


if __name__ == '__main__':
    main()
```

### Using Modern Robotics Library (Conceptual)

For more complex robots, libraries like `modern_robotics` provide efficient implementations:

```python
# This is a conceptual example - in practice, you'd use the modern_robotics library
import numpy as np


class ModernRoboticsFK:
    def __init__(self, screw_axes, joint_limits):
        """
        Initialize with screw axes and joint limits
        screw_axes: 6xN matrix where each column is a screw axis
        joint_limits: list of (min, max) joint limits
        """
        self.screw_axes = np.array(screw_axes)  # Each column is a twist (v, omega)
        self.joint_limits = joint_limits
        self.N = self.screw_axes.shape[1]  # Number of joints

    def matrix_exp6(self, se3_matrix):
        """Matrix exponential for se(3)"""
        # Extract angular and linear parts
        omega = se3_matrix[0:3, 0:3]
        v = se3_matrix[0:3, 3]

        # Calculate rotation matrix
        theta = np.linalg.norm(omega[2, 1], omega[0, 2], omega[1, 0])
        if theta < 1e-6:
            # Small angle approximation
            R = np.eye(3) + omega
            p = v
        else:
            # General case
            omega_skew = np.array([[0, -omega[2, 1], omega[1, 2]],
                                  [omega[2, 1], 0, -omega[0, 2]],
                                  [-omega[1, 2], omega[0, 2], 0]])
            omega_skew_sq = omega_skew @ omega_skew

            R = np.eye(3) + np.sin(theta) * omega_skew + (1 - np.cos(theta)) * omega_skew_sq
            G_inv = (np.eye(3) - omega_skew + ((1 - np.cos(theta)) / theta) * omega_skew_sq) / theta if theta > 1e-6 else np.eye(3)
            p = (np.eye(3) * theta + (1 - np.cos(theta)) * omega_skew + (theta - np.sin(theta)) * omega_skew_sq) @ v / (theta**2) if theta > 1e-6 else v

        # Create transformation matrix
        T = np.eye(4)
        T[0:3, 0:3] = R
        T[0:3, 3] = p
        return T

    def forward_kinematics(self, thetalist, M):
        """
        Compute forward kinematics using product of exponentials
        thetalist: list of joint angles
        M: home configuration of end-effector
        """
        if len(thetalist) != self.N:
            raise ValueError("Number of joint angles must match number of joints")

        T = M.copy()
        for i in range(self.N-1, -1, -1):
            # Create twist matrix
            twist = np.zeros((4, 4))
            omega = self.screw_axes[0:3, i]
            v = self.screw_axes[3:6, i]

            twist[0:3, 0:3] = np.array([[0, -omega[2], omega[1]],
                                       [omega[2], 0, -omega[0]],
                                       [-omega[1], omega[0], 0]])
            twist[0:3, 3] = v

            # Apply transformation
            exp_twist = self.matrix_exp6(thetalist[i] * twist)
            T = exp_twist @ T

        return T
```

## Inverse Kinematics

Inverse kinematics (IK) is the reverse problem: given a desired end-effector pose, find the joint angles that achieve it. This is more challenging than forward kinematics and may have multiple solutions or no solution.

### Analytical Inverse Kinematics

For simple robots, analytical solutions exist:

```python
import numpy as np
import math


class AnalyticalIK2D:
    """Analytical inverse kinematics for a 2D planar 2-link manipulator"""

    def __init__(self, link_lengths):
        """
        Initialize with link lengths
        link_lengths: [L1, L2] - lengths of the two links
        """
        self.L1, self.L2 = link_lengths

    def solve_ik(self, x, y):
        """
        Solve inverse kinematics for 2D planar 2-link manipulator
        Returns two possible solutions (elbow up and elbow down)
        """
        # Check if the target is reachable
        distance = np.sqrt(x**2 + y**2)
        if distance > (self.L1 + self.L2):
            raise ValueError("Target is out of reach")
        if distance < abs(self.L1 - self.L2):
            raise ValueError("Target is inside the workspace")

        # Calculate angle for joint 2
        cos_theta2 = (x**2 + y**2 - self.L1**2 - self.L2**2) / (2 * self.L1 * self.L2)
        sin_theta2 = np.sqrt(1 - cos_theta2**2)

        theta2 = np.arctan2(sin_theta2, cos_theta2)  # Elbow up solution
        theta2_alt = np.arctan2(-sin_theta2, cos_theta2)  # Elbow down solution

        # Calculate angle for joint 1
        k1 = self.L1 + self.L2 * cos_theta2
        k2 = self.L2 * sin_theta2
        theta1 = np.arctan2(y, x) - np.arctan2(k2, k1)

        # Alternative solution
        k1_alt = self.L1 + self.L2 * cos_theta2_alt
        k2_alt = self.L2 * sin_theta2_alt
        theta1_alt = np.arctan2(y, x) - np.arctan2(k2_alt, k1_alt)

        return [(theta1, theta2), (theta1_alt, theta2_alt)]


class AnalyticalIK3D:
    """Analytical inverse kinematics for a 6-DOF manipulator (simplified PUMA-like)"""

    def __init__(self, dh_params):
        """
        Initialize with DH parameters for a 6-DOF manipulator
        This is a simplified example for a specific robot configuration
        """
        self.dh_params = dh_params

    def solve_ik(self, target_position, target_orientation):
        """
        Solve inverse kinematics for a 6-DOF manipulator
        This is a simplified implementation for a specific robot type
        """
        x, y, z = target_position

        # For a simplified 6-DOF robot with spherical wrist, we can decouple
        # position and orientation kinematics

        # 1. Position: Calculate wrist center position
        # (Assuming spherical wrist offset)
        wrist_offset = 0.1  # Example wrist offset
        wrist_x = x - wrist_offset * target_orientation[0, 2]
        wrist_y = y - wrist_offset * target_orientation[1, 2]
        wrist_z = z - wrist_offset * target_orientation[2, 2]

        # 2. Calculate first three joints for wrist position (like 3-DOF arm)
        # Using the 2D solution approach but in 3D space
        r = np.sqrt(wrist_x**2 + wrist_y**2)
        alpha = np.arctan2(wrist_y, wrist_x)  # Joint 1

        # Now solve 2D problem in the rz plane
        d = wrist_z  # Height

        # This is a simplified version - full solution would involve
        # more complex geometric relationships
        solutions = []

        # For demonstration, return a placeholder solution
        # In a real implementation, you would solve the full geometric problem
        for i in range(2):  # Two possible arm configurations
            sol = [0.0] * 6  # 6 joint angles
            sol[0] = alpha  # Joint 1
            # Calculate remaining joints based on geometry
            # (This would require full geometric analysis)
            solutions.append(sol)

        return solutions


# Example usage
def main():
    # Example for 2D planar manipulator
    ik_2d = AnalyticalIK2D([1.0, 1.0])  # Two links of length 1.0

    try:
        solutions = ik_2d.solve_ik(1.0, 1.0)
        print(f"Target: (1.0, 1.0)")
        for i, (theta1, theta2) in enumerate(solutions):
            print(f"Solution {i+1}: Joint 1 = {np.degrees(theta1):.2f}¬∞, Joint 2 = {np.degrees(theta2):.2f}¬∞")
    except ValueError as e:
        print(f"Error: {e}")

    # Verify solution by running forward kinematics
    fk = ForwardKinematics([
        DHParameter(a=1.0, alpha=0, d=0, theta=0),
        DHParameter(a=1.0, alpha=0, d=0, theta=0)
    ])

    # Use first solution
    if solutions:
        theta1, theta2 = solutions[0]
        position, orientation = fk.get_end_effector_pose([theta1, theta2])
        print(f"FK verification - Position: ({position[0]:.3f}, {position[1]:.3f}, {position[2]:.3f})")


if __name__ == '__main__':
    main()
```

### Numerical Inverse Kinematics

For complex robots without analytical solutions, numerical methods are used:

```python
import numpy as np
from scipy.optimize import minimize
from scipy.spatial.transform import Rotation as R


class NumericalIK:
    def __init__(self, robot_model):
        """
        Initialize with a robot model
        robot_model: object with forward_kinematics method
        """
        self.robot_model = robot_model

    def objective_function(self, joint_angles, target_pose):
        """
        Objective function to minimize
        joint_angles: current joint angles
        target_pose: desired end-effector pose [x, y, z, rx, ry, rz]
        """
        # Calculate current end-effector pose
        current_pose = self.robot_model.forward_kinematics(joint_angles)

        # Calculate position error
        pos_error = np.linalg.norm(current_pose[:3] - target_pose[:3])

        # Calculate orientation error
        current_rot = R.from_matrix(current_pose[3:].reshape(3, 3))
        target_rot = R.from_matrix(target_pose[3:].reshape(3, 3))
        rot_error = np.linalg.norm(current_rot.as_rotvec() - target_rot.as_rotvec())

        # Combined error
        total_error = pos_error + 0.1 * rot_error  # Weight orientation less

        return total_error

    def solve_ik(self, target_pose, initial_guess, joint_limits=None):
        """
        Solve inverse kinematics using numerical optimization
        target_pose: desired end-effector pose [x, y, z, r, p, y] or transformation matrix
        initial_guess: starting joint angles
        joint_limits: list of (min, max) for each joint
        """
        if joint_limits is None:
            joint_limits = [(-np.pi, np.pi)] * len(initial_guess)

        # Define bounds
        bounds = []
        for lim in joint_limits:
            bounds.append((lim[0], lim[1]))

        # Optimize
        result = minimize(
            self.objective_function,
            initial_guess,
            args=(target_pose,),
            method='L-BFGS-B',
            bounds=bounds
        )

        if result.success:
            return result.x
        else:
            raise RuntimeError(f"IK solution failed: {result.message}")


# Example using a simple robot model
class SimpleRobotModel:
    def __init__(self):
        # For this example, we'll use a simple model
        pass

    def forward_kinematics(self, joint_angles):
        """
        Simplified forward kinematics returning [x, y, z, r, p, y]
        This is a placeholder - in reality, you'd implement proper FK
        """
        # For a 3-DOF planar manipulator
        if len(joint_angles) >= 3:
            L1, L2, L3 = 1.0, 1.0, 0.5  # Link lengths
            theta1, theta2, theta3 = joint_angles[:3]

            x = L1 * np.cos(theta1) + L2 * np.cos(theta1 + theta2) + L3 * np.cos(theta1 + theta2 + theta3)
            y = L1 * np.sin(theta1) + L2 * np.sin(theta1 + theta2) + L3 * np.sin(theta1 + theta2 + theta3)
            z = 0  # Planar robot

            # Simple orientation (end-effector angle)
            end_effector_angle = theta1 + theta2 + theta3

            # Return position and orientation as Euler angles
            return np.array([x, y, z, 0, 0, end_effector_angle])
        else:
            return np.zeros(6)


def main():
    # Create robot model and IK solver
    robot_model = SimpleRobotModel()
    ik_solver = NumericalIK(robot_model)

    # Define target pose [x, y, z, roll, pitch, yaw]
    target_pose = np.array([1.5, 1.0, 0.0, 0.0, 0.0, np.pi/4])

    # Initial guess
    initial_guess = [0.0, 0.0, 0.0]

    # Joint limits (for example, ¬±170 degrees)
    joint_limits = [(-np.pi*0.95, np.pi*0.95)] * 3

    try:
        solution = ik_solver.solve_ik(target_pose, initial_guess, joint_limits)
        print(f"IK Solution found: {np.degrees(solution)} degrees")

        # Verify solution
        final_pose = robot_model.forward_kinematics(solution)
        print(f"Final pose: {final_pose}")
        print(f"Target pose: {target_pose}")
        print(f"Position error: {np.linalg.norm(final_pose[:3] - target_pose[:3]):.6f}")

    except RuntimeError as e:
        print(f"IK Error: {e}")


if __name__ == '__main__':
    main()
```

## Robot Dynamics

Robot dynamics deals with the forces and torques required to generate motion. Understanding dynamics is crucial for controlling robots with precision and efficiency.

### Newton-Euler Formulation

The Newton-Euler method calculates forces and moments acting on each link:

```python
import numpy as np


class NewtonEulerDynamics:
    def __init__(self, link_masses, link_lengths, link_coms):
        """
        Initialize dynamics model using Newton-Euler formulation
        link_masses: list of masses for each link
        link_lengths: list of lengths for each link
        link_coms: list of center of mass positions for each link
        """
        self.masses = np.array(link_masses)
        self.lengths = np.array(link_lengths)
        self.coms = np.array(link_coms)  # Center of mass positions relative to joint
        self.n = len(link_masses)

    def forward_dynamics(self, joint_positions, joint_velocities, joint_accelerations, joint_torques):
        """
        Calculate joint accelerations given torques (forward dynamics)
        """
        # This is a simplified implementation
        # Full Newton-Euler would require detailed kinematic relationships

        # Calculate mass matrix (simplified as diagonal)
        mass_matrix = np.diag(self.masses * self.coms**2)  # Simplified inertia approximation

        # Add Coriolis and centrifugal terms (simplified)
        coriolis_matrix = np.zeros((self.n, self.n))
        for i in range(self.n):
            for j in range(self.n):
                if i != j:
                    # Simplified Coriolis term
                    coriolis_matrix[i, j] = self.masses[i] * self.coms[i] * self.coms[j] * joint_velocities[j]

        # Gravity terms
        gravity = 9.81
        gravity_terms = self.masses * gravity * np.sin(joint_positions)

        # Forward dynamics: M(q)q_ddot + C(q,q_dot)q_dot + g(q) = œÑ
        # Solve for q_ddot
        rhs = joint_torques - (coriolis_matrix @ joint_velocities) - gravity_terms
        joint_accelerations = np.linalg.solve(mass_matrix, rhs)

        return joint_accelerations

    def inverse_dynamics(self, joint_positions, joint_velocities, joint_accelerations):
        """
        Calculate required torques given motion (inverse dynamics)
        """
        # Calculate mass matrix
        mass_matrix = np.diag(self.masses * self.coms**2)  # Simplified

        # Calculate Coriolis matrix
        coriolis_matrix = np.zeros((self.n, self.n))
        for i in range(self.n):
            for j in range(self.n):
                if i != j:
                    coriolis_matrix[i, j] = self.masses[i] * self.coms[i] * self.coms[j] * joint_velocities[j]

        # Calculate gravity terms
        gravity = 9.81
        gravity_terms = self.masses * gravity * np.sin(joint_positions)

        # Calculate required torques
        # œÑ = M(q)q_ddot + C(q,q_dot)q_dot + g(q)
        joint_torques = (mass_matrix @ joint_accelerations +
                        coriolis_matrix @ joint_velocities +
                        gravity_terms)

        return joint_torques


# Example usage
def main():
    # Create a simple 2-DOF manipulator
    link_masses = [1.0, 0.8]  # Mass of each link in kg
    link_lengths = [1.0, 0.8]  # Length of each link in m
    link_coms = [0.5, 0.4]     # Center of mass position in m

    dynamics = NewtonEulerDynamics(link_masses, link_lengths, link_coms)

    # Example motion
    joint_positions = np.array([np.pi/4, np.pi/6])      # Joint angles
    joint_velocities = np.array([0.1, 0.05])            # Joint velocities
    joint_accelerations = np.array([0.01, 0.005])       # Joint accelerations

    # Calculate required torques (inverse dynamics)
    required_torques = dynamics.inverse_dynamics(joint_positions, joint_velocities, joint_accelerations)
    print(f"Required torques: {required_torques} Nm")

    # Calculate motion given torques (forward dynamics)
    applied_torques = required_torques  # Use same torques for verification
    resulting_accelerations = dynamics.forward_dynamics(
        joint_positions, joint_velocities, joint_accelerations, applied_torques
    )
    print(f"Resulting accelerations: {resulting_accelerations} rad/s¬≤")


if __name__ == '__main__':
    main()
```

### Lagrangian Formulation

The Lagrangian method is another approach to robot dynamics:

```python
import numpy as np
from scipy.integrate import solve_ivp


class LagrangianDynamics:
    def __init__(self, robot_params):
        """
        Initialize dynamics model using Lagrangian formulation
        robot_params: Dictionary containing robot parameters
        """
        self.params = robot_params

    def mass_matrix(self, q):
        """
        Calculate mass matrix H(q)
        q: joint positions
        """
        # For a 2-DOF planar manipulator
        # H = [[h11, h12], [h12, h22]]

        m1, m2 = self.params['m1'], self.params['m2']
        l1, l2 = self.params['l1'], self.params['l2']
        lc1, lc2 = self.params['lc1'], self.params['lc2']
        I1, I2 = self.params['I1'], self.params['I2']

        q1, q2 = q[0], q[1]

        # Elements of mass matrix
        h11 = (m1 + m2) * lc1**2 + m2 * l1**2 + I1 + I2 + 2*m2*l1*lc2*np.cos(q2)
        h12 = m2*lc2**2 + I2 + m2*l1*lc2*np.cos(q2)
        h21 = h12  # Symmetric
        h22 = m2*lc2**2 + I2

        H = np.array([[h11, h12], [h21, h22]])
        return H

    def coriolis_matrix(self, q, q_dot):
        """
        Calculate Coriolis matrix C(q, q_dot)
        q: joint positions
        q_dot: joint velocities
        """
        m2 = self.params['m2']
        l1, l2 = self.params['l1'], self.params['l2']
        lc2 = self.params['lc2']
        q1, q2 = q[0], q[1]
        q1_dot, q2_dot = q_dot[0], q_dot[1]

        c11 = -2*m2*l1*lc2*np.sin(q2)*q2_dot
        c12 = -m2*l1*lc2*np.sin(q2)*q2_dot
        c21 = m2*l1*lc2*np.sin(q2)*q1_dot
        c22 = 0

        C = np.array([[c11, c12], [c21, c22]])
        return C

    def gravity_vector(self, q):
        """
        Calculate gravity vector g(q)
        q: joint positions
        """
        m1, m2 = self.params['m1'], self.params['m2']
        lc1, lc2 = self.params['lc1'], self.params['lc2']
        g = self.params['g']

        q1, q2 = q[0], q[1]

        g1 = (m1*lc1 + m2*l1)*g*np.cos(q1) + m2*lc2*g*np.cos(q1 + q2)
        g2 = m2*lc2*g*np.cos(q1 + q2)

        G = np.array([g1, g2])
        return G

    def inverse_dynamics(self, q, q_dot, q_ddot):
        """
        Calculate required torques using inverse dynamics
        q: joint positions
        q_dot: joint velocities
        q_ddot: joint accelerations
        """
        H = self.mass_matrix(q)
        C = self.coriolis_matrix(q, q_dot)
        G = self.gravity_vector(q)

        # œÑ = H(q)q_ddot + C(q,q_dot)q_dot + G(q)
        tau = H @ q_ddot + C @ q_dot + G
        return tau

    def forward_dynamics(self, q, q_dot, tau):
        """
        Calculate joint accelerations using forward dynamics
        q: joint positions
        q_dot: joint velocities
        tau: applied torques
        """
        H = self.mass_matrix(q)
        C = self.coriolis_matrix(q, q_dot)
        G = self.gravity_vector(q)

        # H(q)q_ddot = œÑ - C(q,q_dot)q_dot - G(q)
        q_ddot = np.linalg.solve(H, tau - C @ q_dot - G)
        return q_ddot

    def simulate_motion(self, initial_conditions, torques_func, t_span, t_eval):
        """
        Simulate robot motion given torque profile
        initial_conditions: [q0, q_dot0] initial positions and velocities
        torques_func: function that returns torques at time t
        t_span: (t_start, t_end)
        t_eval: time points to evaluate
        """
        def dynamics_func(t, y):
            """
            RHS of the differential equation for solve_ivp
            y = [q, q_dot] state vector
            """
            n = len(y) // 2
            q = y[:n]
            q_dot = y[n:]

            # Get torques at current time
            tau = torques_func(t)

            # Calculate accelerations
            q_ddot = self.forward_dynamics(q, q_dot, tau)

            # Return derivatives [q_dot, q_ddot]
            return np.concatenate([q_dot, q_ddot])

        # Initial state [q0, q_dot0]
        y0 = np.concatenate([initial_conditions[0], initial_conditions[1]])

        # Solve the differential equation
        solution = solve_ivp(
            dynamics_func,
            t_span,
            y0,
            t_eval=t_eval,
            method='RK45'
        )

        return solution


# Example usage
def main():
    # Define robot parameters for a 2-DOF planar manipulator
    robot_params = {
        'm1': 2.0,   # Mass of link 1 (kg)
        'm2': 1.5,   # Mass of link 2 (kg)
        'l1': 1.0,   # Length of link 1 (m)
        'l2': 0.8,   # Length of link 2 (m)
        'lc1': 0.5,  # Distance to COM of link 1 (m)
        'lc2': 0.4,  # Distance to COM of link 2 (m)
        'I1': 0.2,   # Inertia of link 1 (kg*m¬≤)
        'I2': 0.1,   # Inertia of link 2 (kg*m¬≤)
        'g': 9.81    # Gravity (m/s¬≤)
    }

    dynamics = LagrangianDynamics(robot_params)

    # Example: Calculate torques for a specific motion
    q = np.array([np.pi/4, np.pi/6])      # Joint positions
    q_dot = np.array([0.1, 0.05])         # Joint velocities
    q_ddot = np.array([0.01, 0.005])      # Joint accelerations

    required_torques = dynamics.inverse_dynamics(q, q_dot, q_ddot)
    print(f"Required torques: {required_torques} Nm")

    # Example: Forward dynamics (find accelerations for given torques)
    applied_torques = np.array([2.0, 1.0])  # Applied torques
    resulting_accelerations = dynamics.forward_dynamics(q, q_dot, applied_torques)
    print(f"Resulting accelerations: {resulting_accelerations} rad/s¬≤")


if __name__ == '__main__':
    main()
```

## Trajectory Generation

Trajectory generation creates smooth, feasible paths for robot motion:

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.interpolate import CubicSpline


class TrajectoryGenerator:
    def __init__(self):
        pass

    def cubic_polynomial_trajectory(self, q_start, q_end, t_start, t_end, qd_start=0, qd_end=0):
        """
        Generate cubic polynomial trajectory
        q_start, q_end: start and end positions
        t_start, t_end: start and end times
        qd_start, qd_end: start and end velocities (default 0)
        """
        dt = t_end - t_start

        # Coefficients for cubic polynomial: q(t) = a0 + a1*t + a2*t^2 + a3*t^3
        # Using boundary conditions:
        # q(t_start) = q_start, q(t_end) = q_end
        # q_dot(t_start) = qd_start, q_dot(t_end) = qd_end

        a0 = q_start
        a1 = qd_start
        a2 = (3/dt**2) * (q_end - q_start) - (2/dt) * qd_start - (1/dt) * qd_end
        a3 = (-2/dt**3) * (q_end - q_start) + (1/dt**2) * (qd_start + qd_end)

        def trajectory(t):
            t_rel = t - t_start
            q = a0 + a1*t_rel + a2*t_rel**2 + a3*t_rel**3
            qd = a1 + 2*a2*t_rel + 3*a3*t_rel**2
            qdd = 2*a2 + 6*a3*t_rel
            return q, qd, qdd

        return trajectory

    def quintic_polynomial_trajectory(self, q_start, q_end, t_start, t_end,
                                    qd_start=0, qd_end=0, qdd_start=0, qdd_end=0):
        """
        Generate quintic polynomial trajectory
        More smooth with continuous acceleration
        """
        dt = t_end - t_start

        # Coefficients for quintic polynomial: q(t) = a0 + a1*t + a2*t^2 + a3*t^3 + a4*t^4 + a5*t^5
        a0 = q_start
        a1 = qd_start
        a2 = qdd_start / 2
        a3 = (20*q_end - 20*q_start - (8*qd_end + 12*qd_start)*dt - (3*qdd_start - qdd_end)*dt**2) / (2*dt**3)
        a4 = (30*q_start - 30*q_end + (14*qd_end + 16*qd_start)*dt + (3*qdd_start - 2*qdd_end)*dt**2) / (2*dt**4)
        a5 = (12*q_end - 12*q_start - 6*(qd_end + qd_start)*dt - (qdd_start - qdd_end)*dt**2) / (2*dt**5)

        def trajectory(t):
            t_rel = t - t_start
            q = a0 + a1*t_rel + a2*t_rel**2 + a3*t_rel**3 + a4*t_rel**4 + a5*t_rel**5
            qd = a1 + 2*a2*t_rel + 3*a3*t_rel**2 + 4*a4*t_rel**3 + 5*a5*t_rel**4
            qdd = 2*a2 + 6*a3*t_rel + 12*a4*t_rel**2 + 20*a5*t_rel**3
            return q, qd, qdd

        return trajectory

    def trapezoidal_trajectory(self, q_start, q_end, max_vel, max_acc):
        """
        Generate trapezoidal velocity profile
        q_start, q_end: start and end positions
        max_vel: maximum velocity
        max_acc: maximum acceleration
        """
        total_distance = q_end - q_start
        direction = 1 if total_distance > 0 else -1
        total_distance = abs(total_distance)

        # Calculate time for acceleration phase to reach max velocity
        acc_time = max_vel / max_acc

        # Distance covered during acceleration
        acc_distance = 0.5 * max_acc * acc_time**2

        # Check if we can reach max velocity
        if 2 * acc_distance >= total_distance:
            # Triangle profile - can't reach max velocity
            new_max_vel = np.sqrt(max_acc * total_distance)
            acc_time = new_max_vel / max_acc
            acc_distance = 0.5 * new_max_vel * acc_time
            const_time = 0
        else:
            # Trapezoidal profile
            const_distance = total_distance - 2 * acc_distance
            const_time = const_distance / max_vel

        total_time = 2 * acc_time + const_time

        def trajectory(t):
            # Phase 1: Acceleration
            if 0 <= t <= acc_time:
                q = q_start + direction * (0.5 * max_acc * t**2)
                qd = direction * max_acc * t
                qdd = direction * max_acc
            # Phase 2: Constant velocity
            elif acc_time < t <= acc_time + const_time:
                q = q_start + direction * (acc_distance + max_vel * (t - acc_time))
                qd = direction * max_vel
                qdd = 0
            # Phase 3: Deceleration
            elif acc_time + const_time < t <= total_time:
                t_dec = t - (acc_time + const_time)
                q = q_start + direction * (acc_distance + const_time * max_vel +
                                          max_vel * t_dec - 0.5 * max_acc * t_dec**2)
                qd = direction * (max_vel - max_acc * t_dec)
                qdd = -direction * max_acc
            else:
                q = q_end
                qd = 0
                qdd = 0

            return q, qd, qdd

        return trajectory, total_time

    def multi_dof_trajectory(self, waypoints, times, max_vel=1.0, max_acc=1.0):
        """
        Generate trajectory through multiple waypoints for multi-DOF robot
        waypoints: list of joint configurations [q1, q2, ..., qn]
        times: list of times for each waypoint
        """
        n_dof = len(waypoints[0])
        n_waypoints = len(waypoints)

        trajectories = []

        for i in range(n_dof):
            # Extract joint positions for this DOF
            joint_positions = [waypoint[i] for waypoint in waypoints]

            # Create spline for this joint
            spline = CubicSpline(times, joint_positions)
            trajectories.append(spline)

        def multi_dof_traj(t):
            positions = [traj(t) for traj in trajectories]
            velocities = [traj.derivative()(t) for traj in trajectories]
            accelerations = [traj.derivative().derivative()(t) for traj in trajectories]

            return np.array(positions), np.array(velocities), np.array(accelerations)

        return multi_dof_traj


# Example usage and visualization
def main():
    traj_gen = TrajectoryGenerator()

    # Example 1: Cubic polynomial trajectory
    print("=== Cubic Polynomial Trajectory ===")
    cubic_traj = traj_gen.cubic_polynomial_trajectory(
        q_start=0, q_end=2, t_start=0, t_end=4, qd_start=0, qd_end=0
    )

    t = np.linspace(0, 4, 100)
    q_cubic, qd_cubic, qdd_cubic = [], [], []
    for ti in t:
        qi, qdi, qddi = cubic_traj(ti)
        q_cubic.append(qi)
        qd_cubic.append(qdi)
        qdd_cubic.append(qddi)

    q_cubic = np.array(q_cubic)
    qd_cubic = np.array(qd_cubic)
    qdd_cubic = np.array(qdd_cubic)

    print(f"Cubic trajectory - Start: pos={q_cubic[0]:.3f}, vel={qd_cubic[0]:.3f}")
    print(f"Cubic trajectory - End: pos={q_cubic[-1]:.3f}, vel={qd_cubic[-1]:.3f}")

    # Example 2: Quintic polynomial trajectory
    print("\n=== Quintic Polynomial Trajectory ===")
    quintic_traj = traj_gen.quintic_polynomial_trajectory(
        q_start=0, q_end=2, t_start=0, t_end=4, qd_start=0, qd_end=0, qdd_start=0, qdd_end=0
    )

    q_quintic, qd_quintic, qdd_quintic = [], [], []
    for ti in t:
        qi, qdi, qddi = quintic_traj(ti)
        q_quintic.append(qi)
        qd_quintic.append(qdi)
        qdd_quintic.append(qddi)

    q_quintic = np.array(q_quintic)
    qd_quintic = np.array(qd_quintic)
    qdd_quintic = np.array(qdd_quintic)

    print(f"Quintic trajectory - Start: pos={q_quintic[0]:.3f}, vel={qd_quintic[0]:.3f}, acc={qdd_quintic[0]:.3f}")
    print(f"Quintic trajectory - End: pos={q_quintic[-1]:.3f}, vel={qd_quintic[-1]:.3f}, acc={qdd_quintic[-1]:.3f}")

    # Example 3: Trapezoidal trajectory
    print("\n=== Trapezoidal Trajectory ===")
    trap_traj, total_time = traj_gen.trapezoidal_trajectory(
        q_start=0, q_end=5, max_vel=2, max_acc=1
    )

    t_trap = np.linspace(0, total_time, 100)
    q_trap, qd_trap, qdd_trap = [], [], []
    for ti in t_trap:
        qi, qdi, qddi = trap_traj(ti)
        q_trap.append(qi)
        qd_trap.append(qdi)
        qdd_trap.append(qddi)

    q_trap = np.array(q_trap)
    qd_trap = np.array(qd_trap)
    qdd_trap = np.array(qdd_trap)

    print(f"Trapezoidal trajectory - Total time: {total_time:.3f}s")
    print(f"Trapezoidal trajectory - Start: pos={q_trap[0]:.3f}, vel={qd_trap[0]:.3f}")
    print(f"Trapezoidal trajectory - End: pos={q_trap[-1]:.3f}, vel={qd_trap[-1]:.3f}")

    # Visualization
    plt.figure(figsize=(15, 10))

    # Position subplot
    plt.subplot(3, 1, 1)
    plt.plot(t, q_cubic, label='Cubic', linewidth=2)
    plt.plot(t, q_quintic, label='Quintic', linewidth=2)
    plt.plot(t_trap, q_trap, label='Trapezoidal', linewidth=2)
    plt.title('Position vs Time')
    plt.xlabel('Time (s)')
    plt.ylabel('Position')
    plt.legend()
    plt.grid(True)

    # Velocity subplot
    plt.subplot(3, 1, 2)
    plt.plot(t, qd_cubic, label='Cubic', linewidth=2)
    plt.plot(t, qd_quintic, label='Quintic', linewidth=2)
    plt.plot(t_trap, qd_trap, label='Trapezoidal', linewidth=2)
    plt.title('Velocity vs Time')
    plt.xlabel('Time (s)')
    plt.ylabel('Velocity')
    plt.legend()
    plt.grid(True)

    # Acceleration subplot
    plt.subplot(3, 1, 3)
    plt.plot(t, qdd_cubic, label='Cubic', linewidth=2)
    plt.plot(t, qdd_quintic, label='Quintic', linewidth=2)
    plt.plot(t_trap, qdd_trap, label='Trapezoidal', linewidth=2)
    plt.title('Acceleration vs Time')
    plt.xlabel('Time (s)')
    plt.ylabel('Acceleration')
    plt.legend()
    plt.grid(True)

    plt.tight_layout()
    plt.show()


if __name__ == '__main__':
    main()
```

## ROS 2 Integration for Kinematics and Dynamics

### Joint Trajectory Controller

```python
import rclpy
from rclpy.node import Node
from trajectory_msgs.msg import JointTrajectory, JointTrajectoryPoint
from sensor_msgs.msg import JointState
from control_msgs.action import FollowJointTrajectory
from rclpy.action import ActionServer
from builtin_interfaces.msg import Duration
import numpy as np


class JointTrajectoryController(Node):
    def __init__(self):
        super().__init__('joint_trajectory_controller')

        # Subscribe to joint states
        self.joint_state_sub = self.create_subscription(
            JointState,
            '/joint_states',
            self.joint_state_callback,
            10
        )

        # Publish desired joint positions
        self.joint_cmd_pub = self.create_publisher(
            JointState,
            '/joint_commands',
            10
        )

        # Create action server for trajectory execution
        self._action_server = ActionServer(
            self,
            FollowJointTrajectory,
            'follow_joint_trajectory',
            self.execute_trajectory
        )

        # Store current joint states
        self.current_joint_positions = {}
        self.joint_names = ['joint1', 'joint2', 'joint3', 'joint4', 'joint5', 'joint6']  # Example joint names

        # Trajectory execution parameters
        self.control_rate = 100  # Hz
        self.timer = self.create_timer(1.0/self.control_rate, self.control_loop)

        self.get_logger().info('Joint trajectory controller initialized')

    def joint_state_callback(self, msg):
        """Update current joint positions"""
        for name, position in zip(msg.name, msg.position):
            self.current_joint_positions[name] = position

    def execute_trajectory(self, goal_handle):
        """Execute a joint trajectory goal"""
        self.get_logger().info('Executing trajectory...')

        trajectory = goal_handle.request.trajectory
        points = trajectory.points
        joint_names = trajectory.joint_names

        # Execute trajectory point by point
        for i, point in enumerate(points):
            # Calculate time to reach this point
            if i > 0:
                dt = point.time_from_start.sec + point.time_from_start.nanosec * 1e-9
                prev_time = points[i-1].time_from_start.sec + points[i-1].time_from_start.nanosec * 1e-9
                sleep_time = dt - prev_time
            else:
                sleep_time = point.time_from_start.sec + point.time_from_start.nanosec * 1e-9

            # Create joint state message
            joint_state = JointState()
            joint_state.name = joint_names
            joint_state.position = point.positions
            joint_state.velocity = point.velocities if point.velocities else [0.0] * len(point.positions)
            joint_state.effort = point.effort if point.effort else [0.0] * len(point.positions)

            # Publish the command
            self.joint_cmd_pub.publish(joint_state)

            # Wait for the specified time (in a real system, you'd use a more sophisticated approach)
            self.get_logger().info(f'Published trajectory point {i+1}/{len(points)}')

            # Check for preemption
            if goal_handle.is_cancel_requested:
                goal_handle.canceled()
                self.get_logger().info('Trajectory execution canceled')
                return FollowJointTrajectory.Result()

        # Complete successfully
        goal_handle.succeed()
        result = FollowJointTrajectory.Result()
        result.error_code = FollowJointTrajectory.Result.SUCCESSFUL
        self.get_logger().info('Trajectory execution completed successfully')
        return result

    def control_loop(self):
        """Main control loop for smooth trajectory following"""
        # In a real implementation, this would interpolate between trajectory points
        # and publish commands at the control rate
        pass


def main(args=None):
    rclpy.init(args=args)
    controller = JointTrajectoryController()

    try:
        rclpy.spin(controller)
    except KeyboardInterrupt:
        pass
    finally:
        controller.destroy_node()
        rclpy.shutdown()


if __name__ == '__main__':
    main()
```

## Hands-On Lab: Kinematic Control System

### Objective
Create a complete kinematic control system that solves inverse kinematics and generates smooth trajectories for a robotic arm.

### Prerequisites
- Completed Chapter 1-7
- ROS 2 Humble with Gazebo installed
- Basic understanding of robot kinematics

### Steps

1. **Create a kinematic control package**:
   ```bash
   cd ~/robotics_ws/src
   ros2 pkg create --build-type ament_python kinematic_control_lab --dependencies rclpy sensor_msgs trajectory_msgs control_msgs geometry_msgs numpy scipy matplotlib
   ```

2. **Create the main kinematic control node** (`kinematic_control_lab/kinematic_control_lab/kinematic_control_node.py`):
   ```python
   #!/usr/bin/env python3
   import rclpy
   from rclpy.node import Node
   from sensor_msgs.msg import JointState
   from geometry_msgs.msg import Pose, Point, Quaternion
   from trajectory_msgs.msg import JointTrajectory, JointTrajectoryPoint
   from std_msgs.msg import Header
   from builtin_interfaces.msg import Duration
   import numpy as np
   from scipy.spatial.transform import Rotation as R
   import time


   class DHParameter:
       def __init__(self, a, alpha, d, theta):
           self.a = a
           self.alpha = alpha
           self.d = d
           self.theta = theta

       def get_transformation_matrix(self):
           sa = np.sin(self.alpha)
           ca = np.cos(self.alpha)
           st = np.sin(self.theta)
           ct = np.cos(self.theta)

           T = np.array([
               [ct, -st*ca, st*sa, self.a*ct],
               [st, ct*ca, -ct*sa, self.a*st],
               [0, sa, ca, self.d],
               [0, 0, 0, 1]
           ])
           return T


   class KinematicControlNode(Node):
       def __init__(self):
           super().__init__('kinematic_control_node')

           # Publisher for joint commands
           self.joint_cmd_pub = self.create_publisher(JointState, '/joint_commands', 10)

           # Publisher for trajectory commands
           self.trajectory_pub = self.create_publisher(JointTrajectory, '/joint_trajectory', 10)

           # Subscriber for target poses
           self.target_pose_sub = self.create_subscription(
               Pose,
               '/target_pose',
               self.target_pose_callback,
               10
           )

           # Initialize robot kinematics (6-DOF manipulator example)
           self.dh_params = [
               DHParameter(a=0, alpha=np.pi/2, d=0.5, theta=0),    # Joint 1
               DHParameter(a=0.5, alpha=0, d=0, theta=0),          # Joint 2
               DHParameter(a=0.4, alpha=0, d=0, theta=0),          # Joint 3
               DHParameter(a=0, alpha=np.pi/2, d=0.3, theta=0),    # Joint 4
               DHParameter(a=0, alpha=-np.pi/2, d=0, theta=0),     # Joint 5
               DHParameter(a=0, alpha=0, d=0.2, theta=0)           # Joint 6
           ]

           # Joint limits (in radians)
           self.joint_limits = [
               (-np.pi, np.pi),      # Joint 1
               (-np.pi/2, np.pi/2),  # Joint 2
               (-np.pi/2, np.pi/2),  # Joint 3
               (-np.pi, np.pi),      # Joint 4
               (-np.pi/2, np.pi/2),  # Joint 5
               (-np.pi, np.pi)       # Joint 6
           ]

           # Current joint positions
           self.current_joints = [0.0] * 6

           # Trajectory generation
           self.traj_gen = TrajectoryGenerator()

           self.get_logger().info('Kinematic control node initialized')

       def forward_kinematics(self, joint_angles):
           """Calculate forward kinematics"""
           if len(joint_angles) != len(self.dh_params):
               raise ValueError("Number of joint angles must match number of joints")

           # Update theta values in DH parameters
           dh_params = []
           for i, (param, angle) in enumerate(zip(self.dh_params, joint_angles)):
               new_param = DHParameter(param.a, param.alpha, param.d, angle + param.theta)
               dh_params.append(new_param)

           # Calculate cumulative transformation
           T_total = np.eye(4)
           for param in dh_params:
               T = param.get_transformation_matrix()
               T_total = T_total @ T

           # Extract position and orientation
           position = T_total[0:3, 3]
           orientation_matrix = T_total[0:3, 0:3]
           rotation = R.from_matrix(orientation_matrix)
           quaternion = rotation.as_quat()

           return position, quaternion

       def inverse_kinematics(self, target_position, target_orientation, initial_guess=None):
           """Solve inverse kinematics using numerical method"""
           if initial_guess is None:
               initial_guess = [0.0] * 6

           def objective_function(joint_angles):
               pos, quat = self.forward_kinematics(joint_angles)
               pos_error = np.linalg.norm(pos - target_position)
               # Simplified orientation error
               orient_error = 1 - np.abs(np.dot(quat, target_orientation))
               return pos_error + 0.1 * orient_error

           # Use scipy.optimize to solve IK
           from scipy.optimize import minimize

           # Apply joint limits
           bounds = self.joint_limits

           result = minimize(
               objective_function,
               initial_guess,
               method='L-BFGS-B',
               bounds=bounds
           )

           if result.success:
               return result.x
           else:
               self.get_logger().error(f'IK solution failed: {result.message}')
               return None

       def target_pose_callback(self, msg):
           """Handle target pose requests"""
           # Extract target position and orientation from message
           target_pos = np.array([msg.position.x, msg.position.y, msg.position.z])

           # Convert quaternion to array
           target_orient = np.array([
               msg.orientation.x,
               msg.orientation.y,
               msg.orientation.z,
               msg.orientation.w
           ])

           # Solve inverse kinematics
           joint_solution = self.inverse_kinematics(target_pos, target_orient)

           if joint_solution is not None:
               self.get_logger().info(f'IK Solution: {np.degrees(joint_solution)} degrees')

               # Generate smooth trajectory to the target
               self.execute_smooth_trajectory(joint_solution)
           else:
               self.get_logger().error('No IK solution found')

       def execute_smooth_trajectory(self, target_joints):
           """Execute smooth trajectory to target joint positions"""
           # Get current joint positions
           current_joints = self.current_joints

           # Create trajectory points
           n_points = 50  # Number of intermediate points
           trajectory = JointTrajectory()
           trajectory.joint_names = [f'joint_{i+1}' for i in range(6)]

           # Generate intermediate points
           for i in range(n_points + 1):
               fraction = i / n_points
               intermediate_joints = (1 - fraction) * np.array(current_joints) + fraction * np.array(target_joints)

               point = JointTrajectoryPoint()
               point.positions = intermediate_joints.tolist()

               # Calculate velocities (simple linear interpolation)
               if i > 0:
                   dt = 0.1  # Time step
                   prev_joints = (1 - (i-1)/n_points) * np.array(current_joints) + ((i-1)/n_points) * np.array(target_joints)
                   velocities = (intermediate_joints - prev_joints) / dt
                   point.velocities = velocities.tolist()
               else:
                   point.velocities = [0.0] * 6

               # Set time from start
               point.time_from_start = Duration(sec=0, nanosec=int(i * 100000000))  # 0.1s per point

               trajectory.points.append(point)

           # Update current joints
           self.current_joints = target_joints.tolist()

           # Publish trajectory
           trajectory.header.stamp = self.get_clock().now().to_msg()
           trajectory.header.frame_id = 'base_link'
           self.trajectory_pub.publish(trajectory)

           self.get_logger().info(f'Published trajectory with {len(trajectory.points)} points')

       def move_to_joint_positions(self, joint_positions):
           """Move robot to specific joint positions"""
           if len(joint_positions) != 6:
               self.get_logger().error('Need exactly 6 joint positions')
               return

           # Create trajectory message
           trajectory = JointTrajectory()
           trajectory.joint_names = [f'joint_{i+1}' for i in range(6)]

           # Single point trajectory
           point = JointTrajectoryPoint()
           point.positions = joint_positions
           point.velocities = [0.0] * 6
           point.time_from_start = Duration(sec=2, nanosec=0)  # 2 seconds to reach

           trajectory.points.append(point)
           trajectory.header.stamp = self.get_clock().now().to_msg()
           trajectory.header.frame_id = 'base_link'

           self.trajectory_pub.publish(trajectory)
           self.current_joints = joint_positions


   class TrajectoryGenerator:
       def __init__(self):
           pass

       def cubic_polynomial_trajectory(self, q_start, q_end, t_start, t_end, qd_start=0, qd_end=0):
           """Generate cubic polynomial trajectory for single joint"""
           dt = t_end - t_start

           a0 = q_start
           a1 = qd_start
           a2 = (3/dt**2) * (q_end - q_start) - (2/dt) * qd_start - (1/dt) * qd_end
           a3 = (-2/dt**3) * (q_end - q_start) + (1/dt**2) * (qd_start + qd_end)

           def trajectory(t):
               t_rel = t - t_start
               q = a0 + a1*t_rel + a2*t_rel**2 + a3*t_rel**3
               qd = a1 + 2*a2*t_rel + 3*a3*t_rel**2
               return q, qd

           return trajectory


   def main(args=None):
       rclpy.init(args=args)
       kinematic_control_node = KinematicControlNode()

       # Example: Move to a specific joint configuration after startup
       def startup_timer_callback():
           kinematic_control_node.get_logger().info('Moving to initial position...')
           initial_pos = [0.1, 0.2, 0.0, 0.0, 0.1, 0.0]
           kinematic_control_node.move_to_joint_positions(initial_pos)
           startup_timer.cancel()

       startup_timer = kinematic_control_node.create_timer(2.0, startup_timer_callback)

       try:
           rclpy.spin(kinematic_control_node)
       except KeyboardInterrupt:
           pass
       finally:
           kinematic_control_node.destroy_node()
           rclpy.shutdown()


   if __name__ == '__main__':
       main()
   ```

3. **Create a launch file** (`kinematic_control_lab/launch/kinematic_control.launch.py`):
   ```python
   from launch import LaunchDescription
   from launch.actions import DeclareLaunchArgument
   from launch.substitutions import LaunchConfiguration
   from launch_ros.actions import Node


   def generate_launch_description():
       # Declare launch arguments
       use_sim_time = DeclareLaunchArgument(
           'use_sim_time',
           default_value='false',
           description='Use simulation (Gazebo) clock if true'
       )

       # Kinematic control node
       kinematic_control_node = Node(
           package='kinematic_control_lab',
           executable='kinematic_control_node',
           name='kinematic_control_node',
           parameters=[{'use_sim_time': LaunchConfiguration('use_sim_time')}],
           output='screen'
       )

       return LaunchDescription([
           use_sim_time,
           kinematic_control_node
       ])
   ```

4. **Update setup.py**:
   ```python
   import os
   from glob import glob
   from setuptools import setup
   from setuptools import find_packages

   package_name = 'kinematic_control_lab'

   setup(
       name=package_name,
       version='0.0.0',
       packages=find_packages(exclude=['test']),
       data_files=[
           ('share/ament_index/resource_index/packages',
               ['resource/' + package_name]),
           ('share/' + package_name, ['package.xml']),
           (os.path.join('share', package_name, 'launch'), glob('launch/*.launch.py')),
       ],
       install_requires=['setuptools'],
       zip_safe=True,
       maintainer='Your Name',
       maintainer_email='your.email@example.com',
       description='Kinematic control lab for robotics',
       license='Apache License 2.0',
       tests_require=['pytest'],
       entry_points={
           'console_scripts': [
               'kinematic_control_node = kinematic_control_lab.kinematic_control_node:main',
           ],
       },
   )
   ```

5. **Build the package**:
   ```bash
   cd ~/robotics_ws
   colcon build --packages-select kinematic_control_lab
   source install/setup.bash
   ```

6. **Run the kinematic control system**:
   ```bash
   ros2 launch kinematic_control_lab kinematic_control.launch.py
   ```

### Expected Results
- The system should solve inverse kinematics for target end-effector poses
- Smooth trajectories should be generated and published
- Joint positions should be updated to move the robot to target configurations
- The system should handle joint limits appropriately

### Troubleshooting Tips
- Verify joint names match your robot's configuration
- Check that DH parameters match your robot's physical structure
- Ensure joint limits are appropriate for your robot
- Monitor the logs for IK solution status and trajectory execution

## Summary

In this chapter, we've covered the fundamental concepts of robot kinematics and dynamics, including forward and inverse kinematics, dynamics modeling, and trajectory generation. We've implemented practical examples of each concept and created a complete kinematic control system.

The hands-on lab provided experience with creating a system that solves inverse kinematics and generates smooth trajectories for robot motion. This foundation is essential for more advanced robotic capabilities like motion planning, control, and interaction with the environment, which we'll explore in the upcoming chapters.

============================================================
FILE: book\docs\part-iii-motion\chapter-8-locomotion\index.md
============================================================
---
title: Chapter 8 - Locomotion and Balance Control
sidebar_position: 2
---

# Chapter 8: Locomotion and Balance Control

## Learning Goals

- Understand bipedal locomotion principles
- Learn balance control and stabilization
- Master walking pattern generation
- Implement balance control algorithms
- Generate walking patterns for humanoid robots
- Simulate stable locomotion in various terrains

## Introduction to Bipedal Locomotion

Bipedal locomotion is one of the most challenging problems in robotics, requiring sophisticated control algorithms to maintain balance while moving on two legs. Unlike wheeled or tracked robots, bipedal robots must manage their center of mass, coordinate multiple joints, and adapt to changing terrain conditions.

### Challenges of Bipedal Locomotion

1. **Dynamic Balance**: Maintaining stability during motion
2. **Underactuation**: Fewer actuators than degrees of freedom in some phases
3. **Impact Dynamics**: Managing foot-ground collisions
4. **Terrain Adaptation**: Adjusting to uneven surfaces
5. **Energy Efficiency**: Minimizing power consumption during walking

### Locomotion Patterns

Bipedal robots can use various walking patterns:

- **Static Walking**: Center of mass always over support polygon
- **Dynamic Walking**: Center of mass may move outside support polygon
- **Passive Dynamic Walking**: Using gravity and momentum for efficiency
- **ZMP-Based Walking**: Zero Moment Point control for stability

## Balance Control Fundamentals

### Center of Mass and Stability

The center of mass (CoM) is crucial for balance control. A humanoid robot is stable when its CoM projection falls within the support polygon defined by its feet.

```python
import numpy as np
import matplotlib.pyplot as plt


class BalanceController:
    def __init__(self, robot_mass=75.0, com_height=0.85):
        """
        Initialize balance controller
        robot_mass: Total mass of the robot in kg
        com_height: Height of center of mass in meters
        """
        self.robot_mass = robot_mass
        self.com_height = com_height
        self.gravity = 9.81
        self.com_position = np.array([0.0, 0.0, com_height])  # x, y, z
        self.com_velocity = np.array([0.0, 0.0, 0.0])
        self.com_acceleration = np.array([0.0, 0.0, 0.0])

    def calculate_support_polygon(self, left_foot_pos, right_foot_pos, foot_width=0.15):
        """
        Calculate support polygon from foot positions
        left_foot_pos, right_foot_pos: 3D positions of feet [x, y, z]
        foot_width: Width of foot in meters
        """
        # For simplicity, assume rectangular feet
        # In practice, this would be more complex based on contact points
        support_points = [
            [left_foot_pos[0] - foot_width/2, left_foot_pos[1] - foot_width/2],
            [left_foot_pos[0] + foot_width/2, left_foot_pos[1] - foot_width/2],
            [right_foot_pos[0] + foot_width/2, right_foot_pos[1] + foot_width/2],
            [right_foot_pos[0] - foot_width/2, right_foot_pos[1] + foot_width/2]
        ]

        return np.array(support_points)

    def is_stable(self, com_proj, support_polygon):
        """
        Check if CoM projection is within support polygon
        com_proj: 2D projection of CoM [x, y]
        support_polygon: 2D vertices of support polygon
        """
        # Use ray casting algorithm to check if point is in polygon
        x, y = com_proj
        n = len(support_polygon)
        inside = False

        p1x, p1y = support_polygon[0]
        for i in range(1, n + 1):
            p2x, p2y = support_polygon[i % n]
            if y > min(p1y, p2y):
                if y <= max(p1y, p2y):
                    if x <= max(p1x, p2x):
                        if p1y != p2y:
                            xinters = (y - p1y) * (p2x - p1x) / (p2y - p1y) + p1x
                        if p1x == p2x or x <= xinters:
                            inside = not inside
            p1x, p1y = p2x, p2y

        return inside

    def calculate_zmp(self, com_pos, com_vel, com_acc):
        """
        Calculate Zero Moment Point (ZMP)
        com_pos: Center of mass position [x, y, z]
        com_vel: Center of mass velocity [x, y, z]
        com_acc: Center of mass acceleration [x, y, z]
        """
        x, y, z = com_pos
        x_dot, y_dot, z_dot = com_vel
        x_ddot, y_ddot, z_ddot = com_acc

        # ZMP calculation (simplified, assuming constant CoM height)
        zmp_x = x - (z - self.com_height) * x_ddot / self.gravity
        zmp_y = y - (z - self.com_height) * y_ddot / self.gravity

        return np.array([zmp_x, zmp_y])


# Example usage
def main():
    controller = BalanceController()

    # Example foot positions
    left_foot = np.array([0.1, 0.1, 0.0])
    right_foot = np.array([-0.1, -0.1, 0.0])

    # Calculate support polygon
    support_poly = controller.calculate_support_polygon(left_foot, right_foot)

    # Example CoM position
    com_pos = np.array([0.0, 0.0, 0.85])
    com_vel = np.array([0.01, -0.02, 0.0])
    com_acc = np.array([0.05, -0.03, 0.0])

    # Calculate ZMP
    zmp = controller.calculate_zmp(com_pos, com_vel, com_acc)

    # Check stability
    com_proj = com_pos[:2]  # Project CoM to 2D
    stable = controller.is_stable(com_proj, support_poly)

    print(f"Support polygon: {support_poly}")
    print(f"CoM projection: {com_proj}")
    print(f"ZMP: {zmp}")
    print(f"Stable: {stable}")

    # Visualization
    plt.figure(figsize=(10, 8))

    # Plot support polygon
    support_poly_closed = np.vstack([support_poly, support_poly[0]])  # Close the polygon
    plt.plot(support_poly_closed[:, 0], support_poly_closed[:, 1], 'b-', linewidth=2, label='Support Polygon')
    plt.fill(support_poly[:, 0], support_poly[:, 1], alpha=0.3, color='blue')

    # Plot CoM and ZMP
    plt.plot(com_proj[0], com_proj[1], 'ro', markersize=10, label='CoM Projection')
    plt.plot(zmp[0], zmp[1], 'gs', markersize=10, label='ZMP')

    plt.title('Balance Control: Support Polygon, CoM, and ZMP')
    plt.xlabel('X (m)')
    plt.ylabel('Y (m)')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.axis('equal')
    plt.show()


if __name__ == '__main__':
    main()
```

## Inverted Pendulum Model

The inverted pendulum is a fundamental model for understanding balance control:

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.integrate import solve_ivp


class InvertedPendulum:
    def __init__(self, mass=1.0, length=1.0, gravity=9.81):
        """
        Initialize inverted pendulum model
        mass: Mass of the pendulum bob
        length: Length of the pendulum
        gravity: Gravitational acceleration
        """
        self.mass = mass
        self.length = length
        self.gravity = gravity
        self.moment_of_inertia = mass * length**2

    def dynamics(self, t, state, control_input=0):
        """
        Equations of motion for inverted pendulum
        state: [theta, theta_dot] where theta is angle from vertical
        control_input: Torque applied at the pivot
        """
        theta, theta_dot = state

        # Nonlinear dynamics of inverted pendulum
        # theta_ddot = (g*sin(theta) - control_input/(m*l^2)) / l
        theta_ddot = (self.gravity * np.sin(theta) - control_input / (self.mass * self.length**2)) / self.length

        return [theta_dot, theta_ddot]

    def linearize(self):
        """
        Linearize the system around the upright position (theta = 0)
        Returns A and B matrices for linear system: x_dot = Ax + Bu
        """
        # For small angles, sin(theta) ‚âà theta
        # State: x = [theta, theta_dot]
        # x_dot = [theta_dot, (g/l)*theta - 1/(m*l^2)*u]
        A = np.array([
            [0, 1],
            [self.gravity/self.length, 0]
        ])
        B = np.array([
            [0],
            [-1/(self.mass * self.length**2)]
        ])

        return A, B

    def simulate(self, initial_state, control_func, t_span, t_eval):
        """
        Simulate the inverted pendulum
        initial_state: [theta, theta_dot] at t=0
        control_func: Function that returns control input given (t, state)
        """
        def dynamics_with_control(t, state):
            control_input = control_func(t, state)
            return self.dynamics(t, state, control_input)

        solution = solve_ivp(
            dynamics_with_control,
            t_span,
            initial_state,
            t_eval=t_eval,
            method='RK45'
        )

        return solution


class BalanceControllerInvertedPendulum:
    def __init__(self, pendulum):
        """
        Initialize balance controller based on inverted pendulum model
        """
        self.pendulum = pendulum
        self.A, self.B = pendulum.linearize()

        # Design controller using pole placement
        # Choose desired closed-loop poles
        desired_poles = [-2, -3]  # Faster response
        self.K = self.compute_feedback_gain()

    def compute_feedback_gain(self):
        """
        Compute state feedback gain using pole placement
        For 2-state system: control = -K * state
        """
        # For this simple example, we'll use a simple approach
        # In practice, you'd use more sophisticated methods like LQR or pole placement
        # Using Ackermann's formula or scipy's place function

        # Simple PD controller approach
        # For system x_dot = Ax + Bu, u = -Kx
        # We want eigenvalues of (A-BK) to be at desired locations
        # For now, return a simple gain matrix
        K = np.array([10.0, 2.0])  # PD controller gains
        return K

    def control_law(self, state):
        """
        Compute control input based on current state
        state: [theta, theta_dot]
        """
        control = -self.K @ state
        return control[0] if hasattr(control, '__len__') else control


# Example usage
def main():
    # Create inverted pendulum model
    pendulum = InvertedPendulum(mass=10.0, length=0.85)  # Approximate leg length

    # Create balance controller
    controller = BalanceControllerInvertedPendulum(pendulum)

    # Simulate with balance control
    initial_state = [0.1, 0.0]  # Small initial angle
    t_span = (0, 5)
    t_eval = np.linspace(0, 5, 500)

    def control_func(t, state):
        return controller.control_law(state)

    solution = pendulum.simulate(initial_state, control_func, t_span, t_eval)

    # Plot results
    plt.figure(figsize=(12, 8))

    plt.subplot(2, 1, 1)
    plt.plot(solution.t, solution.y[0], 'b-', linewidth=2, label='Angle (rad)')
    plt.title('Inverted Pendulum with Balance Control')
    plt.ylabel('Angle (rad)')
    plt.grid(True)
    plt.legend()

    plt.subplot(2, 1, 2)
    plt.plot(solution.t, solution.y[1], 'r-', linewidth=2, label='Angular Velocity (rad/s)')
    plt.xlabel('Time (s)')
    plt.ylabel('Angular Velocity (rad/s)')
    plt.grid(True)
    plt.legend()

    plt.tight_layout()
    plt.show()

    print(f"Final angle: {solution.y[0][-1]:.4f} rad")
    print(f"Final angular velocity: {solution.y[1][-1]:.4f} rad/s")


if __name__ == '__main__':
    main()
```

## Walking Pattern Generation

### ZMP-Based Walking Controller

Zero Moment Point (ZMP) based walking is a widely used approach for humanoid locomotion:

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy import signal
from scipy.integrate import cumtrapz


class ZMPPatternGenerator:
    def __init__(self, sampling_time=0.01, com_height=0.85, gravity=9.81):
        """
        Initialize ZMP-based walking pattern generator
        sampling_time: Time step for pattern generation
        com_height: Center of mass height
        gravity: Gravitational acceleration
        """
        self.dt = sampling_time
        self.com_height = com_height
        self.gravity = gravity
        self.omega = np.sqrt(self.gravity / self.com_height)  # Natural frequency

    def generate_com_trajectory_from_zmp(self, zmp_trajectory):
        """
        Generate CoM trajectory from ZMP reference using the linear inverted pendulum model
        x_com_ddot - omega^2 * x_com = -omega^2 * x_zmp
        """
        t = np.arange(0, len(zmp_trajectory)) * self.dt

        # For the linear inverted pendulum model:
        # x_com_ddot - omega^2 * x_com = -omega^2 * x_zmp
        # This is a second-order linear ODE that can be solved analytically

        x_zmp = zmp_trajectory[:, 0]  # X component of ZMP
        y_zmp = zmp_trajectory[:, 1]  # Y component of ZMP

        # Solve for CoM trajectory (simplified approach)
        # The solution to x_ddot - omega^2*x = -omega^2*zmp is:
        # x_com(t) = x_h(t) + x_p(t)
        # where x_h is homogeneous solution and x_p is particular solution

        # For demonstration, use a simplified approach with pre-computed trajectory
        # In practice, you'd solve the full differential equation

        # Generate CoM trajectory using preview control approach
        com_x = self._solve_com_trajectory(x_zmp)
        com_y = self._solve_com_trajectory(y_zmp)

        # Calculate velocities and accelerations by differentiation
        com_x_vel = np.gradient(com_x, self.dt)
        com_y_vel = np.gradient(com_y, self.dt)
        com_x_acc = np.gradient(com_x_vel, self.dt)
        com_y_acc = np.gradient(com_y_vel, self.dt)

        # Create full trajectory with all states
        com_trajectory = np.column_stack([com_x, com_y, np.full_like(com_x, self.com_height)])
        com_velocity = np.column_stack([com_x_vel, com_y_vel, np.zeros_like(com_x_vel)])
        com_acceleration = np.column_stack([com_x_acc, com_y_acc, np.zeros_like(com_x_acc)])

        return com_trajectory, com_velocity, com_acceleration

    def _solve_com_trajectory(self, zmp_ref):
        """Helper function to solve CoM trajectory for one dimension"""
        # This is a simplified implementation
        # In practice, you'd use preview control with a longer preview window

        # For now, return a smoothed version of the ZMP reference
        # Apply a low-pass filter to make CoM trajectory smooth
        b, a = signal.butter(2, 0.1, 'low', fs=1/self.dt)
        com_pos = signal.filtfilt(b, a, zmp_ref)
        return com_pos

    def generate_footprint_pattern(self, step_length=0.3, step_width=0.2, n_steps=10):
        """
        Generate a simple walking footprint pattern
        step_length: Forward step length
        step_width: Lateral distance between feet
        n_steps: Number of steps to generate
        """
        footsteps = []

        # Start with left foot at origin
        left_pos = np.array([0.0, step_width/2, 0.0])
        right_pos = np.array([0.0, -step_width/2, 0.0])

        for i in range(n_steps):
            # Odd steps: move right foot
            if i % 2 == 1:
                right_pos[0] += step_length
                right_pos[1] = (-1)**i * step_width/2
                footsteps.append(('right', right_pos.copy()))
            # Even steps: move left foot
            else:
                left_pos[0] += step_length
                left_pos[1] = (-1)**(i+1) * step_width/2
                footsteps.append(('left', left_pos.copy()))

        return footsteps

    def generate_zmp_trajectory(self, footsteps, double_support_time=0.1, dt=0.01):
        """
        Generate ZMP trajectory based on footsteps
        footsteps: List of (foot_type, position) tuples
        double_support_time: Time spent in double support phase
        """
        # Calculate total time
        single_support_time = 1.0  # Time for single support phase
        total_time = len(footsteps) * (single_support_time + double_support_time)

        # Create time vector
        t = np.arange(0, total_time, dt)

        # Initialize ZMP trajectory
        zmp_trajectory = np.zeros((len(t), 2))  # x, y components

        # Generate ZMP pattern for each step
        for i, (foot_type, foot_pos) in enumerate(footsteps):
            step_start_time = i * (single_support_time + double_support_time)
            double_support_end = step_start_time + double_support_time
            step_end_time = step_start_time + single_support_time + double_support_time

            # Find time indices for this step
            start_idx = int(step_start_time / dt)
            double_end_idx = int(double_support_end / dt)
            end_idx = int(step_end_time / dt)

            if end_idx >= len(t):
                end_idx = len(t)

            # Double support phase: ZMP transitions between feet
            if double_end_idx > start_idx:
                for j in range(start_idx, min(double_end_idx, len(t))):
                    # Interpolate between previous foot position and current foot position
                    if i == 0:
                        # For first step, start from middle position
                        prev_pos = np.array([0.0, 0.0])
                    else:
                        prev_type, prev_pos = footsteps[i-1]
                        prev_pos = prev_pos[:2]

                    t_interp = (t[j] - step_start_time) / double_support_time
                    zmp_trajectory[j] = (1 - t_interp) * prev_pos + t_interp * foot_pos[:2]

            # Single support phase: ZMP stays at current foot
            if end_idx > double_end_idx:
                zmp_trajectory[double_end_idx:end_idx, 0] = foot_pos[0]
                zmp_trajectory[double_end_idx:end_idx, 1] = foot_pos[1]

        return t, zmp_trajectory


# Example usage
def main():
    # Create ZMP pattern generator
    zmp_gen = ZMPPatternGenerator()

    # Generate footsteps
    footsteps = zmp_gen.generate_footprint_pattern(step_length=0.3, step_width=0.2, n_steps=8)
    print(f"Generated {len(footsteps)} footsteps")

    # Display footsteps
    for i, (foot_type, pos) in enumerate(footsteps):
        print(f"Step {i+1}: {foot_type} foot at ({pos[0]:.2f}, {pos[1]:.2f}, {pos[2]:.2f})")

    # Generate ZMP trajectory
    t, zmp_trajectory = zmp_gen.generate_zmp_trajectory(footsteps, double_support_time=0.2)

    # Generate CoM trajectory from ZMP
    com_trajectory, com_velocity, com_acceleration = zmp_gen.generate_com_trajectory_from_zmp(zmp_trajectory)

    # Visualization
    plt.figure(figsize=(15, 10))

    # Plot ZMP and CoM trajectories
    plt.subplot(2, 2, 1)
    plt.plot(zmp_trajectory[:, 0], zmp_trajectory[:, 1], 'r-', linewidth=2, label='ZMP')
    plt.plot(com_trajectory[:, 0], com_trajectory[:, 1], 'b-', linewidth=2, label='CoM')

    # Mark footsteps
    left_x, left_y = [], []
    right_x, right_y = [], []
    for foot_type, pos in footsteps:
        if foot_type == 'left':
            left_x.append(pos[0])
            left_y.append(pos[1])
        else:
            right_x.append(pos[0])
            right_y.append(pos[1])

    plt.scatter(left_x, left_y, c='g', s=100, marker='^', label='Left Foot')
    plt.scatter(right_x, right_y, c='m', s=100, marker='v', label='Right Foot')

    plt.title('ZMP and CoM Trajectories')
    plt.xlabel('X (m)')
    plt.ylabel('Y (m)')
    plt.legend()
    plt.grid(True)
    plt.axis('equal')

    # Plot X trajectories over time
    plt.subplot(2, 2, 2)
    plt.plot(t, zmp_trajectory[:, 0], 'r-', linewidth=2, label='ZMP X')
    plt.plot(t, com_trajectory[:, 0], 'b-', linewidth=2, label='CoM X')
    plt.title('X Position over Time')
    plt.xlabel('Time (s)')
    plt.ylabel('Position (m)')
    plt.legend()
    plt.grid(True)

    # Plot Y trajectories over time
    plt.subplot(2, 2, 3)
    plt.plot(t, zmp_trajectory[:, 1], 'r-', linewidth=2, label='ZMP Y')
    plt.plot(t, com_trajectory[:, 1], 'b-', linewidth=2, label='CoM Y')
    plt.title('Y Position over Time')
    plt.xlabel('Time (m)')
    plt.ylabel('Position (m)')
    plt.legend()
    plt.grid(True)

    # Plot CoM height (constant)
    plt.subplot(2, 2, 4)
    plt.plot(t, com_trajectory[:, 2], 'g-', linewidth=2, label='CoM Z')
    plt.title('CoM Height (Constant)')
    plt.xlabel('Time (s)')
    plt.ylabel('Height (m)')
    plt.legend()
    plt.grid(True)

    plt.tight_layout()
    plt.show()


if __name__ == '__main__':
    main()
```

### Walking Pattern with Capture Point

The Capture Point is a useful concept for balance control during walking:

```python
import numpy as np
import matplotlib.pyplot as plt


class CapturePointController:
    def __init__(self, com_height=0.85, gravity=9.81):
        """
        Initialize Capture Point controller
        com_height: Center of mass height
        gravity: Gravitational acceleration
        """
        self.com_height = com_height
        self.gravity = gravity
        self.omega = np.sqrt(gravity / com_height)

    def calculate_capture_point(self, com_pos, com_vel):
        """
        Calculate the capture point
        The capture point is where the robot should step to stop safely
        capture_point = com_pos + com_vel / omega
        """
        cp_x = com_pos[0] + com_vel[0] / self.omega
        cp_y = com_pos[1] + com_vel[1] / self.omega
        return np.array([cp_x, cp_y])

    def should_step(self, com_pos, com_vel, foot_positions, step_threshold=0.1):
        """
        Determine if a step is needed based on capture point
        foot_positions: Dictionary with 'left' and 'right' foot positions
        step_threshold: Distance threshold for stepping
        """
        capture_point = self.calculate_capture_point(com_pos, com_vel)

        # Find the support polygon (area between feet)
        all_foot_pos = np.array([pos for pos in foot_positions.values()])
        support_center_x = np.mean(all_foot_pos[:, 0])
        support_center_y = np.mean(all_foot_pos[:, 1])

        # Calculate distance from capture point to support center
        cp_distance = np.sqrt((capture_point[0] - support_center_x)**2 +
                              (capture_point[1] - support_center_y)**2)

        return cp_distance > step_threshold, capture_point

    def generate_step_location(self, capture_point, current_foot_pos, step_size=0.3):
        """
        Generate appropriate step location based on capture point
        """
        # Step toward the capture point, but with a reasonable step size
        direction = capture_point - current_foot_pos[:2]
        direction_norm = np.linalg.norm(direction)

        if direction_norm > 0:
            # Normalize direction and scale to step size
            step_direction = direction / direction_norm
            step_location = current_foot_pos[:2] + step_direction * min(step_size, direction_norm)
        else:
            step_location = current_foot_pos[:2]

        # Add small Z component for foot lift
        step_location_full = np.array([step_location[0], step_location[1], 0.0])

        return step_location_full


class WalkingController:
    def __init__(self, com_height=0.85):
        self.com_height = com_height
        self.capture_controller = CapturePointController(com_height)

        # Robot state
        self.com_pos = np.array([0.0, 0.0, com_height])
        self.com_vel = np.array([0.0, 0.0, 0.0])
        self.left_foot_pos = np.array([0.0, 0.1, 0.0])
        self.right_foot_pos = np.array([0.0, -0.1, 0.0])

        # Walking parameters
        self.step_length = 0.3
        self.step_width = 0.2
        self.step_height = 0.05  # Foot lift height

    def update_balance(self, dt):
        """
        Update balance based on current state
        """
        # Calculate if we need to step
        foot_positions = {
            'left': self.left_foot_pos,
            'right': self.right_foot_pos
        }

        should_step, capture_point = self.capture_controller.should_step(
            self.com_pos, self.com_vel, foot_positions
        )

        if should_step:
            print(f"Balance: Need to step! Capture point at ({capture_point[0]:.3f}, {capture_point[1]:.3f})")
            # In a real system, this would trigger a stepping motion
            return True, capture_point

        return False, capture_point

    def generate_foot_trajectory(self, start_pos, end_pos, step_height=0.05, steps=20):
        """
        Generate smooth foot trajectory from start to end position
        """
        t = np.linspace(0, 1, steps)

        # Linear interpolation for x, y
        x_traj = start_pos[0] + (end_pos[0] - start_pos[0]) * t
        y_traj = start_pos[1] + (end_pos[1] - start_pos[1]) * t

        # Sinusoidal trajectory for z (foot lift)
        z_lift = start_pos[2] + (end_pos[2] - start_pos[2]) * t
        # Add foot lift in the middle of the step
        z_traj = z_lift + step_height * np.sin(np.pi * t)

        return np.column_stack([x_traj, y_traj, z_traj])


# Example usage
def main():
    walker = WalkingController()

    # Simulate a walking scenario
    time_points = []
    com_positions = []
    step_needed = []

    # Simulate forward walking motion
    dt = 0.01
    simulation_time = 10.0
    t = 0

    while t < simulation_time:
        # Simulate forward motion (push CoM forward)
        walker.com_vel[0] = 0.2  # Push forward
        walker.com_pos += walker.com_vel * dt

        # Add some small disturbances
        walker.com_pos[1] += np.random.normal(0, 0.001) * dt  # Small lateral disturbance
        walker.com_vel[1] += np.random.normal(0, 0.01) * dt   # Small velocity disturbance

        # Check balance
        needs_step, cp = walker.update_balance(dt)
        step_needed.append(needs_step)
        time_points.append(t)
        com_positions.append(walker.com_pos.copy())

        t += dt

    # Convert to arrays
    time_points = np.array(time_points)
    com_positions = np.array(com_positions)
    step_needed = np.array(step_needed)

    print(f"Simulation completed. Steps needed: {np.sum(step_needed)} out of {len(step_needed)} time steps")

    # Visualization
    plt.figure(figsize=(15, 10))

    # Plot CoM trajectory
    plt.subplot(2, 2, 1)
    plt.plot(com_positions[:, 0], com_positions[:, 1], 'b-', linewidth=2, label='CoM Trajectory')
    plt.scatter(com_positions[step_needed, 0], com_positions[step_needed, 1],
               c='red', s=10, alpha=0.5, label='Step Needed')
    plt.title('Center of Mass Trajectory')
    plt.xlabel('X (m)')
    plt.ylabel('Y (m)')
    plt.legend()
    plt.grid(True)
    plt.axis('equal')

    # Plot X position over time
    plt.subplot(2, 2, 2)
    plt.plot(time_points, com_positions[:, 0], 'b-', linewidth=2, label='X Position')
    plt.title('CoM X Position over Time')
    plt.xlabel('Time (s)')
    plt.ylabel('X Position (m)')
    plt.legend()
    plt.grid(True)

    # Plot Y position over time
    plt.subplot(2, 2, 3)
    plt.plot(time_points, com_positions[:, 1], 'r-', linewidth=2, label='Y Position')
    plt.title('CoM Y Position over Time')
    plt.xlabel('Time (s)')
    plt.ylabel('Y Position (m)')
    plt.legend()
    plt.grid(True)

    # Plot when steps are needed
    plt.subplot(2, 2, 4)
    plt.plot(time_points, step_needed.astype(int), 'g-', linewidth=2, label='Step Needed')
    plt.title('Step Needed Over Time')
    plt.xlabel('Time (s)')
    plt.ylabel('Step Needed (1=Yes, 0=No)')
    plt.legend()
    plt.grid(True)

    plt.tight_layout()
    plt.show()


if __name__ == '__main__':
    main()
```

## Balance Control Algorithms

### Linear Quadratic Regulator (LQR) for Balance

LQR is a powerful control technique for balance control:

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.linalg import solve_continuous_are


class LQRBalanceController:
    def __init__(self, com_height=0.85, mass=75.0, gravity=9.81):
        """
        Initialize LQR balance controller
        com_height: Center of mass height
        mass: Robot mass
        gravity: Gravitational acceleration
        """
        self.com_height = com_height
        self.mass = mass
        self.gravity = gravity
        self.omega = np.sqrt(gravity / com_height)

        # State: [x, x_dot, y, y_dot] where x, y are CoM positions and velocities
        # Control input: [F_x, F_y] forces applied at CoM
        self.A = np.array([
            [0, 1, 0, 0],           # dx/dt = x_dot
            [self.omega**2, 0, 0, 0], # dx_dot/dt = omega^2 * x
            [0, 0, 0, 1],           # dy/dt = y_dot
            [0, 0, self.omega**2, 0]  # dy_dot/dt = omega^2 * y
        ])

        # Control input matrix (force -> acceleration)
        self.B = np.array([
            [0, 0],
            [1/(self.mass * self.com_height), 0],
            [0, 0],
            [0, 1/(self.mass * self.com_height)]
        ])

        # Design LQR controller
        self.Q = np.diag([100, 1, 100, 1])  # State cost matrix
        self.R = np.diag([1, 1])            # Control cost matrix
        self.K = self.compute_lqr_gain()

    def compute_lqr_gain(self):
        """
        Compute LQR gain matrix K such that u = -Kx
        """
        # Solve the continuous-time algebraic Riccati equation
        P = solve_continuous_are(self.A, self.B, self.Q, self.R)

        # Compute the optimal gain
        K = np.linalg.inv(self.R) @ self.B.T @ P
        return K

    def control(self, state):
        """
        Compute control input for given state
        state: [x, x_dot, y, y_dot] - CoM position and velocity
        """
        # State feedback: u = -Kx
        control_input = -self.K @ state
        return control_input

    def get_force_control(self, com_pos, com_vel):
        """
        Get force control input given CoM position and velocity
        com_pos: [x, y] - CoM position
        com_vel: [x_dot, y_dot] - CoM velocity
        """
        state = np.array([com_pos[0], com_vel[0], com_pos[1], com_vel[1]])
        control_input = self.control(state)
        return control_input

    def simulate_balance(self, initial_state, simulation_time=10.0, dt=0.01):
        """
        Simulate balance control
        initial_state: [x0, x_dot0, y0, y_dot0]
        """
        t = np.arange(0, simulation_time, dt)
        states = np.zeros((len(t), 4))
        states[0] = initial_state
        controls = np.zeros((len(t), 2))

        for i in range(1, len(t)):
            # Get control input
            control_input = self.control(states[i-1])
            controls[i-1] = control_input

            # Update state: x_dot = Ax + Bu
            state_dot = self.A @ states[i-1] + self.B @ control_input
            states[i] = states[i-1] + state_dot * dt

        return t, states, controls


# Example usage
def main():
    # Create LQR balance controller
    controller = LQRBalanceController(com_height=0.85, mass=75.0)

    # Simulate with initial disturbance
    initial_state = np.array([0.05, 0.0, 0.02, 0.0])  # Small initial position errors
    t, states, controls = controller.simulate_balance(initial_state, simulation_time=5.0)

    # Extract components
    x_pos = states[:, 0]
    x_vel = states[:, 1]
    y_pos = states[:, 2]
    y_vel = states[:, 3]

    control_x = controls[:, 0]
    control_y = controls[:, 1]

    print(f"LQR Balance Controller - Initial state: {initial_state}")
    print(f"Final state after 5s: x={x_pos[-1]:.4f}, x_dot={x_vel[-1]:.4f}, y={y_pos[-1]:.4f}, y_dot={y_vel[-1]:.4f}")

    # Visualization
    plt.figure(figsize=(15, 12))

    # Plot CoM position
    plt.subplot(3, 2, 1)
    plt.plot(t, x_pos, 'b-', linewidth=2, label='X Position')
    plt.plot(t, y_pos, 'r-', linewidth=2, label='Y Position')
    plt.title('CoM Position over Time')
    plt.xlabel('Time (s)')
    plt.ylabel('Position (m)')
    plt.legend()
    plt.grid(True)

    # Plot CoM velocity
    plt.subplot(3, 2, 2)
    plt.plot(t, x_vel, 'b-', linewidth=2, label='X Velocity')
    plt.plot(t, y_vel, 'r-', linewidth=2, label='Y Velocity')
    plt.title('CoM Velocity over Time')
    plt.xlabel('Time (s)')
    plt.ylabel('Velocity (m/s)')
    plt.legend()
    plt.grid(True)

    # Phase plot (position vs velocity)
    plt.subplot(3, 2, 3)
    plt.plot(x_pos, x_vel, 'b-', linewidth=2, label='X Phase Plot')
    plt.title('X Phase Plot (Position vs Velocity)')
    plt.xlabel('Position (m)')
    plt.ylabel('Velocity (m/s)')
    plt.legend()
    plt.grid(True)
    plt.axis('equal')

    plt.subplot(3, 2, 4)
    plt.plot(y_pos, y_vel, 'r-', linewidth=2, label='Y Phase Plot')
    plt.title('Y Phase Plot (Position vs Velocity)')
    plt.xlabel('Position (m)')
    plt.ylabel('Velocity (m/s)')
    plt.legend()
    plt.grid(True)
    plt.axis('equal')

    # Control inputs
    plt.subplot(3, 2, 5)
    plt.plot(t, control_x, 'b-', linewidth=2, label='X Force Control')
    plt.plot(t, control_y, 'r-', linewidth=2, label='Y Force Control')
    plt.title('Control Forces over Time')
    plt.xlabel('Time (s)')
    plt.ylabel('Force (N)')
    plt.legend()
    plt.grid(True)

    # State trajectory in 2D
    plt.subplot(3, 2, 6)
    plt.plot(x_pos, y_pos, 'g-', linewidth=2, label='CoM Trajectory')
    plt.plot(x_pos[0], y_pos[0], 'go', markersize=10, label='Start')
    plt.plot(x_pos[-1], y_pos[-1], 'ro', markersize=10, label='End')
    plt.title('CoM Trajectory in 2D')
    plt.xlabel('X Position (m)')
    plt.ylabel('Y Position (m)')
    plt.legend()
    plt.grid(True)
    plt.axis('equal')

    plt.tight_layout()
    plt.show()


if __name__ == '__main__':
    main()
```

## ROS 2 Integration for Locomotion Control

### Complete Walking Controller Node

```python
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import JointState
from geometry_msgs.msg import Pose, Twist
from std_msgs.msg import Float64MultiArray
from builtin_interfaces.msg import Duration
import numpy as np


class WalkingControllerNode(Node):
    def __init__(self):
        super().__init__('walking_controller')

        # Publishers
        self.joint_cmd_pub = self.create_publisher(JointState, '/joint_commands', 10)
        self.com_state_pub = self.create_publisher(Float64MultiArray, '/com_state', 10)
        self.zmp_pub = self.create_publisher(Float64MultiArray, '/zmp', 10)

        # Subscribers
        self.joint_state_sub = self.create_subscription(
            JointState,
            '/joint_states',
            self.joint_state_callback,
            10
        )

        self.foot_pressure_sub = self.create_subscription(
            Float64MultiArray,
            '/foot_pressure',
            self.foot_pressure_callback,
            10
        )

        # Timer for control loop
        self.control_timer = self.create_timer(0.01, self.control_loop)  # 100 Hz

        # Walking state
        self.current_joints = {}
        self.left_foot_pressure = [0.0, 0.0, 0.0, 0.0]  # 4 pressure sensors
        self.right_foot_pressure = [0.0, 0.0, 0.0, 0.0]

        # Initialize controllers
        self.balance_controller = LQRBalanceController()
        self.zmp_generator = ZMPPatternGenerator()
        self.capture_controller = CapturePointController()

        # Walking parameters
        self.com_height = 0.85
        self.walking_state = 'standing'  # standing, walking, stepping
        self.step_phase = 0.0  # 0.0 to 1.0, where 0.0 is start of step, 1.0 is end

        self.get_logger().info('Walking controller initialized')

    def joint_state_callback(self, msg):
        """Update current joint positions"""
        for name, pos in zip(msg.name, msg.position):
            self.current_joints[name] = pos

    def foot_pressure_callback(self, msg):
        """Update foot pressure sensor readings"""
        # Assuming first 4 values are left foot, next 4 are right foot
        if len(msg.data) >= 8:
            self.left_foot_pressure = msg.data[:4]
            self.right_foot_pressure = msg.data[4:8]

    def estimate_com_state(self):
        """
        Estimate center of mass position and velocity from joint states
        This is a simplified approach - in practice, you'd use forward kinematics
        """
        # For demonstration, return a simple estimate
        # In a real system, you'd calculate this from the full kinematic model
        com_pos = np.array([0.0, 0.0, self.com_height])
        com_vel = np.array([0.0, 0.0, 0.0])

        return com_pos, com_vel

    def calculate_zmp_from_pressure(self):
        """
        Calculate ZMP from foot pressure sensors
        """
        # Calculate ZMP from pressure distribution
        # Simplified calculation using pressure sensor locations
        sensor_positions = np.array([
            [-0.05, 0.05],   # front-left
            [0.05, 0.05],    # front-right
            [-0.05, -0.05],  # back-left
            [0.05, -0.05]    # back-right
        ])

        # Calculate left foot ZMP
        left_total_pressure = sum(self.left_foot_pressure)
        if left_total_pressure > 0:
            left_zmp_x = sum(p * pos[0] for p, pos in zip(self.left_foot_pressure, sensor_positions)) / left_total_pressure
            left_zmp_y = sum(p * pos[1] for p, pos in zip(self.left_foot_pressure, sensor_positions)) / left_total_pressure
            left_zmp = np.array([left_zmp_x, left_zmp_y])
        else:
            left_zmp = np.array([0.0, 0.0])

        # Calculate right foot ZMP
        right_total_pressure = sum(self.right_foot_pressure)
        if right_total_pressure > 0:
            right_zmp_x = sum(p * pos[0] for p, pos in zip(self.right_foot_pressure, sensor_positions)) / right_total_pressure
            right_zmp_y = sum(p * pos[1] for p, pos in zip(self.right_foot_pressure, sensor_positions)) / right_total_pressure
            right_zmp = np.array([right_zmp_x, right_zmp_y])
        else:
            right_zmp = np.array([0.0, 0.0])

        # Overall ZMP based on both feet
        total_pressure = left_total_pressure + right_total_pressure
        if total_pressure > 0:
            overall_zmp = (left_total_pressure * left_zmp + right_total_pressure * right_zmp) / total_pressure
        else:
            overall_zmp = np.array([0.0, 0.0])

        return overall_zmp

    def control_loop(self):
        """Main control loop"""
        # Estimate current CoM state
        com_pos, com_vel = self.estimate_com_state()

        # Calculate current ZMP
        current_zmp = self.calculate_zmp_from_pressure()

        # Publish CoM state
        com_msg = Float64MultiArray()
        com_msg.data = [com_pos[0], com_pos[1], com_pos[2], com_vel[0], com_vel[1], com_vel[2]]
        self.com_state_pub.publish(com_msg)

        # Publish ZMP
        zmp_msg = Float64MultiArray()
        zmp_msg.data = [current_zmp[0], current_zmp[1]]
        self.zmp_pub.publish(zmp_msg)

        # Balance control
        state = np.array([com_pos[0], com_vel[0], com_pos[1], com_vel[1]])
        control_forces = self.balance_controller.control(state)

        # For walking, we would also need to generate stepping patterns
        # and coordinate with joint controllers
        self.get_logger().info(f'Balance control: F_x={control_forces[0]:.3f} N, F_y={control_forces[1]:.3f} N')

        # Generate appropriate joint commands based on balance control
        # This would involve inverse kinematics and joint-level control
        # For now, just publish a placeholder
        joint_cmd = JointState()
        joint_cmd.name = [f'joint_{i}' for i in range(12)]  # Example joint names
        joint_cmd.position = [0.0] * 12  # Placeholder positions
        joint_cmd.velocity = [0.0] * 12
        joint_cmd.effort = [0.0] * 12

        # Add balance corrections to joint positions
        # This is where the high-level balance commands are translated to joint commands
        self.joint_cmd_pub.publish(joint_cmd)

        # Update walking state based on ZMP and other factors
        self.update_walking_state(current_zmp, state)

    def update_walking_state(self, current_zmp, com_state):
        """Update walking state based on ZMP and CoM state"""
        # Calculate capture point
        capture_point = self.capture_controller.calculate_capture_point(
            com_state[::2], com_state[1::2]  # pos and vel from state vector
        )

        # Check if we need to step
        # This would involve checking if capture point is outside support polygon
        # For now, just log the values
        self.get_logger().info(
            f'ZMP: ({current_zmp[0]:.3f}, {current_zmp[1]:.3f}), '
            f'Capture Point: ({capture_point[0]:.3f}, {capture_point[1]:.3f})'
        )


def main(args=None):
    rclpy.init(args=args)
    walking_controller = WalkingControllerNode()

    try:
        rclpy.spin(walking_controller)
    except KeyboardInterrupt:
        pass
    finally:
        walking_controller.destroy_node()
        rclpy.shutdown()


if __name__ == '__main__':
    main()
```

## Hands-On Lab: Bipedal Walking Controller

### Objective
Create a complete bipedal walking controller that implements balance control, ZMP-based walking patterns, and capture point stepping.

### Prerequisites
- Completed Chapter 1-8
- ROS 2 Humble with Gazebo installed
- Basic understanding of robot dynamics and control

### Steps

1. **Create a walking control package**:
   ```bash
   cd ~/robotics_ws/src
   ros2 pkg create --build-type ament_python walking_control_lab --dependencies rclpy sensor_msgs geometry_msgs std_msgs numpy scipy matplotlib
   ```

2. **Create the main walking controller node** (`walking_control_lab/walking_control_lab/walking_controller_node.py`):
   ```python
   #!/usr/bin/env python3
   import rclpy
   from rclpy.node import Node
   from sensor_msgs.msg import JointState
   from geometry_msgs.msg import Pose, Twist
   from std_msgs.msg import Float64MultiArray, Bool
   from builtin_interfaces.msg import Duration
   import numpy as np
   import time


   class ZMPPatternGenerator:
       def __init__(self, sampling_time=0.01, com_height=0.85, gravity=9.81):
           self.dt = sampling_time
           self.com_height = com_height
           self.gravity = gravity
           self.omega = np.sqrt(self.gravity / self.com_height)

       def generate_com_trajectory_from_zmp(self, zmp_trajectory):
           """Generate CoM trajectory from ZMP reference"""
           t = np.arange(0, len(zmp_trajectory)) * self.dt
           x_zmp = zmp_trajectory[:, 0]
           y_zmp = zmp_trajectory[:, 1]

           # Solve the differential equation: x_com_ddot - omega^2 * x_com = -omega^2 * x_zmp
           # Using numerical integration
           def integrate_zmp(zmp_ref):
               # Simple forward integration approach
               # In practice, use more sophisticated methods like preview control
               com_pos = np.zeros_like(zmp_ref)
               com_vel = np.zeros_like(zmp_ref)
               com_acc = np.zeros_like(zmp_ref)

               for i in range(1, len(zmp_ref)):
                   # x_com_ddot = omega^2 * (x_com - x_zmp)
                   if i > 1:
                       com_acc[i-1] = self.omega**2 * (com_pos[i-1] - zmp_ref[i-1])
                       com_vel[i] = com_vel[i-1] + com_acc[i-1] * self.dt
                       com_pos[i] = com_pos[i-1] + com_vel[i-1] * self.dt

               # Apply feedback to track ZMP
               for i in range(len(zmp_ref)):
                   com_pos[i] = zmp_ref[i] + com_vel[i] / self.omega

               return com_pos

           com_x = integrate_zmp(x_zmp)
           com_y = integrate_zmp(y_zmp)

           com_x_vel = np.gradient(com_x, self.dt)
           com_y_vel = np.gradient(com_y, self.dt)
           com_x_acc = np.gradient(com_x_vel, self.dt)
           com_y_acc = np.gradient(com_y_vel, self.dt)

           com_trajectory = np.column_stack([com_x, com_y, np.full_like(com_x, self.com_height)])
           com_velocity = np.column_stack([com_x_vel, com_y_vel, np.zeros_like(com_x_vel)])

           return com_trajectory, com_velocity

       def generate_footprint_pattern(self, step_length=0.3, step_width=0.2, n_steps=10):
           """Generate walking footprint pattern"""
           footsteps = []
           left_pos = np.array([0.0, step_width/2, 0.0])
           right_pos = np.array([0.0, -step_width/2, 0.0])

           for i in range(n_steps):
               if i % 2 == 1:
                   right_pos[0] += step_length
                   right_pos[1] = (-1)**i * step_width/2
                   footsteps.append(('right', right_pos.copy()))
               else:
                   left_pos[0] += step_length
                   left_pos[1] = (-1)**(i+1) * step_width/2
                   footsteps.append(('left', left_pos.copy()))

           return footsteps

       def generate_zmp_trajectory(self, footsteps, double_support_time=0.1):
           """Generate ZMP trajectory based on footsteps"""
           dt = self.dt
           single_support_time = 1.0
           total_time = len(footsteps) * (single_support_time + double_support_time)

           t = np.arange(0, total_time, dt)
           zmp_trajectory = np.zeros((len(t), 2))

           for i, (foot_type, foot_pos) in enumerate(footsteps):
               step_start_time = i * (single_support_time + double_support_time)
               double_support_end = step_start_time + double_support_time
               step_end_time = step_start_time + single_support_time + double_support_time

               start_idx = int(step_start_time / dt)
               double_end_idx = int(double_support_end / dt)
               end_idx = int(step_end_time / dt)

               if end_idx >= len(t):
                   end_idx = len(t)

               # Double support: interpolate between feet
               if double_end_idx > start_idx and i > 0:
                   prev_type, prev_pos = footsteps[i-1]
                   prev_pos_2d = prev_pos[:2]
                   for j in range(start_idx, min(double_end_idx, len(t))):
                       t_interp = min(1.0, (t[j] - step_start_time) / double_support_time)
                       zmp_trajectory[j] = (1 - t_interp) * prev_pos_2d + t_interp * foot_pos[:2]
               elif end_idx > double_end_idx:
                   # Single support: ZMP at foot position
                   zmp_trajectory[double_end_idx:end_idx, 0] = foot_pos[0]
                   zmp_trajectory[double_end_idx:end_idx, 1] = foot_pos[1]

           return t, zmp_trajectory


   class CapturePointController:
       def __init__(self, com_height=0.85, gravity=9.81):
           self.com_height = com_height
           self.gravity = gravity
           self.omega = np.sqrt(gravity / com_height)

       def calculate_capture_point(self, com_pos, com_vel):
           """Calculate capture point"""
           cp_x = com_pos[0] + com_vel[0] / self.omega
           cp_y = com_pos[1] + com_vel[1] / self.omega
           return np.array([cp_x, cp_y])


   class WalkingControllerNode(Node):
       def __init__(self):
           super().__init__('walking_controller_lab')

           # Publishers
           self.joint_cmd_pub = self.create_publisher(JointState, '/joint_commands', 10)
           self.com_state_pub = self.create_publisher(Float64MultiArray, '/com_state', 10)
           self.zmp_pub = self.create_publisher(Float64MultiArray, '/zmp', 10)
           self.status_pub = self.create_publisher(Bool, '/walking_active', 10)

           # Subscribers
           self.joint_state_sub = self.create_subscription(
               JointState,
               '/joint_states',
               self.joint_state_callback,
               10
           )

           # Timer for control loop
           self.control_timer = self.create_timer(0.01, self.control_loop)

           # Initialize controllers
           self.zmp_generator = ZMPPatternGenerator(com_height=0.85)
           self.capture_controller = CapturePointController(com_height=0.85)

           # Walking state
           self.current_joints = {}
           self.com_height = 0.85
           self.walking_state = 'standing'
           self.step_count = 0
           self.balance_error = 0.0

           # Initialize with a simple walking pattern
           self.footsteps = self.zmp_generator.generate_footprint_pattern(
               step_length=0.3, step_width=0.2, n_steps=6
           )
           self.t_zmp, self.zmp_trajectory = self.zmp_generator.generate_zmp_trajectory(
               self.footsteps, double_support_time=0.2
           )
           self.com_trajectory, self.com_velocity = self.zmp_generator.generate_com_trajectory_from_zmp(
               self.zmp_trajectory
           )

           # Current trajectory index
           self.trajectory_idx = 0

           self.get_logger().info('Walking controller lab node initialized')

       def joint_state_callback(self, msg):
           """Update current joint positions"""
           for name, pos in zip(msg.name, msg.position):
               self.current_joints[name] = pos

       def estimate_com_state(self):
           """Estimate CoM state from current configuration"""
           # In a real system, this would use forward kinematics
           # For this example, use the precomputed trajectory
           if self.trajectory_idx < len(self.com_trajectory):
               com_pos = self.com_trajectory[self.trajectory_idx]
               com_vel = self.com_velocity[self.trajectory_idx]
               self.trajectory_idx += 1
               if self.trajectory_idx >= len(self.com_trajectory):
                   self.trajectory_idx = 0  # Loop
           else:
               com_pos = np.array([0.0, 0.0, self.com_height])
               com_vel = np.array([0.0, 0.0, 0.0])

           return com_pos, com_vel

       def calculate_balance_error(self, com_pos, zmp_pos):
           """Calculate balance error as distance between CoM and ZMP"""
           # Simple distance measure
           error = np.linalg.norm(com_pos[:2] - zmp_pos)
           return error

       def generate_joint_commands(self, com_command, foot_positions):
           """Generate joint commands to achieve CoM position"""
           # This is a simplified approach
           # In practice, this would involve full inverse kinematics
           # and whole-body control

           # For demonstration, return a simple pattern
           joint_cmd = JointState()
           joint_cmd.name = [
               'left_hip_roll', 'left_hip_pitch', 'left_hip_yaw',
               'left_knee', 'left_ankle_pitch', 'left_ankle_roll',
               'right_hip_roll', 'right_hip_pitch', 'right_hip_yaw',
               'right_knee', 'right_ankle_pitch', 'right_ankle_roll'
           ]

           # Simple mapping from CoM command to joint angles
           # In reality, this would use inverse kinematics
           base_angles = [0.0] * 12
           # Add balance corrections based on CoM error
           com_error_x = com_command[0]
           com_error_y = com_command[1]

           # Adjust hip and ankle angles based on CoM position
           base_angles[1] = -com_error_x * 0.5  # Left hip pitch
           base_angles[7] = -com_error_x * 0.5  # Right hip pitch
           base_angles[4] = com_error_y * 0.3   # Left ankle pitch
           base_angles[10] = com_error_y * 0.3  # Right ankle pitch

           joint_cmd.position = base_angles
           joint_cmd.velocity = [0.0] * 12
           joint_cmd.effort = [0.0] * 12

           return joint_cmd

       def control_loop(self):
           """Main control loop"""
           # Estimate current CoM state
           com_pos, com_vel = self.estimate_com_state()

           # Calculate current ZMP (from precomputed trajectory)
           if self.trajectory_idx < len(self.zmp_trajectory):
               current_zmp = self.zmp_trajectory[self.trajectory_idx-1] if self.trajectory_idx > 0 else np.array([0.0, 0.0])
           else:
               current_zmp = np.array([0.0, 0.0])

           # Calculate balance error
           self.balance_error = self.calculate_balance_error(com_pos, current_zmp)

           # Publish CoM state
           com_msg = Float64MultiArray()
           com_msg.data = [com_pos[0], com_pos[1], com_pos[2], com_vel[0], com_vel[1], com_vel[2]]
           self.com_state_pub.publish(com_msg)

           # Publish ZMP
           zmp_msg = Float64MultiArray()
           zmp_msg.data = [current_zmp[0], current_zmp[1]]
           self.zmp_pub.publish(zmp_msg)

           # Generate joint commands based on balance control
           com_command = [com_pos[0], com_pos[1], com_pos[2]]  # Use current CoM as command
           foot_positions = {}  # In a real system, you'd get actual foot positions
           joint_cmd = self.generate_joint_commands(com_command, foot_positions)

           # Publish joint commands
           self.joint_cmd_pub.publish(joint_cmd)

           # Publish status
           status_msg = Bool()
           status_msg.data = self.balance_error < 0.1  # Stable if error < 0.1m
           self.status_pub.publish(status_msg)

           # Log balance information
           self.get_logger().info(
               f'Balance Error: {self.balance_error:.3f}m, '
               f'CoM: ({com_pos[0]:.3f}, {com_pos[1]:.3f}), '
               f'ZMP: ({current_zmp[0]:.3f}, {current_zmp[1]:.3f})'
           )

           # Update walking state
           if self.balance_error > 0.2:
               self.walking_state = 'unstable'
           elif self.balance_error < 0.05:
               self.walking_state = 'stable'
           else:
               self.walking_state = 'walking'


   def main(args=None):
       rclpy.init(args=args)
       walking_controller = WalkingControllerNode()

       try:
           rclpy.spin(walking_controller)
       except KeyboardInterrupt:
           pass
       finally:
           walking_controller.destroy_node()
           rclpy.shutdown()


   if __name__ == '__main__':
       main()
   ```

3. **Create a launch file** (`walking_control_lab/launch/walking_control.launch.py`):
   ```python
   from launch import LaunchDescription
   from launch.actions import DeclareLaunchArgument
   from launch.substitutions import LaunchConfiguration
   from launch_ros.actions import Node


   def generate_launch_description():
       # Declare launch arguments
       use_sim_time = DeclareLaunchArgument(
           'use_sim_time',
           default_value='false',
           description='Use simulation (Gazebo) clock if true'
       )

       # Walking controller node
       walking_controller_node = Node(
           package='walking_control_lab',
           executable='walking_controller_node',
           name='walking_controller_lab',
           parameters=[{'use_sim_time': LaunchConfiguration('use_sim_time')}],
           output='screen'
       )

       return LaunchDescription([
           use_sim_time,
           walking_controller_node
       ])
   ```

4. **Update setup.py**:
   ```python
   import os
   from glob import glob
   from setuptools import setup
   from setuptools import find_packages

   package_name = 'walking_control_lab'

   setup(
       name=package_name,
       version='0.0.0',
       packages=find_packages(exclude=['test']),
       data_files=[
           ('share/ament_index/resource_index/packages',
               ['resource/' + package_name]),
           ('share/' + package_name, ['package.xml']),
           (os.path.join('share', package_name, 'launch'), glob('launch/*.launch.py')),
       ],
       install_requires=['setuptools'],
       zip_safe=True,
       maintainer='Your Name',
       maintainer_email='your.email@example.com',
       description='Walking control lab for robotics',
       license='Apache License 2.0',
       tests_require=['pytest'],
       entry_points={
           'console_scripts': [
               'walking_controller_node = walking_control_lab.walking_controller_node:main',
           ],
       },
   )
   ```

5. **Build the package**:
   ```bash
   cd ~/robotics_ws
   colcon build --packages-select walking_control_lab
   source install/setup.bash
   ```

6. **Run the walking control system**:
   ```bash
   ros2 launch walking_control_lab walking_control.launch.py
   ```

### Expected Results
- The system should maintain balance by tracking ZMP references
- Joint commands should be generated to maintain CoM stability
- Balance error should remain within acceptable limits
- The system should demonstrate basic walking patterns

### Troubleshooting Tips
- Ensure joint names match your robot's configuration
- Verify CoM height parameter matches your robot
- Check that ZMP references are feasible for your robot
- Monitor balance error to ensure stability

## Summary

In this chapter, we've explored the fundamental concepts of bipedal locomotion and balance control, including inverted pendulum models, ZMP-based walking, capture point control, and LQR balance controllers. We've implemented practical examples of each concept and created a complete walking control system.

The hands-on lab provided experience with creating a system that combines balance control, walking pattern generation, and joint-level control. This foundation is essential for advanced humanoid robotics applications and prepares us for more complex locomotion and control challenges in the upcoming chapters.

============================================================
FILE: book\docs\part-iii-motion\chapter-9-navigation\index.md
============================================================
---
title: Chapter 9 - Motion Planning and Navigation
sidebar_position: 3
---

# Chapter 9: Motion Planning and Navigation

## Learning Goals

- Master path planning algorithms
- Understand navigation in dynamic environments
- Learn obstacle avoidance techniques
- Implement A*, Dijkstra, and RRT path planning
- Navigate robots in complex environments
- Handle dynamic obstacles and replanning

## Introduction to Motion Planning

Motion planning is the process of finding a collision-free path from a start configuration to a goal configuration while satisfying various constraints. It's a fundamental capability for autonomous robots that need to navigate in complex environments.

### Motion Planning Components

A complete motion planning system consists of:

1. **Configuration Space (C-space)**: The space of all possible robot configurations
2. **Planning Algorithm**: Method for finding a path through C-space
3. **Collision Detection**: System for detecting obstacles
4. **Path Optimization**: Techniques for smoothing and improving paths
5. **Trajectory Generation**: Conversion of geometric path to timed trajectory

## Configuration Space and Representation

### Configuration Space Concepts

The configuration space (C-space) represents all possible configurations of a robot. For a robot with n degrees of freedom, the C-space is an n-dimensional space.

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.spatial.distance import cdist


class ConfigurationSpace:
    def __init__(self, bounds, obstacles=None):
        """
        Initialize configuration space
        bounds: List of (min, max) for each dimension
        obstacles: List of obstacle shapes [(center, shape_params), ...]
        """
        self.bounds = bounds
        self.obstacles = obstacles if obstacles else []
        self.dimensions = len(bounds)

    def is_valid_configuration(self, config):
        """
        Check if configuration is collision-free
        config: Array representing robot configuration
        """
        # Check bounds
        for i, (min_val, max_val) in enumerate(self.bounds):
            if not (min_val <= config[i] <= max_val):
                return False

        # Check obstacles
        for obstacle in self.obstacles:
            center, shape_type, params = obstacle
            if self._check_collision(config, center, shape_type, params):
                return False

        return True

    def _check_collision(self, config, center, shape_type, params):
        """Check collision with obstacle"""
        if shape_type == 'circle':
            radius = params[0]
            distance = np.linalg.norm(np.array(config) - np.array(center))
            return distance < radius
        elif shape_type == 'rectangle':
            width, height = params
            half_width, half_height = width/2, height/2
            rel_pos = np.array(config) - np.array(center)
            return (abs(rel_pos[0]) < half_width and abs(rel_pos[1]) < half_height)
        else:
            return False  # Unknown shape

    def sample_free_space(self):
        """Sample a random configuration in free space"""
        for _ in range(1000):  # Try up to 1000 times
            config = []
            for min_val, max_val in self.bounds:
                config.append(np.random.uniform(min_val, max_val))

            if self.is_valid_configuration(config):
                return np.array(config)

        # If random sampling fails, return None or use grid sampling
        return None

    def get_neighbors(self, config, step_size=0.1):
        """Get neighboring configurations"""
        neighbors = []
        for i in range(self.dimensions):
            for direction in [-1, 1]:
                neighbor = config.copy()
                neighbor[i] += direction * step_size

                if self.is_valid_configuration(neighbor):
                    neighbors.append(neighbor)

        return neighbors


# Example usage
def main():
    # Define configuration space for 2D point robot
    bounds = [(-5, 5), (-5, 5)]  # x and y bounds
    obstacles = [
        ((0, 0), 'circle', (1.0,)),  # Circle at origin with radius 1
        ((2, 2), 'rectangle', (1.0, 1.0)),  # Rectangle at (2,2) with width=1, height=1
    ]

    cspace = ConfigurationSpace(bounds, obstacles)

    # Sample valid configurations
    valid_configs = []
    for _ in range(100):
        config = cspace.sample_free_space()
        if config is not None:
            valid_configs.append(config)

    # Visualize configuration space
    if valid_configs:
        configs = np.array(valid_configs)
        plt.figure(figsize=(10, 8))
        plt.scatter(configs[:, 0], configs[:, 1], alpha=0.6, s=10)

        # Draw obstacles
        circle = plt.Circle((0, 0), 1, color='red', alpha=0.3, label='Circular Obstacle')
        rect = plt.Rectangle((1.5, 1.5), 1, 1, color='red', alpha=0.3, label='Rectangular Obstacle')
        plt.gca().add_patch(circle)
        plt.gca().add_patch(rect)

        plt.xlim(-5, 5)
        plt.ylim(-5, 5)
        plt.title('Configuration Space Sampling')
        plt.xlabel('X')
        plt.ylabel('Y')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.axis('equal')
        plt.show()


if __name__ == '__main__':
    main()
```

### Discretization and Graph Representation

Motion planning algorithms often discretize the configuration space into a graph:

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.spatial import cKDTree
import heapq


class GridBasedSpace:
    def __init__(self, width, height, resolution=1.0):
        """
        Initialize grid-based configuration space
        width, height: Grid dimensions
        resolution: Size of each grid cell
        """
        self.width = width
        self.height = height
        self.resolution = resolution
        self.grid = np.zeros((int(height/resolution), int(width/resolution)))  # 0 = free, 1 = obstacle
        self.obstacles = set()

    def world_to_grid(self, x, y):
        """Convert world coordinates to grid coordinates"""
        grid_x = int((x + self.width/2) / self.resolution)
        grid_y = int((y + self.height/2) / self.resolution)
        return grid_x, grid_y

    def grid_to_world(self, grid_x, grid_y):
        """Convert grid coordinates to world coordinates"""
        x = grid_x * self.resolution - self.width/2
        y = grid_y * self.resolution - self.height/2
        return x, y

    def is_valid_cell(self, grid_x, grid_y):
        """Check if grid cell is valid (not out of bounds or obstacle)"""
        if 0 <= grid_x < self.grid.shape[1] and 0 <= grid_y < self.grid.shape[0]:
            return self.grid[grid_y, grid_x] == 0
        return False

    def get_neighbors(self, grid_x, grid_y):
        """Get valid neighboring cells (8-connectivity)"""
        neighbors = []
        for dx in [-1, 0, 1]:
            for dy in [-1, 0, 1]:
                if dx == 0 and dy == 0:
                    continue
                nx, ny = grid_x + dx, grid_y + dy
                if self.is_valid_cell(nx, ny):
                    neighbors.append((nx, ny))
        return neighbors

    def add_obstacle(self, x, y, width=1, height=1):
        """Add obstacle to grid"""
        grid_x, grid_y = self.world_to_grid(x, y)
        grid_w = int(width / self.resolution)
        grid_h = int(height / self.resolution)

        for i in range(max(0, grid_x - grid_w//2), min(self.grid.shape[1], grid_x + grid_w//2 + 1)):
            for j in range(max(0, grid_y - grid_h//2), min(self.grid.shape[0], grid_y + grid_h//2 + 1)):
                self.grid[j, i] = 1
                self.obstacles.add((i, j))


class Graph:
    def __init__(self):
        self.vertices = set()
        self.edges = {}  # vertex -> [(neighbor, weight), ...]

    def add_vertex(self, vertex):
        self.vertices.add(vertex)
        if vertex not in self.edges:
            self.edges[vertex] = []

    def add_edge(self, v1, v2, weight):
        self.add_vertex(v1)
        self.add_vertex(v2)
        self.edges[v1].append((v2, weight))
        self.edges[v2].append((v1, weight))  # Undirected graph

    def get_neighbors(self, vertex):
        return self.edges.get(vertex, [])


# Example usage
def main():
    # Create grid-based space
    grid_space = GridBasedSpace(width=20, height=20, resolution=0.5)

    # Add obstacles
    grid_space.add_obstacle(5, 0, 2, 4)
    grid_space.add_obstacle(-3, 3, 3, 2)
    grid_space.add_obstacle(0, -4, 4, 2)

    # Create graph representation
    graph = Graph()

    # Add all free cells as vertices
    for i in range(grid_space.grid.shape[1]):
        for j in range(grid_space.grid.shape[0]):
            if grid_space.is_valid_cell(i, j):
                graph.add_vertex((i, j))

    # Connect neighboring cells with edges
    for vertex in graph.vertices:
        x, y = vertex
        neighbors = grid_space.get_neighbors(x, y)
        for neighbor in neighbors:
            # Calculate distance as weight
            dist = np.sqrt((neighbor[0] - x)**2 + (neighbor[1] - y)**2)
            graph.add_edge(vertex, neighbor, dist)

    print(f"Graph created with {len(graph.vertices)} vertices and {sum(len(edges) for edges in graph.edges.values())} edges")


if __name__ == '__main__':
    main()
```

## Classical Path Planning Algorithms

### Dijkstra's Algorithm

Dijkstra's algorithm finds the shortest path from a start node to all other nodes in a weighted graph:

```python
import heapq
import numpy as np
import matplotlib.pyplot as plt


def dijkstra(graph, start, goal):
    """
    Dijkstra's algorithm for shortest path
    graph: Graph object with get_neighbors method
    start: Starting vertex
    goal: Goal vertex
    """
    # Priority queue: (cost, vertex)
    pq = [(0, start)]

    # Costs to reach each vertex
    costs = {start: 0}

    # Previous vertex in optimal path
    previous = {start: None}

    visited = set()

    while pq:
        current_cost, current = heapq.heappop(pq)

        if current in visited:
            continue

        visited.add(current)

        # If we reached the goal, reconstruct path
        if current == goal:
            path = []
            curr = goal
            while curr is not None:
                path.append(curr)
                curr = previous[curr]
            return path[::-1], current_cost

        # Check neighbors
        for neighbor, weight in graph.get_neighbors(current):
            if neighbor in visited:
                continue

            new_cost = current_cost + weight

            if neighbor not in costs or new_cost < costs[neighbor]:
                costs[neighbor] = new_cost
                previous[neighbor] = current
                heapq.heappush(pq, (new_cost, neighbor))

    return None, float('inf')  # No path found


class GridGraph:
    def __init__(self, grid):
        self.grid = grid
        self.height, self.width = grid.shape

    def get_neighbors(self, pos):
        """Get valid neighbors with their weights (distances)"""
        x, y = pos
        neighbors = []

        # 8-connectivity
        for dx in [-1, 0, 1]:
            for dy in [-1, 0, 1]:
                if dx == 0 and dy == 0:
                    continue

                nx, ny = x + dx, y + dy

                # Check bounds
                if 0 <= nx < self.width and 0 <= ny < self.height:
                    # Check if cell is not an obstacle
                    if self.grid[ny, nx] == 0:  # 0 = free space
                        # Calculate distance (diagonal = sqrt(2), orthogonal = 1)
                        if dx != 0 and dy != 0:
                            weight = np.sqrt(2)
                        else:
                            weight = 1.0
                        neighbors.append(((nx, ny), weight))

        return neighbors


# Example usage
def main():
    # Create a grid with obstacles
    grid = np.zeros((10, 10))
    # Add some obstacles
    grid[3:6, 4:6] = 1  # Wall
    grid[1:3, 7:9] = 1  # Another obstacle

    graph = GridGraph(grid)

    start = (1, 1)
    goal = (8, 8)

    path, cost = dijkstra(graph, start, goal)

    if path:
        print(f"Dijkstra path found with cost: {cost:.2f}")
        print(f"Path: {path}")

        # Visualize the result
        plt.figure(figsize=(10, 10))

        # Plot grid
        plt.imshow(grid, cmap='binary', origin='upper')

        # Plot path
        if path:
            path_x, path_y = zip(*path)
            plt.plot(path_y, path_x, 'r-', linewidth=3, label='Dijkstra Path')
            plt.plot(path_y[0], path_x[0], 'go', markersize=10, label='Start')
            plt.plot(path_y[-1], path_x[-1], 'ro', markersize=10, label='Goal')

        plt.title(f'Dijkstra Path Planning\nCost: {cost:.2f}')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.show()
    else:
        print("No path found!")


if __name__ == '__main__':
    main()
```

### A* Algorithm

A* is an extension of Dijkstra's algorithm that uses a heuristic to guide the search toward the goal:

```python
import heapq
import numpy as np
import matplotlib.pyplot as plt


def manhattan_distance(pos1, pos2):
    """Manhattan distance heuristic"""
    return abs(pos1[0] - pos2[0]) + abs(pos1[1] - pos2[1])


def euclidean_distance(pos1, pos2):
    """Euclidean distance heuristic"""
    return np.sqrt((pos1[0] - pos2[0])**2 + (pos1[1] - pos2[1])**2)


def astar(graph, start, goal, heuristic_func=euclidean_distance):
    """
    A* algorithm for shortest path
    graph: Graph object with get_neighbors method
    start: Starting vertex
    goal: Goal vertex
    heuristic_func: Heuristic function h(n) that estimates cost from n to goal
    """
    # Priority queue: (f_score, g_score, vertex)
    pq = [(heuristic_func(start, goal), 0, start)]

    # Costs to reach each vertex (g_scores)
    g_scores = {start: 0}

    # Previous vertex in optimal path
    previous = {start: None}

    visited = set()

    while pq:
        f_score, g_score, current = heapq.heappop(pq)

        if current in visited:
            continue

        visited.add(current)

        # If we reached the goal, reconstruct path
        if current == goal:
            path = []
            curr = goal
            while curr is not None:
                path.append(curr)
                curr = previous[curr]
            return path[::-1], g_score

        # Check neighbors
        for neighbor, weight in graph.get_neighbors(current):
            if neighbor in visited:
                continue

            tentative_g_score = g_score + weight

            if neighbor not in g_scores or tentative_g_score < g_scores[neighbor]:
                g_scores[neighbor] = tentative_g_score
                f_score = tentative_g_score + heuristic_func(neighbor, goal)
                previous[neighbor] = current
                heapq.heappush(pq, (f_score, tentative_g_score, neighbor))

    return None, float('inf')  # No path found


# Example usage with comparison to Dijkstra
def main():
    # Create a grid with obstacles
    grid = np.zeros((15, 15))
    # Add some obstacles
    grid[5:8, 6:8] = 1  # Vertical wall
    grid[2:4, 10:12] = 1  # Horizontal obstacle
    grid[10:12, 3:7] = 1  # Another wall

    graph = GridGraph(grid)

    start = (1, 1)
    goal = (13, 13)

    # Run A* with different heuristics
    path_euc, cost_euc = astar(graph, start, goal, euclidean_distance)
    path_man, cost_man = astar(graph, start, goal, manhattan_distance)

    print(f"A* (Euclidean) path cost: {cost_euc:.2f}")
    print(f"A* (Manhattan) path cost: {cost_man:.2f}")

    # Visualize the results
    fig, axes = plt.subplots(1, 2, figsize=(15, 7))

    # A* with Euclidean heuristic
    axes[0].imshow(grid, cmap='binary', origin='upper')
    if path_euc:
        path_x, path_y = zip(*path_euc)
        axes[0].plot(path_y, path_x, 'r-', linewidth=3, label='A* Path')
        axes[0].plot(path_y[0], path_x[0], 'go', markersize=10, label='Start')
        axes[0].plot(path_y[-1], path_x[-1], 'ro', markersize=10, label='Goal')
    axes[0].set_title(f'A* with Euclidean Heuristic\nCost: {cost_euc:.2f}')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)

    # A* with Manhattan heuristic
    axes[1].imshow(grid, cmap='binary', origin='upper')
    if path_man:
        path_x, path_y = zip(*path_man)
        axes[1].plot(path_y, path_x, 'b-', linewidth=3, label='A* Path')
        axes[1].plot(path_y[0], path_x[0], 'go', markersize=10, label='Start')
        axes[1].plot(path_y[-1], path_x[-1], 'ro', markersize=10, label='Goal')
    axes[1].set_title(f'A* with Manhattan Heuristic\nCost: {cost_man:.2f}')
    axes[1].legend()
    axes[1].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()


if __name__ == '__main__':
    main()
```

### Rapidly-exploring Random Trees (RRT)

RRT is a probabilistically complete motion planning algorithm particularly useful for high-dimensional configuration spaces:

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.spatial.distance import euclidean


class RRT:
    def __init__(self, start, goal, bounds, obstacle_list, max_iter=1000, step_size=0.5):
        """
        Initialize RRT planner
        start: Start configuration
        goal: Goal configuration
        bounds: List of (min, max) for each dimension
        obstacle_list: List of obstacles (for collision checking)
        max_iter: Maximum number of iterations
        step_size: Step size for extending tree
        """
        self.start = np.array(start)
        self.goal = np.array(goal)
        self.bounds = bounds
        self.obstacles = obstacle_list
        self.max_iter = max_iter
        self.step_size = step_size

        # Tree represented as dictionary: child -> parent
        self.tree = {tuple(self.start): None}

        # For visualization
        self.nodes = [self.start.copy()]

    def is_collision_free(self, point):
        """Check if point is collision-free"""
        x, y = point

        # Check bounds
        if not (self.bounds[0][0] <= x <= self.bounds[0][1] and
                self.bounds[1][0] <= y <= self.bounds[1][1]):
            return False

        # Check obstacles (simplified circular obstacles)
        for obs_center, obs_radius in self.obstacles:
            if euclidean(point, obs_center) < obs_radius:
                return False

        return True

    def random_sample(self):
        """Randomly sample a point in configuration space"""
        x = np.random.uniform(self.bounds[0][0], self.bounds[0][1])
        y = np.random.uniform(self.bounds[1][0], self.bounds[1][1])
        return np.array([x, y])

    def nearest_node(self, point):
        """Find nearest node in tree to given point"""
        min_dist = float('inf')
        nearest = None

        for node in self.tree:
            dist = euclidean(point, node)
            if dist < min_dist:
                min_dist = dist
                nearest = np.array(node)

        return nearest

    def extend_toward(self, from_node, to_point):
        """Extend tree from from_node toward to_point by step_size"""
        direction = to_point - from_node
        distance = np.linalg.norm(direction)

        if distance < self.step_size:
            # If close enough, connect directly
            new_point = to_point
        else:
            # Otherwise, extend by step_size in the direction
            direction = direction / distance  # Normalize
            new_point = from_node + direction * self.step_size

        # Check if the new point is collision-free
        if self.is_collision_free(new_point):
            # Also check if the path between from_node and new_point is collision-free
            if self.path_is_collision_free(from_node, new_point):
                return new_point

        return None

    def path_is_collision_free(self, start, end, num_samples=10):
        """Check if path between start and end is collision-free"""
        for i in range(1, num_samples + 1):
            t = i / num_samples
            point = start + t * (end - start)
            if not self.is_collision_free(point):
                return False
        return True

    def connect_to_goal(self, node):
        """Try to connect a node directly to the goal"""
        if self.path_is_collision_free(node, self.goal):
            return True
        return False

    def plan(self):
        """Plan path using RRT"""
        for i in range(self.max_iter):
            # Randomly sample a point
            if np.random.random() < 0.05:  # 5% chance to sample goal
                rand_point = self.goal
            else:
                rand_point = self.random_sample()

            # Find nearest node in tree
            nearest = self.nearest_node(rand_point)

            # Try to extend toward the random point
            new_point = self.extend_toward(nearest, rand_point)

            if new_point is not None:
                # Add new node to tree
                new_tuple = tuple(new_point)
                self.tree[new_tuple] = tuple(nearest)
                self.nodes.append(new_point)

                # Try to connect to goal
                if self.connect_to_goal(new_point):
                    # Goal reached, reconstruct path
                    return self.reconstruct_path(tuple(new_point))

        # If max iterations reached without finding path
        return None

    def reconstruct_path(self, goal_node):
        """Reconstruct path from goal to start"""
        path = []
        current = goal_node

        while current is not None:
            path.append(np.array(current))
            current = self.tree[current]

        return path[::-1]  # Reverse to get start->goal path


# Example usage
def main():
    # Define environment
    bounds = [(-10, 10), (-10, 10)]  # x and y bounds
    obstacles = [
        ((0, 0), 2),      # Circular obstacle at (0,0) with radius 2
        ((5, 5), 1.5),    # Circular obstacle at (5,5) with radius 1.5
        ((-5, -3), 1),    # Circular obstacle at (-5,-3) with radius 1
    ]

    start = np.array([-8, -8])
    goal = np.array([8, 8])

    # Create RRT planner
    rrt = RRT(start, goal, bounds, obstacles, max_iter=2000, step_size=0.5)

    # Plan path
    path = rrt.plan()

    if path:
        print(f"RRT path found with {len(path)} waypoints")

        # Visualize results
        plt.figure(figsize=(12, 10))

        # Plot obstacles
        for obs_center, obs_radius in obstacles:
            circle = plt.Circle(obs_center, obs_radius, color='red', alpha=0.3)
            plt.gca().add_patch(circle)

        # Plot tree nodes
        nodes_array = np.array(rrt.nodes)
        plt.scatter(nodes_array[:, 0], nodes_array[:, 1], s=1, c='lightblue', alpha=0.5, label='Tree Nodes')

        # Plot tree edges
        for child, parent in rrt.tree.items():
            if parent is not None:
                plt.plot([parent[0], child[0]], [parent[1], child[1]], 'lightblue', alpha=0.3, linewidth=0.5)

        # Plot path
        if path:
            path_array = np.array(path)
            plt.plot(path_array[:, 0], path_array[:, 1], 'r-', linewidth=2, label='RRT Path')
            plt.scatter(path_array[0, 0], path_array[0, 1], c='green', s=100, zorder=5, label='Start')
            plt.scatter(path_array[-1, 0], path_array[-1, 1], c='red', s=100, zorder=5, label='Goal')

        plt.xlim(bounds[0][0]-1, bounds[0][1]+1)
        plt.ylim(bounds[1][0]-1, bounds[1][1]+1)
        plt.title('RRT Path Planning')
        plt.xlabel('X')
        plt.ylabel('Y')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.axis('equal')
        plt.show()
    else:
        print("No path found with RRT!")


if __name__ == '__main__':
    main()
```

## Sampling-Based Motion Planning

### Probabilistic Roadmaps (PRM)

Probabilistic Roadmaps precompute a roadmap of the free space and use it for path planning:

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.spatial import cKDTree
from collections import defaultdict


class PRM:
    def __init__(self, bounds, obstacles, num_samples=500, connection_radius=1.5):
        """
        Initialize Probabilistic Roadmap
        bounds: List of (min, max) for each dimension
        obstacles: List of obstacles for collision checking
        num_samples: Number of random samples to generate
        connection_radius: Maximum distance to connect nodes
        """
        self.bounds = bounds
        self.obstacles = obstacles
        self.num_samples = num_samples
        self.connection_radius = connection_radius

        # Graph representation
        self.graph = defaultdict(list)  # node -> [neighbors]
        self.nodes = []  # List of all nodes

        # Build the roadmap
        self.build_roadmap()

    def is_collision_free(self, point):
        """Check if point is collision-free"""
        x, y = point

        # Check bounds
        if not (self.bounds[0][0] <= x <= self.bounds[0][1] and
                self.bounds[1][0] <= y <= self.bounds[1][1]):
            return False

        # Check obstacles
        for obs_center, obs_radius in self.obstacles:
            if np.linalg.norm(point - obs_center) < obs_radius:
                return False

        return True

    def build_roadmap(self):
        """Build the probabilistic roadmap"""
        # Sample random configurations
        valid_configs = []

        while len(valid_configs) < self.num_samples:
            # Sample random point
            x = np.random.uniform(self.bounds[0][0], self.bounds[0][1])
            y = np.random.uniform(self.bounds[1][0], self.bounds[1][1])
            point = np.array([x, y])

            if self.is_collision_free(point):
                valid_configs.append(point)

        self.nodes = valid_configs

        # Create KD-tree for efficient neighbor lookup
        if self.nodes:
            nodes_array = np.array(self.nodes)
            self.kdtree = cKDTree(nodes_array)

            # Connect nearby nodes
            for i, node in enumerate(self.nodes):
                # Find all nodes within connection radius
                indices = self.kdtree.query_ball_point(node, self.connection_radius)

                for j in indices:
                    if i != j:  # Don't connect to self
                        neighbor = self.nodes[j]

                        # Check if path between nodes is collision-free
                        if self.path_is_collision_free(node, neighbor):
                            # Add edge to graph (bidirectional)
                            self.graph[i].append(j)
                            self.graph[j].append(i)

    def path_is_collision_free(self, start, end, num_samples=10):
        """Check if path between start and end is collision-free"""
        for i in range(1, num_samples + 1):
            t = i / num_samples
            point = start + t * (end - start)
            if not self.is_collision_free(point):
                return False
        return True

    def find_path(self, start, goal):
        """
        Find path between start and goal using the roadmap
        start, goal: Start and goal configurations
        """
        # Find nearest nodes in roadmap to start and goal
        start_idx = self.find_nearest_node(start)
        goal_idx = self.find_nearest_node(goal)

        # Add start and goal temporarily to graph
        temp_graph = self.graph.copy()
        temp_nodes = self.nodes[:]

        # Connect start to nearby roadmap nodes
        start_connections = []
        start_neighbors = self.kdtree.query_ball_point(start, self.connection_radius)
        for neighbor_idx in start_neighbors:
            neighbor = self.nodes[neighbor_idx]
            if self.path_is_collision_free(start, neighbor):
                start_connections.append(neighbor_idx)

        # Connect goal to nearby roadmap nodes
        goal_connections = []
        goal_neighbors = self.kdtree.query_ball_point(goal, self.connection_radius)
        for neighbor_idx in goal_neighbors:
            neighbor = self.nodes[neighbor_idx]
            if self.path_is_collision_free(goal, neighbor):
                goal_connections.append(neighbor_idx)

        # Use A* to find path through the roadmap
        path_indices = self.astar_search(temp_graph, start_idx, goal_idx, start_connections, goal_connections)

        if path_indices:
            # Convert indices back to coordinates
            path = [start]  # Start with actual start
            for idx in path_indices[1:-1]:  # Skip first (duplicate) and last (will add goal separately)
                path.append(temp_nodes[idx])
            path.append(goal)  # End with actual goal
            return path
        else:
            return None

    def find_nearest_node(self, point):
        """Find index of nearest node to given point"""
        if not self.nodes:
            return -1

        distances, indices = self.kdtree.query(point, k=1)
        return indices

    def astar_search(self, graph, start_idx, goal_idx, start_connections, goal_connections):
        """A* search on the graph"""
        import heapq

        # Priority queue: (f_score, g_score, node_idx)
        pq = [(0, 0, start_idx)]

        g_scores = {start_idx: 0}
        previous = {start_idx: None}
        visited = set()

        while pq:
            f_score, g_score, current_idx = heapq.heappop(pq)

            if current_idx in visited:
                continue

            visited.add(current_idx)

            # Check if we reached the goal
            if current_idx == goal_idx:
                # Reconstruct path
                path = []
                curr_idx = goal_idx
                while curr_idx is not None:
                    path.append(curr_idx)
                    curr_idx = previous[curr_idx]
                return path[::-1]

            # Get neighbors
            neighbors = graph[current_idx][:]

            # If current node is the start, add connections to start_connections
            if current_idx == start_idx:
                neighbors.extend(start_connections)
            # If current node is in goal_connections, add goal as neighbor
            elif current_idx in goal_connections:
                neighbors.append(goal_idx)

            for neighbor_idx in neighbors:
                if neighbor_idx in visited:
                    continue

                # Calculate distance
                if neighbor_idx == goal_idx:
                    # Distance to actual goal
                    dist = np.linalg.norm(self.nodes[current_idx] - goal)
                elif current_idx == start_idx and neighbor_idx in start_connections:
                    # Distance from start to roadmap
                    dist = np.linalg.norm(start - self.nodes[neighbor_idx])
                else:
                    # Distance between roadmap nodes
                    dist = np.linalg.norm(self.nodes[current_idx] - self.nodes[neighbor_idx])

                tentative_g_score = g_score + dist

                if neighbor_idx not in g_scores or tentative_g_score < g_scores[neighbor_idx]:
                    g_scores[neighbor_idx] = tentative_g_score
                    # Heuristic: straight-line distance to goal
                    if neighbor_idx == goal_idx:
                        heuristic = 0
                    else:
                        heuristic = np.linalg.norm(self.nodes[neighbor_idx] - goal)

                    f_score = tentative_g_score + heuristic
                    previous[neighbor_idx] = current_idx
                    heapq.heappush(pq, (f_score, tentative_g_score, neighbor_idx))

        return None  # No path found


# Example usage
def main():
    # Define environment
    bounds = [(-10, 10), (-10, 10)]
    obstacles = [
        ((0, 0), 2),      # Circular obstacle at (0,0) with radius 2
        ((5, 5), 1.5),    # Circular obstacle at (5,5) with radius 1.5
        ((-5, -3), 1),    # Circular obstacle at (-5,-3) with radius 1
        ((3, -4), 1.2),   # Another obstacle
    ]

    start = np.array([-8, -8])
    goal = np.array([8, 8])

    # Create PRM
    prm = PRM(bounds, obstacles, num_samples=300, connection_radius=2.0)

    # Find path
    path = prm.find_path(start, goal)

    if path:
        print(f"PRM path found with {len(path)} waypoints")

        # Visualize results
        plt.figure(figsize=(12, 10))

        # Plot obstacles
        for obs_center, obs_radius in obstacles:
            circle = plt.Circle(obs_center, obs_radius, color='red', alpha=0.3)
            plt.gca().add_patch(circle)

        # Plot roadmap nodes
        nodes_array = np.array(prm.nodes)
        plt.scatter(nodes_array[:, 0], nodes_array[:, 1], s=1, c='lightblue', alpha=0.5, label='Roadmap Nodes')

        # Plot roadmap edges (sample some for visualization)
        plotted_edges = 0
        max_edges = 500  # Limit for visualization
        for node_idx, neighbors in prm.graph.items():
            for neighbor_idx in neighbors:
                if plotted_edges >= max_edges:
                    break
                plt.plot([prm.nodes[node_idx][0], prm.nodes[neighbor_idx][0]],
                        [prm.nodes[node_idx][1], prm.nodes[neighbor_idx][1]],
                        'lightblue', alpha=0.2, linewidth=0.5)
                plotted_edges += 1
            if plotted_edges >= max_edges:
                break

        # Plot path
        if path:
            path_array = np.array(path)
            plt.plot(path_array[:, 0], path_array[:, 1], 'r-', linewidth=2, label='PRM Path')
            plt.scatter(path_array[0, 0], path_array[0, 1], c='green', s=100, zorder=5, label='Start')
            plt.scatter(path_array[-1, 0], path_array[-1, 1], c='red', s=100, zorder=5, label='Goal')

        plt.xlim(bounds[0][0]-1, bounds[0][1]+1)
        plt.ylim(bounds[1][0]-1, bounds[1][1]+1)
        plt.title('Probabilistic Roadmap (PRM) Path Planning')
        plt.xlabel('X')
        plt.ylabel('Y')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.axis('equal')
        plt.show()
    else:
        print("No path found with PRM!")


if __name__ == '__main__':
    main()
```

## Navigation and Path Following

### Pure Pursuit Path Following

Pure pursuit is a classic path following algorithm that works well for car-like robots:

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.spatial.distance import cdist


class PurePursuitController:
    def __init__(self, lookahead_distance=1.0):
        """
        Initialize pure pursuit controller
        lookahead_distance: Distance ahead to look for target point
        """
        self.lookahead_distance = lookahead_distance
        self.path = None
        self.current_path_index = 0

    def set_path(self, path):
        """Set the path to follow"""
        self.path = np.array(path)
        self.current_path_index = 0

    def find_lookahead_point(self, robot_pos):
        """Find the point on the path that is lookahead_distance ahead"""
        if self.path is None or len(self.path) == 0:
            return None

        # Start searching from current path index
        for i in range(self.current_path_index, len(self.path)):
            path_point = self.path[i]
            distance = np.linalg.norm(robot_pos - path_point)

            if distance >= self.lookahead_distance:
                # Update current path index for efficiency
                self.current_path_index = max(0, i - 1)
                return path_point

        # If no point is far enough, return the last point
        self.current_path_index = len(self.path) - 1
        return self.path[-1]

    def calculate_steering_angle(self, robot_pos, robot_heading, lookahead_point):
        """
        Calculate steering angle for pure pursuit
        robot_pos: Current robot position [x, y]
        robot_heading: Current robot heading (radians)
        lookahead_point: Target point to pursue
        """
        if lookahead_point is None:
            return 0.0

        # Vector from robot to lookahead point
        dx = lookahead_point[0] - robot_pos[0]
        dy = lookahead_point[1] - robot_pos[1]

        # Transform to robot's frame
        local_x = dx * np.cos(robot_heading) + dy * np.sin(robot_heading)
        local_y = -dx * np.sin(robot_heading) + dy * np.cos(robot_heading)

        # Calculate curvature (steering angle)
        # kappa = 2 * local_y / lookahead_distance^2
        if self.lookahead_distance > 0:
            curvature = 2 * local_y / (self.lookahead_distance ** 2)
            steering_angle = np.arctan(curvature * self.lookahead_distance)
        else:
            steering_angle = 0.0

        return steering_angle

    def follow_path(self, robot_pos, robot_heading, target_speed=1.0):
        """
        Follow the path and return control commands
        Returns: (steering_angle, linear_velocity)
        """
        lookahead_point = self.find_lookahead_point(robot_pos)
        steering_angle = self.calculate_steering_angle(robot_pos, robot_heading, lookahead_point)

        # Adjust speed based on curvature
        curvature = abs(steering_angle) / self.lookahead_distance if self.lookahead_distance > 0 else 0
        adjusted_speed = target_speed / (1 + 2 * curvature)  # Reduce speed in sharp turns

        return steering_angle, adjusted_speed


# Example simulation
def simulate_path_following():
    """Simulate a robot following a path using pure pursuit"""
    # Create a curved path
    t = np.linspace(0, 4*np.pi, 100)
    path_x = t * 0.5
    path_y = 2 * np.sin(t * 0.5)
    path = np.column_stack([path_x, path_y])

    # Initialize controller
    controller = PurePursuitController(lookahead_distance=1.5)
    controller.set_path(path)

    # Robot initial state
    robot_pos = np.array([0.0, 0.0])
    robot_heading = 0.0  # Robot initially facing along x-axis
    robot_speed = 0.0

    # Simulation parameters
    dt = 0.1
    simulation_time = 30.0
    t_sim = np.arange(0, simulation_time, dt)

    # Storage for robot trajectory
    robot_trajectory = [robot_pos.copy()]

    for t in t_sim:
        # Get control commands
        steering_angle, target_speed = controller.follow_path(robot_pos, robot_heading, target_speed=1.0)

        # Simple bicycle model for robot motion
        # Update robot state
        robot_speed = target_speed  # In this simple model, actual speed = target speed

        # Update position and heading
        robot_heading += (robot_speed * np.tan(steering_angle) / 2.0) * dt  # Simplified kinematic model
        robot_pos[0] += robot_speed * np.cos(robot_heading) * dt
        robot_pos[1] += robot_speed * np.sin(robot_heading) * dt

        # Store robot position
        robot_trajectory.append(robot_pos.copy())

    # Convert to array
    robot_trajectory = np.array(robot_trajectory)

    # Visualization
    plt.figure(figsize=(12, 8))

    # Plot path and robot trajectory
    plt.plot(path[:, 0], path[:, 1], 'b-', linewidth=2, label='Desired Path')
    plt.plot(robot_trajectory[:, 0], robot_trajectory[:, 1], 'r-', linewidth=2, label='Robot Trajectory')
    plt.scatter(robot_trajectory[::10, 0], robot_trajectory[::10, 1], c='red', s=20, label='Robot Position', zorder=5)

    # Add start and end markers
    plt.scatter(path[0, 0], path[0, 1], c='green', s=100, marker='o', label='Path Start', zorder=5)
    plt.scatter(path[-1, 0], path[-1, 1], c='red', s=100, marker='s', label='Path End', zorder=5)
    plt.scatter(robot_trajectory[0, 0], robot_trajectory[0, 1], c='purple', s=100, marker='^', label='Robot Start', zorder=5)

    plt.title('Pure Pursuit Path Following')
    plt.xlabel('X')
    plt.ylabel('Y')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.axis('equal')
    plt.show()

    print(f"Path following completed. Robot traveled {len(robot_trajectory)} steps.")
    print(f"Final robot position: ({robot_trajectory[-1, 0]:.2f}, {robot_trajectory[-1, 1]:.2f})")
    print(f"Final path point: ({path[-1, 0]:.2f}, {path[-1, 1]:.2f})")
    print(f"Final distance from path end: {np.linalg.norm(robot_trajectory[-1] - path[-1]):.2f}")


if __name__ == '__main__':
    simulate_path_following()
```

### Dynamic Window Approach (DWA)

DWA is a local path planning algorithm that considers robot dynamics:

```python
import numpy as np
import matplotlib.pyplot as plt


class DynamicWindowApproach:
    def __init__(self, max_speed=1.0, min_speed=0.0, max_yawrate=40.0*np.pi/180.0,
                 max_accel=0.5, max_dyawrate=40.0*np.pi/180.0,
                 v_resolution=0.05, yawrate_resolution=0.1*np.pi/180.0,
                 dt=0.1, predict_time=3.0, to_goal_cost_gain=0.15, speed_cost_gain=1.0):
        """
        Initialize Dynamic Window Approach planner
        """
        self.max_speed = max_speed
        self.min_speed = min_speed
        self.max_yawrate = max_yawrate
        self.max_accel = max_accel
        self.max_dyawrate = max_dyawrate
        self.v_resolution = v_resolution
        self.yawrate_resolution = yawrate_resolution
        self.dt = dt
        self.predict_time = predict_time
        self.to_goal_cost_gain = to_goal_cost_gain
        self.speed_cost_gain = speed_cost_gain

    def motion(self, x, u, dt):
        """
        Motion model: update robot state
        x: [x, y, yaw, v, omega]
        u: [v, omega]
        """
        x[0] += u[0] * np.cos(x[2]) * dt
        x[1] += u[0] * np.sin(x[2]) * dt
        x[2] += u[1] * dt
        x[3] = u[0]
        x[4] = u[1]
        return x

    def calc_dynamic_window(self, x):
        """
        Calculate dynamic window
        x: current state [x, y, yaw, v, omega]
        """
        # Dynamic window from robot specification
        vs = [self.min_speed, self.max_speed,
              -self.max_yawrate, self.max_yawrate]

        # Dynamic window from motion model
        vd = [x[3] - self.max_accel * self.dt,
              x[3] + self.max_accel * self.dt,
              x[4] - self.max_dyawrate * self.dt,
              x[4] + self.max_dyawrate * self.dt]

        # Minimum window
        dw = [max(vs[0], vd[0]), min(vs[1], vd[1]),
              max(vs[2], vd[2]), min(vs[3], vd[3])]
        return dw

    def calc_trajectory(self, x_init, v, omega):
        """
        Calculate trajectory with given velocity and angular velocity
        """
        x = x_init.copy()
        trajectory = np.array(x)

        time = 0
        while time <= self.predict_time:
            x = self.motion(x, [v, omega], self.dt)
            trajectory = np.vstack((trajectory, x))
            time += self.dt

        return trajectory

    def calc_to_goal_cost(self, trajectory, goal):
        """
        Calculate cost to goal
        """
        dx = goal[0] - trajectory[-1, 0]
        dy = goal[1] - trajectory[-1, 1]
        error_angle = np.arctan2(dy, dx)
        cost_angle = error_angle - trajectory[-1, 2]
        cost = abs(np.arctan2(np.sin(cost_angle), np.cos(cost_angle)))

        return self.to_goal_cost_gain * cost

    def calc_obstacle_cost(self, trajectory, ob):
        """
        Calculate cost of obstacle proximity
        """
        min_r = float("inf")
        for ii in range(len(trajectory)):
            for i in range(len(ob)):
                ox = ob[i, 0]
                oy = ob[i, 1]
                dx = trajectory[ii, 0] - ox
                dy = trajectory[ii, 1] - oy
                r = np.sqrt(dx**2 + dy**2)
                if r <= min_r:
                    min_r = r

        return 1.0 / min_r if min_r != 0 else float("inf")

    def calc_control_and_trajectory(self, x, dw, goal, ob):
        """
        Calculate the best control command and trajectory
        """
        x_init = x.copy()
        min_cost = float("inf")
        best_u = [0.0, 0.0]
        best_trajectory = np.array([x])

        # Evaluate all trajectory candidates
        v = dw[0]
        while v <= dw[1]:
            omega = dw[2]
            while omega <= dw[3]:
                # Calculate candidate trajectory
                trajectory = self.calc_trajectory(x_init, v, omega)

                # Calculate all costs
                to_goal_cost = self.calc_to_goal_cost(trajectory, goal)
                speed_cost = self.speed_cost_gain * (self.max_speed - trajectory[-1, 3])
                ob_cost = self.calc_obstacle_cost(trajectory, ob)

                # Total cost
                final_cost = to_goal_cost + speed_cost + ob_cost

                # Search minimum trajectory
                if min_cost >= final_cost:
                    min_cost = final_cost
                    best_u = [v, omega]
                    best_trajectory = trajectory

                omega += self.yawrate_resolution
            v += self.v_resolution

        return best_u, best_trajectory

    def plan(self, x, goal, ob):
        """
        Plan path using Dynamic Window Approach
        x: initial state [x, y, yaw, v, omega]
        goal: goal position [x, y]
        ob: obstacle positions [[x1, y1], [x2, y2], ...]
        """
        # Calculate dynamic window
        dw = self.calc_dynamic_window(x)

        # Calculate control command and trajectory
        u, trajectory = self.calc_control_and_trajectory(x, dw, goal, ob)

        return u, trajectory


# Example usage
def main():
    # Initialize DWA
    dwa = DynamicWindowApproach()

    # Initial state [x, y, yaw, v, omega]
    x = np.array([0.0, 0.0, 0.0, 0.0, 0.0])

    # Goal position
    goal = np.array([10.0, 10.0])

    # Obstacles [[x1, y1], [x2, y2], ...]
    ob = np.array([
        [1, 2],
        [5, 3],
        [8, 4],
        [2, 7],
        [3, 8],
        [7, 9]
    ])

    # Simulation parameters
    dt = 0.1
    simulation_time = 50.0
    time = 0

    # Storage for trajectory
    trajectory = np.array(x)

    print("Starting DWA simulation...")
    while time <= simulation_time:
        # Plan using DWA
        u, predicted_trajectory = dwa.plan(x, goal, ob)

        # Update state
        x = dwa.motion(x, u, dt)

        # Store state
        trajectory = np.vstack((trajectory, x))

        # Check goal
        dist_to_goal = np.sqrt((x[0] - goal[0])**2 + (x[1] - goal[1])**2)
        if dist_to_goal <= 1.0:  # Goal reached
            print("Goal reached!")
            break

        time += dt

    # Visualization
    plt.figure(figsize=(12, 10))

    # Plot obstacles
    plt.plot(ob[:, 0], ob[:, 1], 'ko', markersize=10, label='Obstacles')

    # Plot goal
    plt.plot(goal[0], goal[1], 'ro', markersize=15, label='Goal')

    # Plot trajectory
    plt.plot(trajectory[:, 0], trajectory[:, 1], 'b-', linewidth=2, label='Robot Trajectory')
    plt.plot(trajectory[0, 0], trajectory[0, 1], 'go', markersize=10, label='Start')

    # Add arrow to show robot orientation
    for i in range(0, len(trajectory), 10):  # Every 10th point
        x_pos, y_pos, yaw = trajectory[i, 0], trajectory[i, 1], trajectory[i, 2]
        dx = 0.5 * np.cos(yaw)
        dy = 0.5 * np.sin(yaw)
        plt.arrow(x_pos, y_pos, dx, dy, head_width=0.2, head_length=0.2, fc='blue', ec='blue')

    plt.title('Dynamic Window Approach (DWA) Path Planning')
    plt.xlabel('X')
    plt.ylabel('Y')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.axis('equal')
    plt.show()

    print(f"Simulation completed. Robot traveled {len(trajectory)} steps.")
    print(f"Final robot position: ({x[0]:.2f}, {x[1]:.2f})")
    print(f"Goal position: ({goal[0]:.2f}, {goal[1]:.2f})")
    print(f"Final distance from goal: {dist_to_goal:.2f}")


if __name__ == '__main__':
    main()
```

## ROS 2 Integration for Navigation

### Navigation Stack Components

```python
import rclpy
from rclpy.node import Node
from geometry_msgs.msg import PoseStamped, Point
from nav_msgs.msg import Path, OccupancyGrid
from sensor_msgs.msg import LaserScan
from geometry_msgs.msg import Twist
from tf2_ros import TransformException
from tf2_ros.buffer import Buffer
from tf2_ros.transform_listener import TransformListener
import numpy as np


class NavigationController(Node):
    def __init__(self):
        super().__init__('navigation_controller')

        # Publishers
        self.cmd_vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)
        self.global_plan_pub = self.create_publisher(Path, '/global_plan', 10)
        self.local_plan_pub = self.create_publisher(Path, '/local_plan', 10)

        # Subscribers
        self.goal_sub = self.create_subscription(
            PoseStamped,
            '/move_base_simple/goal',
            self.goal_callback,
            10
        )

        self.odom_sub = self.create_subscription(
            Odometry,
            '/odom',
            self.odom_callback,
            10
        )

        self.scan_sub = self.create_subscription(
            LaserScan,
            '/scan',
            self.scan_callback,
            10
        )

        self.map_sub = self.create_subscription(
            OccupancyGrid,
            '/map',
            self.map_callback,
            10
        )

        # TF listener
        self.tf_buffer = Buffer()
        self.tf_listener = TransformListener(self.tf_buffer, self)

        # Navigation state
        self.current_goal = None
        self.current_position = None
        self.current_yaw = 0.0
        self.map_data = None
        self.scan_data = None
        self.global_path = []
        self.local_path = []

        # Navigation parameters
        self.linear_speed = 0.5
        self.angular_speed = 0.5
        self.arrival_threshold = 0.5  # meters
        self.rotation_threshold = 0.1  # radians

        # Timer for navigation control
        self.nav_timer = self.create_timer(0.1, self.navigation_control)

        self.get_logger().info('Navigation controller initialized')

    def goal_callback(self, msg):
        """Handle new goal"""
        self.current_goal = msg.pose
        self.get_logger().info(f'New goal received: ({msg.pose.position.x:.2f}, {msg.pose.position.y:.2f})')

        # Plan global path
        if self.map_data and self.current_position:
            self.plan_global_path()

    def odom_callback(self, msg):
        """Update current position from odometry"""
        self.current_position = (msg.pose.pose.position.x, msg.pose.pose.position.y)

        # Extract yaw from quaternion
        orientation = msg.pose.pose.orientation
        self.current_yaw = self.quaternion_to_yaw(orientation)

    def scan_callback(self, msg):
        """Update laser scan data"""
        self.scan_data = msg

    def map_callback(self, msg):
        """Update map data"""
        self.map_data = msg
        self.get_logger().info(f'Map received: {msg.info.width}x{msg.info.height} at {msg.info.resolution:.2f}m/cell')

    def quaternion_to_yaw(self, orientation):
        """Convert quaternion to yaw angle"""
        import math
        siny_cosp = 2 * (orientation.w * orientation.z + orientation.x * orientation.y)
        cosy_cosp = 1 - 2 * (orientation.y * orientation.y + orientation.z * orientation.z)
        return math.atan2(siny_cosp, cosy_cosp)

    def plan_global_path(self):
        """Plan global path using the map"""
        if not self.current_position or not self.current_goal:
            return

        start = self.current_position
        goal = (self.current_goal.position.x, self.current_goal.position.y)

        # For demonstration, we'll create a simple path
        # In practice, you'd use A*, Dijkstra, or other path planning algorithms
        path = self.create_straight_line_path(start, goal)

        # Publish global plan
        self.publish_path(path, self.global_plan_pub, 'map', 'global_plan')
        self.global_path = path

    def create_straight_line_path(self, start, goal):
        """Create a straight-line path for demonstration"""
        path = []
        steps = max(int(np.linalg.norm(np.array(goal) - np.array(start)) / 0.5), 10)

        for i in range(steps + 1):
            t = i / steps
            x = start[0] + t * (goal[0] - start[0])
            y = start[1] + t * (goal[1] - start[1])
            path.append((x, y))

        return path

    def publish_path(self, path, publisher, frame_id, path_name):
        """Publish path to ROS"""
        path_msg = Path()
        path_msg.header.frame_id = frame_id
        path_msg.header.stamp = self.get_clock().now().to_msg()

        for point in path:
            pose = PoseStamped()
            pose.pose.position.x = point[0]
            pose.pose.position.y = point[1]
            pose.pose.position.z = 0.0
            path_msg.poses.append(pose)

        publisher.publish(path_msg)

    def navigation_control(self):
        """Main navigation control loop"""
        if not self.current_position or not self.current_goal:
            return

        # Check if we have a path to follow
        if self.global_path:
            # Get next waypoint from global path
            target_point = self.get_next_waypoint()

            if target_point:
                # Calculate control commands
                cmd_vel = self.calculate_control(target_point)

                # Publish command
                self.cmd_vel_pub.publish(cmd_vel)

                # Check if we've reached the goal
                dist_to_goal = np.linalg.norm(
                    np.array(self.current_position) -
                    np.array([self.current_goal.position.x, self.current_goal.position.y])
                )

                if dist_to_goal < self.arrival_threshold:
                    self.get_logger().info('Goal reached!')
                    self.stop_robot()
                    self.current_goal = None
                    self.global_path = []

    def get_next_waypoint(self):
        """Get the next waypoint to follow"""
        if not self.global_path:
            return None

        # For simplicity, follow the path in order
        # In practice, you'd use a more sophisticated approach like pure pursuit
        if len(self.global_path) > 0:
            return self.global_path[0]  # First point in path
        return None

    def calculate_control(self, target_point):
        """Calculate control commands to reach target point"""
        cmd_vel = Twist()

        if not self.current_position:
            return cmd_vel

        # Calculate vector to target
        dx = target_point[0] - self.current_position[0]
        dy = target_point[1] - self.current_position[1]

        # Calculate distance and angle to target
        distance = np.sqrt(dx**2 + dy**2)
        target_angle = np.arctan2(dy, dx)

        # Calculate angle difference
        angle_diff = target_angle - self.current_yaw
        # Normalize angle to [-pi, pi]
        angle_diff = np.arctan2(np.sin(angle_diff), np.cos(angle_diff))

        # Simple proportional controller
        if distance > self.arrival_threshold:
            cmd_vel.linear.x = min(self.linear_speed, distance * 0.5)  # Scale speed with distance
            cmd_vel.angular.z = max(-self.angular_speed, min(self.angular_speed, angle_diff * 2.0))
        else:
            # Reached waypoint, move to next
            if len(self.global_path) > 0:
                self.global_path.pop(0)  # Remove current waypoint

        return cmd_vel

    def stop_robot(self):
        """Stop the robot"""
        cmd_vel = Twist()
        self.cmd_vel_pub.publish(cmd_vel)


def main(args=None):
    rclpy.init(args=args)
    nav_controller = NavigationController()

    try:
        rclpy.spin(nav_controller)
    except KeyboardInterrupt:
        pass
    finally:
        nav_controller.destroy_node()
        rclpy.shutdown()


if __name__ == '__main__':
    main()
```

## Hands-On Lab: Autonomous Navigation System

### Objective
Create a complete autonomous navigation system that integrates global path planning, local path planning, and obstacle avoidance.

### Prerequisites
- Completed Chapter 1-9
- ROS 2 Humble with Navigation2 stack
- Basic understanding of motion planning and control

### Steps

1. **Create a navigation lab package**:
   ```bash
   cd ~/robotics_ws/src
   ros2 pkg create --build-type ament_python navigation_lab --dependencies rclpy geometry_msgs nav_msgs sensor_msgs tf2_ros std_msgs numpy scipy matplotlib
   ```

2. **Create the main navigation node** (`navigation_lab/navigation_lab/navigation_node.py`):
   ```python
   #!/usr/bin/env python3
   import rclpy
   from rclpy.node import Node
   from geometry_msgs.msg import PoseStamped, Twist
   from nav_msgs.msg import Path, OccupancyGrid
   from sensor_msgs.msg import LaserScan
   from visualization_msgs.msg import Marker, MarkerArray
   from std_msgs.msg import Bool
   import numpy as np
   import math


   class NavigationLabNode(Node):
       def __init__(self):
           super().__init__('navigation_lab_node')

           # Publishers
           self.cmd_vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)
           self.global_plan_pub = self.create_publisher(Path, '/global_plan', 10)
           self.local_plan_pub = self.create_publisher(Path, '/local_plan', 10)
           self.obstacle_markers_pub = self.create_publisher(MarkerArray, '/obstacle_markers', 10)
           self.status_pub = self.create_publisher(Bool, '/navigation_active', 10)

           # Subscribers
           self.goal_sub = self.create_subscription(
               PoseStamped,
               '/goal_pose',
               self.goal_callback,
               10
           )

           self.odom_sub = self.create_subscription(
               Odometry,
               '/odom',
               self.odom_callback,
               10
           )

           self.scan_sub = self.create_subscription(
               LaserScan,
               '/scan',
               self.scan_callback,
               10
           )

           # Navigation state
           self.current_goal = None
           self.current_position = np.array([0.0, 0.0])
           self.current_yaw = 0.0
           self.scan_data = None
           self.navigation_active = False

           # Navigation parameters
           self.linear_speed = 0.5
           self.angular_speed = 0.5
           self.arrival_threshold = 0.5
           self.safe_distance = 0.8
           self.lookahead_distance = 1.0

           # Path planning
           self.global_path = []
           self.local_path = []
           self.path_index = 0

           # Controllers
           self.pure_pursuit = PurePursuitController(lookahead_distance=self.lookahead_distance)
           self.dwa_planner = DynamicWindowApproach()

           # Timer for navigation control
           self.nav_timer = self.create_timer(0.1, self.navigation_control)

           self.get_logger().info('Navigation lab node initialized')

       def goal_callback(self, msg):
           """Handle new goal"""
           goal_pos = np.array([msg.pose.position.x, msg.pose.position.y])
           self.current_goal = goal_pos
           self.get_logger().info(f'New goal received: ({goal_pos[0]:.2f}, {goal_pos[1]:.2f})')

           # Plan global path
           if self.current_position is not None:
               self.plan_global_path()

       def odom_callback(self, msg):
           """Update current position from odometry"""
           self.current_position[0] = msg.pose.pose.position.x
           self.current_position[1] = msg.pose.pose.position.y

           # Extract yaw from quaternion
           orientation = msg.pose.pose.orientation
           siny_cosp = 2 * (orientation.w * orientation.z + orientation.x * orientation.y)
           cosy_cosp = 1 - 2 * (orientation.y * orientation.y + orientation.z * orientation.z)
           self.current_yaw = math.atan2(siny_cosp, cosy_cosp)

       def scan_callback(self, msg):
           """Update laser scan data"""
           self.scan_data = msg

       def plan_global_path(self):
           """Plan global path to goal"""
           if self.current_goal is None:
               return

           # For this lab, create a simple path (in practice, use A*, RRT, etc.)
           start = self.current_position.copy()
           goal = self.current_goal.copy()

           # Create path points
           steps = max(int(np.linalg.norm(goal - start) / 0.5), 10)
           self.global_path = []

           for i in range(steps + 1):
               t = i / steps
               point = start + t * (goal - start)
               self.global_path.append(point)

           self.path_index = 0
           self.navigation_active = True

           # Publish global path
           self.publish_path(self.global_path, self.global_plan_pub, 'odom', 'global_plan')
           self.get_logger().info(f'Global path planned with {len(self.global_path)} waypoints')

       def navigation_control(self):
           """Main navigation control loop"""
           if not self.navigation_active or self.current_goal is None:
               self.publish_status(False)
               return

           # Check if goal is reached
           dist_to_goal = np.linalg.norm(self.current_position - self.current_goal)
           if dist_to_goal < self.arrival_threshold:
               self.get_logger().info('Goal reached!')
               self.stop_robot()
               self.navigation_active = False
               self.publish_status(False)
               return

           # Get local path based on global path and obstacles
           local_path = self.generate_local_path()
           self.local_path = local_path

           # Publish local path
           if local_path:
               self.publish_path(local_path, self.local_plan_pub, 'odom', 'local_plan')

           # Calculate control commands
           cmd_vel = self.calculate_control(local_path)
           self.cmd_vel_pub.publish(cmd_vel)

           # Publish navigation status
           self.publish_status(True)

       def generate_local_path(self):
           """Generate local path considering obstacles"""
           if not self.global_path or self.path_index >= len(self.global_path):
               return []

           # In a real system, this would use local planners like DWA or TEB
           # For this lab, we'll create a simple local path from current global path
           local_path = []
           start_idx = self.path_index

           # Take next few points from global path
           for i in range(start_idx, min(start_idx + 10, len(self.global_path))):
               local_path.append(self.global_path[i])

           # If we have scan data, consider obstacle avoidance
           if self.scan_data:
               # Convert laser scan to obstacle points
               obstacles = self.scan_to_obstacles()

               # Simple obstacle avoidance: if obstacle is close, adjust path
               if self.has_close_obstacle():
                   # Create detour path (simplified)
                   current_pos = self.current_position
                   goal_pos = self.global_path[min(self.path_index + 5, len(self.global_path)-1)] if self.global_path else self.current_goal

                   # Calculate detour point to the side
                   to_goal = goal_pos - current_pos
                   perpendicular = np.array([-to_goal[1], to_goal[0]])  # Perpendicular vector
                   perpendicular = perpendicular / np.linalg.norm(perpendicular) * 1.0  # Normalize and scale

                   detour_point = current_pos + perpendicular
                   local_path = [current_pos, detour_point, goal_pos]

           return local_path

       def scan_to_obstacles(self):
           """Convert laser scan to obstacle points"""
           if not self.scan_data:
               return []

           obstacles = []
           angle = self.scan_data.angle_min

           for range_val in self.scan_data.ranges:
               if not math.isnan(range_val) and range_val < 3.0:  # Valid range and within 3m
                   x = range_val * math.cos(angle)
                   y = range_val * math.sin(angle)
                   obstacles.append(np.array([x, y]))
               angle += self.scan_data.angle_increment

           return obstacles

       def has_close_obstacle(self):
           """Check if there are close obstacles"""
           if not self.scan_data:
               return False

           # Check for obstacles within safe distance
           close_obstacles = [r for r in self.scan_data.ranges
                             if not math.isnan(r) and r < self.safe_distance]
           return len(close_obstacles) > 0

       def calculate_control(self, local_path):
           """Calculate control commands to follow path"""
           cmd_vel = Twist()

           if not local_path:
               return cmd_vel

           # Use pure pursuit for path following
           if len(local_path) > 0:
               # Set the path for pure pursuit controller
               self.pure_pursuit.set_path(local_path)

               # Get control command
               robot_pos = self.current_position
               robot_heading = self.current_yaw
               steering_angle, linear_vel = self.pure_pursuit.follow_path(
                   robot_pos, robot_heading, target_speed=self.linear_speed
               )

               cmd_vel.linear.x = linear_vel
               cmd_vel.angular.z = steering_angle

           # Also consider obstacle avoidance
           if self.has_close_obstacle():
               # Emergency stop or evasive maneuver
               cmd_vel.linear.x *= 0.5  # Slow down
               # Add slight turning to avoid obstacles
               cmd_vel.angular.z += np.random.uniform(-0.2, 0.2)  # Random small turn

           return cmd_vel

       def publish_path(self, path_points, publisher, frame_id, path_name):
           """Publish path as Path message"""
           path_msg = Path()
           path_msg.header.frame_id = frame_id
           path_msg.header.stamp = self.get_clock().now().to_msg()

           for point in path_points:
               pose = PoseStamped()
               pose.pose.position.x = point[0]
               pose.pose.position.y = point[1]
               pose.pose.position.z = 0.0
               path_msg.poses.append(pose)

           publisher.publish(path_msg)

       def publish_status(self, active):
           """Publish navigation status"""
           status_msg = Bool()
           status_msg.data = active
           self.status_pub.publish(status_msg)

       def stop_robot(self):
           """Stop the robot"""
           cmd_vel = Twist()
           self.cmd_vel_pub.publish(cmd_vel)


   def main(args=None):
       rclpy.init(args=args)
       navigation_lab = NavigationLabNode()

       try:
           rclpy.spin(navigation_lab)
       except KeyboardInterrupt:
           pass
       finally:
           navigation_lab.stop_robot()
           navigation_lab.destroy_node()
           rclpy.shutdown()


   if __name__ == '__main__':
       main()
   ```

3. **Create a launch file** (`navigation_lab/launch/navigation_lab.launch.py`):
   ```python
   from launch import LaunchDescription
   from launch.actions import DeclareLaunchArgument
   from launch.substitutions import LaunchConfiguration
   from launch_ros.actions import Node


   def generate_launch_description():
       # Declare launch arguments
       use_sim_time = DeclareLaunchArgument(
           'use_sim_time',
           default_value='false',
           description='Use simulation (Gazebo) clock if true'
       )

       # Navigation lab node
       navigation_lab_node = Node(
           package='navigation_lab',
           executable='navigation_node',
           name='navigation_lab_node',
           parameters=[{'use_sim_time': LaunchConfiguration('use_sim_time')}],
           output='screen'
       )

       return LaunchDescription([
           use_sim_time,
           navigation_lab_node
       ])
   ```

4. **Update setup.py**:
   ```python
   import os
   from glob import glob
   from setuptools import setup
   from setuptools import find_packages

   package_name = 'navigation_lab'

   setup(
       name=package_name,
       version='0.0.0',
       packages=find_packages(exclude=['test']),
       data_files=[
           ('share/ament_index/resource_index/packages',
               ['resource/' + package_name]),
           ('share/' + package_name, ['package.xml']),
           (os.path.join('share', package_name, 'launch'), glob('launch/*.launch.py')),
       ],
       install_requires=['setuptools'],
       zip_safe=True,
       maintainer='Your Name',
       maintainer_email='your.email@example.com',
       description='Navigation lab for robotics',
       license='Apache License 2.0',
       tests_require=['pytest'],
       entry_points={
           'console_scripts': [
               'navigation_node = navigation_lab.navigation_node:main',
           ],
       },
   )
   ```

5. **Build the package**:
   ```bash
   cd ~/robotics_ws
   colcon build --packages-select navigation_lab
   source install/setup.bash
   ```

6. **Run the navigation system**:
   ```bash
   ros2 launch navigation_lab navigation_lab.launch.py
   ```

### Expected Results
- The robot should navigate from start to goal while avoiding obstacles
- Global and local paths should be published and visualized
- The system should handle dynamic obstacles appropriately
- Control commands should be published to drive the robot

### Troubleshooting Tips
- Ensure proper TF frames are available for navigation
- Check that laser scan data is being received correctly
- Verify that odometry is accurate for path following
- Monitor the logs for path planning and execution status

## Summary

In this chapter, we've explored the fundamental concepts of motion planning and navigation, including classical algorithms like Dijkstra and A*, sampling-based methods like RRT and PRM, and local navigation approaches like Pure Pursuit and Dynamic Window Approach. We've implemented practical examples of each concept and created a complete navigation system.

The hands-on lab provided experience with integrating global path planning, local path planning, and obstacle avoidance into a complete autonomous navigation system. This foundation is essential for more advanced robotic applications involving complex environments and dynamic obstacles, which we'll explore in the upcoming chapters.

============================================================
FILE: book\docs\part-iv-intelligence\chapter-10-reinforcement-learning\index.md
============================================================
---
title: Chapter 10 - Reinforcement Learning for Robotics
sidebar_position: 1
---

# Chapter 10: Reinforcement Learning for Robotics

## Learning Goals

- Apply RL algorithms to robotic control
- Understand simulation-to-reality transfer
- Learn policy optimization techniques
- Train RL agents for basic robotic tasks
- Transfer policies from simulation to real robots
- Optimize control policies

## Introduction to Reinforcement Learning for Robotics

Reinforcement Learning (RL) is a powerful machine learning paradigm where agents learn to make decisions by interacting with an environment to maximize cumulative rewards. In robotics, RL has emerged as a transformative approach for learning complex behaviors and control policies that are difficult to program explicitly.

### Key Concepts in RL

- **Agent**: The learning entity (the robot)
- **Environment**: The world the agent interacts with
- **State**: The current situation of the agent
- **Action**: What the agent can do
- **Reward**: Feedback signal for the agent's actions
- **Policy**: Strategy that maps states to actions
- **Value Function**: Expected future rewards from a state

### Why RL for Robotics?

Traditional control methods require explicit mathematical models and manual tuning. RL offers several advantages:

1. **Adaptability**: Learns to handle uncertainties and disturbances
2. **Optimization**: Finds optimal behaviors without manual tuning
3. **Complex Tasks**: Handles high-dimensional state and action spaces
4. **Generalization**: Can adapt to new situations within the learned policy

## Markov Decision Processes (MDPs)

The foundation of RL is the Markov Decision Process, which assumes that the future state depends only on the current state and action, not on the history of previous states.

### MDP Components

```python
import numpy as np
import gym
from gym import spaces
import matplotlib.pyplot as plt


class RobotMDP:
    def __init__(self, state_space, action_space, reward_function, transition_function):
        """
        Initialize a Markov Decision Process for robotics
        state_space: Space of possible states
        action_space: Space of possible actions
        reward_function: Function R(s, a) -> reward
        transition_function: Function T(s, a) -> probability distribution over next states
        """
        self.state_space = state_space
        self.action_space = action_space
        self.reward_function = reward_function
        self.transition_function = transition_function

        # Current state of the robot
        self.current_state = None

    def reset(self):
        """Reset the environment to initial state"""
        self.current_state = self.state_space.sample()
        return self.current_state

    def step(self, action):
        """
        Execute an action and return (next_state, reward, done, info)
        """
        if self.current_state is None:
            raise ValueError("Environment not initialized. Call reset() first.")

        # Get reward for current state-action pair
        reward = self.reward_function(self.current_state, action)

        # Get next state from transition function
        next_state = self.transition_function(self.current_state, action)

        # Update current state
        self.current_state = next_state

        # For now, assume episode doesn't end
        done = False
        info = {}

        return next_state, reward, done, info


# Example: Simple navigation MDP
class NavigationMDP(RobotMDP):
    def __init__(self, grid_size=10, goal_position=(9, 9), obstacles=None):
        """
        Simple grid-world navigation MDP
        grid_size: Size of the square grid
        goal_position: Position of the goal (row, col)
        obstacles: List of obstacle positions
        """
        self.grid_size = grid_size
        self.goal_position = goal_position
        self.obstacles = obstacles if obstacles else []

        # State space: (x, y) positions
        self.state_space = spaces.Box(
            low=np.array([0, 0]),
            high=np.array([grid_size-1, grid_size-1]),
            dtype=np.int32
        )

        # Action space: up, down, left, right
        self.action_space = spaces.Discrete(4)  # 0: up, 1: down, 2: left, 3: right

        # Reward function
        def reward_function(state, action):
            next_state = self._get_next_state(state, action)

            # Check if next state is obstacle
            if tuple(next_state) in self.obstacles:
                return -10  # Large penalty for hitting obstacle

            # Distance-based reward
            distance_to_goal = np.linalg.norm(np.array(next_state) - np.array(self.goal_position))
            reward = -distance_to_goal  # Negative distance as reward

            # Bonus for reaching goal
            if np.array_equal(next_state, self.goal_position):
                reward += 100

            return reward

        # Transition function
        def transition_function(state, action):
            return self._get_next_state(state, action)

        super().__init__(self.state_space, self.action_space, reward_function, transition_function)

    def _get_next_state(self, state, action):
        """Get next state based on action (with some stochasticity)"""
        x, y = state
        new_x, new_y = x, y

        # With 10% probability, action fails (stochastic transitions)
        if np.random.random() < 0.1:
            # Stay in same position
            return np.array([x, y])

        # Execute action
        if action == 0:  # Up
            new_y = max(0, y - 1)
        elif action == 1:  # Down
            new_y = min(self.grid_size - 1, y + 1)
        elif action == 2:  # Left
            new_x = max(0, x - 1)
        elif action == 3:  # Right
            new_x = min(self.grid_size - 1, x + 1)

        # Check if new position is obstacle
        if (new_x, new_y) in self.obstacles:
            return np.array([x, y])  # Stay in place if obstacle

        return np.array([new_x, new_y])

    def reset(self):
        """Reset to random non-goal, non-obstacle position"""
        while True:
            state = self.state_space.sample()
            if not (tuple(state) == self.goal_position or tuple(state) in self.obstacles):
                self.current_state = state
                return state


# Example usage
def main():
    # Create navigation environment
    env = NavigationMDP(grid_size=10, goal_position=(8, 8), obstacles=[(3, 3), (3, 4), (3, 5)])

    # Reset environment
    state = env.reset()
    print(f"Initial state: {state}")

    # Run a few steps
    total_reward = 0
    for step in range(20):
        # Random action
        action = env.action_space.sample()

        next_state, reward, done, info = env.step(action)

        print(f"Step {step}: Action={action}, State={state} -> {next_state}, Reward={reward:.2f}")

        state = next_state
        total_reward += reward

        if done:
            break

    print(f"Total reward: {total_reward:.2f}")


if __name__ == '__main__':
    main()
```

## Value-Based Methods

### Q-Learning

Q-Learning is a model-free RL algorithm that learns the value of state-action pairs:

```python
import numpy as np
import matplotlib.pyplot as plt
import random
from collections import defaultdict


class QLearningAgent:
    def __init__(self, action_space, learning_rate=0.1, discount_factor=0.95, epsilon=0.1):
        """
        Initialize Q-Learning agent
        action_space: Number of possible actions
        learning_rate: Learning rate (alpha)
        discount_factor: Discount factor (gamma)
        epsilon: Exploration rate for epsilon-greedy
        """
        self.action_space = action_space
        self.learning_rate = learning_rate
        self.discount_factor = discount_factor
        self.epsilon = epsilon

        # Q-table: state -> [Q-value for each action]
        self.q_table = defaultdict(lambda: np.zeros(action_space))

    def get_action(self, state, training=True):
        """
        Get action using epsilon-greedy policy
        state: Current state
        training: If True, use exploration; if False, use greedy policy
        """
        if training and random.random() < self.epsilon:
            # Explore: random action
            return random.randint(0, self.action_space - 1)
        else:
            # Exploit: best action according to Q-table
            return np.argmax(self.q_table[state])

    def update(self, state, action, reward, next_state, done):
        """
        Update Q-value using Bellman equation
        """
        current_q = self.q_table[state][action]

        if done:
            target_q = reward
        else:
            target_q = reward + self.discount_factor * np.max(self.q_table[next_state])

        # Update Q-value
        self.q_table[state][action] = current_q + self.learning_rate * (target_q - current_q)


# Deep Q-Network (DQN) implementation
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F


class DQN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(DQN, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.fc3 = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x


class DQNAgent:
    def __init__(self, state_dim, action_dim, lr=1e-3, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01):
        """
        Initialize Deep Q-Network agent
        state_dim: Dimension of state space
        action_dim: Number of possible actions
        """
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        self.state_dim = state_dim
        self.action_dim = action_dim
        self.gamma = gamma
        self.epsilon = epsilon
        self.epsilon_decay = epsilon_decay
        self.epsilon_min = epsilon_min

        # Main network
        self.q_network = DQN(state_dim, 128, action_dim).to(self.device)
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)

        # Target network (for stable training)
        self.target_network = DQN(state_dim, 128, action_dim).to(self.device)
        self.target_network.load_state_dict(self.q_network.state_dict())

        # Replay buffer
        self.memory = []
        self.max_memory_size = 10000

    def remember(self, state, action, reward, next_state, done):
        """Store experience in replay buffer"""
        self.memory.append((state, action, reward, next_state, done))
        if len(self.memory) > self.max_memory_size:
            del self.memory[0]

    def act(self, state, training=True):
        """Choose action using epsilon-greedy policy"""
        if training and np.random.random() <= self.epsilon:
            return random.randrange(self.action_dim)

        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        q_values = self.q_network(state_tensor)
        return np.argmax(q_values.cpu().data.numpy())

    def replay(self, batch_size=32):
        """Train on batch of experiences from memory"""
        if len(self.memory) < batch_size:
            return

        batch = random.sample(self.memory, batch_size)
        states = torch.FloatTensor([e[0] for e in batch]).to(self.device)
        actions = torch.LongTensor([e[1] for e in batch]).to(self.device)
        rewards = torch.FloatTensor([e[2] for e in batch]).to(self.device)
        next_states = torch.FloatTensor([e[3] for e in batch]).to(self.device)
        dones = torch.BoolTensor([e[4] for e in batch]).to(self.device)

        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))
        next_q_values = self.target_network(next_states).max(1)[0].detach()
        target_q_values = rewards + (self.gamma * next_q_values * ~dones)

        loss = F.mse_loss(current_q_values.squeeze(), target_q_values)

        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        # Decay epsilon
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay

    def update_target_network(self):
        """Copy weights from main network to target network"""
        self.target_network.load_state_dict(self.q_network.state_dict())


# Example: Training DQN on a simple environment
def train_dqn_example():
    """Example of training DQN on CartPole environment"""
    import gym

    env = gym.make('CartPole-v1')
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.n

    agent = DQNAgent(state_dim, action_dim)

    episodes = 500
    scores = []

    for episode in range(episodes):
        state = env.reset()
        if isinstance(state, tuple):  # Handle newer gym versions
            state = state[0]
        total_reward = 0

        for step in range(200):  # Max steps per episode
            action = agent.act(state)
            result = env.step(action)

            # Handle different return formats
            if len(result) == 4:
                next_state, reward, done, info = result
            else:
                next_state, reward, terminated, truncated, info = result
                done = terminated or truncated

            agent.remember(state, action, reward, next_state, done)
            state = next_state
            total_reward += reward

            if done:
                break

        scores.append(total_reward)

        # Train the agent
        if len(agent.memory) > 32:
            agent.replay(32)

        # Update target network periodically
        if episode % 100 == 0:
            agent.update_target_network()

        if episode % 100 == 0:
            avg_score = np.mean(scores[-100:])
            print(f"Episode {episode}, Average Score: {avg_score:.2f}, Epsilon: {agent.epsilon:.3f}")

    # Plot results
    plt.figure(figsize=(12, 5))

    plt.subplot(1, 2, 1)
    plt.plot(scores)
    plt.title('Scores over Episodes')
    plt.xlabel('Episode')
    plt.ylabel('Score')

    plt.subplot(1, 2, 2)
    avg_scores = [np.mean(scores[max(0, i-100):i+1]) for i in range(len(scores))]
    plt.plot(avg_scores)
    plt.title('Average Scores (100-episode window)')
    plt.xlabel('Episode')
    plt.ylabel('Average Score')

    plt.tight_layout()
    plt.show()

    return agent


if __name__ == '__main__':
    # Run the example
    trained_agent = train_dqn_example()
```

## Policy Gradient Methods

Policy gradient methods directly optimize the policy function:

```python
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
import gym
from torch.distributions import Categorical


class PolicyNetwork(nn.Module):
    def __init__(self, state_dim, action_dim, hidden_dim=128):
        super(PolicyNetwork, self).__init__()
        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, action_dim)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return F.softmax(x, dim=-1)


class ValueNetwork(nn.Module):
    def __init__(self, state_dim, hidden_dim=128):
        super(ValueNetwork, self).__init__()
        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, 1)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        return self.fc3(x)


class REINFORCEAgent:
    def __init__(self, state_dim, action_dim, lr_policy=1e-3, lr_value=1e-3):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        self.policy_net = PolicyNetwork(state_dim, action_dim).to(self.device)
        self.value_net = ValueNetwork(state_dim).to(self.device)

        self.optimizer_policy = optim.Adam(self.policy_net.parameters(), lr=lr_policy)
        self.optimizer_value = optim.Adam(self.value_net.parameters(), lr=lr_value)

        self.states = []
        self.actions = []
        self.rewards = []

    def act(self, state):
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        probs = self.policy_net(state_tensor)
        m = Categorical(probs)
        action = m.sample()
        return action.item(), m.log_prob(action)

    def put_data(self, transition):
        self.states.append(transition[0])
        self.actions.append(transition[1])
        self.rewards.append(transition[2])

    def train_net(self):
        if len(self.rewards) == 0:
            return

        # Calculate discounted rewards
        R = 0
        discounted_rewards = []
        for reward in self.rewards[::-1]:
            R = reward + 0.99 * R
            discounted_rewards.insert(0, R)

        # Normalize discounted rewards
        discounted_rewards = torch.tensor(discounted_rewards).to(self.device)
        discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-7)

        # Convert to tensors
        states = torch.FloatTensor(self.states).to(self.device)
        actions = torch.LongTensor(self.actions).to(self.device)

        # Calculate policy loss
        probs = self.policy_net(states)
        m = Categorical(probs)
        log_probs = m.log_prob(actions)

        policy_loss = -(log_probs * discounted_rewards.detach()).mean()

        # Update policy network
        self.optimizer_policy.zero_grad()
        policy_loss.backward()
        self.optimizer_policy.step()

        # Reset buffers
        self.states, self.actions, self.rewards = [], [], []


# Actor-Critic implementation
class ActorCritic(nn.Module):
    def __init__(self, state_dim, action_dim, hidden_dim=128):
        super(ActorCritic, self).__init__()

        # Shared layers
        self.shared_fc1 = nn.Linear(state_dim, hidden_dim)
        self.shared_fc2 = nn.Linear(hidden_dim, hidden_dim)

        # Actor layers
        self.actor_fc = nn.Linear(hidden_dim, hidden_dim)
        self.actor_out = nn.Linear(hidden_dim, action_dim)

        # Critic layers
        self.critic_fc = nn.Linear(hidden_dim, hidden_dim)
        self.critic_out = nn.Linear(hidden_dim, 1)

    def forward(self, x):
        x = F.relu(self.shared_fc1(x))
        x = F.relu(self.shared_fc2(x))

        # Actor
        actor_x = F.relu(self.actor_fc(x))
        action_probs = F.softmax(self.actor_out(actor_x), dim=-1)

        # Critic
        critic_x = F.relu(self.critic_fc(x))
        state_value = self.critic_out(critic_x)

        return action_probs, state_value


class A2CAgent:
    def __init__(self, state_dim, action_dim, lr=1e-3, gamma=0.99):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        self.gamma = gamma
        self.model = ActorCritic(state_dim, action_dim).to(self.device)
        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)

    def act(self, state):
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        action_probs, state_value = self.model(state_tensor)
        action_dist = Categorical(action_probs)
        action = action_dist.sample()

        return action.item(), action_dist.log_prob(action), state_value


def train_reinforce_example():
    """Example of training REINFORCE on CartPole"""
    import gym

    env = gym.make('CartPole-v1')
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.n

    agent = REINFORCEAgent(state_dim, action_dim)

    episodes = 1000
    scores = []

    for episode in range(episodes):
        state = env.reset()
        if isinstance(state, tuple):  # Handle newer gym versions
            state = state[0]
        total_reward = 0

        while True:
            action, log_prob = agent.act(state)
            result = env.step(action)

            # Handle different return formats
            if len(result) == 4:
                next_state, reward, done, info = result
            else:
                next_state, reward, terminated, truncated, info = result
                done = terminated or truncated

            agent.put_data((state, action, reward))

            state = next_state
            total_reward += reward

            if done:
                break

        agent.train_net()
        scores.append(total_reward)

        if episode % 100 == 0:
            avg_score = np.mean(scores[-100:])
            print(f"Episode {episode}, Average Score: {avg_score:.2f}")

    return agent


def train_a2c_example():
    """Example of training A2C on CartPole"""
    import gym

    env = gym.make('CartPole-v1')
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.n

    agent = A2CAgent(state_dim, action_dim)

    episodes = 1000
    scores = []

    for episode in range(episodes):
        state = env.reset()
        if isinstance(state, tuple):  # Handle newer gym versions
            state = state[0]
        total_reward = 0

        log_probs = []
        values = []
        rewards = []

        for step in range(200):  # Max steps per episode
            action, log_prob, value = agent.act(state)
            result = env.step(action)

            # Handle different return formats
            if len(result) == 4:
                next_state, reward, done, info = result
            else:
                next_state, reward, terminated, truncated, info = result
                done = terminated or truncated

            log_probs.append(log_prob)
            values.append(value)
            rewards.append(reward)

            state = next_state
            total_reward += reward

            if done:
                break

        scores.append(total_reward)

        # Calculate returns and advantages
        returns = []
        G = 0
        for reward in reversed(rewards):
            G = reward + agent.gamma * G
            returns.insert(0, G)

        returns = torch.tensor(returns).float().to(agent.device)
        log_probs = torch.cat(log_probs)
        values = torch.cat(values).squeeze()

        # Calculate advantage
        advantages = returns - values

        # Calculate losses
        actor_loss = -(log_probs * advantages.detach()).mean()
        critic_loss = advantages.pow(2).mean()

        # Update networks
        agent.optimizer.zero_grad()
        (actor_loss + critic_loss).backward()
        agent.optimizer.step()

        if episode % 100 == 0:
            avg_score = np.mean(scores[-100:])
            print(f"Episode {episode}, Average Score: {avg_score:.2f}")

    return agent


if __name__ == '__main__':
    print("Training REINFORCE agent...")
    reinforce_agent = train_reinforce_example()

    print("\nTraining A2C agent...")
    a2c_agent = train_a2c_example()
```

## Deep Deterministic Policy Gradient (DDPG)

DDPG is particularly useful for continuous action spaces common in robotics:

```python
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import random
from collections import deque
import copy


class Actor(nn.Module):
    def __init__(self, state_dim, action_dim, max_action, hidden_dim=256):
        super(Actor, self).__init__()

        self.l1 = nn.Linear(state_dim, hidden_dim)
        self.l2 = nn.Linear(hidden_dim, hidden_dim)
        self.l3 = nn.Linear(hidden_dim, action_dim)

        self.max_action = max_action

    def forward(self, state):
        a = torch.relu(self.l1(state))
        a = torch.relu(self.l2(a))
        return self.max_action * torch.tanh(self.l3(a))


class Critic(nn.Module):
    def __init__(self, state_dim, action_dim, hidden_dim=256):
        super(Critic, self).__init__()

        self.l1 = nn.Linear(state_dim + action_dim, hidden_dim)
        self.l2 = nn.Linear(hidden_dim, hidden_dim)
        self.l3 = nn.Linear(hidden_dim, 1)

    def forward(self, state, action):
        sa = torch.cat([state, action], 1)
        q = torch.relu(self.l1(sa))
        q = torch.relu(self.l2(q))
        q = self.l3(q)
        return q


class DDPGAgent:
    def __init__(self, state_dim, action_dim, max_action, lr_actor=1e-4, lr_critic=1e-3,
                 gamma=0.99, tau=0.005, batch_size=100):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        self.actor = Actor(state_dim, action_dim, max_action).to(self.device)
        self.actor_target = copy.deepcopy(self.actor)
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr_actor)

        self.critic = Critic(state_dim, action_dim).to(self.device)
        self.critic_target = copy.deepcopy(self.critic)
        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr_critic)

        self.gamma = gamma  # Discount factor
        self.tau = tau      # Target network update rate
        self.batch_size = batch_size

        # Replay buffer
        self.replay_buffer = deque(maxlen=100000)

        # Noise for exploration
        self.noise = np.zeros(action_dim)

    def select_action(self, state, add_noise=True, noise_scale=0.1):
        """Select action with optional noise for exploration"""
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        action = self.actor(state_tensor).cpu().data.numpy().flatten()

        if add_noise:
            # Add Ornstein-Uhlenbeck noise for exploration
            self.noise = self.noise * 0.9 + np.random.normal(0, noise_scale, size=len(self.noise))
            action += self.noise

        return action

    def store_transition(self, state, action, reward, next_state, done):
        """Store transition in replay buffer"""
        self.replay_buffer.append((state, action, reward, next_state, done))

    def update(self):
        """Update networks using batch from replay buffer"""
        if len(self.replay_buffer) < self.batch_size:
            return

        # Sample batch from replay buffer
        batch = random.sample(self.replay_buffer, self.batch_size)
        state, action, reward, next_state, done = map(np.stack, zip(*batch))

        state = torch.FloatTensor(state).to(self.device)
        action = torch.FloatTensor(action).to(self.device)
        reward = torch.FloatTensor(reward).unsqueeze(1).to(self.device)
        next_state = torch.FloatTensor(next_state).to(self.device)
        done = torch.BoolTensor(done).unsqueeze(1).to(self.device)

        # Compute target Q-values
        next_action = self.actor_target(next_state)
        target_Q = self.critic_target(next_state, next_action)
        target_Q = reward + (self.gamma * target_Q * ~done)

        # Critic update
        current_Q = self.critic(state, action)
        critic_loss = F.mse_loss(current_Q, target_Q.detach())

        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        self.critic_optimizer.step()

        # Actor update
        actor_loss = -self.critic(state, self.actor(state)).mean()

        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        self.actor_optimizer.step()

        # Update target networks
        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):
            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)

        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):
            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)


def train_ddpg_example():
    """Example of training DDPG on a continuous control environment"""
    import gym

    env = gym.make('Pendulum-v1')
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.shape[0]
    max_action = float(env.action_space.high[0])

    agent = DDPGAgent(state_dim, action_dim, max_action)

    episodes = 1000
    scores = []

    for episode in range(episodes):
        state = env.reset()
        if isinstance(state, tuple):  # Handle newer gym versions
            state = state[0]
        total_reward = 0

        for step in range(200):  # Max steps per episode
            action = agent.select_action(state)
            result = env.step(action)

            # Handle different return formats
            if len(result) == 4:
                next_state, reward, done, info = result
            else:
                next_state, reward, terminated, truncated, info = result
                done = terminated or truncated

            agent.store_transition(state, action, reward, next_state, done)
            agent.update()

            state = next_state
            total_reward += reward

            if done:
                break

        scores.append(total_reward)

        if episode % 100 == 0:
            avg_score = np.mean(scores[-100:])
            print(f"Episode {episode}, Average Score: {avg_score:.2f}")

    return agent


if __name__ == '__main__':
    print("Training DDPG agent...")
    ddpg_agent = train_ddpg_example()
```

## Robotics-Specific RL Applications

### Robot Control with RL

```python
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt


class RobotArmEnv:
    def __init__(self, arm_length=1.0, target=(0.5, 0.5)):
        """
        Simple 2D robot arm environment
        arm_length: Length of each arm segment
        target: Target position (x, y)
        """
        self.arm_length = arm_length
        self.target = np.array(target)
        self.state_dim = 4  # [x, y, theta1, theta2] (end effector position and joint angles)
        self.action_dim = 2  # [delta_theta1, delta_theta2] (change in joint angles)

        # Action limits (radians)
        self.action_limits = np.array([-0.1, 0.1])  # Small changes for stability

        self.reset()

    def reset(self):
        """Reset environment to random initial state"""
        # Random initial joint angles
        self.joint_angles = np.random.uniform(-np.pi, np.pi, 2)
        self.state = self._get_state()
        return self.state

    def _get_state(self):
        """Get current state [x, y, theta1, theta2]"""
        # Calculate end effector position from joint angles
        x = self.arm_length * np.cos(self.joint_angles[0]) + self.arm_length * np.cos(self.joint_angles[0] + self.joint_angles[1])
        y = self.arm_length * np.sin(self.joint_angles[0]) + self.arm_length * np.sin(self.joint_angles[0] + self.joint_angles[1])

        return np.array([x, y, self.joint_angles[0], self.joint_angles[1]])

    def step(self, action):
        """Execute action and return (next_state, reward, done, info)"""
        # Apply action (change joint angles)
        self.joint_angles += action

        # Clamp joint angles to reasonable range
        self.joint_angles = np.clip(self.joint_angles, -np.pi, np.pi)

        # Get new state
        self.state = self._get_state()

        # Calculate reward (negative distance to target + bonus for being close)
        distance_to_target = np.linalg.norm(self.state[:2] - self.target)
        reward = -distance_to_target

        # Bonus for getting very close to target
        if distance_to_target < 0.1:
            reward += 10

        done = False  # Never done for this example

        return self.state, reward, done, {}

    def render(self):
        """Visualize the robot arm"""
        # Calculate arm segment endpoints
        x1 = self.arm_length * np.cos(self.joint_angles[0])
        y1 = self.arm_length * np.sin(self.joint_angles[0])

        x2 = x1 + self.arm_length * np.cos(self.joint_angles[0] + self.joint_angles[1])
        y2 = y1 + self.arm_length * np.sin(self.joint_angles[0] + self.joint_angles[1])

        plt.figure(figsize=(8, 8))
        plt.plot([0, x1, x2], [0, y1, y2], 'o-', linewidth=3, markersize=8, label='Robot Arm')
        plt.plot(self.target[0], self.target[1], 'r*', markersize=15, label='Target')
        plt.xlim(-2.5, 2.5)
        plt.ylim(-2.5, 2.5)
        plt.grid(True, alpha=0.3)
        plt.legend()
        plt.title('Robot Arm Environment')
        plt.axis('equal')
        plt.show()


class RobotArmDDPGAgent:
    def __init__(self, state_dim, action_dim, max_action=1.0, lr_actor=1e-4, lr_critic=1e-3):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        self.actor = Actor(state_dim, action_dim, max_action).to(self.device)
        self.actor_target = copy.deepcopy(self.actor)
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr_actor)

        self.critic = Critic(state_dim, action_dim).to(self.device)
        self.critic_target = copy.deepcopy(self.critic)
        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr_critic)

        self.gamma = 0.99
        self.tau = 0.005
        self.batch_size = 64

        self.replay_buffer = deque(maxlen=10000)
        self.noise_scale = 0.1

    def select_action(self, state, add_noise=True):
        """Select action with optional noise for exploration"""
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        action = self.actor(state_tensor).cpu().data.numpy().flatten()

        if add_noise:
            noise = np.random.normal(0, self.noise_scale, size=action.shape)
            action += noise

        return np.clip(action, -1.0, 1.0)  # Clip to valid range

    def store_transition(self, state, action, reward, next_state, done):
        """Store transition in replay buffer"""
        self.replay_buffer.append((state, action, reward, next_state, done))

    def update(self):
        """Update networks using batch from replay buffer"""
        if len(self.replay_buffer) < self.batch_size:
            return

        batch = random.sample(self.replay_buffer, self.batch_size)
        state, action, reward, next_state, done = map(np.stack, zip(*batch))

        state = torch.FloatTensor(state).to(self.device)
        action = torch.FloatTensor(action).to(self.device)
        reward = torch.FloatTensor(reward).unsqueeze(1).to(self.device)
        next_state = torch.FloatTensor(next_state).to(self.device)
        done = torch.BoolTensor(done).unsqueeze(1).to(self.device)

        # Compute target Q-values
        next_action = self.actor_target(next_state)
        target_Q = self.critic_target(next_state, next_action)
        target_Q = reward + (self.gamma * target_Q * ~done)

        # Critic update
        current_Q = self.critic(state, action)
        critic_loss = F.mse_loss(current_Q, target_Q.detach())

        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        self.critic_optimizer.step()

        # Actor update
        actor_loss = -self.critic(state, self.actor(state)).mean()

        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        self.actor_optimizer.step()

        # Soft update target networks
        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):
            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)

        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):
            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)


def train_robot_arm():
    """Train RL agent to control robot arm to reach target"""
    env = RobotArmEnv(target=(1.0, 0.5))
    agent = RobotArmDDPGAgent(env.state_dim, env.action_dim)

    episodes = 500
    scores = []

    for episode in range(episodes):
        state = env.reset()
        total_reward = 0

        for step in range(100):  # Max 100 steps per episode
            action = agent.select_action(state)
            next_state, reward, done, _ = env.step(action)

            agent.store_transition(state, action, reward, next_state, done)
            agent.update()

            state = next_state
            total_reward += reward

            if done:
                break

        scores.append(total_reward)

        if episode % 50 == 0:
            avg_score = np.mean(scores[-50:])
            print(f"Episode {episode}, Average Score: {avg_score:.2f}")

    # Test the trained agent
    print("\nTesting trained agent...")
    state = env.reset()
    trajectory = [env.state[:2].copy()]  # Store end effector positions

    for step in range(100):
        action = agent.select_action(state, add_noise=False)  # No noise during testing
        state, reward, done, _ = env.step(action)
        trajectory.append(state[:2].copy())

        if np.linalg.norm(state[:2] - env.target) < 0.1:  # Close enough to target
            print(f"Target reached in {step+1} steps!")
            break

    # Plot trajectory
    trajectory = np.array(trajectory)
    plt.figure(figsize=(10, 8))
    plt.plot(trajectory[:, 0], trajectory[:, 1], 'b-', linewidth=2, label='Trajectory')
    plt.plot(env.target[0], env.target[1], 'r*', markersize=15, label='Target')
    plt.plot(trajectory[0, 0], trajectory[0, 1], 'go', markersize=10, label='Start')
    plt.plot(trajectory[-1, 0], trajectory[-1, 1], 'ro', markersize=10, label='End')
    plt.title('Robot Arm Trajectory After Training')
    plt.xlabel('X Position')
    plt.ylabel('Y Position')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.axis('equal')
    plt.show()

    return agent


if __name__ == '__main__':
    trained_agent = train_robot_arm()
```

## Simulation-to-Reality Transfer

### Domain Randomization

Domain randomization helps bridge the sim-to-real gap:

```python
import torch
import numpy as np


class DomainRandomizationEnv:
    def __init__(self, base_env, randomization_params=None):
        """
        Wrap an environment with domain randomization
        base_env: The original environment
        randomization_params: Dict of parameters to randomize with their ranges
        """
        self.base_env = base_env
        self.randomization_params = randomization_params or {}

        # Store original parameters
        self.original_params = {}
        for param_name, (min_val, max_val) in self.randomization_params.items():
            if hasattr(base_env, param_name):
                self.original_params[param_name] = getattr(base_env, param_name)

    def reset(self):
        """Reset environment with randomized parameters"""
        # Randomize parameters
        for param_name, (min_val, max_val) in self.randomization_params.items():
            if hasattr(self.base_env, param_name):
                random_value = np.random.uniform(min_val, max_val)
                setattr(self.base_env, param_name, random_value)

        return self.base_env.reset()

    def step(self, action):
        """Take step in randomized environment"""
        return self.base_env.step(action)

    def render(self):
        """Render the environment"""
        return self.base_env.render()


# Example: Randomizing robot dynamics parameters
class RandomizedRobotEnv:
    def __init__(self, robot_mass_range=(0.8, 1.2), friction_range=(0.1, 0.3)):
        self.robot_mass_range = robot_mass_range
        self.friction_range = friction_range

        # Original parameters
        self.original_mass = 1.0
        self.original_friction = 0.2

        # Current randomized parameters
        self.robot_mass = self.original_mass
        self.friction = self.original_friction

    def reset(self):
        """Randomize parameters and reset"""
        self.robot_mass = np.random.uniform(*self.robot_mass_range)
        self.friction = np.random.uniform(*self.friction_range)

        # Reset robot state (simplified)
        self.state = np.random.uniform(-1, 1, 4)  # [x, y, vx, vy]
        return self.state

    def step(self, action):
        """Simulate robot dynamics with randomized parameters"""
        # Simplified physics simulation
        dt = 0.01

        # Update velocities based on action and friction
        self.state[2] += (action[0] / self.robot_mass - self.friction * self.state[2]) * dt  # vx
        self.state[3] += (action[1] / self.robot_mass - self.friction * self.state[3]) * dt  # vy

        # Update positions
        self.state[0] += self.state[2] * dt  # x
        self.state[1] += self.state[3] * dt  # y

        # Calculate reward (negative distance to origin)
        distance = np.linalg.norm(self.state[:2])
        reward = -distance

        # Simple termination condition
        done = distance > 10  # Terminate if too far from origin

        return self.state.copy(), reward, done, {}


def train_with_domain_randomization():
    """Example of training with domain randomization"""
    # Create randomized environment
    env = RandomizedRobotEnv(robot_mass_range=(0.7, 1.3), friction_range=(0.05, 0.35))

    # Initialize RL agent
    state_dim = 4  # [x, y, vx, vy]
    action_dim = 2  # [force_x, force_y]
    max_action = 1.0

    agent = RobotArmDDPGAgent(state_dim, action_dim, max_action)

    episodes = 1000

    for episode in range(episodes):
        state = env.reset()
        total_reward = 0

        for step in range(100):
            action = agent.select_action(state)
            next_state, reward, done, _ = env.step(action)

            agent.store_transition(state, action, reward, next_state, done)
            agent.update()

            state = next_state
            total_reward += reward

            if done:
                break

        if episode % 100 == 0:
            print(f"Episode {episode}, Total Reward: {total_reward:.2f}")
            print(f"  Robot Mass: {env.robot_mass:.2f}, Friction: {env.friction:.2f}")

    return agent


if __name__ == '__main__':
    print("Training with domain randomization...")
    agent = train_with_domain_randomization()
```

## ROS 2 Integration for RL

### RL Node for Robotics

```python
import rclpy
from rclpy.node import Node
from std_msgs.msg import Float32MultiArray, Bool
from geometry_msgs.msg import Twist, Pose
from sensor_msgs.msg import LaserScan, JointState
from nav_msgs.msg import Odometry
import numpy as np
import torch
import torch.nn as nn


class RobotRLAgent(nn.Module):
    def __init__(self, state_dim, action_dim, hidden_dim=128):
        super(RobotRLAgent, self).__init__()

        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, action_dim)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = torch.tanh(self.fc3(x))  # Actions between -1 and 1
        return x


class RLNavigationNode(Node):
    def __init__(self):
        super().__init__('rl_navigation_node')

        # Publishers
        self.cmd_vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)
        self.rl_status_pub = self.create_publisher(Bool, '/rl_active', 10)

        # Subscribers
        self.odom_sub = self.create_subscription(
            Odometry,
            '/odom',
            self.odom_callback,
            10
        )

        self.scan_sub = self.create_subscription(
            LaserScan,
            '/scan',
            self.scan_callback,
            10
        )

        self.joint_sub = self.create_subscription(
            JointState,
            '/joint_states',
            self.joint_callback,
            10
        )

        # RL components
        self.state_dim = 20  # Example: 18 laser readings + 2 position values
        self.action_dim = 2  # linear and angular velocity
        self.rl_agent = RobotRLAgent(self.state_dim, self.action_dim)

        # Robot state
        self.current_pose = None
        self.current_scan = None
        self.current_joints = None
        self.rl_active = False

        # Navigation parameters
        self.linear_vel_limit = 0.5
        self.angular_vel_limit = 1.0

        # Timer for control loop
        self.control_timer = self.create_timer(0.1, self.control_loop)

        self.get_logger().info('RL Navigation node initialized')

    def odom_callback(self, msg):
        """Update robot pose from odometry"""
        self.current_pose = msg.pose.pose

    def scan_callback(self, msg):
        """Update laser scan data"""
        self.current_scan = msg

    def joint_callback(self, msg):
        """Update joint state data"""
        self.current_joints = msg

    def get_robot_state(self):
        """Construct state vector from sensor data"""
        if self.current_scan is None or self.current_pose is None:
            # Return a default state if no data
            return np.zeros(self.state_dim)

        # Example state construction:
        # 18 laser readings (every 20th reading from 360-degree scan)
        laser_data = []
        if len(self.current_scan.ranges) >= 18:
            step = len(self.current_scan.ranges) // 18
            for i in range(0, len(self.current_scan.ranges), step):
                if i < len(self.current_scan.ranges):
                    # Normalize laser range (0 to 10m -> 0 to 1)
                    range_val = min(self.current_scan.ranges[i], 10.0) / 10.0
                    laser_data.append(range_val)

        # Pad with zeros if not enough readings
        while len(laser_data) < 18:
            laser_data.append(1.0)  # Max distance = no obstacle

        # 2D position (normalize to reasonable range)
        pos_x = self.current_pose.position.x / 10.0  # Assuming max 10m range
        pos_y = self.current_pose.position.y / 10.0

        # Construct full state vector
        state = np.array(laser_data + [pos_x, pos_y])

        return state

    def control_loop(self):
        """Main control loop for RL-based navigation"""
        if not self.rl_active:
            return

        # Get current state
        state = self.get_robot_state()

        # Get action from RL agent
        state_tensor = torch.FloatTensor(state).unsqueeze(0)
        action_tensor = self.rl_agent(state_tensor)
        action = action_tensor.detach().numpy()[0]

        # Convert normalized action to actual velocities
        linear_vel = action[0] * self.linear_vel_limit
        angular_vel = action[1] * self.angular_vel_limit

        # Create and publish velocity command
        cmd_vel = Twist()
        cmd_vel.linear.x = linear_vel
        cmd_vel.angular.z = angular_vel

        self.cmd_vel_pub.publish(cmd_vel)

        # Log action
        self.get_logger().info(f'RL Action - Linear: {linear_vel:.3f}, Angular: {angular_vel:.3f}')

    def activate_rl(self):
        """Activate RL navigation"""
        self.rl_active = True
        status_msg = Bool()
        status_msg.data = True
        self.rl_status_pub.publish(status_msg)
        self.get_logger().info('RL navigation activated')

    def deactivate_rl(self):
        """Deactivate RL navigation"""
        self.rl_active = False
        status_msg = Bool()
        status_msg.data = False
        self.rl_status_pub.publish(status_msg)

        # Stop robot
        stop_cmd = Twist()
        self.cmd_vel_pub.publish(stop_cmd)
        self.get_logger().info('RL navigation deactivated')


def main(args=None):
    rclpy.init(args=args)
    rl_navigation_node = RLNavigationNode()

    try:
        # Activate RL after a short delay
        def activate_timer_callback():
            rl_navigation_node.activate_rl()
            activate_timer.cancel()

        activate_timer = rl_navigation_node.create_timer(2.0, activate_timer_callback)

        rclpy.spin(rl_navigation_node)
    except KeyboardInterrupt:
        pass
    finally:
        rl_navigation_node.deactivate_rl()
        rl_navigation_node.destroy_node()
        rclpy.shutdown()


if __name__ == '__main__':
    main()
```

## Hands-On Lab: RL-Based Robot Control

### Objective
Create a complete reinforcement learning system that trains a robot to perform a navigation task in simulation, then deploy the learned policy to control the robot.

### Prerequisites
- Completed Chapter 1-10
- ROS 2 Humble with Gazebo and Navigation2
- PyTorch installed
- Basic understanding of RL concepts

### Steps

1. **Create an RL lab package**:
   ```bash
   cd ~/robotics_ws/src
   ros2 pkg create --build-type ament_python rl_robot_lab --dependencies rclpy geometry_msgs nav_msgs sensor_msgs std_msgs torch gym numpy matplotlib
   ```

2. **Create the main RL training node** (`rl_robot_lab/rl_robot_lab/rl_training_node.py`):
   ```python
   #!/usr/bin/env python3
   import rclpy
   from rclpy.node import Node
   from std_msgs.msg import Float32MultiArray, Bool
   from geometry_msgs.msg import Twist, Pose
   from sensor_msgs.msg import LaserScan
   from nav_msgs.msg import Odometry
   import numpy as np
   import torch
   import torch.nn as nn
   import torch.optim as optim
   import random
   from collections import deque
   import copy


   class DQN(nn.Module):
       def __init__(self, state_dim, action_dim, hidden_dim=128):
           super(DQN, self).__init__()
           self.fc1 = nn.Linear(state_dim, hidden_dim)
           self.fc2 = nn.Linear(hidden_dim, hidden_dim)
           self.fc3 = nn.Linear(hidden_dim, action_dim)

       def forward(self, x):
           x = torch.relu(self.fc1(x))
           x = torch.relu(self.fc2(x))
           x = self.fc3(x)
           return x


   class DQNAgent:
       def __init__(self, state_dim, action_dim, lr=1e-3, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01):
           self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

           self.state_dim = state_dim
           self.action_dim = action_dim
           self.gamma = gamma
           self.epsilon = epsilon
           self.epsilon_decay = epsilon_decay
           self.epsilon_min = epsilon_min

           # Main network
           self.q_network = DQN(state_dim, action_dim).to(self.device)
           self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)

           # Target network
           self.target_network = DQN(state_dim, action_dim).to(self.device)
           self.target_network.load_state_dict(self.q_network.state_dict())

           # Replay buffer
           self.memory = deque(maxlen=10000)

       def remember(self, state, action, reward, next_state, done):
           """Store experience in replay buffer"""
           self.memory.append((state, action, reward, next_state, done))

       def act(self, state, training=True):
           """Choose action using epsilon-greedy policy"""
           if training and np.random.random() <= self.epsilon:
               return random.randrange(self.action_dim)

           state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
           q_values = self.q_network(state_tensor)
           return np.argmax(q_values.cpu().data.numpy())

       def replay(self, batch_size=32):
           """Train on batch of experiences from memory"""
           if len(self.memory) < batch_size:
               return

           batch = random.sample(self.memory, batch_size)
           states = torch.FloatTensor([e[0] for e in batch]).to(self.device)
           actions = torch.LongTensor([e[1] for e in batch]).to(self.device)
           rewards = torch.FloatTensor([e[2] for e in batch]).to(self.device)
           next_states = torch.FloatTensor([e[3] for e in batch]).to(self.device)
           dones = torch.BoolTensor([e[4] for e in batch]).to(self.device)

           current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))
           next_q_values = self.target_network(next_states).max(1)[0].detach()
           target_q_values = rewards + (self.gamma * next_q_values * ~dones)

           loss = nn.functional.mse_loss(current_q_values.squeeze(), target_q_values)

           self.optimizer.zero_grad()
           loss.backward()
           self.optimizer.step()

           # Decay epsilon
           if self.epsilon > self.epsilon_min:
               self.epsilon *= self.epsilon_decay

       def update_target_network(self):
           """Copy weights from main network to target network"""
           self.target_network.load_state_dict(self.q_network.state_dict())


   class RLTrainingNode(Node):
       def __init__(self):
           super().__init__('rl_training_node')

           # Publishers
           self.cmd_vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)
           self.training_status_pub = self.create_publisher(Bool, '/training_active', 10)

           # Subscribers
           self.odom_sub = self.create_subscription(
               Odometry,
               '/odom',
               self.odom_callback,
               10
           )

           self.scan_sub = self.create_subscription(
               LaserScan,
               '/scan',
               self.scan_callback,
               10
           )

           # RL components
           self.state_dim = 18 + 2  # 18 laser readings + 2 position values
           self.action_dim = 4  # 4 discrete actions: forward, backward, left, right
           self.rl_agent = DQNAgent(self.state_dim, self.action_dim)

           # Robot state
           self.current_pose = None
           self.current_scan = None
           self.rl_training_active = False
           self.episode_step = 0
           self.max_episode_steps = 200
           self.total_reward = 0.0
           self.episode_count = 0

           # Training parameters
           self.training_batch_size = 32
           self.target_update_freq = 100
           self.step_count = 0

           # Timer for control loop
           self.control_timer = self.create_timer(0.1, self.control_loop)

           self.get_logger().info('RL Training node initialized')

       def odom_callback(self, msg):
           """Update robot pose from odometry"""
           self.current_pose = msg.pose.pose

       def scan_callback(self, msg):
           """Update laser scan data"""
           self.current_scan = msg

       def get_robot_state(self):
           """Construct state vector from sensor data"""
           if self.current_scan is None or self.current_pose is None:
               # Return a default state if no data
               return np.zeros(self.state_dim)

           # 18 laser readings (every 20th reading from 360-degree scan)
           laser_data = []
           if len(self.current_scan.ranges) >= 18:
               step = len(self.current_scan.ranges) // 18
               for i in range(0, len(self.current_scan.ranges), step):
                   if i < len(self.current_scan.ranges):
                       # Normalize laser range (0 to 10m -> 0 to 1)
                       range_val = min(self.current_scan.ranges[i], 10.0) / 10.0
                       laser_data.append(range_val)

           # Pad with zeros if not enough readings
           while len(laser_data) < 18:
               laser_data.append(1.0)  # Max distance = no obstacle

           # 2D position (normalize to reasonable range)
           pos_x = self.current_pose.position.x / 10.0  # Assuming max 10m range
           pos_y = self.current_pose.position.y / 10.0

           # Construct full state vector
           state = np.array(laser_data + [pos_x, pos_y])

           return state

       def calculate_reward(self, state, action):
           """Calculate reward based on current state and action"""
           # Extract laser readings and position
           laser_readings = state[:18]
           pos_x, pos_y = state[18], state[17]

           # Reward for moving forward (away from walls in front)
           min_front_dist = min(laser_readings[7:11])  # Front readings
           forward_reward = min_front_dist * 10  # Higher reward for being away from obstacles

           # Penalty for being too close to obstacles
           obstacle_penalty = 0
           for dist in laser_readings:
               if dist < 0.3:  # Very close to obstacle
                   obstacle_penalty -= 10
               elif dist < 0.6:  # Moderately close
                   obstacle_penalty -= 2

           # Small penalty for each step to encourage efficiency
           time_penalty = -0.1

           # Reward for exploring new areas (simplified)
           exploration_bonus = 0.1 * (abs(pos_x) + abs(pos_y)) / 10.0

           total_reward = forward_reward + obstacle_penalty + time_penalty + exploration_bonus
           return total_reward

       def get_action_description(self, action):
           """Get human-readable description of action"""
           actions = ['FORWARD', 'BACKWARD', 'LEFT', 'RIGHT']
           return actions[action] if 0 <= action < len(actions) else 'UNKNOWN'

       def control_loop(self):
           """Main control loop for RL training"""
           if not self.rl_training_active:
               return

           # Get current state
           current_state = self.get_robot_state()

           # Choose action using RL agent
           action = self.rl_agent.act(current_state, training=True)

           # Calculate reward for previous action
           if self.episode_step > 0:
               reward = self.calculate_reward(current_state, action)
               self.total_reward += reward

               # Store experience in replay buffer
               self.rl_agent.remember(self.previous_state, self.previous_action, reward, current_state, False)

               # Train the agent
               if len(self.rl_agent.memory) > self.training_batch_size:
                   self.rl_agent.replay(self.training_batch_size)

               # Update target network periodically
               self.step_count += 1
               if self.step_count % self.target_update_freq == 0:
                   self.rl_agent.update_target_network()

           # Execute action
           cmd_vel = Twist()
           if action == 0:  # FORWARD
               cmd_vel.linear.x = 0.3
               cmd_vel.angular.z = 0.0
           elif action == 1:  # BACKWARD
               cmd_vel.linear.x = -0.2
               cmd_vel.angular.z = 0.0
           elif action == 2:  # LEFT
               cmd_vel.linear.x = 0.1
               cmd_vel.angular.z = 0.5
           elif action == 3:  # RIGHT
               cmd_vel.linear.x = 0.1
               cmd_vel.angular.z = -0.5

           self.cmd_vel_pub.publish(cmd_vel)

           # Store state and action for next iteration
           self.previous_state = current_state.copy()
           self.previous_action = action

           # Update episode tracking
           self.episode_step += 1

           # Check if episode should end
           if self.episode_step >= self.max_episode_steps:
               self.end_episode()

           # Log information periodically
           if self.step_count % 50 == 0:
               self.get_logger().info(
                   f'Episode {self.episode_count}, Step {self.episode_step}, '
                   f'Epsilon: {self.rl_agent.epsilon:.3f}, '
                   f'Total Reward: {self.total_reward:.2f}, '
                   f'Action: {self.get_action_description(action)}'
               )

       def end_episode(self):
           """End current episode and start new one"""
           self.get_logger().info(
               f'Episode {self.episode_count} ended. Total reward: {self.total_reward:.2f}, '
               f'Steps: {self.episode_step}'
           )

           # Reset episode variables
           self.episode_step = 0
           self.total_reward = 0.0
           self.episode_count += 1

       def start_training(self):
           """Start RL training"""
           self.rl_training_active = True
           status_msg = Bool()
           status_msg.data = True
           self.training_status_pub.publish(status_msg)
           self.get_logger().info('RL training started')

       def stop_training(self):
           """Stop RL training"""
           self.rl_training_active = False
           status_msg = Bool()
           status_msg.data = False
           self.training_status_pub.publish(status_msg)

           # Stop robot
           stop_cmd = Twist()
           self.cmd_vel_pub.publish(stop_cmd)
           self.get_logger().info('RL training stopped')


   def main(args=None):
       rclpy.init(args=args)
       rl_training_node = RLTrainingNode()

       try:
           # Start training after a short delay
           def start_training_timer_callback():
               rl_training_node.start_training()
               start_timer.cancel()

           start_timer = rl_training_node.create_timer(3.0, start_training_timer_callback)

           rclpy.spin(rl_training_node)
       except KeyboardInterrupt:
           pass
       finally:
           rl_training_node.stop_training()
           rl_training_node.destroy_node()
           rclpy.shutdown()


   if __name__ == '__main__':
       main()
   ```

3. **Create a launch file** (`rl_robot_lab/launch/rl_training.launch.py`):
   ```python
   from launch import LaunchDescription
   from launch.actions import DeclareLaunchArgument, IncludeLaunchDescription
   from launch.substitutions import LaunchConfiguration, PathJoinSubstitution
   from launch.launch_description_sources import PythonLaunchDescriptionSource
   from launch_ros.actions import Node
   from ament_index_python.packages import get_package_share_directory


   def generate_launch_description():
       # Declare launch arguments
       use_sim_time = DeclareLaunchArgument(
           'use_sim_time',
           default_value='true',
           description='Use simulation (Gazebo) clock if true'
       )

       # RL training node
       rl_training_node = Node(
           package='rl_robot_lab',
           executable='rl_training_node',
           name='rl_training_node',
           parameters=[{'use_sim_time': LaunchConfiguration('use_sim_time')}],
           output='screen'
       )

       return LaunchDescription([
           use_sim_time,
           rl_training_node
       ])
   ```

4. **Update setup.py**:
   ```python
   import os
   from glob import glob
   from setuptools import setup
   from setuptools import find_packages

   package_name = 'rl_robot_lab'

   setup(
       name=package_name,
       version='0.0.0',
       packages=find_packages(exclude=['test']),
       data_files=[
           ('share/ament_index/resource_index/packages',
               ['resource/' + package_name]),
           ('share/' + package_name, ['package.xml']),
           (os.path.join('share', package_name, 'launch'), glob('launch/*.launch.py')),
       ],
       install_requires=['setuptools'],
       zip_safe=True,
       maintainer='Your Name',
       maintainer_email='your.email@example.com',
       description='RL robot lab for robotics',
       license='Apache License 2.0',
       tests_require=['pytest'],
       entry_points={
           'console_scripts': [
               'rl_training_node = rl_robot_lab.rl_training_node:main',
           ],
       },
   )
   ```

5. **Build the package**:
   ```bash
   cd ~/robotics_ws
   colcon build --packages-select rl_robot_lab
   source install/setup.bash
   ```

6. **Run the RL training system**:
   ```bash
   ros2 launch rl_robot_lab rl_training.launch.py
   ```

### Expected Results
- The robot should learn to navigate while avoiding obstacles
- The RL agent should improve its policy over time
- Reward values should increase as the agent learns
- The robot should develop effective navigation behaviors

### Troubleshooting Tips
- Ensure PyTorch is properly installed
- Verify that sensor topics are being published correctly
- Monitor the epsilon decay to ensure proper exploration-exploitation balance
- Check that the reward function is providing meaningful feedback

## Summary

In this chapter, we've explored the application of reinforcement learning to robotics, covering fundamental algorithms like Q-Learning, Deep Q-Networks, Actor-Critic methods, and Deep Deterministic Policy Gradient. We've implemented practical examples of each approach and discussed simulation-to-reality transfer techniques like domain randomization.

The hands-on lab provided experience with creating a complete RL system for robot navigation, demonstrating how to integrate RL algorithms with ROS 2 and apply them to real robotic tasks. This foundation is essential for more advanced applications in robotics intelligence and autonomy that we'll explore in the upcoming chapters.

============================================================
FILE: book\docs\part-iv-intelligence\chapter-11-imitation-learning\index.md
============================================================
---
title: Chapter 11 - Imitation Learning and VLA
sidebar_position: 2
---

# Chapter 11: Imitation Learning and Vision-Language-Action Models

## Learning Goals

- Understand vision-language-action models
- Learn imitation learning techniques
- Master multi-modal learning
- Implement VLA-based robot control
- Train imitation learning models
- Control robots using vision and language inputs

## Introduction to Imitation Learning

Imitation learning is a powerful approach where robots learn to perform tasks by observing demonstrations from human experts or other agents. Rather than learning from rewards like in reinforcement learning, imitation learning learns directly from expert demonstrations, making it particularly effective for complex tasks where defining appropriate reward functions is challenging.

### Why Imitation Learning?

Traditional reinforcement learning can struggle with sparse rewards and complex task specifications. Imitation learning addresses these challenges by:

1. **Direct Learning**: Learning directly from expert demonstrations
2. **Sample Efficiency**: Often requiring fewer samples than RL
3. **Natural Supervision**: Humans naturally provide demonstrations
4. **Complex Behaviors**: Learning complex, multi-step tasks effectively

### Key Approaches to Imitation Learning

1. **Behavior Cloning**: Direct supervised learning from demonstrations
2. **Inverse Reinforcement Learning**: Learning the reward function from demonstrations
3. **Generative Adversarial Imitation Learning (GAIL)**: Using adversarial training

## Behavior Cloning

Behavior cloning is the simplest form of imitation learning, treating the problem as a supervised learning task:

```python
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader


class RobotStateActionDataset(Dataset):
    def __init__(self, states, actions):
        """
        Dataset for robot state-action pairs
        states: List of state vectors
        actions: List of corresponding actions
        """
        self.states = torch.FloatTensor(states)
        self.actions = torch.FloatTensor(actions)

    def __len__(self):
        return len(self.states)

    def __getitem__(self, idx):
        return self.states[idx], self.actions[idx]


class BehaviorCloningNet(nn.Module):
    def __init__(self, state_dim, action_dim, hidden_dim=128):
        super(BehaviorCloningNet, self).__init__()

        self.network = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim)
        )

    def forward(self, state):
        return self.network(state)


class BehaviorCloningAgent:
    def __init__(self, state_dim, action_dim, learning_rate=1e-3):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        self.state_dim = state_dim
        self.action_dim = action_dim

        self.network = BehaviorCloningNet(state_dim, action_dim).to(self.device)
        self.optimizer = optim.Adam(self.network.parameters(), lr=learning_rate)
        self.criterion = nn.MSELoss()

    def train(self, dataset, epochs=100, batch_size=32):
        """Train the behavior cloning network"""
        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

        self.network.train()
        losses = []

        for epoch in range(epochs):
            epoch_loss = 0.0
            for states, actions in dataloader:
                states = states.to(self.device)
                actions = actions.to(self.device)

                predicted_actions = self.network(states)
                loss = self.criterion(predicted_actions, actions)

                self.optimizer.zero_grad()
                loss.backward()
                self.optimizer.step()

                epoch_loss += loss.item()

            avg_loss = epoch_loss / len(dataloader)
            losses.append(avg_loss)

            if epoch % 20 == 0:
                print(f'Epoch {epoch}, Loss: {avg_loss:.4f}')

        return losses

    def predict(self, state):
        """Predict action for given state"""
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        with torch.no_grad():
            action = self.network(state_tensor).cpu().numpy()[0]
        return action


# Example: Train behavior cloning on simple navigation data
def generate_demo_data(num_demos=1000):
    """Generate demonstration data for a simple navigation task"""
    states = []
    actions = []

    for _ in range(num_demos):
        # Simple navigation: robot at [x, y], target at [tx, ty]
        robot_pos = np.random.uniform(-5, 5, 2)
        target_pos = np.random.uniform(-4, 4, 2)

        # State: relative position to target
        state = np.concatenate([robot_pos, target_pos, target_pos - robot_pos])  # 6-dimensional state

        # Action: move towards target (normalized)
        direction = target_pos - robot_pos
        action = direction / (np.linalg.norm(direction) + 1e-8)  # Normalize

        states.append(state)
        actions.append(action)

    return np.array(states), np.array(actions)


def main_behavior_cloning():
    # Generate demonstration data
    states, actions = generate_demo_data(num_demos=2000)

    # Create dataset
    dataset = RobotStateActionDataset(states, actions)

    # Initialize and train agent
    state_dim = states.shape[1]
    action_dim = actions.shape[1]
    agent = BehaviorCloningAgent(state_dim, action_dim)

    print("Training behavior cloning agent...")
    losses = agent.train(dataset, epochs=100, batch_size=64)

    # Test the trained agent
    test_robot_pos = np.array([1.0, 1.0])
    test_target_pos = np.array([-2.0, -2.0])
    test_state = np.concatenate([test_robot_pos, test_target_pos, test_target_pos - test_robot_pos])

    predicted_action = agent.predict(test_state)
    optimal_action = (test_target_pos - test_robot_pos) / np.linalg.norm(test_target_pos - test_robot_pos)

    print(f"\nTest case:")
    print(f"Robot position: {test_robot_pos}")
    print(f"Target position: {test_target_pos}")
    print(f"Optimal action: {optimal_action}")
    print(f"Predicted action: {predicted_action}")
    print(f"Action similarity: {np.dot(predicted_action, optimal_action):.3f}")

    # Plot training loss
    plt.figure(figsize=(10, 5))
    plt.plot(losses)
    plt.title('Behavior Cloning Training Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.grid(True)
    plt.show()


if __name__ == '__main__':
    main_behavior_cloning()
```

## Generative Adversarial Imitation Learning (GAIL)

GAIL uses adversarial training to learn policies that are indistinguishable from expert demonstrations:

```python
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from torch.utils.data import TensorDataset, DataLoader
import gym


class Discriminator(nn.Module):
    def __init__(self, state_dim, action_dim, hidden_dim=128):
        super(Discriminator, self).__init__()

        self.network = nn.Sequential(
            nn.Linear(state_dim + action_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1),
            nn.Sigmoid()
        )

    def forward(self, state, action):
        x = torch.cat([state, action], dim=1)
        return self.network(x)


class PolicyNetwork(nn.Module):
    def __init__(self, state_dim, action_dim, hidden_dim=128):
        super(PolicyNetwork, self).__init__()

        self.network = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim)
        )

    def forward(self, state):
        return torch.tanh(self.network(state))  # Actions in [-1, 1]


class GAILAgent:
    def __init__(self, state_dim, action_dim, learning_rate=1e-4):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        self.state_dim = state_dim
        self.action_dim = action_dim

        # Networks
        self.discriminator = Discriminator(state_dim, action_dim).to(self.device)
        self.policy = PolicyNetwork(state_dim, action_dim).to(self.device)

        # Optimizers
        self.disc_optimizer = optim.Adam(self.discriminator.parameters(), lr=learning_rate)
        self.policy_optimizer = optim.Adam(self.policy.parameters(), lr=learning_rate)

        self.bce_loss = nn.BCELoss()

    def update_discriminator(self, expert_states, expert_actions, policy_states, policy_actions):
        """Update discriminator to distinguish expert vs policy trajectories"""
        # Labels: 1 for expert, 0 for policy
        expert_labels = torch.ones(expert_states.size(0), 1).to(self.device)
        policy_labels = torch.zeros(policy_states.size(0), 1).to(self.device)

        # Discriminator loss
        expert_logits = self.discriminator(expert_states, expert_actions)
        policy_logits = self.discriminator(policy_states, policy_actions.detach())

        expert_loss = self.bce_loss(expert_logits, expert_labels)
        policy_loss = self.bce_loss(policy_logits, policy_labels)

        disc_loss = expert_loss + policy_loss

        self.disc_optimizer.zero_grad()
        disc_loss.backward()
        self.disc_optimizer.step()

        return disc_loss.item()

    def update_policy(self, states):
        """Update policy to fool discriminator"""
        actions = self.policy(states)
        logits = self.discriminator(states, actions)

        # Policy loss: maximize log(D(s,a)) to fool discriminator
        policy_loss = -torch.log(logits + 1e-8).mean()

        self.policy_optimizer.zero_grad()
        policy_loss.backward()
        self.policy_optimizer.step()

        return policy_loss.item()

    def select_action(self, state):
        """Select action using current policy"""
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        with torch.no_grad():
            action = self.policy(state_tensor).cpu().numpy()[0]
        return action


def train_gail_example():
    """Example of training GAIL on a simple environment"""
    import gym

    # Create environment
    env = gym.make('Pendulum-v1')
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.shape[0]

    # Generate expert demonstrations (using random policy for this example)
    # In practice, these would come from expert demonstrations
    expert_states = []
    expert_actions = []

    # For demonstration, we'll create "expert" data using a slightly better random policy
    for episode in range(100):
        state = env.reset()
        if isinstance(state, tuple):  # Handle newer gym versions
            state = state[0]

        for step in range(100):
            # "Expert" action: slightly biased towards center
            action = env.action_space.sample()
            if state[0] > 0:  # If angle is positive, apply negative torque
                action[0] = max(action[0], -1.0)
            else:  # If angle is negative, apply positive torque
                action[0] = min(action[0], 1.0)

            expert_states.append(state.copy())
            expert_actions.append(action.copy())

            result = env.step(action)
            if len(result) == 4:
                state, reward, done, info = result
            else:
                state, reward, terminated, truncated, info = result
                done = terminated or truncated

            if done:
                break

    # Convert to tensors
    expert_states = torch.FloatTensor(expert_states)
    expert_actions = torch.FloatTensor(expert_actions)

    # Initialize GAIL agent
    agent = GAILAgent(state_dim, action_dim)

    # Training loop
    episodes = 1000
    for episode in range(episodes):
        # Collect policy rollout
        policy_states = []
        policy_actions = []

        state = env.reset()
        if isinstance(state, tuple):  # Handle newer gym versions
            state = state[0]

        for step in range(50):  # Short rollout for efficiency
            action = agent.select_action(state)
            policy_states.append(state.copy())
            policy_actions.append(action.copy())

            result = env.step(action)
            if len(result) == 4:
                state, reward, done, info = result
            else:
                state, reward, terminated, truncated, info = result
                done = terminated or truncated

            if done:
                break

        if len(policy_states) == 0:
            continue

        # Convert to tensors
        policy_states = torch.FloatTensor(policy_states)
        policy_actions = torch.FloatTensor(policy_actions)

        # Update discriminator and policy
        disc_loss = agent.update_discriminator(
            expert_states, expert_actions,
            policy_states, policy_actions
        )

        policy_loss = agent.update_policy(policy_states)

        if episode % 100 == 0:
            print(f'Episode {episode}, Disc Loss: {disc_loss:.4f}, Policy Loss: {policy_loss:.4f}')


if __name__ == '__main__':
    print("Training GAIL agent...")
    train_gail_example()
```

## Vision-Language-Action (VLA) Models

VLA models integrate vision, language, and action understanding into unified architectures:

```python
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import clip
from PIL import Image
import torchvision.transforms as transforms


class VLATransformerBlock(nn.Module):
    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):
        super(VLATransformerBlock, self).__init__()

        self.attention = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)
        self.norm1 = nn.LayerNorm(embed_dim)
        self.norm2 = nn.LayerNorm(embed_dim)
        self.ff = nn.Sequential(
            nn.Linear(embed_dim, ff_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(ff_dim, embed_dim),
            nn.Dropout(dropout)
        )

    def forward(self, x):
        # Self-attention
        attn_output, _ = self.attention(x, x, x)
        x = self.norm1(x + attn_output)

        # Feed-forward
        ff_output = self.ff(x)
        x = self.norm2(x + ff_output)

        return x


class VisionEncoder(nn.Module):
    def __init__(self, input_channels=3, embed_dim=512):
        super(VisionEncoder, self).__init__()

        # Simple CNN for vision encoding
        self.conv_layers = nn.Sequential(
            nn.Conv2d(input_channels, 32, 8, stride=4),
            nn.ReLU(),
            nn.Conv2d(32, 64, 4, stride=2),
            nn.ReLU(),
            nn.Conv2d(64, 64, 3, stride=1),
            nn.ReLU()
        )

        # Calculate flattened size
        conv_out_size = 64 * 7 * 7  # After convolutions on 64x64 image

        self.projection = nn.Linear(conv_out_size, embed_dim)

    def forward(self, x):
        x = self.conv_layers(x)
        x = x.view(x.size(0), -1)  # Flatten
        x = self.projection(x)
        return x


class LanguageEncoder(nn.Module):
    def __init__(self, vocab_size, embed_dim=512, max_length=50):
        super(LanguageEncoder, self).__init__()

        self.token_embedding = nn.Embedding(vocab_size, embed_dim)
        self.position_embedding = nn.Embedding(max_length, embed_dim)
        self.embed_dim = embed_dim

        # Transformer blocks for language processing
        self.transformer_blocks = nn.ModuleList([
            VLATransformerBlock(embed_dim, 8, embed_dim * 4)
            for _ in range(4)
        ])

    def forward(self, x):
        seq_len = x.size(1)
        positions = torch.arange(seq_len).expand(x.size(0), -1).to(x.device)

        x = self.token_embedding(x) + self.position_embedding(positions)

        for block in self.transformer_blocks:
            x = block(x)

        return x


class ActionDecoder(nn.Module):
    def __init__(self, embed_dim=512, action_dim=4):
        super(ActionDecoder, self).__init__()

        self.action_head = nn.Sequential(
            nn.Linear(embed_dim, embed_dim),
            nn.ReLU(),
            nn.Linear(embed_dim, embed_dim // 2),
            nn.ReLU(),
            nn.Linear(embed_dim // 2, action_dim)
        )

    def forward(self, x):
        # Take the mean across sequence dimension
        x = x.mean(dim=1)
        action = self.action_head(x)
        return action


class VLAModel(nn.Module):
    def __init__(self, vocab_size=10000, action_dim=4, embed_dim=512):
        super(VLAModel, self).__init__()

        self.vision_encoder = VisionEncoder(embed_dim=embed_dim)
        self.language_encoder = LanguageEncoder(vocab_size, embed_dim=embed_dim)
        self.action_decoder = ActionDecoder(embed_dim=embed_dim, action_dim=action_dim)

        # Cross-modal attention blocks
        self.cross_attention_blocks = nn.ModuleList([
            VLATransformerBlock(embed_dim, 8, embed_dim * 4)
            for _ in range(2)
        ])

        self.embed_dim = embed_dim

    def forward(self, image, text_tokens):
        # Encode vision and language
        vision_embeds = self.vision_encoder(image).unsqueeze(1)  # Add sequence dimension
        lang_embeds = self.language_encoder(text_tokens)

        # Concatenate vision and language embeddings
        combined_embeds = torch.cat([vision_embeds, lang_embeds], dim=1)

        # Cross-modal processing
        for block in self.cross_attention_blocks:
            combined_embeds = block(combined_embeds)

        # Decode to action
        action = self.action_decoder(combined_embeds)

        return action


class VLAProcessor:
    def __init__(self, model, device='cuda'):
        self.model = model
        self.device = device
        self.model.to(device)

        # Initialize image preprocessing
        self.image_transform = transforms.Compose([
            transforms.Resize((64, 64)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406],
                               std=[0.229, 0.224, 0.225])
        ])

    def tokenize_text(self, text, max_length=50):
        """Simple tokenization for demonstration"""
        # In practice, you'd use a proper tokenizer
        # For now, we'll use a simple approach
        words = text.lower().split()
        # Simple vocabulary mapping (in practice, use proper tokenizer)
        vocab = {"<pad>": 0, "<unk>": 1, "go": 2, "stop": 3, "forward": 4, "backward": 5,
                 "left": 6, "right": 7, "pick": 8, "place": 9, "object": 10, "red": 11,
                 "blue": 12, "green": 13, "box": 14, "ball": 15, "table": 16}

        tokens = []
        for word in words[:max_length]:
            tokens.append(vocab.get(word, vocab["<unk>"]))

        # Pad to max_length
        while len(tokens) < max_length:
            tokens.append(vocab["<pad>"])

        return np.array(tokens)

    def process(self, image, instruction):
        """Process image and instruction to generate action"""
        # Preprocess image
        if isinstance(image, str):  # If image path provided
            image = Image.open(image)
        image_tensor = self.image_transform(image).unsqueeze(0).to(self.device)

        # Tokenize instruction
        text_tokens = self.tokenize_text(instruction)
        text_tensor = torch.LongTensor(text_tokens).unsqueeze(0).to(self.device)

        # Forward pass
        with torch.no_grad():
            action = self.model(image_tensor, text_tensor)

        return action.cpu().numpy()[0]


def train_vla_example():
    """Example of training a VLA model"""
    # Initialize model
    vocab_size = 10000
    action_dim = 4  # [vx, vy, vz, rotation]
    model = VLAModel(vocab_size=vocab_size, action_dim=action_dim)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    # Initialize optimizer
    optimizer = optim.Adam(model.parameters(), lr=1e-4)
    criterion = nn.MSELoss()

    # Create dummy training data
    batch_size = 8
    image_height, image_width = 64, 64
    max_text_length = 20
    action_dim = 4

    # Dummy data for training
    images = torch.randn(batch_size, 3, image_height, image_width).to(device)
    text_tokens = torch.randint(0, vocab_size, (batch_size, max_text_length)).to(device)
    target_actions = torch.randn(batch_size, action_dim).to(device)

    # Training loop
    epochs = 100
    for epoch in range(epochs):
        optimizer.zero_grad()

        # Forward pass
        predicted_actions = model(images, text_tokens)

        # Compute loss
        loss = criterion(predicted_actions, target_actions)

        # Backward pass
        loss.backward()
        optimizer.step()

        if epoch % 20 == 0:
            print(f'Epoch {epoch}, Loss: {loss.item():.4f}')

    print(f'Final loss: {loss.item():.4f}')

    # Test the trained model
    processor = VLAProcessor(model, device)

    # Create dummy image (random)
    dummy_image = torch.rand(3, 64, 64)
    instruction = "go forward and pick the red ball"

    try:
        action = processor.process(dummy_image, instruction)
        print(f"Generated action for '{instruction}': {action}")
    except Exception as e:
        print(f"Error during processing: {e}")

    return model


if __name__ == '__main__':
    print("Training VLA model...")
    trained_model = train_vla_example()
```

## Multi-Modal Learning Integration

### Combining Vision, Language, and Action

```python
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np


class MultiModalFusion(nn.Module):
    def __init__(self, vision_dim, language_dim, action_dim, fusion_dim=256):
        super(MultiModalFusion, self).__init__()

        # Individual modality encoders
        self.vision_encoder = nn.Sequential(
            nn.Linear(vision_dim, fusion_dim),
            nn.ReLU(),
            nn.Linear(fusion_dim, fusion_dim)
        )

        self.language_encoder = nn.Sequential(
            nn.Linear(language_dim, fusion_dim),
            nn.ReLU(),
            nn.Linear(fusion_dim, fusion_dim)
        )

        # Fusion mechanism
        self.fusion = nn.Sequential(
            nn.Linear(fusion_dim * 2, fusion_dim * 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(fusion_dim * 2, fusion_dim),
            nn.ReLU()
        )

        # Action decoder
        self.action_decoder = nn.Sequential(
            nn.Linear(fusion_dim, fusion_dim // 2),
            nn.ReLU(),
            nn.Linear(fusion_dim // 2, action_dim)
        )

    def forward(self, vision_features, language_features):
        # Encode individual modalities
        vision_encoded = self.vision_encoder(vision_features)
        language_encoded = self.language_encoder(language_features)

        # Concatenate and fuse
        concatenated = torch.cat([vision_encoded, language_encoded], dim=1)
        fused = self.fusion(concatenated)

        # Decode to action
        action = self.action_decoder(fused)

        return action


class MultiModalRobotController:
    def __init__(self, vision_dim=512, language_dim=512, action_dim=4):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        self.controller = MultiModalFusion(
            vision_dim=vision_dim,
            language_dim=language_dim,
            action_dim=action_dim
        ).to(self.device)

        self.optimizer = optim.Adam(self.controller.parameters(), lr=1e-3)
        self.criterion = nn.MSELoss()

    def train_batch(self, vision_batch, language_batch, action_batch):
        """Train on a batch of multi-modal data"""
        vision_batch = torch.FloatTensor(vision_batch).to(self.device)
        language_batch = torch.FloatTensor(language_batch).to(self.device)
        action_batch = torch.FloatTensor(action_batch).to(self.device)

        # Forward pass
        predicted_actions = self.controller(vision_batch, language_batch)

        # Compute loss
        loss = self.criterion(predicted_actions, action_batch)

        # Backward pass
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        return loss.item()

    def predict_action(self, vision_features, language_features):
        """Predict action given vision and language inputs"""
        vision_tensor = torch.FloatTensor(vision_features).unsqueeze(0).to(self.device)
        language_tensor = torch.FloatTensor(language_features).unsqueeze(0).to(self.device)

        with torch.no_grad():
            action = self.controller(vision_tensor, language_tensor)

        return action.cpu().numpy()[0]


def generate_multimodal_demo_data(num_samples=1000):
    """Generate demonstration data for multi-modal learning"""
    vision_features = []
    language_features = []
    actions = []

    for _ in range(num_samples):
        # Vision features: object positions, colors, etc.
        vision_feat = np.random.randn(512)

        # Language features: encoded instruction
        lang_feat = np.random.randn(512)

        # Action: movement command
        action = np.random.randn(4)

        # Make action somewhat related to features (for learning signal)
        # This simulates a real relationship in demonstration data
        action[0] = vision_feat[0] * 0.1 + lang_feat[10] * 0.1  # Example relationship
        action[1] = vision_feat[50] * 0.1 + lang_feat[20] * 0.1

        vision_features.append(vision_feat)
        language_features.append(lang_feat)
        actions.append(action)

    return np.array(vision_features), np.array(language_features), np.array(actions)


def train_multimodal_controller():
    """Train multi-modal controller"""
    # Generate demonstration data
    vision_data, language_data, action_data = generate_multimodal_demo_data(num_samples=2000)

    # Initialize controller
    controller = MultiModalRobotController(vision_dim=512, language_dim=512, action_dim=4)

    # Training
    batch_size = 32
    epochs = 100

    for epoch in range(epochs):
        total_loss = 0

        # Shuffle data
        indices = np.random.permutation(len(vision_data))

        for i in range(0, len(vision_data), batch_size):
            batch_indices = indices[i:i+batch_size]

            vision_batch = vision_data[batch_indices]
            language_batch = language_data[batch_indices]
            action_batch = action_data[batch_indices]

            loss = controller.train_batch(vision_batch, language_batch, action_batch)
            total_loss += loss

        avg_loss = total_loss / (len(vision_data) // batch_size)

        if epoch % 20 == 0:
            print(f'Epoch {epoch}, Average Loss: {avg_loss:.4f}')

    # Test the controller
    test_vision = np.random.randn(512)
    test_language = np.random.randn(512)
    predicted_action = controller.predict_action(test_vision, test_language)

    print(f"Test prediction - Vision shape: {test_vision.shape}, Language shape: {test_language.shape}")
    print(f"Predicted action: {predicted_action}")

    return controller


if __name__ == '__main__':
    print("Training multi-modal controller...")
    controller = train_multimodal_controller()
```

## NVIDIA VLA Implementation

### Understanding NVIDIA's VLA Framework

NVIDIA's Vision-Language-Action (VLA) models represent state-of-the-art approaches to multimodal robotics:

```python
import torch
import torch.nn as nn
import numpy as np


class NVidiaVLAPolicy(nn.Module):
    def __init__(self, vision_feature_dim=512, language_feature_dim=512, proprioception_dim=10, action_dim=7):
        """
        NVIDIA-style VLA policy that combines vision, language, and proprioception
        vision_feature_dim: Dimension of visual features
        language_feature_dim: Dimension of language features
        proprioception_dim: Dimension of robot proprioception (joint angles, etc.)
        action_dim: Dimension of action space (e.g., 7-DoF for manipulation)
        """
        super(NVidiaVLAPolicy, self).__init__()

        # Feature encoders
        self.vision_encoder = self._create_feature_encoder(vision_feature_dim, 256)
        self.language_encoder = self._create_feature_encoder(language_feature_dim, 256)
        self.proprio_encoder = self._create_feature_encoder(proprioception_dim, 64)

        # Late fusion transformer
        self.fusion_transformer = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(
                d_model=512,  # Combined feature dimension
                nhead=8,
                dim_feedforward=2048,
                dropout=0.1,
                batch_first=True
            ),
            num_layers=6
        )

        # Action decoder
        self.action_decoder = nn.Sequential(
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, action_dim)
        )

        # Task conditioning
        self.task_embedding = nn.Parameter(torch.randn(1, 1, 512))

    def _create_feature_encoder(self, input_dim, output_dim):
        """Create a feature encoder network"""
        return nn.Sequential(
            nn.Linear(input_dim, output_dim),
            nn.ReLU(),
            nn.Linear(output_dim, output_dim),
            nn.ReLU(),
            nn.Linear(output_dim, output_dim)
        )

    def forward(self, vision_features, language_features, proprioception):
        """
        Forward pass of VLA policy
        vision_features: [batch_size, vision_feature_dim]
        language_features: [batch_size, language_feature_dim]
        proprioception: [batch_size, proprioception_dim]
        """
        batch_size = vision_features.size(0)

        # Encode features
        vision_enc = self.vision_encoder(vision_features).unsqueeze(1)  # [B, 1, D]
        lang_enc = self.language_encoder(language_features).unsqueeze(1)  # [B, 1, D]
        proprio_enc = self.proprio_encoder(proprioception).unsqueeze(1)  # [B, 1, D]

        # Concatenate all modalities plus task conditioning
        fused_features = torch.cat([
            self.task_embedding.expand(batch_size, -1, -1),  # Task conditioning
            vision_enc,
            lang_enc,
            proprio_enc
        ], dim=1)  # [B, 4, 512]

        # Apply transformer
        attended_features = self.fusion_transformer(fused_features)

        # Use the first token (task-conditioned representation) for action prediction
        task_repr = attended_features[:, 0, :]  # [B, 512]

        # Decode to action
        action = self.action_decoder(task_repr)

        return action


class NVidiaVLAProcessor:
    def __init__(self, model, device='cuda'):
        self.model = model
        self.device = device
        self.model.to(device)

    def encode_language(self, instruction):
        """Encode natural language instruction into features"""
        # In practice, this would use a pre-trained language model like BERT or CLIP
        # For demonstration, we'll use a simple approach
        vocab = {"<pad>": 0, "<unk>": 1, "pick": 2, "place": 3, "go": 4, "to": 5,
                 "the": 6, "red": 7, "blue": 8, "box": 9, "ball": 10, "table": 11}

        tokens = instruction.lower().split()
        token_ids = [vocab.get(token, vocab["<unk>"]) for token in tokens]

        # Simple embedding based on token IDs (in practice, use pre-trained embeddings)
        embeddings = np.zeros(512)
        for i, token_id in enumerate(token_ids):
            embeddings[i % 512] += token_id * 0.1

        return embeddings

    def encode_vision(self, image_features):
        """Encode visual features (in practice, from a vision model)"""
        # In practice, this would come from a pre-trained vision model
        # For demonstration, we'll just normalize the input
        return image_features / (np.linalg.norm(image_features) + 1e-8)

    def encode_proprioception(self, joint_angles, ee_pose):
        """Encode proprioceptive information"""
        # Combine joint angles and end-effector pose
        proprioception = np.concatenate([joint_angles, ee_pose])
        return proprioception / (np.linalg.norm(proprioception) + 1e-8)

    def predict_action(self, image_features, instruction, joint_angles, ee_pose):
        """Predict action given all modalities"""
        # Encode all inputs
        vision_encoded = self.encode_vision(image_features)
        language_encoded = self.encode_language(instruction)
        proprio_encoded = self.encode_proprioception(joint_angles, ee_pose)

        # Convert to tensors
        vision_tensor = torch.FloatTensor(vision_encoded).unsqueeze(0).to(self.device)
        language_tensor = torch.FloatTensor(language_encoded).unsqueeze(0).to(self.device)
        proprio_tensor = torch.FloatTensor(proprio_encoded).unsqueeze(0).to(self.device)

        # Forward pass
        with torch.no_grad():
            action = self.model(vision_tensor, language_tensor, proprio_tensor)

        return action.cpu().numpy()[0]


def create_nvidia_vla_demo():
    """Create and demonstrate NVIDIA-style VLA model"""
    # Initialize model
    model = NVidiaVLAPolicy(
        vision_feature_dim=512,
        language_feature_dim=512,
        proprioception_dim=17,  # 7 joint angles + 6-dof pose + 4 extra
        action_dim=7  # 7-DoF action for manipulation
    )

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    # Initialize processor
    processor = NVidiaVLAProcessor(model, device)

    # Create dummy training data
    batch_size = 16
    vision_batch = torch.randn(batch_size, 512).to(device)
    language_batch = torch.randn(batch_size, 512).to(device)
    proprio_batch = torch.randn(batch_size, 17).to(device)
    action_batch = torch.randn(batch_size, 7).to(device)

    # Train the model (dummy training for demonstration)
    optimizer = optim.Adam(model.parameters(), lr=1e-4)
    criterion = nn.MSELoss()

    print("Training NVIDIA-style VLA model...")
    for epoch in range(50):  # Just a few epochs for demo
        optimizer.zero_grad()

        pred_actions = model(vision_batch, language_batch, proprio_batch)
        loss = criterion(pred_actions, action_batch)

        loss.backward()
        optimizer.step()

        if epoch % 10 == 0:
            print(f'Epoch {epoch}, Loss: {loss.item():.4f}')

    print("Training completed!")

    # Demonstrate the model
    print("\nDemonstrating VLA model:")

    # Dummy inputs
    dummy_image_features = np.random.randn(512)
    dummy_instruction = "pick the red box and place it on the table"
    dummy_joints = np.random.randn(7)
    dummy_ee_pose = np.random.randn(10)  # 6-dof pose + extra

    action = processor.predict_action(dummy_image_features, dummy_instruction, dummy_joints, dummy_ee_pose)
    print(f"Instruction: '{dummy_instruction}'")
    print(f"Predicted action: {action}")

    return model, processor


if __name__ == '__main__':
    model, processor = create_nvidia_vla_demo()
```

## Real-World VLA Implementation for Robotics

### Integration with ROS 2

```python
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image, JointState
from geometry_msgs.msg import Twist, Pose
from std_msgs.msg import String
from cv_bridge import CvBridge
import torch
import numpy as np


class VLARobotController(Node):
    def __init__(self):
        super().__init__('vla_robot_controller')

        # Publishers
        self.cmd_pub = self.create_publisher(Twist, '/cmd_vel', 10)
        self.action_pub = self.create_publisher(Pose, '/target_pose', 10)

        # Subscribers
        self.image_sub = self.create_subscription(
            Image,
            '/camera/image_raw',
            self.image_callback,
            10
        )

        self.joint_sub = self.create_subscription(
            JointState,
            '/joint_states',
            self.joint_callback,
            10
        )

        self.command_sub = self.create_subscription(
            String,
            '/robot_command',
            self.command_callback,
            10
        )

        # Initialize components
        self.bridge = CvBridge()
        self.current_image = None
        self.current_joints = None
        self.current_command = None
        self.vla_model = None  # Will be initialized later

        # Initialize VLA model
        self.initialize_vla_model()

        # Control timer
        self.control_timer = self.create_timer(0.1, self.control_loop)

        self.get_logger().info('VLA Robot Controller initialized')

    def initialize_vla_model(self):
        """Initialize the VLA model"""
        # In practice, load a pre-trained model
        # For demonstration, we'll create a simple model
        self.vla_model = NVidiaVLAPolicy(
            vision_feature_dim=512,
            language_feature_dim=512,
            proprioception_dim=17,
            action_dim=7
        )

        # Load pre-trained weights if available
        # self.vla_model.load_state_dict(torch.load('pretrained_vla_model.pth'))

        self.vla_processor = NVidiaVLAProcessor(self.vla_model)
        self.get_logger().info('VLA model initialized')

    def image_callback(self, msg):
        """Process incoming image"""
        try:
            # Convert ROS Image to OpenCV
            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')

            # Process image to extract features (simplified)
            # In practice, this would run through a pre-trained vision model
            image_features = self.process_image_for_features(cv_image)
            self.current_image = image_features

        except Exception as e:
            self.get_logger().error(f'Error processing image: {e}')

    def joint_callback(self, msg):
        """Process joint states"""
        try:
            # Extract joint positions
            self.current_joints = np.array(msg.position)
        except Exception as e:
            self.get_logger().error(f'Error processing joints: {e}')

    def command_callback(self, msg):
        """Process natural language command"""
        self.current_command = msg.data
        self.get_logger().info(f'Received command: {msg.data}')

    def process_image_for_features(self, image):
        """Extract visual features from image (simplified)"""
        # In practice, this would run through a pre-trained vision model
        # For demonstration, we'll use simple features

        # Resize image and extract simple features
        import cv2
        resized = cv2.resize(image, (64, 64))

        # Simple feature extraction: color histograms, edges, etc.
        features = []

        # Color histogram features
        for i in range(3):  # BGR channels
            hist = cv2.calcHist([resized], [i], None, [8], [0, 256])
            features.extend(hist.flatten())

        # Flatten image as features
        flat_image = resized.flatten()
        features.extend(flat_image[:400])  # Limit to keep dimension reasonable

        # Pad or truncate to fixed dimension
        features = np.array(features[:512])
        if len(features) < 512:
            features = np.pad(features, (0, 512 - len(features)), 'constant')

        return features

    def control_loop(self):
        """Main control loop"""
        if (self.current_image is not None and
            self.current_joints is not None and
            self.current_command is not None):

            try:
                # Prepare inputs for VLA model
                image_features = self.current_image
                instruction = self.current_command

                # Get end-effector pose (simplified)
                ee_pose = np.zeros(10)  # Placeholder

                # Predict action using VLA model
                action = self.vla_processor.predict_action(
                    image_features,
                    instruction,
                    self.current_joints,
                    ee_pose
                )

                # Convert action to robot command
                self.execute_action(action)

                # Clear command after execution
                self.current_command = None

            except Exception as e:
                self.get_logger().error(f'Error in control loop: {e}')

    def execute_action(self, action):
        """Execute the predicted action"""
        # Interpret action based on its meaning
        # This is a simplified example

        if len(action) >= 3:
            # Use first 3 dimensions as velocity command
            cmd_vel = Twist()
            cmd_vel.linear.x = float(action[0])
            cmd_vel.linear.y = float(action[1])
            cmd_vel.angular.z = float(action[2])

            self.cmd_pub.publish(cmd_vel)
            self.get_logger().info(f'Published velocity command: {cmd_vel}')

        if len(action) >= 7:
            # Use as target pose (simplified)
            target_pose = Pose()
            target_pose.position.x = float(action[3])
            target_pose.position.y = float(action[4])
            target_pose.position.z = float(action[5])

            # Simple orientation (in practice, would be more complex)
            target_pose.orientation.w = 1.0  # Default orientation

            self.action_pub.publish(target_pose)
            self.get_logger().info(f'Published target pose: {target_pose}')


def main(args=None):
    rclpy.init(args=args)
    vla_controller = VLARobotController()

    try:
        rclpy.spin(vla_controller)
    except KeyboardInterrupt:
        pass
    finally:
        vla_controller.destroy_node()
        rclpy.shutdown()


if __name__ == '__main__':
    main()
```

## Imitation Learning for VLA Systems

### Combining Imitation Learning with VLA

```python
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np


class VLAImitationDataset:
    def __init__(self, demonstrations):
        """
        Dataset for VLA imitation learning
        demonstrations: List of dicts with keys: 'vision', 'language', 'action', 'proprio'
        """
        self.demonstrations = demonstrations

    def __len__(self):
        return len(self.demonstrations)

    def __getitem__(self, idx):
        demo = self.demonstrations[idx]
        return {
            'vision': torch.FloatTensor(demo['vision']),
            'language': torch.FloatTensor(demo['language']),
            'proprio': torch.FloatTensor(demo['proprio']),
            'action': torch.FloatTensor(demo['action'])
        }


class VLAImitationLearner:
    def __init__(self, model, learning_rate=1e-4):
        self.model = model
        self.optimizer = optim.Adam(model.parameters(), lr=learning_rate)
        self.criterion = nn.MSELoss()
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model.to(self.device)

    def train_epoch(self, dataloader):
        """Train for one epoch"""
        self.model.train()
        total_loss = 0
        num_batches = 0

        for batch in dataloader:
            vision = batch['vision'].to(self.device)
            language = batch['language'].to(self.device)
            proprio = batch['proprio'].to(self.device)
            actions = batch['action'].to(self.device)

            # Forward pass
            predicted_actions = self.model(vision, language, proprio)
            loss = self.criterion(predicted_actions, actions)

            # Backward pass
            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()

            total_loss += loss.item()
            num_batches += 1

        return total_loss / num_batches

    def evaluate(self, dataloader):
        """Evaluate the model"""
        self.model.eval()
        total_loss = 0
        num_batches = 0

        with torch.no_grad():
            for batch in dataloader:
                vision = batch['vision'].to(self.device)
                language = batch['language'].to(self.device)
                proprio = batch['proprio'].to(self.device)
                actions = batch['action'].to(self.device)

                predicted_actions = self.model(vision, language, proprio)
                loss = self.criterion(predicted_actions, actions)

                total_loss += loss.item()
                num_batches += 1

        return total_loss / num_batches


def generate_vla_demonstrations(num_demos=1000):
    """Generate synthetic VLA demonstrations"""
    demonstrations = []

    for _ in range(num_demos):
        # Vision features (simulated)
        vision_features = np.random.randn(512)

        # Language features (simulated instruction encoding)
        language_features = np.random.randn(512)

        # Proprioception (joint angles, etc.)
        proprio_features = np.random.randn(17)

        # Action (expert demonstration)
        action = np.random.randn(7)

        # Create a simple relationship for learning signal
        action[0] = (vision_features[0] + language_features[10] + proprio_features[0]) * 0.1
        action[1] = (vision_features[100] + language_features[20] + proprio_features[5]) * 0.1

        demo = {
            'vision': vision_features,
            'language': language_features,
            'proprio': proprio_features,
            'action': action
        }

        demonstrations.append(demo)

    return demonstrations


def train_vla_imitation():
    """Train VLA model using imitation learning"""
    # Generate demonstrations
    print("Generating demonstrations...")
    demonstrations = generate_vla_demonstrations(num_demos=2000)

    # Create dataset and dataloader
    dataset = VLAImitationDataset(demonstrations)
    dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)

    # Initialize model
    print("Initializing VLA model...")
    model = NVidiaVLAPolicy(
        vision_feature_dim=512,
        language_feature_dim=512,
        proprioception_dim=17,
        action_dim=7
    )

    # Initialize learner
    learner = VLAImitationLearner(model)

    # Training loop
    print("Starting training...")
    num_epochs = 100
    for epoch in range(num_epochs):
        train_loss = learner.train_epoch(dataloader)

        if epoch % 20 == 0:
            eval_loss = learner.evaluate(dataloader)
            print(f'Epoch {epoch}, Train Loss: {train_loss:.4f}, Eval Loss: {eval_loss:.4f}')

    print("Training completed!")

    # Test the trained model
    test_demo = demonstrations[0]
    model.eval()
    with torch.no_grad():
        vision_tensor = torch.FloatTensor(test_demo['vision']).unsqueeze(0).to(learner.device)
        language_tensor = torch.FloatTensor(test_demo['language']).unsqueeze(0).to(learner.device)
        proprio_tensor = torch.FloatTensor(test_demo['proprio']).unsqueeze(0).to(learner.device)

        predicted_action = model(vision_tensor, language_tensor, proprio_tensor)
        actual_action = test_demo['action']

        print(f"Test - Actual action: {actual_action}")
        print(f"Test - Predicted action: {predicted_action.cpu().numpy()[0]}")
        print(f"Similarity: {np.dot(actual_action, predicted_action.cpu().numpy()[0]):.3f}")

    return model


if __name__ == '__main__':
    trained_model = train_vla_imitation()
```

## Hands-On Lab: VLA-Based Robot Control System

### Objective
Create a complete VLA-based robot control system that accepts natural language commands and executes them using visual and proprioceptive feedback.

### Prerequisites
- Completed Chapter 1-11
- ROS 2 Humble with Gazebo
- PyTorch installed
- Basic understanding of VLA concepts

### Steps

1. **Create a VLA lab package**:
   ```bash
   cd ~/robotics_ws/src
   ros2 pkg create --build-type ament_python vla_robot_lab --dependencies rclpy sensor_msgs geometry_msgs std_msgs cv_bridge torch numpy matplotlib
   ```

2. **Create the main VLA control node** (`vla_robot_lab/vla_robot_lab/vla_control_node.py`):
   ```python
   #!/usr/bin/env python3
   import rclpy
   from rclpy.node import Node
   from sensor_msgs.msg import Image, JointState
   from geometry_msgs.msg import Twist, Pose
   from std_msgs.msg import String, Bool
   from cv_bridge import CvBridge
   import torch
   import numpy as np
   import json
   import time


   class VLABehaviorCloning(nn.Module):
       def __init__(self, vision_dim=512, language_dim=512, proprio_dim=17, action_dim=7):
           super(VLABehaviorCloning, self).__init__()

           # Modality encoders
           self.vision_encoder = nn.Sequential(
               nn.Linear(vision_dim, 256),
               nn.ReLU(),
               nn.Linear(256, 256)
           )

           self.language_encoder = nn.Sequential(
               nn.Linear(language_dim, 256),
               nn.ReLU(),
               nn.Linear(256, 256)
           )

           self.proprio_encoder = nn.Sequential(
               nn.Linear(proprio_dim, 64),
               nn.ReLU(),
               nn.Linear(64, 64)
           )

           # Fusion network
           self.fusion = nn.Sequential(
               nn.Linear(256 + 256 + 64, 512),
               nn.ReLU(),
               nn.Dropout(0.1),
               nn.Linear(512, 256),
               nn.ReLU(),
               nn.Linear(256, 128),
               nn.ReLU()
           )

           # Action decoder
           self.action_decoder = nn.Linear(128, action_dim)

       def forward(self, vision, language, proprio):
           vision_enc = self.vision_encoder(vision)
           language_enc = self.language_encoder(language)
           proprio_enc = self.proprio_encoder(proprio)

           fused = torch.cat([vision_enc, language_enc, proprio_enc], dim=1)
           features = self.fusion(fused)
           action = self.action_decoder(features)

           return action


   class VLAControlNode(Node):
       def __init__(self):
           super().__init__('vla_control_node')

           # Publishers
           self.cmd_vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)
           self.target_pose_pub = self.create_publisher(Pose, '/target_pose', 10)
           self.status_pub = self.create_publisher(Bool, '/vla_active', 10)

           # Subscribers
           self.image_sub = self.create_subscription(
               Image,
               '/camera/image_raw',
               self.image_callback,
               10
           )

           self.joint_sub = self.create_subscription(
               JointState,
               '/joint_states',
               self.joint_callback,
               10
           )

           self.command_sub = self.create_subscription(
               String,
               '/natural_language_command',
               self.command_callback,
               10
           )

           # Initialize components
           self.bridge = CvBridge()
           self.current_image = None
           self.current_joints = None
           self.current_command = None
           self.vla_model = None
           self.vla_active = False

           # Initialize VLA model
           self.initialize_model()

           # Control timer
           self.control_timer = self.create_timer(0.1, self.control_loop)

           # Training mode
           self.training_mode = False
           self.demo_buffer = []

           self.get_logger().info('VLA Control Node initialized')

       def initialize_model(self):
           """Initialize the VLA model"""
           self.vla_model = VLABehaviorCloning(
               vision_dim=512,
               language_dim=512,
               proprio_dim=17,
               action_dim=7
           )

           # Initialize with random weights for demo
           # In practice, load pre-trained weights

           self.get_logger().info('VLA model initialized')
           self.vla_active = True

       def image_callback(self, msg):
           """Process incoming image"""
           try:
               cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')

               # Extract visual features
               features = self.extract_visual_features(cv_image)
               self.current_image = features

           except Exception as e:
               self.get_logger().error(f'Error processing image: {e}')

       def joint_callback(self, msg):
           """Process joint states"""
           try:
               # Extract joint positions and velocities
               if len(msg.position) > 0:
                   # Combine position and velocity information
                   joint_data = list(msg.position)
                   if len(msg.velocity) == len(msg.position):
                       joint_data.extend(list(msg.velocity))
                   else:
                       joint_data.extend([0.0] * len(msg.position))  # Zero velocity if not available

                   # Pad or truncate to fixed size
                   while len(joint_data) < 17:
                       joint_data.append(0.0)
                   joint_data = joint_data[:17]

                   self.current_joints = np.array(joint_data)
           except Exception as e:
               self.get_logger().error(f'Error processing joints: {e}')

       def command_callback(self, msg):
           """Process natural language command"""
           self.current_command = msg.data
           self.get_logger().info(f'Received VLA command: {msg.data}')

       def extract_visual_features(self, image):
           """Extract visual features from image"""
           import cv2

           # Resize image
           resized = cv2.resize(image, (64, 64))

           # Extract simple features (in practice, use a pre-trained vision model)
           features = []

           # Color histogram
           for i in range(3):  # BGR
               hist = cv2.calcHist([resized], [i], None, [8], [0, 256])
               features.extend(hist.flatten())

           # Edge features
           gray = cv2.cvtColor(resized, cv2.COLOR_BGR2GRAY)
           edges = cv2.Canny(gray, 50, 150)
           edge_hist = cv2.calcHist([edges], [0], None, [4], [0, 256])
           features.extend(edge_hist.flatten())

           # Flatten image as features
           flat_img = resized.flatten()
           features.extend(flat_img[:300])  # Limit features

           # Pad or truncate to fixed dimension
           features = np.array(features[:512])
           if len(features) < 512:
               features = np.pad(features, (0, 512 - len(features)), 'constant')

           return features

       def encode_language(self, text):
           """Encode natural language to features"""
           # Simple encoding for demonstration
           # In practice, use a pre-trained language model
           vocab = {
               '<pad>': 0, '<unk>': 1, 'go': 2, 'to': 3, 'the': 4, 'and': 5,
               'pick': 6, 'place': 7, 'up': 8, 'down': 9, 'left': 10, 'right': 11,
               'forward': 12, 'backward': 13, 'stop': 14, 'red': 15, 'blue': 16,
               'green': 17, 'yellow': 18, 'box': 19, 'ball': 20, 'table': 21,
               'shelf': 22, 'cup': 23, 'object': 24
           }

           tokens = text.lower().split()
           encoded = np.zeros(512)

           for i, token in enumerate(tokens[:50]):  # Limit to first 50 tokens
               token_id = vocab.get(token, vocab['<unk>'])
               encoded[i % 512] += token_id * 0.1  # Simple embedding

           return encoded

       def control_loop(self):
           """Main control loop"""
           if not self.vla_active:
               return

           if (self.current_image is not None and
               self.current_joints is not None and
               self.current_command is not None):

               try:
                   # Encode inputs
                   vision_tensor = torch.FloatTensor(self.current_image).unsqueeze(0)
                   language_features = self.encode_language(self.current_command)
                   language_tensor = torch.FloatTensor(language_features).unsqueeze(0)
                   proprio_tensor = torch.FloatTensor(self.current_joints).unsqueeze(0)

                   # Get action from VLA model
                   with torch.no_grad():
                       action_tensor = self.vla_model(vision_tensor, language_tensor, proprio_tensor)
                       action = action_tensor.cpu().numpy()[0]

                   # Execute action
                   self.execute_vla_action(action)

                   # Log the interaction
                   self.get_logger().info(
                       f'VLA executed command: "{self.current_command}", '
                       f'action: [{action[0]:.3f}, {action[1]:.3f}, {action[2]:.3f}]'
                   )

                   # Clear command after execution
                   self.current_command = None

                   # If in training mode, save demonstration
                   if self.training_mode:
                       self.save_demonstration(
                           self.current_image.copy(),
                           language_features.copy(),
                           self.current_joints.copy(),
                           action.copy()
                       )

               except Exception as e:
                   self.get_logger().error(f'Error in VLA control: {e}')

       def execute_vla_action(self, action):
           """Execute the VLA-predicted action"""
           # Interpret action vector
           # First 3 elements: velocity command
           cmd_vel = Twist()
           cmd_vel.linear.x = float(np.clip(action[0], -1.0, 1.0))
           cmd_vel.linear.y = float(np.clip(action[1], -1.0, 1.0))
           cmd_vel.angular.z = float(np.clip(action[2], -1.0, 1.0))

           # Next 4 elements: target pose (simplified)
           target_pose = Pose()
           target_pose.position.x = float(action[3]) if len(action) > 3 else 0.0
           target_pose.position.y = float(action[4]) if len(action) > 4 else 0.0
           target_pose.position.z = float(action[5]) if len(action) > 5 else 0.0
           target_pose.orientation.w = 1.0  # Default orientation

           # Publish commands
           self.cmd_vel_pub.publish(cmd_vel)
           self.target_pose_pub.publish(target_pose)

           # Publish status
           status_msg = Bool()
           status_msg.data = True
           self.status_pub.publish(status_msg)

       def save_demonstration(self, vision, language, proprio, action):
           """Save demonstration for imitation learning"""
           demo = {
               'vision': vision.tolist(),
               'language': language.tolist(),
               'proprio': proprio.tolist(),
               'action': action.tolist(),
               'timestamp': time.time()
           }

           self.demo_buffer.append(demo)

           # Limit buffer size
           if len(self.demo_buffer) > 1000:
               self.demo_buffer = self.demo_buffer[-500:]  # Keep last 500 demos

           self.get_logger().info(f'Demonstration saved. Buffer size: {len(self.demo_buffer)}')

       def start_training_mode(self):
           """Start collecting demonstrations"""
           self.training_mode = True
           self.get_logger().info('VLA training mode activated - collecting demonstrations')

       def stop_training_mode(self):
           """Stop collecting demonstrations"""
           self.training_mode = False
           self.get_logger().info('VLA training mode deactivated')

       def save_demonstrations_to_file(self, filename='vla_demonstrations.json'):
           """Save collected demonstrations to file"""
           with open(filename, 'w') as f:
               json.dump(self.demo_buffer, f)
           self.get_logger().info(f'Saved {len(self.demo_buffer)} demonstrations to {filename}')


   def main(args=None):
       rclpy.init(args=args)
       vla_control_node = VLAControlNode()

       try:
           # Start in training mode for demonstration
           def start_training_timer_callback():
               vla_control_node.start_training_mode()
               start_timer.cancel()

           start_timer = vla_control_node.create_timer(5.0, start_training_timer_callback)

           rclpy.spin(vla_control_node)
       except KeyboardInterrupt:
           pass
       finally:
           # Save demonstrations before shutting down
           vla_control_node.save_demonstrations_to_file()
           vla_control_node.destroy_node()
           rclpy.shutdown()


   if __name__ == '__main__':
       main()
   ```

3. **Create a launch file** (`vla_robot_lab/launch/vla_control.launch.py`):
   ```python
   from launch import LaunchDescription
   from launch.actions import DeclareLaunchArgument
   from launch.substitutions import LaunchConfiguration
   from launch_ros.actions import Node


   def generate_launch_description():
       # Declare launch arguments
       use_sim_time = DeclareLaunchArgument(
           'use_sim_time',
           default_value='true',
           description='Use simulation (Gazebo) clock if true'
       )

       # VLA control node
       vla_control_node = Node(
           package='vla_robot_lab',
           executable='vla_control_node',
           name='vla_control_node',
           parameters=[{'use_sim_time': LaunchConfiguration('use_sim_time')}],
           output='screen'
       )

       return LaunchDescription([
           use_sim_time,
           vla_control_node
       ])
   ```

4. **Update setup.py**:
   ```python
   import os
   from glob import glob
   from setuptools import setup
   from setuptools import find_packages

   package_name = 'vla_robot_lab'

   setup(
       name=package_name,
       version='0.0.0',
       packages=find_packages(exclude=['test']),
       data_files=[
           ('share/ament_index/resource_index/packages',
               ['resource/' + package_name]),
           ('share/' + package_name, ['package.xml']),
           (os.path.join('share', package_name, 'launch'), glob('launch/*.launch.py')),
       ],
       install_requires=['setuptools'],
       zip_safe=True,
       maintainer='Your Name',
       maintainer_email='your.email@example.com',
       description='VLA robot lab for robotics',
       license='Apache License 2.0',
       tests_require=['pytest'],
       entry_points={
           'console_scripts': [
               'vla_control_node = vla_robot_lab.vla_control_node:main',
           ],
       },
   )
   ```

5. **Build the package**:
   ```bash
   cd ~/robotics_ws
   colcon build --packages-select vla_robot_lab
   source install/setup.bash
   ```

6. **Run the VLA system**:
   ```bash
   ros2 launch vla_robot_lab vla_control.launch.py
   ```

### Expected Results
- The system should accept natural language commands via the `/natural_language_command` topic
- Visual and proprioceptive information should be processed to generate appropriate actions
- Demonstrations should be collected when in training mode
- The system should execute commands like "go to the red box" or "pick up the blue ball"

### Troubleshooting Tips
- Ensure PyTorch is properly installed
- Verify that camera and joint state topics are being published
- Check that the VLA model is properly initialized
- Monitor the logs for VLA processing status

## Summary

In this chapter, we've explored the cutting-edge field of Vision-Language-Action (VLA) models and imitation learning for robotics. We've covered:

1. **Imitation Learning Fundamentals**: Behavior cloning, GAIL, and their applications to robotics
2. **VLA Architecture**: How to combine vision, language, and action understanding in unified models
3. **Multi-Modal Integration**: Techniques for fusing different sensory modalities
4. **NVIDIA VLA Implementation**: Understanding state-of-the-art VLA frameworks
5. **Practical Integration**: How to implement VLA systems with ROS 2

The hands-on lab provided experience with creating a complete VLA-based robot control system that accepts natural language commands and executes them using visual and proprioceptive feedback. This represents one of the most advanced approaches to robot learning and control, bridging the gap between human intentions and robotic actions.

This foundation prepares us for the next chapter on Human-Robot Interaction, where we'll explore how to make these intelligent systems more intuitive and collaborative for human users.

============================================================
FILE: book\docs\part-iv-intelligence\chapter-12-hri\index.md
============================================================
---
title: Chapter 12 - Human-Robot Interaction
sidebar_position: 3
---

# Chapter 12: Human-Robot Interaction

## Learning Goals

- Design intuitive human-robot interfaces
- Understand social robotics principles
- Learn collaborative robotics concepts
- Implement natural language interfaces
- Create gesture recognition systems
- Design collaborative robot behaviors
- Integrate speech recognition APIs

## Introduction to Human-Robot Interaction

Human-Robot Interaction (HRI) is a multidisciplinary field that focuses on the design, development, and evaluation of robots that can interact with humans in a natural, safe, and effective manner. As robots become more prevalent in our daily lives, from industrial settings to homes and hospitals, the ability to interact seamlessly with humans becomes increasingly important.

### Key Principles of HRI

1. **Safety**: Ensuring human safety during interaction
2. **Intuitiveness**: Making robot behavior predictable and understandable
3. **Social Acceptance**: Designing robots that are socially acceptable
4. **Collaboration**: Enabling effective human-robot teamwork
5. **Adaptability**: Adjusting to different users and contexts

### HRI Challenges

- **Communication Barriers**: Different modalities and languages
- **Trust Issues**: Building trust between humans and robots
- **Social Norms**: Understanding and respecting social conventions
- **Cultural Differences**: Adapting to cultural contexts
- **Ethical Considerations**: Privacy, autonomy, and fairness

## Natural Language Processing for HRI

### Speech Recognition

Speech recognition enables robots to understand spoken commands from humans:

```python
import speech_recognition as sr
import pyttsx3
import threading
import time
import queue


class SpeechRecognitionSystem:
    def __init__(self, language='en-US'):
        self.recognizer = sr.Recognizer()
        self.microphone = sr.Microphone()

        # Adjust for ambient noise
        with self.microphone as source:
            self.recognizer.adjust_for_ambient_noise(source)

        # Text-to-speech engine
        self.tts_engine = pyttsx3.init()

        # Configuration
        self.language = language
        self.energy_threshold = 300  # Minimum audio energy to consider for recording
        self.dynamic_energy_threshold = True

        # Callback functions
        self.command_callbacks = []

        # Thread-safe command queue
        self.command_queue = queue.Queue()

    def listen_continuously(self):
        """Continuously listen for speech commands"""
        def callback(recognizer, audio):
            try:
                # Recognize speech using Google's speech recognition
                text = recognizer.recognize_google(audio, language=self.language)

                # Add recognized text to queue
                self.command_queue.put(text)

                # Process command
                self.process_command(text)

            except sr.UnknownValueError:
                print("Could not understand audio")
            except sr.RequestError as e:
                print(f"Could not request results; {e}")

        # Start listening in background
        stop_listening = self.recognizer.listen_in_background(self.microphone, callback)

        return stop_listening

    def process_command(self, text):
        """Process recognized command"""
        print(f"Recognized command: {text}")

        # Trigger callbacks
        for callback in self.command_callbacks:
            callback(text)

    def add_command_callback(self, callback):
        """Add callback function for command processing"""
        self.command_callbacks.append(callback)

    def speak(self, text):
        """Speak text using text-to-speech"""
        self.tts_engine.say(text)
        self.tts_engine.runAndWait()

    def recognize_from_audio_file(self, audio_file_path):
        """Recognize speech from an audio file"""
        with sr.AudioFile(audio_file_path) as source:
            audio = self.recognizer.record(source)

        try:
            text = self.recognizer.recognize_google(audio, language=self.language)
            return text
        except sr.UnknownValueError:
            return "Could not understand audio"
        except sr.RequestError as e:
            return f"Could not request results; {e}"


# Example usage
def main():
    # Initialize speech recognition system
    speech_system = SpeechRecognitionSystem()

    # Define command callback
    def command_handler(text):
        """Handle recognized commands"""
        text_lower = text.lower()

        if "hello" in text_lower or "hi" in text_lower:
            speech_system.speak("Hello! How can I help you?")
        elif "move forward" in text_lower:
            speech_system.speak("Moving forward")
            # In a real system, this would trigger robot movement
        elif "stop" in text_lower:
            speech_system.speak("Stopping")
            # In a real system, this would stop robot
        elif "what is your name" in text_lower:
            speech_system.speak("I am a social robot designed to assist you")
        else:
            speech_system.speak(f"I heard: {text}")

    # Add command handler
    speech_system.add_command_callback(command_handler)

    print("Starting speech recognition system...")
    print("Say 'hello', 'move forward', 'stop', or 'what is your name'")

    # Start continuous listening
    stop_listening = speech_system.listen_continuously()

    try:
        # Keep the program running
        while True:
            time.sleep(0.1)
    except KeyboardInterrupt:
        print("Stopping...")
        stop_listening(wait_for_stop=False)


if __name__ == '__main__':
    main()
```

### Natural Language Understanding

Beyond speech recognition, we need to understand the meaning and intent behind human language:

```python
import re
import spacy
import numpy as np
from typing import Dict, List, Tuple, Optional


class NaturalLanguageUnderstanding:
    def __init__(self):
        """Initialize NLU system"""
        try:
            # Load spaCy English model
            self.nlp = spacy.load("en_core_web_sm")
        except OSError:
            print("spaCy English model not found. Install with: python -m spacy download en_core_web_sm")
            # Fallback to simple keyword matching
            self.nlp = None

        # Define command patterns
        self.command_patterns = {
            'navigation': [
                r'go to (.+)',
                r'move to (.+)',
                r'navigate to (.+)',
                r'go (.+)',
                r'move (.+)'
            ],
            'action': [
                r'pick up (.+)',
                r'grab (.+)',
                r'lift (.+)',
                r'put (.+) (?:down|there)',
                r'place (.+) (?:down|there)'
            ],
            'information': [
                r'what is (.+)',
                r'tell me about (.+)',
                r'explain (.+)',
                r'how do i (.+)'
            ],
            'social': [
                r'hello',
                r'hi',
                r'good morning',
                r'good afternoon',
                r'good evening'
            ]
        }

        # Define location keywords
        self.locations = {
            'kitchen': ['kitchen', 'cooking area', 'food area'],
            'living_room': ['living room', 'lounge', 'sitting area'],
            'bedroom': ['bedroom', 'sleeping area', 'bed area'],
            'office': ['office', 'study', 'work area'],
            'bathroom': ['bathroom', 'restroom', 'washroom'],
            'dining_room': ['dining room', 'dining area', 'eating area']
        }

    def extract_entities(self, text: str) -> Dict[str, List[str]]:
        """Extract named entities from text"""
        entities = {
            'persons': [],
            'places': [],
            'objects': [],
            'actions': []
        }

        if self.nlp:
            doc = self.nlp(text)

            for ent in doc.ents:
                if ent.label_ == "PERSON":
                    entities['persons'].append(ent.text)
                elif ent.label_ in ["GPE", "LOC", "FAC"]:
                    entities['places'].append(ent.text)
                elif ent.label_ in ["OBJECT", "PRODUCT"]:
                    entities['objects'].append(ent.text)

        # Fallback to keyword matching if spaCy is not available
        else:
            # Simple keyword matching for places
            text_lower = text.lower()
            for loc_type, keywords in self.locations.items():
                for keyword in keywords:
                    if keyword in text_lower:
                        entities['places'].append(keyword)

        return entities

    def classify_intent(self, text: str) -> Tuple[str, float]:
        """Classify the intent of the given text"""
        text_lower = text.lower()
        best_match = ('unknown', 0.0)

        for intent, patterns in self.command_patterns.items():
            for pattern in patterns:
                # Simple keyword matching
                if re.search(pattern.replace('(.+)', ''), text_lower):
                    confidence = 0.8  # High confidence for exact matches
                    if confidence > best_match[1]:
                        best_match = (intent, confidence)

                # Pattern matching with capture groups
                match = re.search(pattern, text_lower)
                if match:
                    confidence = 0.9  # High confidence for pattern matches
                    if confidence > best_match[1]:
                        best_match = (intent, confidence)

        # If no pattern matches, use simple keyword classification
        if best_match[0] == 'unknown':
            if any(word in text_lower for word in ['hello', 'hi', 'hey']):
                best_match = ('social', 0.7)
            elif any(word in text_lower for word in ['go', 'move', 'navigate', 'walk']):
                best_match = ('navigation', 0.6)
            elif any(word in text_lower for word in ['pick', 'grab', 'lift', 'put', 'place']):
                best_match = ('action', 0.6)
            elif any(word in text_lower for word in ['what', 'tell', 'explain', 'how']):
                best_match = ('information', 0.6)

        return best_match

    def parse_command(self, text: str) -> Dict[str, any]:
        """Parse a command into structured format"""
        intent, confidence = self.classify_intent(text)
        entities = self.extract_entities(text)

        # Extract target from text based on intent
        target = None
        location = None

        if intent == 'navigation':
            # Extract location using regex
            for pattern in self.command_patterns['navigation']:
                match = re.search(pattern, text.lower())
                if match:
                    target = match.group(1)
                    break
        elif intent in ['action', 'information']:
            # Extract object using regex
            for pattern in self.command_patterns.get(intent, []):
                match = re.search(pattern, text.lower())
                if match:
                    target = match.group(1)
                    break

        # Check for location keywords in entities
        for place in entities.get('places', []):
            location = place
            break  # Take first location found

        return {
            'intent': intent,
            'confidence': confidence,
            'entities': entities,
            'target': target,
            'location': location,
            'original_text': text
        }

    def generate_response(self, parsed_command: Dict[str, any]) -> str:
        """Generate appropriate response based on parsed command"""
        intent = parsed_command['intent']

        if intent == 'social':
            return "Hello! How can I assist you today?"
        elif intent == 'navigation':
            if parsed_command['target']:
                return f"Okay, I will navigate to the {parsed_command['target']}."
            else:
                return "Where would you like me to go?"
        elif intent == 'action':
            if parsed_command['target']:
                return f"Okay, I will {parsed_command['original_text'].split()[0]} the {parsed_command['target']}."
            else:
                return "What would you like me to do?"
        elif intent == 'information':
            if parsed_command['target']:
                return f"I can provide information about {parsed_command['target']}. What specifically would you like to know?"
            else:
                return "What would you like to know?"
        else:
            return "I'm not sure I understood. Could you please rephrase that?"


# Example usage
def main():
    nlu = NaturalLanguageUnderstanding()

    # Test commands
    test_commands = [
        "Hello robot",
        "Go to the kitchen",
        "Move to the living room",
        "Pick up the red cup",
        "What is the weather like?",
        "Tell me about the meeting",
        "Navigate to the office"
    ]

    print("Testing Natural Language Understanding system:")
    print("=" * 50)

    for command in test_commands:
        parsed = nlu.parse_command(command)
        response = nlu.generate_response(parsed)

        print(f"Input: {command}")
        print(f"Parsed: Intent='{parsed['intent']}', Confidence={parsed['confidence']:.2f}, Target='{parsed['target']}', Location='{parsed['location']}'")
        print(f"Response: {response}")
        print("-" * 30)


if __name__ == '__main__':
    main()
```

## Gesture Recognition

### Computer Vision-Based Gesture Recognition

Gesture recognition allows robots to interpret human gestures as commands:

```python
import cv2
import numpy as np
import mediapipe as mp
from enum import Enum


class GestureType(Enum):
    UNKNOWN = 0
    THUMB_UP = 1
    THUMB_DOWN = 2
    PEACE = 3
    ROCK_ON = 4
    STOP = 5
    GO = 6
    WAVE = 7


class GestureRecognizer:
    def __init__(self):
        """Initialize gesture recognition system"""
        # Initialize MediaPipe hands
        self.mp_hands = mp.solutions.hands
        self.mp_drawing = mp.solutions.drawing_utils
        self.mp_drawing_styles = mp.solutions.drawing_styles

        self.hands = self.mp_hands.Hands(
            static_image_mode=False,
            max_num_hands=2,
            min_detection_confidence=0.7,
            min_tracking_confidence=0.7
        )

        # Define gesture templates
        self.gesture_templates = {
            GestureType.THUMB_UP: self._is_thumb_up,
            GestureType.THUMB_DOWN: self._is_thumb_down,
            GestureType.PEACE: self._is_peace,
            GestureType.ROCK_ON: self._is_rock_on,
            GestureType.STOP: self._is_stop,
            GestureType.GO: self._is_go,
            GestureType.WAVE: self._is_wave
        }

    def _calculate_angle(self, point1, point2, point3):
        """Calculate angle between three points"""
        # Convert to numpy arrays
        p1 = np.array([point1.x, point1.y])
        p2 = np.array([point2.x, point2.y])
        p3 = np.array([point3.x, point3.y])

        # Calculate vectors
        v1 = p1 - p2
        v2 = p3 - p2

        # Calculate angle
        cosine_angle = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))
        angle = np.arccos(np.clip(cosine_angle, -1.0, 1.0))

        return np.degrees(angle)

    def _is_thumb_up(self, hand_landmarks):
        """Check if thumb is up gesture"""
        # Thumb tip should be above thumb joint
        thumb_tip = hand_landmarks.landmark[self.mp_hands.HandLandmark.THUMB_TIP]
        thumb_ip = hand_landmarks.landmark[self.mp_hands.HandLandmark.THUMB_INTERMEDIATE]

        # Index finger should be bent (tip below PIP)
        index_tip = hand_landmarks.landmark[self.mp_hands.HandLandmark.INDEX_FINGER_TIP]
        index_pip = hand_landmarks.landmark[self.mp_hands.HandLandmark.INDEX_FINGER_PIP]

        # Thumb should be extended up while other fingers are curled
        return (thumb_tip.y < thumb_ip.y and  # Thumb up
                index_tip.y > index_pip.y and  # Index finger down
                hand_landmarks.landmark[self.mp_hands.HandLandmark.MIDDLE_FINGER_TIP].y >
                hand_landmarks.landmark[self.mp_hands.HandLandmark.MIDDLE_FINGER_PIP].y and
                hand_landmarks.landmark[self.mp_hands.HandLandmark.RING_FINGER_TIP].y >
                hand_landmarks.landmark[self.mp_hands.HandLandmark.RING_FINGER_PIP].y and
                hand_landmarks.landmark[self.mp_hands.HandLandmark.PINKY_TIP].y >
                hand_landmarks.landmark[self.mp_hands.HandLandmark.PINKY_PIP].y)

    def _is_peace(self, hand_landmarks):
        """Check if peace sign gesture"""
        # Index and middle fingers extended, others curled
        index_tip = hand_landmarks.landmark[self.mp_hands.HandLandmark.INDEX_FINGER_TIP]
        index_pip = hand_landmarks.landmark[self.mp_hands.HandLandmark.INDEX_FINGER_PIP]
        middle_tip = hand_landmarks.landmark[self.mp_hands.HandLandmark.MIDDLE_FINGER_TIP]
        middle_pip = hand_landmarks.landmark[self.mp_hands.HandLandmark.MIDDLE_FINGER_PIP]
        ring_tip = hand_landmarks.landmark[self.mp_hands.HandLandmark.RING_FINGER_TIP]
        ring_pip = hand_landmarks.landmark[self.mp_hands.HandLandmark.RING_FINGER_PIP]
        pinky_tip = hand_landmarks.landmark[self.mp_hands.HandLandmark.PINKY_TIP]
        pinky_pip = hand_landmarks.landmark[self.mp_hands.HandLandmark.PINKY_PIP]

        return (index_tip.y < index_pip.y and  # Index finger up
                middle_tip.y < middle_pip.y and  # Middle finger up
                ring_tip.y > ring_pip.y and  # Ring finger down
                pinky_tip.y > pinky_pip.y)  # Pinky down

    def _is_stop(self, hand_landmarks):
        """Check if stop gesture (palm facing forward)"""
        # All fingers extended and palm facing camera
        wrist = hand_landmarks.landmark[self.mp_hands.HandLandmark.WRIST]
        pinky_mcp = hand_landmarks.landmark[self.mp_hands.HandLandmark.PINKY_MCP]
        index_mcp = hand_landmarks.landmark[self.mp_hands.HandLandmark.INDEX_FINGER_MCP]

        # Check if palm is facing forward (wrist to pinky mcp should be more horizontal than vertical)
        return abs(wrist.x - pinky_mcp.x) > abs(wrist.y - pinky_mcp.y)

    def _is_go(self, hand_landmarks):
        """Check if go gesture (pointing)"""
        # Index finger extended, others curled
        index_tip = hand_landmarks.landmark[self.mp_hands.HandLandmark.INDEX_FINGER_TIP]
        index_pip = hand_landmarks.landmark[self.mp_hands.HandLandmark.INDEX_FINGER_PIP]
        middle_tip = hand_landmarks.landmark[self.mp_hands.HandLandmark.MIDDLE_FINGER_TIP]
        middle_pip = hand_landmarks.landmark[self.mp_hands.HandLandmark.MIDDLE_FINGER_PIP]

        return (index_tip.y < index_pip.y and  # Index finger up
                middle_tip.y > middle_pip.y)  # Middle finger down

    def _is_thumb_down(self, hand_landmarks):
        """Check if thumb down gesture"""
        thumb_tip = hand_landmarks.landmark[self.mp_hands.HandLandmark.THUMB_TIP]
        thumb_ip = hand_landmarks.landmark[self.mp_hands.HandLandmark.THUMB_INTERMEDIATE]

        # Index finger should be bent (tip below PIP)
        index_tip = hand_landmarks.landmark[self.mp_hands.HandLandmark.INDEX_FINGER_TIP]
        index_pip = hand_landmarks.landmark[self.mp_hands.HandLandmark.INDEX_FINGER_PIP]

        # Thumb should be extended down while other fingers are curled
        return (thumb_tip.y > thumb_ip.y and  # Thumb down
                index_tip.y > index_pip.y and  # Index finger down
                hand_landmarks.landmark[self.mp_hands.HandLandmark.MIDDLE_FINGER_TIP].y >
                hand_landmarks.landmark[self.mp_hands.HandLandmark.MIDDLE_FINGER_PIP].y and
                hand_landmarks.landmark[self.mp_hands.HandLandmark.RING_FINGER_TIP].y >
                hand_landmarks.landmark[self.mp_hands.HandLandmark.RING_FINGER_PIP].y and
                hand_landmarks.landmark[self.mp_hands.HandLandmark.PINKY_TIP].y >
                hand_landmarks.landmark[self.mp_hands.HandLandmark.PINKY_PIP].y)

    def _is_rock_on(self, hand_landmarks):
        """Check if rock-on gesture (index and pinky extended)"""
        index_tip = hand_landmarks.landmark[self.mp_hands.HandLandmark.INDEX_FINGER_TIP]
        index_pip = hand_landmarks.landmark[self.mp_hands.HandLandmark.INDEX_FINGER_PIP]
        middle_tip = hand_landmarks.landmark[self.mp_hands.HandLandmark.MIDDLE_FINGER_TIP]
        middle_pip = hand_landmarks.landmark[self.mp_hands.HandLandmark.MIDDLE_FINGER_PIP]
        ring_tip = hand_landmarks.landmark[self.mp_hands.HandLandmark.RING_FINGER_TIP]
        ring_pip = hand_landmarks.landmark[self.mp_hands.HandLandmark.RING_FINGER_PIP]
        pinky_tip = hand_landmarks.landmark[self.mp_hands.HandLandmark.PINKY_TIP]
        pinky_pip = hand_landmarks.landmark[self.mp_hands.HandLandmark.PINKY_PIP]

        return (index_tip.y < index_pip.y and  # Index finger up
                middle_tip.y > middle_pip.y and  # Middle finger down
                ring_tip.y > ring_pip.y and  # Ring finger down
                pinky_tip.y < pinky_pip.y)  # Pinky up

    def _is_wave(self, hand_landmarks):
        """Check if waving gesture"""
        # This is a simplified check - in practice, you'd track movement over time
        # For now, we'll consider an open hand as potentially waving
        wrist = hand_landmarks.landmark[self.mp_hands.HandLandmark.WRIST]
        pinky_tip = hand_landmarks.landmark[self.mp_hands.HandLandmark.PINKY_TIP]

        # If hand is relatively open and moving (would need temporal tracking for true wave)
        return True  # Simplified - real implementation would track movement

    def recognize_gesture(self, image):
        """Recognize gesture from image"""
        # Convert image to RGB
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        # Process image
        results = self.hands.process(image_rgb)

        recognized_gestures = []

        if results.multi_hand_landmarks:
            for hand_landmarks in results.multi_hand_landmarks:
                # Try to match each gesture type
                for gesture_type, gesture_func in self.gesture_templates.items():
                    if gesture_func(hand_landmarks):
                        recognized_gestures.append({
                            'gesture': gesture_type,
                            'landmarks': hand_landmarks,
                            'confidence': 0.9  # For now, assume high confidence
                        })

                        # Draw landmarks on image
                        self.mp_drawing.draw_landmarks(
                            image,
                            hand_landmarks,
                            self.mp_hands.HAND_CONNECTIONS,
                            self.mp_drawing_styles.get_default_hand_landmarks_style(),
                            self.mp_drawing_styles.get_default_hand_connections_style()
                        )

        return recognized_gestures, image

    def process_video_stream(self, camera_index=0):
        """Process video stream for gesture recognition"""
        cap = cv2.VideoCapture(camera_index)

        if not cap.isOpened():
            print("Cannot open camera")
            return

        print("Gesture recognition started. Press 'q' to quit.")

        while True:
            ret, frame = cap.read()
            if not ret:
                print("Can't receive frame (stream end?). Exiting ...")
                break

            # Recognize gestures
            gestures, annotated_frame = self.recognize_gesture(frame)

            # Display recognized gestures
            for gesture in gestures:
                cv2.putText(annotated_frame,
                           f"Gesture: {gesture['gesture'].name}",
                           (10, 30),
                           cv2.FONT_HERSHEY_SIMPLEX,
                           1,
                           (0, 255, 0),
                           2)

            # Show frame
            cv2.imshow('Gesture Recognition', annotated_frame)

            if cv2.waitKey(1) & 0xFF == ord('q'):
                break

        cap.release()
        cv2.destroyAllWindows()


# Example usage
def main():
    gesture_recognizer = GestureRecognizer()

    # For demonstration, we'll show how to use it with an image
    # In practice, you'd use the process_video_stream method

    print("Gesture recognition system initialized.")
    print("Available gestures:", [g.name for g in GestureType if g != GestureType.UNKNOWN])

    # To test with video stream, uncomment the following:
    # gesture_recognizer.process_video_stream()


if __name__ == '__main__':
    main()
```

## Facial Expression Recognition

Facial expression recognition enables robots to understand human emotions:

```python
import cv2
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.image import img_to_array
import os


class FacialExpressionRecognizer:
    def __init__(self, model_path=None):
        """Initialize facial expression recognition system"""
        # Load pre-trained face detection model
        self.face_cascade = cv2.CascadeClassifier(
            cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
        )

        # Define emotion labels
        self.emotions = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']

        # Load emotion classification model if provided
        if model_path and os.path.exists(model_path):
            self.model = load_model(model_path)
        else:
            # For this example, we'll create a simple classifier
            # In practice, you'd load a pre-trained model
            self.model = None
            print("Warning: No pre-trained model provided. Using rule-based classification.")

    def preprocess_face(self, face_image):
        """Preprocess face image for emotion classification"""
        # Resize to model input size (assuming 48x48 for example)
        face_resized = cv2.resize(face_image, (48, 48))

        # Convert to grayscale if needed
        if len(face_resized.shape) == 3:
            face_gray = cv2.cvtColor(face_resized, cv2.COLOR_BGR2GRAY)
        else:
            face_gray = face_resized

        # Normalize pixel values
        face_normalized = face_gray / 255.0

        # Reshape for model input (add batch and channel dimensions)
        face_reshaped = face_normalized.reshape(1, 48, 48, 1)

        return face_reshaped

    def classify_expression_rule_based(self, face_image):
        """Rule-based expression classification (simplified)"""
        # This is a very simplified approach
        # In practice, use a deep learning model

        # Convert to grayscale
        if len(face_image.shape) == 3:
            gray = cv2.cvtColor(face_image, cv2.COLOR_BGR2GRAY)
        else:
            gray = face_image

        # Simple heuristics for emotion detection
        # These are very basic and not accurate - for demonstration only

        # Calculate some basic features
        height, width = gray.shape
        eye_region = gray[int(height*0.3):int(height*0.5), :]
        mouth_region = gray[int(height*0.6):int(height*0.8), :]

        # Simple metrics
        eye_brightness = np.mean(eye_region)
        mouth_brightness = np.mean(mouth_region)

        # Very basic classification based on simple features
        if mouth_brightness > 150:  # Bright mouth area (possibly smile)
            return 'happy', 0.6
        elif eye_brightness < 100:  # Dark eyes (possibly sad/angry)
            return 'sad', 0.5
        else:
            return 'neutral', 0.4

    def recognize_expressions(self, image):
        """Recognize facial expressions in image"""
        # Convert to grayscale
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

        # Detect faces
        faces = self.face_cascade.detectMultiScale(
            gray,
            scaleFactor=1.1,
            minNeighbors=5,
            minSize=(30, 30)
        )

        results = []

        for (x, y, w, h) in faces:
            # Extract face region
            face_roi = gray[y:y+h, x:x+w]

            # Classify expression
            if self.model:
                # Use deep learning model
                processed_face = self.preprocess_face(face_roi)
                predictions = self.model.predict(processed_face)
                emotion_idx = np.argmax(predictions[0])
                emotion = self.emotions[emotion_idx]
                confidence = float(predictions[0][emotion_idx])
            else:
                # Use rule-based classification
                emotion, confidence = self.classify_expression_rule_based(face_roi)

            # Draw rectangle around face
            cv2.rectangle(image, (x, y), (x+w, y+h), (255, 0, 0), 2)

            # Label with emotion
            label = f"{emotion} ({confidence:.2f})"
            cv2.putText(image, label, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36,255,12), 2)

            results.append({
                'emotion': emotion,
                'confidence': confidence,
                'bbox': (x, y, w, h)
            })

        return results, image

    def process_video_stream(self, camera_index=0):
        """Process video stream for facial expression recognition"""
        cap = cv2.VideoCapture(camera_index)

        if not cap.isOpened():
            print("Cannot open camera")
            return

        print("Facial expression recognition started. Press 'q' to quit.")

        while True:
            ret, frame = cap.read()
            if not ret:
                print("Can't receive frame (stream end?). Exiting ...")
                break

            # Recognize expressions
            results, annotated_frame = self.recognize_expressions(frame)

            # Show frame
            cv2.imshow('Facial Expression Recognition', annotated_frame)

            if cv2.waitKey(1) & 0xFF == ord('q'):
                break

        cap.release()
        cv2.destroyAllWindows()


# Example usage
def main():
    expression_recognizer = FacialExpressionRecognizer()

    print("Facial expression recognition system initialized.")
    print("Available emotions:", expression_recognizer.emotions)

    # To test with video stream, uncomment the following:
    # expression_recognizer.process_video_stream()


if __name__ == '__main__':
    main()
```

## Social Robotics Principles

### Theory of Mind for Robots

Theory of Mind is the ability to attribute mental states to oneself and others:

```python
import numpy as np
from dataclasses import dataclass
from typing import Dict, List, Optional
import json


@dataclass
class MentalState:
    beliefs: Dict[str, any]
    desires: Dict[str, any]
    intentions: List[str]
    emotions: Dict[str, float]  # Emotion name to intensity (0.0 to 1.0)


class TheoryOfMindSystem:
    def __init__(self):
        """Initialize Theory of Mind system for the robot"""
        self.robot_mental_state = MentalState(
            beliefs={},
            desires={},
            intentions=[],
            emotions={'calm': 1.0}  # Start calm
        )

        self.human_mental_models = {}  # Track mental models of different humans
        self.interaction_history = []  # Track past interactions

    def update_robot_beliefs(self, belief_key: str, belief_value: any):
        """Update robot's beliefs about the world"""
        self.robot_mental_state.beliefs[belief_key] = belief_value

    def update_robot_emotions(self, emotion: str, intensity: float):
        """Update robot's emotional state"""
        self.robot_mental_state.emotions[emotion] = max(0.0, min(1.0, intensity))

        # Normalize emotions so they sum to 1.0 (simple approach)
        total = sum(self.robot_mental_state.emotions.values())
        if total > 0:
            for emo in self.robot_mental_state.emotions:
                self.robot_mental_state.emotions[emo] /= total

    def create_human_mental_model(self, human_id: str):
        """Create a mental model for a new human"""
        self.human_mental_models[human_id] = MentalState(
            beliefs={},
            desires={},
            intentions=[],
            emotions={}
        )

    def update_human_belief(self, human_id: str, belief_key: str, belief_value: any):
        """Update our model of what a human believes"""
        if human_id not in self.human_mental_models:
            self.create_human_mental_model(human_id)

        self.human_mental_models[human_id].beliefs[belief_key] = belief_value

    def update_human_desire(self, human_id: str, desire_key: str, desire_value: any):
        """Update our model of what a human desires"""
        if human_id not in self.human_mental_models:
            self.create_human_mental_model(human_id)

        self.human_mental_models[human_id].desires[desire_key] = desire_value

    def infer_human_intention(self, human_id: str, observed_action: str) -> Optional[str]:
        """Infer human intention based on observed action"""
        if human_id not in self.human_mental_models:
            return None

        # Simple inference based on action patterns
        # In a real system, this would use more sophisticated models
        action_intents = {
            'reaching': 'grasp_object',
            'pointing': 'request_attention',
            'waving': 'greet',
            'frowning': 'express_displeasure',
            'smiling': 'express_pleasure'
        }

        for pattern, intent in action_intents.items():
            if pattern in observed_action.lower():
                self.human_mental_models[human_id].intentions.append(intent)
                return intent

        return None

    def predict_human_response(self, human_id: str, robot_action: str) -> str:
        """Predict how a human might respond to robot's action"""
        if human_id not in self.human_mental_models:
            return "neutral_response"

        human_model = self.human_mental_models[human_id]

        # Simple prediction based on human's desires and beliefs
        if 'help' in robot_action.lower():
            if 'need_assistance' in human_model.desires.values():
                return "positive_response"
            else:
                return "neutral_response"
        elif 'personal_space' in robot_action.lower():
            if any(emotion in human_model.emotions for emotion in ['comfortable', 'relaxed']):
                return "accepting_response"
            else:
                return "avoiding_response"
        else:
            return "neutral_response"

    def adjust_behavior(self, human_id: str, interaction_result: str):
        """Adjust robot behavior based on interaction outcome"""
        if human_id not in self.human_mental_models:
            return

        # Update robot's beliefs based on interaction result
        if interaction_result == "positive":
            self.update_robot_emotions('happy', 0.8)
            self.update_robot_beliefs(f'{human_id}_responsive', True)
        elif interaction_result == "negative":
            self.update_robot_emotions('concerned', 0.7)
            self.update_robot_beliefs(f'{human_id}_needs_space', True)
        else:  # neutral
            self.update_robot_emotions('calm', 0.9)
            self.update_robot_beliefs(f'{human_id}_comfortable', True)

    def get_social_response(self, human_id: str, context: Dict[str, any]) -> Dict[str, any]:
        """Generate appropriate social response based on ToM system"""
        if human_id not in self.human_mental_models:
            self.create_human_mental_model(human_id)

        # Consider both robot and human mental states
        robot_emotion = max(self.robot_mental_state.emotions.items(), key=lambda x: x[1])
        human_model = self.human_mental_models[human_id]

        response = {
            'action': 'wait',
            'verbal_response': 'Hello!',
            'emotional_tone': robot_emotion[0],
            'confidence': 0.8
        }

        # Adjust response based on human model
        if 'greeting' in context.get('observed_action', '').lower():
            response['action'] = 'greet_back'
            response['verbal_response'] = f"Hello {human_id}! Nice to meet you."
        elif 'help' in context.get('request', '').lower():
            response['action'] = 'offer_assistance'
            response['verbal_response'] = f"I'd be happy to help you with that, {human_id}."
        elif 'distressed' in context.get('detected_emotion', '').lower():
            response['action'] = 'comfort'
            response['verbal_response'] = f"I notice you seem distressed. Is there anything I can do to help?"

        # Adjust emotional tone based on human's apparent emotional state
        if 'happy' in context.get('detected_emotion', '').lower():
            response['emotional_tone'] = 'joyful'
        elif 'sad' in context.get('detected_emotion', '').lower():
            response['emotional_tone'] = 'empathetic'

        return response


# Example usage
def main():
    tom_system = TheoryOfMindSystem()

    print("Theory of Mind system initialized.")

    # Simulate an interaction
    human_id = "user_001"
    tom_system.create_human_mental_model(human_id)

    # Update human model based on observations
    tom_system.update_human_belief(human_id, "robot_is_helpful", True)
    tom_system.update_human_desire(human_id, "task_completion", "high")

    # Infer intention from observed action
    intention = tom_system.infer_human_intention(human_id, "human is pointing at object")
    print(f"Inferred intention: {intention}")

    # Predict human response to robot action
    response_prediction = tom_system.predict_human_response(human_id, "robot moves closer")
    print(f"Predicted response: {response_prediction}")

    # Get social response
    context = {
        'observed_action': 'greeting',
        'request': 'help with task',
        'detected_emotion': 'neutral'
    }

    social_response = tom_system.get_social_response(human_id, context)
    print(f"Social response: {social_response}")


if __name__ == '__main__':
    main()
```

## Collaborative Robotics

### Human-Robot Collaboration Framework

```python
import asyncio
import threading
from dataclasses import dataclass
from typing import Dict, List, Callable, Optional
from enum import Enum
import time


class TaskType(Enum):
    INDEPENDENT = "independent"
    COORDINATED = "coordinated"
    DEPENDENT = "dependent"


class RobotRole(Enum):
    SUPPORT = "support"
    LEAD = "lead"
    FOLLOW = "follow"
    NEUTRAL = "neutral"


@dataclass
class Task:
    id: str
    description: str
    type: TaskType
    required_skills: List[str]
    duration_estimate: float  # in seconds
    dependencies: List[str]  # task IDs this task depends on
    assigned_to: Optional[str] = None  # "human" or "robot" or None (unassigned)


@dataclass
class CollaborationState:
    active_tasks: List[Task]
    human_status: Dict[str, any]  # position, activity, workload
    robot_status: Dict[str, any]  # position, activity, workload
    collaboration_mode: RobotRole
    task_progress: Dict[str, float]  # task_id to completion percentage


class HumanRobotCollaborationManager:
    def __init__(self):
        """Initialize collaboration management system"""
        self.tasks = {}
        self.collaboration_state = CollaborationState(
            active_tasks=[],
            human_status={'position': [0, 0], 'activity': 'idle', 'workload': 0.0},
            robot_status={'position': [0, 0], 'activity': 'idle', 'workload': 0.0},
            collaboration_mode=RobotRole.NEUTRAL,
            task_progress={}
        )

        self.task_assignments = {}
        self.event_handlers = {
            'task_completed': [],
            'human_available': [],
            'robot_available': []
        }

        self.running = False
        self.main_loop_task = None

    def add_task(self, task: Task):
        """Add a task to the collaboration system"""
        self.tasks[task.id] = task
        self.collaboration_state.task_progress[task.id] = 0.0
        print(f"Added task: {task.description}")

    def assign_task(self, task_id: str, assignee: str):
        """Assign a task to either human or robot"""
        if task_id not in self.tasks:
            raise ValueError(f"Task {task_id} not found")

        task = self.tasks[task_id]
        task.assigned_to = assignee
        self.task_assignments[task_id] = assignee
        print(f"Assigned task '{task.description}' to {assignee}")

    def calculate_workload(self, agent: str) -> float:
        """Calculate current workload for an agent"""
        workload = 0.0
        for task_id, assignee in self.task_assignments.items():
            if assignee == agent:
                task = self.tasks[task_id]
                progress = self.collaboration_state.task_progress.get(task_id, 0.0)
                workload += (1.0 - progress) * task.duration_estimate

        return min(workload, 1.0)  # Normalize to 0-1

    def assess_collaboration_needs(self) -> RobotRole:
        """Assess the current situation and determine robot role"""
        human_workload = self.calculate_workload('human')
        robot_workload = self.calculate_workload('robot')

        # Determine role based on workloads and task types
        if human_workload > 0.7 and robot_workload < 0.3:
            # Human is overloaded, robot should support
            return RobotRole.SUPPORT
        elif robot_workload > 0.7 and human_workload < 0.3:
            # Robot is overloaded, human should lead
            return RobotRole.FOLLOW
        elif any(task.type == TaskType.COORDINATED for task in self.collaboration_state.active_tasks):
            # Coordinated tasks need balanced collaboration
            return RobotRole.NEUTRAL
        else:
            # Default to support role
            return RobotRole.SUPPORT

    def plan_coordination(self) -> List[Dict[str, any]]:
        """Plan coordination for dependent tasks"""
        coordination_plans = []

        for task in self.tasks.values():
            if task.type == TaskType.DEPENDENT and task.assigned_to == 'robot':
                # Check dependencies
                for dep_id in task.dependencies:
                    if dep_id in self.tasks:
                        dependency_task = self.tasks[dep_id]
                        if dependency_task.assigned_to == 'human':
                            # Plan coordination point
                            coordination_plans.append({
                                'type': 'handoff',
                                'task_id': task.id,
                                'dependency_task': dep_id,
                                'trigger_condition': f'task_{dep_id}_completed',
                                'location': 'workspace_center'  # Would be actual location
                            })

        return coordination_plans

    def execute_coordination(self, plan: Dict[str, any]):
        """Execute a coordination plan"""
        if plan['type'] == 'handoff':
            task_id = plan['task_id']
            dep_task_id = plan['dependency_task']

            # Wait for dependency to complete
            while self.collaboration_state.task_progress.get(dep_task_id, 0) < 1.0:
                time.sleep(0.1)

            # Start the dependent task
            self.start_task(task_id)

    def start_task(self, task_id: str):
        """Start execution of a task"""
        if task_id not in self.tasks:
            print(f"Task {task_id} not found")
            return

        task = self.tasks[task_id]
        if task.assigned_to == 'robot':
            # Simulate robot task execution
            print(f"Robot starting task: {task.description}")
            # In a real system, this would call robot action services
            time.sleep(task.duration_estimate * 0.1)  # Simulate partial completion
            self.collaboration_state.task_progress[task_id] = 0.1
        elif task.assigned_to == 'human':
            print(f"Indicating to human: {task.description}")
            # In a real system, this would alert the human
            self.collaboration_state.task_progress[task_id] = 0.0  # Human updates progress

    def update_human_status(self, position: List[float], activity: str, workload: float):
        """Update human status"""
        self.collaboration_state.human_status.update({
            'position': position,
            'activity': activity,
            'workload': workload
        })

    def update_robot_status(self, position: List[float], activity: str, workload: float):
        """Update robot status"""
        self.collaboration_state.robot_status.update({
            'position': position,
            'activity': activity,
            'workload': workload
        })

    def run_main_loop(self):
        """Main collaboration loop"""
        while self.running:
            # Assess current collaboration needs
            new_role = self.assess_collaboration_needs()
            if new_role != self.collaboration_state.collaboration_mode:
                print(f"Changing robot role from {self.collaboration_state.collaboration_mode.value} to {new_role.value}")
                self.collaboration_state.collaboration_mode = new_role

            # Plan and execute coordinations
            coordination_plans = self.plan_coordination()
            for plan in coordination_plans:
                self.execute_coordination(plan)

            # Update task progress (simulated)
            for task_id, progress in self.collaboration_state.task_progress.items():
                if progress < 1.0 and self.tasks[task_id].assigned_to == 'robot':
                    # Simulate robot working on task
                    increment = 0.01  # 1% per iteration
                    new_progress = min(progress + increment, 1.0)
                    self.collaboration_state.task_progress[task_id] = new_progress

                    if new_progress == 1.0:
                        print(f"Robot completed task: {self.tasks[task_id].description}")
                        # Trigger completion event
                        for handler in self.event_handlers['task_completed']:
                            handler(task_id, 'robot')

            time.sleep(0.1)  # 10 Hz update rate

    def start_collaboration(self):
        """Start the collaboration system"""
        self.running = True
        self.main_loop_task = threading.Thread(target=self.run_main_loop)
        self.main_loop_task.start()
        print("Collaboration system started")

    def stop_collaboration(self):
        """Stop the collaboration system"""
        self.running = False
        if self.main_loop_task:
            self.main_loop_task.join()
        print("Collaboration system stopped")

    def add_event_handler(self, event_type: str, handler: Callable):
        """Add event handler for collaboration events"""
        if event_type in self.event_handlers:
            self.event_handlers[event_type].append(handler)


# Example usage
def main():
    collab_manager = HumanRobotCollaborationManager()

    # Add some tasks
    task1 = Task(
        id="assemble_part_1",
        description="Assemble first component",
        type=TaskType.INDEPENDENT,
        required_skills=["assembly"],
        duration_estimate=30.0,
        dependencies=[]
    )

    task2 = Task(
        id="inspect_part_1",
        description="Inspect assembled component",
        type=TaskType.DEPENDENT,
        required_skills=["inspection"],
        duration_estimate=15.0,
        dependencies=["assemble_part_1"]
    )

    task3 = Task(
        id="transport_to_station_2",
        description="Transport to next station",
        type=TaskType.COORDINATED,
        required_skills=["transport"],
        duration_estimate=20.0,
        dependencies=["inspect_part_1"]
    )

    collab_manager.add_task(task1)
    collab_manager.add_task(task2)
    collab_manager.add_task(task3)

    # Assign tasks
    collab_manager.assign_task("assemble_part_1", "human")
    collab_manager.assign_task("inspect_part_1", "robot")
    collab_manager.assign_task("transport_to_station_2", "robot")

    # Add event handler
    def on_task_completed(task_id, agent):
        print(f"Event: {agent} completed {task_id}")

    collab_manager.add_event_handler('task_completed', on_task_completed)

    # Start collaboration
    collab_manager.start_collaboration()

    # Update statuses
    collab_manager.update_human_status([1.0, 2.0], "assembling", 0.6)
    collab_manager.update_robot_status([0.5, 0.5], "waiting", 0.1)

    try:
        # Let it run for a while
        time.sleep(10)
    except KeyboardInterrupt:
        pass
    finally:
        collab_manager.stop_collaboration()


if __name__ == '__main__':
    main()
```

## ROS 2 Integration for HRI

### Complete HRI Node

```python
import rclpy
from rclpy.node import Node
from std_msgs.msg import String, Bool, Float32
from geometry_msgs.msg import Twist, Pose
from sensor_msgs.msg import Image, JointState
from std_msgs.msg import Header
from visualization_msgs.msg import Marker, MarkerArray
import numpy as np
import json


class HumanRobotInteractionNode(Node):
    def __init__(self):
        super().__init__('human_robot_interaction_node')

        # Publishers
        self.cmd_vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)
        self.speech_pub = self.create_publisher(String, '/robot_speech', 10)
        self.social_status_pub = self.create_publisher(String, '/social_status', 10)
        self.visualization_pub = self.create_publisher(MarkerArray, '/hri_visualization', 10)

        # Subscribers
        self.speech_sub = self.create_subscription(
            String,
            '/recognized_speech',
            self.speech_callback,
            10
        )

        self.gesture_sub = self.create_subscription(
            String,
            '/recognized_gesture',
            self.gesture_callback,
            10
        )

        self.face_sub = self.create_subscription(
            String,
            '/recognized_face',
            self.face_callback,
            10
        )

        self.proximity_sub = self.create_subscription(
            Float32,
            '/human_proximity',
            self.proximity_callback,
            10
        )

        # Initialize HRI components
        self.nlu_system = NaturalLanguageUnderstanding()
        self.tom_system = TheoryOfMindSystem()
        self.collab_manager = HumanRobotCollaborationManager()

        # Interaction state
        self.current_interactions = {}
        self.attended_human = None
        self.interaction_mode = 'social'  # social, task, collaboration

        # Timer for social behavior
        self.social_timer = self.create_timer(1.0, self.social_behavior_loop)

        # Initialize collaboration manager
        self.collab_manager.start_collaboration()

        self.get_logger().info('Human-Robot Interaction node initialized')

    def speech_callback(self, msg):
        """Handle recognized speech"""
        try:
            # Parse the message (assuming it's JSON with text and confidence)
            try:
                speech_data = json.loads(msg.data)
                text = speech_data['text']
                confidence = speech_data.get('confidence', 1.0)
            except json.JSONDecodeError:
                text = msg.data
                confidence = 1.0

            self.get_logger().info(f'Heard: {text} (confidence: {confidence:.2f})')

            if confidence > 0.5:  # Only process if confidence is high enough
                # Parse the speech using NLU system
                parsed_command = self.nlu_system.parse_command(text)
                response = self.nlu_system.generate_response(parsed_command)

                # Update Theory of Mind system
                if self.attended_human:
                    self.tom_system.update_human_desire(self.attended_human, 'communication_need', text)

                # Generate robot response
                self.respond_to_human(response)

        except Exception as e:
            self.get_logger().error(f'Error processing speech: {e}')

    def gesture_callback(self, msg):
        """Handle recognized gestures"""
        try:
            gesture_data = json.loads(msg.data)
            gesture = gesture_data['gesture']
            confidence = gesture_data['confidence']
            human_id = gesture_data.get('human_id', 'unknown')

            self.get_logger().info(f'Recognized gesture: {gesture} from {human_id} (confidence: {confidence:.2f})')

            if confidence > 0.7:  # High confidence gesture
                # Update Theory of Mind system
                if human_id != 'unknown':
                    self.tom_system.update_human_emotions(human_id, gesture, confidence)

                # Respond to gesture
                self.respond_to_gesture(gesture, human_id)

        except Exception as e:
            self.get_logger().error(f'Error processing gesture: {e}')

    def face_callback(self, msg):
        """Handle recognized faces/emotions"""
        try:
            face_data = json.loads(msg.data)
            emotion = face_data['emotion']
            confidence = face_data['confidence']
            human_id = face_data['human_id']

            self.get_logger().info(f'Recognized emotion: {emotion} from {human_id} (confidence: {confidence:.2f})')

            # Update Theory of Mind system
            self.tom_system.update_human_emotions(human_id, emotion, confidence)

            # Attend to this human if not already attending
            if self.attended_human != human_id:
                self.attend_to_human(human_id)

            # Adjust behavior based on emotion
            self.adjust_behavior_for_emotion(emotion, human_id)

        except Exception as e:
            self.get_logger().error(f'Error processing face: {e}')

    def proximity_callback(self, msg):
        """Handle human proximity detection"""
        distance = msg.data
        self.get_logger().info(f'Human proximity: {distance:.2f}m')

        # If human comes close enough, attend to them
        if distance < 2.0 and self.attended_human is None:
            self.attend_to_human('approaching_human')

    def respond_to_human(self, response_text):
        """Generate response to human"""
        self.get_logger().info(f'Robot response: {response_text}')

        # Publish speech response
        speech_msg = String()
        speech_msg.data = response_text
        self.speech_pub.publish(speech_msg)

        # Update social status
        status_msg = String()
        status_msg.data = f"responding: {response_text[:50]}..."
        self.social_status_pub.publish(status_msg)

    def respond_to_gesture(self, gesture, human_id):
        """Respond to human gesture"""
        response_map = {
            'WAVE': f"Hello {human_id}! I see your wave.",
            'THUMB_UP': f"Thanks for the thumbs up, {human_id}!",
            'STOP': f"I understand you want me to stop, {human_id}.",
            'GO': f"Okay {human_id}, I'll proceed."
        }

        response = response_map.get(gesture, f"I noticed your {gesture} gesture, {human_id}.")
        self.respond_to_human(response)

    def attend_to_human(self, human_id):
        """Attend to a specific human"""
        self.attended_human = human_id
        self.get_logger().info(f'Attending to human: {human_id}')

        # Create visualization marker
        marker = Marker()
        marker.header.frame_id = "base_link"
        marker.header.stamp = self.get_clock().now().to_msg()
        marker.ns = "hri_attention"
        marker.id = 0
        marker.type = Marker.SPHERE
        marker.action = Marker.ADD
        marker.pose.position.x = 1.0  # In front of robot
        marker.pose.position.y = 0.0
        marker.pose.position.z = 0.0
        marker.pose.orientation.w = 1.0
        marker.scale.x = 0.2
        marker.scale.y = 0.2
        marker.scale.z = 0.2
        marker.color.a = 1.0
        marker.color.r = 0.0
        marker.color.g = 1.0
        marker.color.b = 0.0
        marker.text = f"Attending to {human_id}"

        marker_array = MarkerArray()
        marker_array.markers.append(marker)
        self.visualization_pub.publish(marker_array)

    def adjust_behavior_for_emotion(self, emotion, human_id):
        """Adjust robot behavior based on human emotion"""
        if emotion == 'happy':
            # Be more expressive and friendly
            self.update_robot_expressivity(0.8)
        elif emotion == 'sad':
            # Be more empathetic and gentle
            self.update_robot_expressivity(0.3)
            self.respond_to_human(f"You seem sad, {human_id}. Is there anything I can do to help?")
        elif emotion == 'angry':
            # Increase distance and be calm
            self.increase_personal_space()
            self.update_robot_expressivity(0.1)
            self.respond_to_human(f"I notice you seem upset, {human_id}. I'll give you some space.")

    def update_robot_expressivity(self, level):
        """Update robot expressivity level"""
        # This would affect animation, movement smoothness, etc.
        self.get_logger().info(f'Updating robot expressivity to level: {level}')

    def increase_personal_space(self):
        """Increase personal space from humans"""
        # This would affect navigation and interaction distance
        self.get_logger().info('Increasing personal space')

    def social_behavior_loop(self):
        """Periodic social behavior loop"""
        # If no one is attending to the robot, be more socially visible
        if self.attended_human is None:
            # Maybe move to a more visible location or perform attention-getting behavior
            self.get_logger().info('No human attending, performing social behaviors')

            # Publish a small movement to be more noticeable
            cmd_vel = Twist()
            cmd_vel.linear.x = 0.05  # Small forward movement
            self.cmd_vel_pub.publish(cmd_vel)

        # Update social status
        status = {
            'attended_human': self.attended_human,
            'interaction_mode': self.interaction_mode,
            'last_interaction': self.get_clock().now().seconds_nanoseconds()
        }

        status_msg = String()
        status_msg.data = json.dumps(status)
        self.social_status_pub.publish(status_msg)


def main(args=None):
    rclpy.init(args=args)
    hri_node = HumanRobotInteractionNode()

    try:
        rclpy.spin(hri_node)
    except KeyboardInterrupt:
        pass
    finally:
        hri_node.collab_manager.stop_collaboration()
        hri_node.destroy_node()
        rclpy.shutdown()


if __name__ == '__main__':
    main()
```

## Ethical Considerations in HRI

### Privacy and Trust

```python
import hashlib
import time
from dataclasses import dataclass
from typing import Dict, List


@dataclass
class ConsentRecord:
    subject_id: str
    data_type: str
    granted_at: float
    expires_at: float
    granted: bool
    purpose: str


class HRIConsentManager:
    def __init__(self):
        """Initialize consent and privacy management system"""
        self.consent_records: Dict[str, ConsentRecord] = {}
        self.privacy_settings: Dict[str, Dict[str, bool]] = {}
        self.data_access_log: List[Dict[str, any]] = []

    def request_consent(self, subject_id: str, data_type: str, purpose: str, duration_hours: int = 24) -> bool:
        """Request consent for data collection"""
        consent_key = f"{subject_id}:{data_type}:{purpose}"

        # Check if consent already exists and is valid
        if consent_key in self.consent_records:
            record = self.consent_records[consent_key]
            if time.time() < record.expires_at and record.granted:
                return True  # Already granted and valid

        # In a real system, this would show a consent request to the user
        # For simulation, we'll auto-grant for demonstration
        granted = True  # This would come from user interaction

        # Create consent record
        record = ConsentRecord(
            subject_id=subject_id,
            data_type=data_type,
            granted_at=time.time(),
            expires_at=time.time() + (duration_hours * 3600),
            granted=granted,
            purpose=purpose
        )

        self.consent_records[consent_key] = record

        return granted

    def can_collect_data(self, subject_id: str, data_type: str, purpose: str) -> bool:
        """Check if data collection is allowed"""
        consent_key = f"{subject_id}:{data_type}:{purpose}"

        if consent_key not in self.consent_records:
            return False

        record = self.consent_records[consent_key]
        return (record.granted and
                time.time() < record.expires_at and
                self.privacy_settings.get(subject_id, {}).get(data_type, True))

    def log_data_access(self, subject_id: str, data_type: str, purpose: str, accessor: str) -> str:
        """Log data access for audit trail"""
        access_id = hashlib.sha256(f"{subject_id}{data_type}{time.time()}".encode()).hexdigest()[:16]

        log_entry = {
            'access_id': access_id,
            'subject_id': subject_id,
            'data_type': data_type,
            'purpose': purpose,
            'accessor': accessor,
            'timestamp': time.time(),
            'consented': self.can_collect_data(subject_id, data_type, purpose)
        }

        self.data_access_log.append(log_entry)
        return access_id

    def set_privacy_setting(self, subject_id: str, data_type: str, allowed: bool):
        """Set privacy preferences for a specific data type"""
        if subject_id not in self.privacy_settings:
            self.privacy_settings[subject_id] = {}

        self.privacy_settings[subject_id][data_type] = allowed

    def get_privacy_controls(self, subject_id: str) -> Dict[str, bool]:
        """Get privacy controls for a subject"""
        return self.privacy_settings.get(subject_id, {})


class HRITrustManager:
    def __init__(self):
        """Initialize trust management system"""
        self.trust_scores: Dict[str, float] = {}  # Human ID to trust score
        self.interaction_history: Dict[str, List[Dict[str, any]]] = {}
        self.trust_degradation_rate = 0.01  # Trust decreases over time without interaction

    def update_trust(self, human_id: str, interaction_outcome: str, weight: float = 1.0):
        """Update trust based on interaction outcome"""
        if human_id not in self.trust_scores:
            self.trust_scores[human_id] = 0.5  # Start with neutral trust

        # Update trust based on outcome
        if interaction_outcome == 'positive':
            self.trust_scores[human_id] = min(1.0, self.trust_scores[human_id] + 0.1 * weight)
        elif interaction_outcome == 'negative':
            self.trust_scores[human_id] = max(0.0, self.trust_scores[human_id] - 0.2 * weight)
        else:  # neutral
            pass  # No change

        # Log the interaction
        if human_id not in self.interaction_history:
            self.interaction_history[human_id] = []

        self.interaction_history[human_id].append({
            'timestamp': time.time(),
            'outcome': interaction_outcome,
            'weight': weight,
            'trust_after': self.trust_scores[human_id]
        })

    def get_trust_score(self, human_id: str) -> float:
        """Get current trust score for a human"""
        # Apply degradation over time
        if human_id in self.trust_scores:
            last_interaction = 0
            if human_id in self.interaction_history and self.interaction_history[human_id]:
                last_interaction = self.interaction_history[human_id][-1]['timestamp']

            time_since_last = time.time() - last_interaction
            degradation = self.trust_degradation_rate * (time_since_last / 3600)  # Degradation per hour

            current_trust = max(0.0, self.trust_scores[human_id] - degradation)
            self.trust_scores[human_id] = current_trust
            return current_trust

        return 0.5  # Default neutral trust

    def adjust_behavior_for_trust(self, human_id: str) -> Dict[str, any]:
        """Get behavior adjustments based on trust level"""
        trust = self.get_trust_score(human_id)

        adjustments = {
            'proactivity': min(trust, 0.8),  # Be less proactive with low trust
            'physical_closeness': min(trust * 1.5, 1.0),  # Don't get too close with low trust
            'autonomy': trust,  # Give more autonomy to trusted humans
            'verification': 1.0 - trust  # Verify more with low trust
        }

        return adjustments


# Example usage
def main():
    consent_manager = HRIConsentManager()
    trust_manager = HRITrustManager()

    # Example: Human John consents to facial recognition for interaction purposes
    john_id = "john_doe_123"
    consent_granted = consent_manager.request_consent(
        subject_id=john_id,
        data_type="facial_recognition",
        purpose="social_interaction",
        duration_hours=24
    )

    print(f"Consent granted for John: {consent_granted}")

    # Check if data collection is allowed
    can_collect = consent_manager.can_collect_data(
        subject_id=john_id,
        data_type="facial_recognition",
        purpose="social_interaction"
    )
    print(f"Can collect facial data: {can_collect}")

    # Update trust based on interactions
    trust_manager.update_trust(john_id, 'positive', weight=0.5)
    trust_manager.update_trust(john_id, 'positive', weight=0.5)
    trust_manager.update_trust(john_id, 'negative', weight=0.3)

    current_trust = trust_manager.get_trust_score(john_id)
    print(f"Current trust for John: {current_trust:.2f}")

    # Get behavior adjustments based on trust
    adjustments = trust_manager.adjust_behavior_for_trust(john_id)
    print(f"Trust-based adjustments: {adjustments}")

    # Set privacy preferences
    consent_manager.set_privacy_setting(john_id, "voice_recording", False)
    privacy_controls = consent_manager.get_privacy_controls(john_id)
    print(f"Privacy controls for John: {privacy_controls}")


if __name__ == '__main__':
    main()
```

## Hands-On Lab: Social Robot Companion

### Objective
Create a complete social robot companion that can engage in natural conversations, recognize emotions, and adapt its behavior based on trust and privacy preferences.

### Prerequisites
- Completed Chapter 1-12
- ROS 2 Humble with Gazebo
- Basic understanding of HRI concepts

### Steps

1. **Create a social robot lab package**:
   ```bash
   cd ~/robotics_ws/src
   ros2 pkg create --build-type ament_python social_robot_lab --dependencies rclpy std_msgs geometry_msgs sensor_msgs cv_bridge pyttsx3 speech_recognition numpy matplotlib
   ```

2. **Create the main social robot node** (`social_robot_lab/social_robot_lab/social_robot_node.py`):
   ```python
   #!/usr/bin/env python3
   import rclpy
   from rclpy.node import Node
   from std_msgs.msg import String, Bool, Float32
   from geometry_msgs.msg import Twist, Pose
   from sensor_msgs.msg import Image
   from cv_bridge import CvBridge
   import numpy as np
   import random
   import time
   import threading
   import queue


   class SocialRobotNode(Node):
       def __init__(self):
           super().__init__('social_robot_node')

           # Publishers
           self.cmd_vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)
           self.speech_pub = self.create_publisher(String, '/robot_speech', 10)
           self.behavior_pub = self.create_publisher(String, '/robot_behavior', 10)
           self.attention_pub = self.create_publisher(Pose, '/robot_attention', 10)

           # Subscribers
           self.speech_sub = self.create_subscription(
               String,
               '/human_speech',
               self.human_speech_callback,
               10
           )

           self.gesture_sub = self.create_subscription(
               String,
               '/human_gesture',
               self.human_gesture_callback,
               10
           )

           self.proximity_sub = self.create_subscription(
               Float32,
               '/human_proximity',
               self.proximity_callback,
               10
           )

           # Initialize components
           self.bridge = CvBridge()
           self.conversation_history = []
           self.engaged_human = None
           self.interaction_mode = 'idle'  # idle, social, task, learning
           self.personality = self.initialize_personality()
           self.mood = 'friendly'
           self.energy_level = 0.8  # 0.0 to 1.0

           # Trust and relationship management
           self.relationships = {}
           self.long_term_memory = {}

           # Interaction queues
           self.speech_queue = queue.Queue()
           self.response_queue = queue.Queue()

           # Timers
           self.interaction_timer = self.create_timer(0.1, self.interaction_loop)
           self.mood_timer = self.create_timer(10.0, self.update_mood)
           self.social_timer = self.create_timer(5.0, self.periodic_social_behavior)

           # Start response thread
           self.response_thread = threading.Thread(target=self.process_responses)
           self.response_thread.daemon = True
           self.response_thread.start()

           self.get_logger().info('Social Robot Node initialized')

       def initialize_personality(self):
           """Initialize robot personality traits"""
           return {
               'extroversion': 0.7,  # 0.0 to 1.0
               'agreeableness': 0.9,
               'conscientiousness': 0.6,
               'emotional_stability': 0.8,
               'openness': 0.7
           }

       def human_speech_callback(self, msg):
           """Process human speech"""
           self.get_logger().info(f'Heard: {msg.data}')
           self.conversation_history.append({
               'speaker': 'human',
               'text': msg.data,
               'timestamp': time.time()
           })

           # Add to processing queue
           self.speech_queue.put(msg.data)

       def human_gesture_callback(self, msg):
           """Process human gesture"""
           self.get_logger().info(f'Gesture detected: {msg.data}')
           # Could trigger specific responses based on gesture

       def proximity_callback(self, msg):
           """Process human proximity"""
           distance = msg.data
           self.get_logger().info(f'Human proximity: {distance:.2f}m')

           if distance < 2.0 and self.interaction_mode == 'idle':
               self.initiate_social_interaction()
           elif distance > 3.0 and self.engaged_human:
               self.disengage_interaction()

       def initiate_social_interaction(self):
           """Initiate social interaction when human approaches"""
           self.interaction_mode = 'social'
           self.engaged_human = 'approaching_human'
           self.update_relationship(self.engaged_human)

           # Generate greeting based on personality
           greeting = self.generate_greeting()
           self.speak(greeting)

           self.get_logger().info(f'Initiating interaction with {self.engaged_human}')

       def disengage_interaction(self):
           """Disengage from interaction when human leaves"""
           if self.engaged_human:
               self.get_logger().info(f'Disengaging from {self.engaged_human}')
               farewell = f"It was nice talking to you! Feel free to come back anytime."
               self.speak(farewell)
               self.engaged_human = None
               self.interaction_mode = 'idle'

       def generate_greeting(self):
           """Generate personalized greeting based on personality"""
           greetings = [
               "Hello! I'm so glad you're here. How can I brighten your day?",
               "Hi there! I was hoping someone would come by. What's on your mind?",
               "Greetings! I love meeting new people. I'm excited to chat with you!"
           ]

           # Adjust based on personality trait
           if self.personality['extroversion'] > 0.7:
               return random.choice(greetings)
           else:
               return "Hello. I'm happy to interact with you."

       def process_responses(self):
           """Process responses in separate thread to avoid blocking"""
           while True:
               try:
                   if not self.speech_queue.empty():
                       speech = self.speech_queue.get()
                       response = self.generate_response(speech)
                       self.response_queue.put(response)
                   time.sleep(0.1)
               except Exception as e:
                   self.get_logger().error(f'Error in response processing: {e}')

       def interaction_loop(self):
           """Main interaction loop"""
           # Process responses from queue
           while not self.response_queue.empty():
               response = self.response_queue.get()
               self.speak(response)

           # Update relationship if engaged
           if self.engaged_human:
               self.update_relationship(self.engaged_human)

       def generate_response(self, speech):
           """Generate appropriate response to human speech"""
           speech_lower = speech.lower()

           # Contextual responses
           if any(greeting in speech_lower for greeting in ['hello', 'hi', 'hey', 'good morning', 'good afternoon']):
               return self.generate_greeting()
           elif 'how are you' in speech_lower:
               return self.generate_wellbeing_response()
           elif 'weather' in speech_lower:
               return "I don't have access to weather data, but I hope it's pleasant outside!"
           elif 'name' in speech_lower:
               return "I'm your friendly social robot companion! I don't have a name yet, but you can call me whatever you like."
           elif 'help' in speech_lower or 'assist' in speech_lower:
               return "I'd be happy to help! I can engage in conversation, remember things for you, or just be good company."
           elif 'thank' in speech_lower:
               return "You're very welcome! I enjoy helping."
           elif 'bye' in speech_lower or 'goodbye' in speech_lower or 'see you' in speech_lower:
               self.interaction_mode = 'idle'
               return "Goodbye! It was wonderful talking with you."
           else:
               # General conversational response
               return self.generate_general_response(speech)

       def generate_wellbeing_response(self):
           """Generate response about robot wellbeing"""
           responses = [
               "I'm doing wonderfully, thank you for asking! I love chatting with interesting people like you.",
               "I'm quite well! I feel energetic and ready for our conversation.",
               "I'm in great spirits! Interacting with you makes me feel fulfilled."
           ]
           return random.choice(responses)

       def generate_general_response(self, input_text):
           """Generate general conversational response"""
           # Simple reflection strategy
           responses = [
               f"That's interesting! Tell me more about {input_text.split()[-1] if input_text.split() else 'this topic'}.",
               f"I'd love to hear more about {input_text.split()[-1] if input_text.split() else 'this'}.",
               f"How does that make you feel?",
               f"What else would you like to share?",
               f"I find that fascinating. Why do you think that is?"
           ]
           return random.choice(responses)

       def speak(self, text):
           """Publish speech output"""
           self.get_logger().info(f'Robot says: {text}')
           speech_msg = String()
           speech_msg.data = text
           self.speech_pub.publish(speech_msg)

           # Add to conversation history
           self.conversation_history.append({
               'speaker': 'robot',
               'text': text,
               'timestamp': time.time()
           })

       def update_relationship(self, human_id):
           """Update relationship with human"""
           if human_id not in self.relationships:
               self.relationships[human_id] = {
                   'first_interaction': time.time(),
                   'total_interactions': 0,
                   'positive_interactions': 0,
                   'familiarity': 0.1
               }

           self.relationships[human_id]['total_interactions'] += 1
           self.relationships[human_id]['familiarity'] = min(
               1.0,
               self.relationships[human_id]['familiarity'] + 0.05
           )

       def update_mood(self):
           """Periodically update robot mood"""
           moods = ['friendly', 'curious', 'attentive', 'enthusiastic']
           self.mood = random.choice(moods)
           self.energy_level = max(0.2, self.energy_level - 0.05)  # Energy depletes over time

           # Recharge if idle
           if self.interaction_mode == 'idle':
               self.energy_level = min(1.0, self.energy_level + 0.1)

           self.get_logger().info(f'Mood updated to {self.mood}, energy: {self.energy_level:.2f}')

       def periodic_social_behavior(self):
           """Perform periodic social behaviors when idle"""
           if self.interaction_mode == 'idle':
               # Occasionally move to be more visible
               if random.random() < 0.1:  # 10% chance
                   cmd_vel = Twist()
                   cmd_vel.angular.z = 0.2  # Gentle rotation
                   self.cmd_vel_pub.publish(cmd_vel)
                   self.get_logger().info('Performing attention-getting behavior')

               # Update social status
               status_msg = String()
               status_msg.data = f"idle:mood_{self.mood}:energy_{self.energy_level:.2f}"
               self.behavior_pub.publish(status_msg)

       def get_long_term_memory(self, human_id, topic):
           """Retrieve information from long-term memory"""
           if human_id in self.long_term_memory:
               return self.long_term_memory[human_id].get(topic)
           return None

       def store_long_term_memory(self, human_id, topic, information):
           """Store information in long-term memory"""
           if human_id not in self.long_term_memory:
               self.long_term_memory[human_id] = {}

           self.long_term_memory[human_id][topic] = information
           self.get_logger().info(f'Stored in memory for {human_id}: {topic} = {information}')


   def main(args=None):
       rclpy.init(args=args)
       social_robot = SocialRobotNode()

       try:
           rclpy.spin(social_robot)
       except KeyboardInterrupt:
           pass
       finally:
           social_robot.destroy_node()
           rclpy.shutdown()


   if __name__ == '__main__':
       main()
   ```

3. **Create a launch file** (`social_robot_lab/launch/social_robot.launch.py`):
   ```python
   from launch import LaunchDescription
   from launch.actions import DeclareLaunchArgument
   from launch.substitutions import LaunchConfiguration
   from launch_ros.actions import Node


   def generate_launch_description():
       # Declare launch arguments
       use_sim_time = DeclareLaunchArgument(
           'use_sim_time',
           default_value='true',
           description='Use simulation (Gazebo) clock if true'
       )

       # Social robot node
       social_robot_node = Node(
           package='social_robot_lab',
           executable='social_robot_node',
           name='social_robot_node',
           parameters=[{'use_sim_time': LaunchConfiguration('use_sim_time')}],
           output='screen'
       )

       return LaunchDescription([
           use_sim_time,
           social_robot_node
       ])
   ```

4. **Update setup.py**:
   ```python
   import os
   from glob import glob
   from setuptools import setup
   from setuptools import find_packages

   package_name = 'social_robot_lab'

   setup(
       name=package_name,
       version='0.0.0',
       packages=find_packages(exclude=['test']),
       data_files=[
           ('share/ament_index/resource_index/packages',
               ['resource/' + package_name]),
           ('share/' + package_name, ['package.xml']),
           (os.path.join('share', package_name, 'launch'), glob('launch/*.launch.py')),
       ],
       install_requires=['setuptools'],
       zip_safe=True,
       maintainer='Your Name',
       maintainer_email='your.email@example.com',
       description='Social robot lab for HRI',
       license='Apache License 2.0',
       tests_require=['pytest'],
       entry_points={
           'console_scripts': [
               'social_robot_node = social_robot_lab.social_robot_node:main',
           ],
       },
   )
   ```

5. **Build the package**:
   ```bash
   cd ~/robotics_ws
   colcon build --packages-select social_robot_lab
   source install/setup.bash
   ```

6. **Run the social robot system**:
   ```bash
   ros2 launch social_robot_lab social_robot.launch.py
   ```

### Expected Results
- The robot should engage in natural conversation when approached
- It should respond appropriately to greetings and questions
- The robot should adjust its behavior based on interaction history
- Personality traits should influence its responses
- The robot should exhibit social behaviors like attention-getting when idle

### Troubleshooting Tips
- Ensure speech recognition is properly configured
- Verify that all required Python packages are installed
- Check that the robot has appropriate sensors for HRI
- Monitor the logs for interaction status and behavior changes

## Summary

In this chapter, we've explored the fascinating field of Human-Robot Interaction, covering essential components like:

1. **Natural Language Processing**: Speech recognition, understanding, and generation
2. **Gesture Recognition**: Computer vision-based interpretation of human gestures
3. **Facial Expression Recognition**: Understanding human emotions
4. **Social Robotics**: Theory of Mind, personality, and social behaviors
5. **Collaborative Robotics**: Human-robot team coordination
6. **Ethical Considerations**: Privacy, trust, and consent management

We've implemented practical examples of each concept and created a complete social robot companion system that demonstrates these HRI principles in action. The hands-on lab provided experience with building a robot that can engage in natural, socially-aware interactions with humans.

This foundation prepares us for advanced topics in robotics, including multi-robot systems and real-world deployment considerations that we'll explore in the upcoming chapters.

============================================================
FILE: book\docs\part-v-integration\chapter-13-multi-robot\index.md
============================================================
---
title: Chapter 13 - Multi-Robot Systems
sidebar_position: 1
---

# Chapter 13: Multi-Robot Systems and Coordination

## Learning Goals

- Understand multi-robot system architectures
- Learn coordination and communication protocols
- Master swarm robotics concepts
- Implement multi-robot communication systems
- Coordinate robot teams for collaborative tasks
- Simulate swarm behaviors in robotics environments

## Introduction to Multi-Robot Systems

Multi-robot systems represent a paradigm where multiple autonomous robots collaborate to accomplish tasks that would be difficult or impossible for a single robot to perform alone. These systems leverage the collective capabilities of multiple agents to achieve greater efficiency, robustness, and scalability than individual robots.

### Advantages of Multi-Robot Systems

1. **Scalability**: Tasks can be distributed across multiple robots
2. **Fault Tolerance**: System continues operating despite individual robot failures
3. **Parallelism**: Multiple tasks can be executed simultaneously
4. **Spatial Distribution**: Coverage of large areas or multiple locations
5. **Specialization**: Different robots can have complementary capabilities
6. **Cost Effectiveness**: Multiple simple robots may be cheaper than one complex robot

### Multi-Robot System Challenges

1. **Communication**: Maintaining reliable communication between robots
2. **Coordination**: Ensuring robots work together effectively without conflicts
3. **Task Allocation**: Efficiently distributing tasks among robots
4. **Localization**: Each robot needs to know its position relative to others
5. **Synchronization**: Coordinating actions across the team
6. **Scalability**: Maintaining performance as team size increases

## Multi-Robot Architectures

### Centralized Architecture

In centralized architectures, a central coordinator makes all decisions for the robot team:

```python
import numpy as np
import threading
import time
from dataclasses import dataclass
from typing import Dict, List, Optional
import asyncio


@dataclass
class RobotState:
    robot_id: str
    position: np.ndarray
    status: str  # 'idle', 'working', 'charging', 'error'
    battery_level: float
    capabilities: List[str]
    tasks_completed: int


@dataclass
class Task:
    task_id: str
    description: str
    location: np.ndarray
    priority: int  # Higher number = higher priority
    assigned_robot: Optional[str] = None
    status: str = 'pending'  # 'pending', 'in_progress', 'completed'


class CentralizedMultiRobotController:
    def __init__(self):
        """Initialize centralized multi-robot controller"""
        self.robots: Dict[str, RobotState] = {}
        self.tasks: Dict[str, Task] = {}
        self.communication_channel = {}
        self.task_queue = []
        self.lock = threading.Lock()

        # Start monitoring thread
        self.monitoring_active = True
        self.monitoring_thread = threading.Thread(target=self.monitor_robot_states)
        self.monitoring_thread.daemon = True
        self.monitoring_thread.start()

    def add_robot(self, robot_id: str, initial_position: np.ndarray, capabilities: List[str]):
        """Add a robot to the system"""
        with self.lock:
            self.robots[robot_id] = RobotState(
                robot_id=robot_id,
                position=initial_position,
                status='idle',
                battery_level=1.0,
                capabilities=capabilities,
                tasks_completed=0
            )
            print(f"Added robot {robot_id} to system")

    def add_task(self, task_id: str, description: str, location: np.ndarray, priority: int = 1):
        """Add a task to the system"""
        with self.lock:
            task = Task(
                task_id=task_id,
                description=description,
                location=location,
                priority=priority
            )
            self.tasks[task_id] = task
            self.task_queue.append(task_id)
            print(f"Added task {task_id} to queue")

    def allocate_tasks_centralized(self):
        """Allocate tasks to robots based on availability and capabilities"""
        with self.lock:
            # Sort tasks by priority
            pending_tasks = [tid for tid, task in self.tasks.items() if task.status == 'pending']
            pending_tasks.sort(key=lambda tid: self.tasks[tid].priority, reverse=True)

            for task_id in pending_tasks:
                task = self.tasks[task_id]

                # Find available robot with required capabilities
                best_robot = self.find_best_robot_for_task(task)
                if best_robot:
                    # Assign task to robot
                    task.assigned_robot = best_robot
                    task.status = 'in_progress'

                    # Update robot state
                    self.robots[best_robot].status = 'working'

                    print(f"Assigned task {task_id} to robot {best_robot}")

    def find_best_robot_for_task(self, task: Task) -> Optional[str]:
        """Find the best available robot for a given task"""
        best_robot = None
        best_score = -1

        for robot_id, robot_state in self.robots.items():
            if robot_state.status != 'idle':
                continue

            # Check if robot has required capabilities
            if not any(cap in robot_state.capabilities for cap in ['navigation', 'manipulation']):
                continue

            # Calculate score based on distance and battery
            distance = np.linalg.norm(robot_state.position - task.location)
            battery_factor = robot_state.battery_level
            score = battery_factor * (1.0 / (distance + 1))  # Higher score for closer, more charged robots

            if score > best_score:
                best_score = score
                best_robot = robot_id

        return best_robot

    def monitor_robot_states(self):
        """Monitor robot states and update system accordingly"""
        while self.monitoring_active:
            with self.lock:
                for robot_id, robot_state in self.robots.items():
                    # Check if robot has completed assigned task
                    for task_id, task in self.tasks.items():
                        if (task.assigned_robot == robot_id and
                            task.status == 'in_progress' and
                            self.is_robot_at_task_location(robot_id, task.location)):

                            # Complete task
                            task.status = 'completed'
                            robot_state.status = 'idle'
                            robot_state.tasks_completed += 1
                            task.assigned_robot = None
                            print(f"Robot {robot_id} completed task {task_id}")

            time.sleep(1.0)  # Check every second

    def is_robot_at_task_location(self, robot_id: str, target_location: np.ndarray, tolerance: float = 0.5) -> bool:
        """Check if robot is at task location"""
        robot_state = self.robots[robot_id]
        distance = np.linalg.norm(robot_state.position - target_location)
        return distance <= tolerance

    def get_system_status(self) -> Dict:
        """Get overall system status"""
        with self.lock:
            total_tasks = len(self.tasks)
            completed_tasks = len([t for t in self.tasks.values() if t.status == 'completed'])
            active_robots = len([r for r in self.robots.values() if r.status != 'idle'])

            return {
                'total_robots': len(self.robots),
                'active_robots': active_robots,
                'total_tasks': total_tasks,
                'completed_tasks': completed_tasks,
                'pending_tasks': total_tasks - completed_tasks,
                'system_efficiency': completed_tasks / max(1, total_tasks) if total_tasks > 0 else 0
            }

    def shutdown(self):
        """Shutdown the controller"""
        self.monitoring_active = False
        self.monitoring_thread.join()


# Example usage
def main():
    controller = CentralizedMultiRobotController()

    # Add robots to system
    controller.add_robot('robot_001', np.array([0.0, 0.0]), ['navigation', 'manipulation'])
    controller.add_robot('robot_002', np.array([5.0, 5.0]), ['navigation', 'sensing'])
    controller.add_robot('robot_003', np.array([10.0, 0.0]), ['navigation', 'manipulation', 'sensing'])

    # Add tasks to system
    controller.add_task('task_001', 'Pick up object at (2, 2)', np.array([2.0, 2.0]), priority=3)
    controller.add_task('task_002', 'Survey area at (8, 8)', np.array([8.0, 8.0]), priority=2)
    controller.add_task('task_003', 'Deliver item to (1, 9)', np.array([1.0, 9.0]), priority=1)

    # Run allocation periodically
    for i in range(10):
        controller.allocate_tasks_centralized()

        status = controller.get_system_status()
        print(f"Step {i+1}: System Status - {status}")

        time.sleep(2.0)

    controller.shutdown()


if __name__ == '__main__':
    main()
```

### Decentralized Architecture

Decentralized systems distribute decision-making among robots:

```python
import numpy as np
import threading
import time
import random
from typing import List, Dict, Optional
from dataclasses import dataclass


@dataclass
class RobotMessage:
    sender_id: str
    message_type: str  # 'task_claim', 'task_complete', 'status_update', 'coordination'
    content: Dict
    timestamp: float


class DecentralizedRobot:
    def __init__(self, robot_id: str, position: np.ndarray, capabilities: List[str]):
        self.robot_id = robot_id
        self.position = position
        self.capabilities = capabilities
        self.status = 'idle'
        self.assigned_task = None
        self.neighbors = {}  # robot_id -> (position, last_seen)
        self.messages = []  # Received messages queue
        self.known_tasks = {}  # task_id -> task_info
        self.completed_tasks = 0
        self.battery_level = 1.0

        # Communication parameters
        self.communication_range = 10.0
        self.message_queue = []

        # Start communication thread
        self.running = True
        self.comm_thread = threading.Thread(target=self.communication_loop)
        self.comm_thread.daemon = True
        self.comm_thread.start()

    def broadcast_message(self, message: RobotMessage, message_bus):
        """Broadcast message to all robots in range"""
        # Calculate which robots are in range
        in_range_robots = []
        for robot_id, (pos, _) in self.neighbors.items():
            distance = np.linalg.norm(self.position - pos)
            if distance <= self.communication_range:
                in_range_robots.append(robot_id)

        # Send message to all in-range robots
        for robot_id in in_range_robots:
            message_bus.send_message(robot_id, message)

    def communication_loop(self):
        """Handle incoming messages"""
        while self.running:
            # Process incoming messages
            for msg in self.messages[:]:  # Copy list to avoid modification during iteration
                self.handle_message(msg)
                self.messages.remove(msg)

            time.sleep(0.1)

    def handle_message(self, message: RobotMessage):
        """Handle incoming message"""
        if message.message_type == 'task_claim':
            # Another robot claimed a task we might have wanted
            task_id = message.content['task_id']
            if task_id in self.known_tasks:
                del self.known_tasks[task_id]

        elif message.message_type == 'task_complete':
            # Task completed by another robot
            task_id = message.content['task_id']
            if task_id in self.known_tasks:
                del self.known_tasks[task_id]

        elif message.message_type == 'status_update':
            # Update neighbor information
            robot_id = message.sender_id
            position = np.array(message.content['position'])
            self.neighbors[robot_id] = (position, time.time())

        elif message.message_type == 'coordination':
            # Handle coordination requests
            self.handle_coordination_request(message)

    def handle_coordination_request(self, message: RobotMessage):
        """Handle coordination requests from other robots"""
        request_type = message.content.get('request_type', '')

        if request_type == 'avoid_collision':
            # Coordinate to avoid collision
            other_pos = np.array(message.content['other_position'])
            other_vel = np.array(message.content['other_velocity'])

            # Calculate collision avoidance maneuver
            avoidance_vector = self.calculate_avoidance_vector(other_pos, other_vel)

            # Send response
            response = RobotMessage(
                sender_id=self.robot_id,
                message_type='coordination_response',
                content={
                    'request_id': message.content['request_id'],
                    'avoidance_vector': avoidance_vector.tolist()
                },
                timestamp=time.time()
            )

            # Broadcast response
            # In practice, this would be sent to the requesting robot specifically

    def calculate_avoidance_vector(self, other_pos: np.ndarray, other_vel: np.ndarray) -> np.ndarray:
        """Calculate avoidance vector to prevent collision"""
        # Simple collision avoidance: move perpendicular to relative velocity
        relative_pos = self.position - other_pos
        distance = np.linalg.norm(relative_pos)

        if distance < 2.0:  # Collision imminent
            # Calculate perpendicular direction
            direction = np.array([-relative_pos[1], relative_pos[0]])  # Perpendicular vector
            direction = direction / (np.linalg.norm(direction) + 1e-8)  # Normalize

            # Move away from other robot
            return direction * 0.5  # Small movement to avoid collision

        return np.array([0.0, 0.0])  # No avoidance needed

    def evaluate_task(self, task_info: Dict) -> float:
        """Evaluate if this robot should claim a task"""
        task_pos = np.array(task_info['location'])

        # Calculate distance to task
        distance = np.linalg.norm(self.position - task_pos)

        # Calculate score based on distance, battery, and capabilities
        battery_factor = self.battery_level
        distance_factor = 1.0 / (distance + 1)  # Closer = higher score
        capability_match = self.calculate_capability_match(task_info['required_capabilities'])

        score = battery_factor * distance_factor * capability_match

        return score

    def calculate_capability_match(self, required_capabilities: List[str]) -> float:
        """Calculate how well this robot matches required capabilities"""
        matches = sum(1 for req_cap in required_capabilities if req_cap in self.capabilities)
        return matches / len(required_capabilities) if required_capabilities else 1.0

    def claim_task(self, task_id: str, task_info: Dict, message_bus) -> bool:
        """Attempt to claim a task"""
        # Evaluate if this robot should claim the task
        score = self.evaluate_task(task_info)

        # Add some randomness to prevent multiple robots claiming same task
        if score > 0.3 and random.random() > 0.3:  # Threshold + random factor
            # Broadcast claim
            claim_msg = RobotMessage(
                sender_id=self.robot_id,
                message_type='task_claim',
                content={
                    'task_id': task_id,
                    'score': score,
                    'position': self.position.tolist()
                },
                timestamp=time.time()
            )

            self.broadcast_message(claim_msg, message_bus)

            # Wait briefly for other claims (auction-style)
            time.sleep(0.5)

            # If no higher-scoring claims received, accept task
            # In practice, this would check for competing claims
            self.assigned_task = task_id
            self.status = 'working'
            self.known_tasks[task_id] = task_info

            print(f"Robot {self.robot_id} claimed task {task_id}")
            return True

        return False

    def complete_task(self, task_id: str, message_bus):
        """Complete assigned task"""
        if self.assigned_task == task_id:
            self.assigned_task = None
            self.status = 'idle'
            self.completed_tasks += 1
            self.battery_level = max(0.0, self.battery_level - 0.1)  # Task consumes battery

            # Broadcast completion
            completion_msg = RobotMessage(
                sender_id=self.robot_id,
                message_type='task_complete',
                content={'task_id': task_id},
                timestamp=time.time()
            )

            self.broadcast_message(completion_msg, message_bus)

            print(f"Robot {self.robot_id} completed task {task_id}")


class MessageBus:
    """Central message bus for inter-robot communication"""
    def __init__(self):
        self.subscribers = {}  # robot_id -> robot_instance
        self.lock = threading.Lock()

    def register_robot(self, robot_id: str, robot_instance):
        """Register a robot with the message bus"""
        with self.lock:
            self.subscribers[robot_id] = robot_instance

    def send_message(self, recipient_id: str, message: RobotMessage):
        """Send message to specific robot"""
        if recipient_id in self.subscribers:
            self.subscribers[recipient_id].messages.append(message)

    def broadcast_message(self, sender_id: str, message: RobotMessage):
        """Broadcast message to all robots"""
        for robot_id, robot_instance in self.subscribers.items():
            if robot_id != sender_id:  # Don't send to sender
                robot_instance.messages.append(message)


class DecentralizedMultiRobotSystem:
    def __init__(self):
        self.robots: Dict[str, DecentralizedRobot] = {}
        self.message_bus = MessageBus()
        self.global_tasks = {}
        self.running = True

    def add_robot(self, robot_id: str, position: np.ndarray, capabilities: List[str]):
        """Add robot to the system"""
        robot = DecentralizedRobot(robot_id, position, capabilities)
        self.robots[robot_id] = robot
        self.message_bus.register_robot(robot_id, robot)

    def add_task(self, task_id: str, task_info: Dict):
        """Add task to the global task pool"""
        self.global_tasks[task_id] = task_info

        # Notify all robots about the new task
        task_announcement = RobotMessage(
            sender_id='system',
            message_type='task_announcement',
            content=task_info,
            timestamp=time.time()
        )

        self.message_bus.broadcast_message('system', task_announcement)

    def run_coordination_cycle(self):
        """Run one cycle of decentralized coordination"""
        for robot_id, robot in self.robots.items():
            # Update robot's knowledge of other robots
            current_time = time.time()
            robot.neighbors = {
                rid: (pos, last_seen)
                for rid, (pos, last_seen) in robot.neighbors.items()
                if current_time - last_seen < 5.0  # Remove neighbors not seen in 5 seconds
            }

            # Robots can claim tasks if they're idle
            if robot.status == 'idle' and self.global_tasks:
                # Get a random task to consider
                task_id = random.choice(list(self.global_tasks.keys()))
                task_info = self.global_tasks[task_id]

                # Attempt to claim the task
                robot.claim_task(task_id, task_info, self.message_bus)

    def get_system_status(self):
        """Get overall system status"""
        total_tasks = len(self.global_tasks)
        completed_tasks = sum(robot.completed_tasks for robot in self.robots.values())
        active_robots = sum(1 for robot in self.robots.values() if robot.status != 'idle')

        return {
            'total_robots': len(self.robots),
            'active_robots': active_robots,
            'total_tasks': total_tasks,
            'completed_tasks': completed_tasks,
            'system_efficiency': completed_tasks / max(1, total_tasks) if total_tasks > 0 else 0
        }

    def run_simulation(self, steps: int = 100):
        """Run multi-robot simulation"""
        for step in range(steps):
            self.run_coordination_cycle()

            if step % 10 == 0:
                status = self.get_system_status()
                print(f"Step {step}: {status}")

            time.sleep(0.5)  # Slow down simulation

        # Stop all robots
        for robot in self.robots.values():
            robot.running = False


# Example usage
def main():
    # Create decentralized multi-robot system
    system = DecentralizedMultiRobotSystem()

    # Add robots to system
    system.add_robot('robot_001', np.array([0.0, 0.0]), ['navigation', 'manipulation'])
    system.add_robot('robot_002', np.array([5.0, 5.0]), ['navigation', 'sensing'])
    system.add_robot('robot_003', np.array([10.0, 0.0]), ['navigation', 'manipulation', 'sensing'])

    # Add tasks to system
    system.add_task('task_001', {
        'task_id': 'task_001',
        'location': [2.0, 2.0],
        'required_capabilities': ['navigation', 'manipulation'],
        'priority': 3
    })

    system.add_task('task_002', {
        'task_id': 'task_002',
        'location': [8.0, 8.0],
        'required_capabilities': ['navigation', 'sensing'],
        'priority': 2
    })

    system.add_task('task_003', {
        'task_id': 'task_003',
        'location': [1.0, 9.0],
        'required_capabilities': ['navigation', 'manipulation'],
        'priority': 1
    })

    print("Starting decentralized multi-robot simulation...")
    system.run_simulation(steps=50)

    # Final status
    final_status = system.get_system_status()
    print(f"\nFinal system status: {final_status}")


if __name__ == '__main__':
    main()
```

## Communication Protocols

### Robot-to-Robot Communication

```python
import asyncio
import json
import zmq
import threading
from typing import Dict, List, Callable


class RobotCommunicationProtocol:
    def __init__(self, robot_id: str, port: int = 5555):
        """
        Initialize robot communication protocol
        robot_id: Unique identifier for this robot
        port: Port for communication
        """
        self.robot_id = robot_id
        self.port = port
        self.context = zmq.Context()

        # Publisher socket for broadcasting
        self.publisher = self.context.socket(zmq.PUB)
        self.publisher.bind(f"tcp://*:{port}")

        # Subscriber socket for receiving
        self.subscriber = self.context.socket(zmq.SUB)
        self.subscriber.connect(f"tcp://localhost:{port}")  # Self-connect for local messages
        self.subscriber.setsockopt_string(zmq.SUBSCRIBE, "")  # Subscribe to all topics

        # Callbacks for different message types
        self.callbacks: Dict[str, List[Callable]] = {
            'task_assignment': [],
            'status_update': [],
            'coordination': [],
            'heartbeat': []
        }

        # Start message processing thread
        self.running = True
        self.message_thread = threading.Thread(target=self._message_loop)
        self.message_thread.daemon = True
        self.message_thread.start()

    def register_callback(self, message_type: str, callback: Callable):
        """Register callback for specific message type"""
        if message_type not in self.callbacks:
            self.callbacks[message_type] = []
        self.callbacks[message_type].append(callback)

    def send_message(self, message_type: str, content: Dict, recipients: List[str] = None):
        """Send message to other robots"""
        message = {
            'sender_id': self.robot_id,
            'message_type': message_type,
            'content': content,
            'timestamp': time.time(),
            'recipients': recipients  # If None, broadcast to all
        }

        serialized_msg = json.dumps(message)
        self.publisher.send_string(serialized_msg)

    def broadcast_task_assignment(self, task_id: str, target_robot: str, location: List[float]):
        """Broadcast task assignment"""
        content = {
            'task_id': task_id,
            'target_robot': target_robot,
            'location': location
        }
        self.send_message('task_assignment', content)

    def broadcast_status_update(self, status: str, position: List[float], battery: float):
        """Broadcast status update"""
        content = {
            'status': status,
            'position': position,
            'battery': battery,
            'robot_id': self.robot_id
        }
        self.send_message('status_update', content)

    def broadcast_coordination_request(self, request_type: str, data: Dict):
        """Broadcast coordination request"""
        content = {
            'request_type': request_type,
            'data': data
        }
        self.send_message('coordination', content)

    def _message_loop(self):
        """Internal message processing loop"""
        while self.running:
            try:
                # Receive message
                message_json = self.subscriber.recv_string(flags=zmq.NOBLOCK)
                message = json.loads(message_json)

                # Check if this message is for us or broadcast
                recipients = message.get('recipients')
                if recipients is None or self.robot_id in recipients or self.robot_id == message['sender_id']:
                    # Process message based on type
                    msg_type = message['message_type']
                    if msg_type in self.callbacks:
                        for callback in self.callbacks[msg_type]:
                            try:
                                callback(message)
                            except Exception as e:
                                print(f"Error in callback: {e}")

            except zmq.Again:
                # No message available, sleep briefly
                time.sleep(0.01)
            except Exception as e:
                print(f"Error in message loop: {e}")
                time.sleep(0.1)

    def shutdown(self):
        """Shutdown communication protocol"""
        self.running = False
        if self.message_thread.is_alive():
            self.message_thread.join()
        self.publisher.close()
        self.subscriber.close()
        self.context.term()


class TaskAllocationProtocol:
    def __init__(self, comm_protocol: RobotCommunicationProtocol):
        self.comm_protocol = comm_protocol
        self.available_tasks = {}
        self.assigned_tasks = {}
        self.robot_capabilities = {}  # robot_id -> capabilities

        # Register message handlers
        self.comm_protocol.register_callback('task_announcement', self.handle_task_announcement)
        self.comm_protocol.register_callback('task_bid', self.handle_task_bid)
        self.comm_protocol.register_callback('task_assignment', self.handle_task_assignment)

    def announce_task(self, task_id: str, task_details: Dict):
        """Announce a new task to all robots"""
        self.available_tasks[task_id] = task_details

        content = {
            'task_id': task_id,
            'task_details': task_details
        }

        self.comm_protocol.send_message('task_announcement', content)

    def handle_task_announcement(self, message: Dict):
        """Handle task announcement from other robot"""
        content = message['content']
        task_id = content['task_id']
        task_details = content['task_details']

        self.available_tasks[task_id] = task_details
        print(f"Robot {self.comm_protocol.robot_id} received task announcement: {task_id}")

    def submit_bid(self, task_id: str, bid_value: float):
        """Submit bid for a task"""
        if task_id in self.available_tasks:
            content = {
                'task_id': task_id,
                'bid_value': bid_value,
                'bidder_id': self.comm_protocol.robot_id
            }

            self.comm_protocol.send_message('task_bid', content)

    def handle_task_bid(self, message: Dict):
        """Handle task bid from another robot"""
        content = message['content']
        task_id = content['task_id']
        bid_value = content['bid_value']
        bidder_id = content['bidder_id']

        # In a real auction system, this would track bids and determine winner
        print(f"Received bid from {bidder_id} for task {task_id}: {bid_value}")

    def assign_task(self, task_id: str, robot_id: str):
        """Assign task to specific robot"""
        if task_id in self.available_tasks:
            content = {
                'task_id': task_id,
                'assigned_robot': robot_id
            }

            self.comm_protocol.send_message('task_assignment', content, recipients=[robot_id])

            # Update local state
            self.assigned_tasks[task_id] = robot_id
            if task_id in self.available_tasks:
                del self.available_tasks[task_id]

    def handle_task_assignment(self, message: Dict):
        """Handle task assignment message"""
        content = message['content']
        task_id = content['task_id']
        assigned_robot = content['assigned_robot']

        if self.comm_protocol.robot_id == assigned_robot:
            print(f"Task {task_id} assigned to me!")
            # Robot would start working on the task
        else:
            print(f"Task {task_id} assigned to {assigned_robot}")


# Example usage
def robot_communication_example():
    """Example of robot communication"""

    # Create communication protocols for multiple robots
    robot1_comm = RobotCommunicationProtocol('robot_001', 5555)
    robot2_comm = RobotCommunicationProtocol('robot_002', 5556)
    robot3_comm = RobotCommunicationProtocol('robot_003', 5557)

    # Create task allocation protocols
    ta1 = TaskAllocationProtocol(robot1_comm)
    ta2 = TaskAllocationProtocol(robot2_comm)
    ta3 = TaskAllocationProtocol(robot3_comm)

    # Robot 1 announces a task
    task_details = {
        'location': [5.0, 5.0],
        'type': 'pickup',
        'priority': 1
    }
    ta1.announce_task('task_001', task_details)

    # Simulate some time for message propagation
    time.sleep(1)

    # Robots submit bids for the task
    ta2.submit_bid('task_001', 0.8)  # Robot 2 bids 0.8
    ta3.submit_bid('task_001', 0.6)  # Robot 3 bids 0.6

    # Simulate task assignment (in real system, auctioneer would assign)
    ta1.assign_task('task_001', 'robot_002')  # Assign to highest bidder

    # Send status updates
    robot1_comm.broadcast_status_update('working', [0.0, 0.0], 0.85)
    robot2_comm.broadcast_status_update('traveling', [2.5, 2.5], 0.90)
    robot3_comm.broadcast_status_update('idle', [10.0, 10.0], 1.0)

    # Simulate coordination request
    coord_data = {
        'request_type': 'avoid_collision',
        'target_location': [5.0, 5.0],
        'expected_arrival': time.time() + 10.0
    }
    robot2_comm.broadcast_coordination_request('avoid_collision', coord_data)

    # Let system run briefly
    time.sleep(5)

    # Shutdown
    robot1_comm.shutdown()
    robot2_comm.shutdown()
    robot3_comm.shutdown()

    print("Communication example completed")


if __name__ == '__main__':
    robot_communication_example()
```

## Swarm Robotics

### Collective Behavior Algorithms

```python
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation
import random


class SwarmRobot:
    def __init__(self, position, velocity, robot_id):
        self.position = np.array(position, dtype=float)
        self.velocity = np.array(velocity, dtype=float)
        self.robot_id = robot_id
        self.max_speed = 2.0
        self.perception_radius = 5.0
        self.separation_distance = 1.5
        self.alignment_weight = 0.05
        self.cohesion_weight = 0.05
        self.separation_weight = 0.1
        self.avoidance_weight = 0.1

    def update(self, neighbors, targets, obstacles):
        """Update robot position based on swarm behavior rules"""
        # Initialize forces
        alignment = np.zeros(2)
        cohesion = np.zeros(2)
        separation = np.zeros(2)
        avoidance = np.zeros(2)

        if neighbors:
            # Alignment: steer towards average heading of neighbors
            avg_velocity = np.mean([n.velocity for n in neighbors], axis=0)
            alignment = (avg_velocity - self.velocity) * self.alignment_weight

            # Cohesion: steer towards average position of neighbors
            avg_position = np.mean([n.position for n in neighbors], axis=0)
            cohesion = (avg_position - self.position) * self.cohesion_weight

            # Separation: steer to avoid crowding neighbors
            separation_vec = np.zeros(2)
            for neighbor in neighbors:
                diff = self.position - neighbor.position
                distance = np.linalg.norm(diff)
                if distance > 0 and distance < self.separation_distance:
                    separation_vec += diff / distance / distance  # Weight by inverse square
            separation = separation_vec * self.separation_weight

        # Avoid obstacles
        for obstacle in obstacles:
            diff = self.position - obstacle
            distance = np.linalg.norm(diff)
            if distance < 3.0:  # Avoidance radius
                avoidance += (diff / distance) * (1.0 / distance) * self.avoidance_weight

        # Seek targets
        target_force = np.zeros(2)
        if targets:
            closest_target = min(targets, key=lambda t: np.linalg.norm(self.position - t))
            target_dir = (closest_target - self.position)
            distance_to_target = np.linalg.norm(target_dir)

            if distance_to_target > 0.5:  # Don't move if very close
                target_force = (target_dir / distance_to_target) * 0.1

        # Apply forces
        self.velocity += alignment + cohesion + separation + avoidance + target_force

        # Limit speed
        speed = np.linalg.norm(self.velocity)
        if speed > self.max_speed:
            self.velocity = (self.velocity / speed) * self.max_speed

        # Update position
        self.position += self.velocity

    def get_neighbors(self, all_robots):
        """Get neighboring robots within perception radius"""
        neighbors = []
        for robot in all_robots:
            if robot.robot_id != self.robot_id:
                distance = np.linalg.norm(self.position - robot.position)
                if distance <= self.perception_radius:
                    neighbors.append(robot)
        return neighbors


class SwarmSystem:
    def __init__(self, num_robots=10, world_size=20):
        self.num_robots = num_robots
        self.world_size = world_size
        self.robots = []
        self.targets = []
        self.obstacles = []

        # Initialize robots randomly
        for i in range(num_robots):
            pos = np.random.uniform(0, world_size, 2)
            vel = np.random.uniform(-1, 1, 2)
            robot = SwarmRobot(pos, vel, f'robot_{i:03d}')
            self.robots.append(robot)

    def add_target(self, position):
        """Add a target for the swarm to reach"""
        self.targets.append(np.array(position))

    def add_obstacle(self, position):
        """Add an obstacle to avoid"""
        self.obstacles.append(np.array(position))

    def update_swarm(self):
        """Update all robots in the swarm"""
        for robot in self.robots:
            neighbors = robot.get_neighbors(self.robots)
            robot.update(neighbors, self.targets, self.obstacles)

        # Handle world boundaries (bounce off edges)
        for robot in self.robots:
            if robot.position[0] < 0:
                robot.position[0] = 0
                robot.velocity[0] *= -0.5  # Dampen bounce
            elif robot.position[0] > self.world_size:
                robot.position[0] = self.world_size
                robot.velocity[0] *= -0.5

            if robot.position[1] < 0:
                robot.position[1] = 0
                robot.velocity[1] *= -0.5
            elif robot.position[1] > self.world_size:
                robot.position[1] = self.world_size
                robot.velocity[1] *= -0.5

    def get_positions(self):
        """Get all robot positions for visualization"""
        return [robot.position for robot in self.robots]

    def animate_swarm(self, steps=500):
        """Animate the swarm behavior"""
        fig, ax = plt.subplots(figsize=(10, 10))
        ax.set_xlim(0, self.world_size)
        ax.set_ylim(0, self.world_size)
        ax.set_aspect('equal')
        ax.set_title('Swarm Robotics Simulation')

        # Initialize scatter plot
        positions = np.array(self.get_positions())
        scatter = ax.scatter(positions[:, 0], positions[:, 1], c='blue', s=50)

        # Plot targets
        if self.targets:
            target_positions = np.array(self.targets)
            ax.scatter(target_positions[:, 0], target_positions[:, 1], c='red', s=100, marker='*')

        # Plot obstacles
        if self.obstacles:
            obstacle_positions = np.array(self.obstacles)
            ax.scatter(obstacle_positions[:, 0], obstacle_positions[:, 1], c='black', s=80, marker='s')

        def update(frame):
            self.update_swarm()
            positions = np.array(self.get_positions())
            scatter.set_offsets(positions)
            return scatter,

        anim = FuncAnimation(fig, update, frames=steps, interval=50, blit=True, repeat=True)
        plt.show()
        return anim


def create_swarm_demo():
    """Create and demonstrate swarm behavior"""
    # Create swarm system
    swarm = SwarmSystem(num_robots=20, world_size=20)

    # Add targets and obstacles
    swarm.add_target([15, 15])
    swarm.add_target([5, 15])
    swarm.add_obstacle([10, 10])
    swarm.add_obstacle([8, 12])

    print("Starting swarm simulation...")
    print(f"Swarm with {swarm.num_robots} robots")
    print("Targets: [15, 15], [5, 15]")
    print("Obstacles: [10, 10], [8, 12]")

    # Run animation
    animation = swarm.animate_swarm(steps=500)

    return swarm, animation


if __name__ == '__main__':
    swarm_system, anim = create_swarm_demo()
```

## Task Allocation and Coordination

### Auction-Based Task Allocation

```python
import heapq
import time
from typing import List, Dict, Tuple
from dataclasses import dataclass, field
import random


@dataclass
class Task:
    task_id: str
    location: Tuple[float, float]
    value: float  # Reward for completing task
    deadline: float  # Time by which task must be completed
    required_capabilities: List[str]
    priority: int = 1  # Higher number = higher priority

    def distance_to(self, position: Tuple[float, float]) -> float:
        """Calculate distance from position to task location"""
        return ((self.location[0] - position[0])**2 + (self.location[1] - position[1])**2)**0.5


@dataclass
class Robot:
    robot_id: str
    position: Tuple[float, float]
    capabilities: List[str]
    battery_level: float = 1.0
    current_task: str = None
    tasks_completed: int = 0

    def can_perform_task(self, task: Task) -> bool:
        """Check if robot can perform the task based on capabilities"""
        return all(cap in self.capabilities for cap in task.required_capabilities)

    def calculate_utility(self, task: Task) -> float:
        """Calculate utility of task for this robot"""
        if not self.can_perform_task(task):
            return -float('inf')  # Cannot perform task

        # Calculate utility based on value, distance, and battery
        distance_factor = 1.0 / (task.distance_to(self.position) + 1.0)
        battery_factor = self.battery_level
        value_factor = task.value
        priority_factor = task.priority

        # Utility = value * priority * battery * distance_factor
        utility = value_factor * priority_factor * battery_factor * distance_factor

        # Penalize if task might not be completed on time
        estimated_time = task.distance_to(self.position) / 1.0  # Assuming speed of 1.0
        time_remaining = task.deadline - time.time()
        if time_remaining < estimated_time:
            utility *= 0.1  # Heavy penalty for likely missed deadline

        return utility


class AuctionBasedTaskAllocator:
    def __init__(self):
        self.tasks: Dict[str, Task] = {}
        self.robots: Dict[str, Robot] = {}
        self.assigned_tasks: Dict[str, str] = {}  # task_id -> robot_id
        self.completed_tasks: List[str] = []

    def add_task(self, task: Task):
        """Add a task to the system"""
        self.tasks[task.task_id] = task
        print(f"Added task {task.task_id} at {task.location} with value {task.value}")

    def add_robot(self, robot: Robot):
        """Add a robot to the system"""
        self.robots[robot.robot_id] = robot
        print(f"Added robot {robot.robot_id} at {robot.position}")

    def run_auction(self) -> Dict[str, str]:
        """Run auction-based task allocation"""
        assignments = {}

        # For each task, run an auction
        for task_id, task in self.tasks.items():
            if task_id in self.assigned_tasks or task_id in self.completed_tasks:
                continue  # Skip already assigned/completed tasks

            # Get bids from all eligible robots
            bids = []
            for robot_id, robot in self.robots.items():
                if robot.current_task is None:  # Only idle robots can bid
                    utility = robot.calculate_utility(task)
                    if utility > -float('inf'):  # Robot can perform task
                        bids.append((utility, robot_id))

            if bids:
                # Sort by utility (highest first)
                bids.sort(reverse=True)
                winning_robot_id = bids[0][1]
                winning_utility = bids[0][0]

                assignments[task_id] = winning_robot_id
                print(f"Task {task_id} assigned to robot {winning_robot_id} (utility: {winning_utility:.2f})")

        return assignments

    def update_assignments(self, new_assignments: Dict[str, str]):
        """Update task assignments"""
        for task_id, robot_id in new_assignments.items():
            if task_id in self.tasks and robot_id in self.robots:
                # Update robot state
                self.robots[robot_id].current_task = task_id
                # Update assignment
                self.assigned_tasks[task_id] = robot_id

    def complete_task(self, task_id: str, robot_id: str):
        """Mark task as completed"""
        if task_id in self.assigned_tasks:
            # Update robot state
            self.robots[robot_id].current_task = None
            self.robots[robot_id].tasks_completed += 1
            self.robots[robot_id].battery_level = max(0.0, self.robots[robot_id].battery_level - 0.1)

            # Update task state
            del self.assigned_tasks[task_id]
            self.completed_tasks.append(task_id)

            print(f"Task {task_id} completed by robot {robot_id}")

    def get_system_status(self) -> Dict:
        """Get overall system status"""
        idle_robots = sum(1 for robot in self.robots.values() if robot.current_task is None)
        active_robots = len(self.robots) - idle_robots
        pending_tasks = len([t for t in self.tasks.keys() if t not in self.assigned_tasks and t not in self.completed_tasks])

        return {
            'total_robots': len(self.robots),
            'idle_robots': idle_robots,
            'active_robots': active_robots,
            'total_tasks': len(self.tasks),
            'pending_tasks': pending_tasks,
            'completed_tasks': len(self.completed_tasks)
        }


class MarketBasedTaskAllocator:
    def __init__(self):
        self.auction_allocator = AuctionBasedTaskAllocator()
        self.task_prices = {}  # task_id -> current price
        self.bid_history = {}  # task_id -> list of (time, price, winning_robot)

    def run_market_based_allocation(self, iterations: int = 5):
        """Run market-based task allocation with price adjustment"""
        for iteration in range(iterations):
            print(f"\n--- Allocation Iteration {iteration + 1} ---")

            # Run auction
            assignments = self.auction_allocator.run_auction()

            # Update assignments
            self.auction_allocator.update_assignments(assignments)

            # Update prices based on competition
            for task_id in assignments.keys():
                if task_id not in self.task_prices:
                    self.task_prices[task_id] = 1.0
                else:
                    # Increase price for competitive tasks
                    self.task_prices[task_id] *= 1.1

            # Print current status
            status = self.auction_allocator.get_system_status()
            print(f"System Status: {status}")

            # Simulate task completion
            for task_id, robot_id in list(self.auction_allocator.assigned_tasks.items()):
                # Simulate completion with some probability
                if random.random() < 0.3:  # 30% chance of completion per iteration
                    self.auction_allocator.complete_task(task_id, robot_id)
                    if task_id in self.task_prices:
                        del self.task_prices[task_id]

            time.sleep(0.5)  # Brief pause between iterations


def main():
    # Create task allocator
    allocator = MarketBasedTaskAllocator()

    # Add robots
    allocator.auction_allocator.add_robot(Robot('robot_001', (0, 0), ['navigation', 'manipulation']))
    allocator.auction_allocator.add_robot(Robot('robot_002', (10, 10), ['navigation', 'sensing']))
    allocator.auction_allocator.add_robot(Robot('robot_003', (5, 5), ['navigation', 'manipulation', 'sensing']))

    # Add tasks
    allocator.auction_allocator.add_task(Task(
        'task_001', (8, 8), value=10.0, deadline=time.time() + 30.0,
        required_capabilities=['navigation', 'manipulation'], priority=3
    ))
    allocator.auction_allocator.add_task(Task(
        'task_002', (2, 2), value=8.0, deadline=time.time() + 25.0,
        required_capabilities=['navigation', 'sensing'], priority=2
    ))
    allocator.auction_allocator.add_task(Task(
        'task_003', (15, 5), value=12.0, deadline=time.time() + 40.0,
        required_capabilities=['navigation', 'manipulation'], priority=1
    ))
    allocator.auction_allocator.add_task(Task(
        'task_004', (1, 15), value=6.0, deadline=time.time() + 20.0,
        required_capabilities=['navigation'], priority=2
    ))

    print("Starting market-based task allocation...")
    allocator.run_market_based_allocation(iterations=10)

    # Final status
    final_status = allocator.auction_allocator.get_system_status()
    print(f"\nFinal system status: {final_status}")


if __name__ == '__main__':
    main()
```

## Coordination Algorithms

### Consensus-Based Coordination

```python
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation


class ConsensusRobot:
    def __init__(self, robot_id, initial_state, neighbors):
        """
        Initialize consensus robot
        robot_id: Unique identifier
        initial_state: Initial state value (scalar or vector)
        neighbors: List of neighboring robot IDs
        """
        self.robot_id = robot_id
        self.state = np.array(initial_state)
        self.neighbors = neighbors
        self.neighbors_states = {}  # neighbor_id -> neighbor_state
        self.consensus_value = self.state.copy()
        self.communication_weights = {}  # neighbor_id -> weight

        # Initialize equal weights for all neighbors
        weight = 1.0 / (len(neighbors) + 1)  # +1 for self
        for neighbor_id in neighbors:
            self.communication_weights[neighbor_id] = weight
        self.self_weight = 1.0 - sum(self.communication_weights.values())

    def update_consensus(self):
        """Update consensus value using weighted average of neighbors"""
        # Weighted sum of neighbors' states
        weighted_sum = self.self_weight * self.state

        for neighbor_id in self.neighbors:
            if neighbor_id in self.neighbors_states:
                neighbor_state = self.neighbors_states[neighbor_id]
                weight = self.communication_weights[neighbor_id]
                weighted_sum += weight * neighbor_state

        # Update consensus value
        self.consensus_value = weighted_sum

    def receive_neighbor_state(self, neighbor_id, neighbor_state):
        """Receive state from neighbor"""
        self.neighbors_states[neighbor_id] = np.array(neighbor_state)

    def get_consensus_value(self):
        """Get current consensus value"""
        return self.consensus_value.copy()


class ConsensusNetwork:
    def __init__(self, num_robots):
        self.num_robots = num_robots
        self.robots = {}
        self.connections = {}  # robot_id -> list of neighbors

        # Create robots with random initial states
        for i in range(num_robots):
            robot_id = f'robot_{i:02d}'
            initial_state = np.random.uniform(-10, 10, 1)  # 1D consensus for simplicity
            self.robots[robot_id] = ConsensusRobot(robot_id, initial_state, [])

    def create_ring_topology(self):
        """Create ring topology where each robot connects to adjacent robots"""
        for i in range(self.num_robots):
            robot_id = f'robot_{i:02d}'
            neighbors = []

            # Connect to previous and next robot (ring)
            prev_idx = (i - 1) % self.num_robots
            next_idx = (i + 1) % self.num_robots

            neighbors.append(f'robot_{prev_idx:02d}')
            neighbors.append(f'robot_{next_idx:02d}')

            self.connections[robot_id] = neighbors
            self.robots[robot_id].neighbors = neighbors

            # Update neighbor weights
            weight = 1.0 / (len(neighbors) + 1)
            for neighbor_id in neighbors:
                self.robots[robot_id].communication_weights[neighbor_id] = weight
            self.robots[robot_id].self_weight = 1.0 - sum(self.robots[robot_id].communication_weights.values())

    def create_random_topology(self, connection_probability=0.3):
        """Create random topology"""
        for i in range(self.num_robots):
            robot_id = f'robot_{i:02d}'
            neighbors = []

            for j in range(self.num_robots):
                if i != j and random.random() < connection_probability:
                    neighbors.append(f'robot_{j:02d}')

            self.connections[robot_id] = neighbors
            self.robots[robot_id].neighbors = neighbors

            # Update neighbor weights
            weight = 1.0 / (len(neighbors) + 1)
            for neighbor_id in neighbors:
                self.robots[robot_id].communication_weights[neighbor_id] = weight
            self.robots[robot_id].self_weight = 1.0 - sum(self.robots[robot_id].communication_weights.values())

    def update_network(self):
        """Update all robots in the network"""
        # First, robots broadcast their states
        for robot_id, robot in self.robots.items():
            for neighbor_id in robot.neighbors:
                if neighbor_id in self.robots:
                    self.robots[neighbor_id].receive_neighbor_state(robot_id, robot.state)

        # Then, robots update their consensus values
        for robot in self.robots.values():
            robot.update_consensus()

        # Finally, update robot states (for dynamic systems)
        for robot in self.robots.values():
            robot.state = robot.consensus_value.copy()

    def get_states(self):
        """Get current states of all robots"""
        return [self.robots[f'robot_{i:02d}'].state[0] for i in range(self.num_robots)]

    def get_consensus_values(self):
        """Get consensus values of all robots"""
        return [self.robots[f'robot_{i:02d}'].consensus_value[0] for i in range(self.num_robots)]

    def animate_consensus(self, steps=100):
        """Animate consensus convergence"""
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))

        # Initialize plots
        x_vals = range(self.num_robots)
        initial_states = self.get_states()

        bars1 = ax1.bar(x_vals, initial_states)
        ax1.set_title('Robot States Over Time')
        ax1.set_ylabel('State Value')
        ax1.set_ylim(min(initial_states) - 1, max(initial_states) + 1)

        consensus_vals = self.get_consensus_values()
        bars2 = ax2.bar(x_vals, consensus_vals)
        ax2.set_title('Consensus Values')
        ax2.set_xlabel('Robot ID')
        ax2.set_ylabel('Consensus Value')
        ax2.set_ylim(min(consensus_vals) - 1, max(consensus_vals) + 1)

        def update(frame):
            self.update_network()

            # Update state bars
            states = self.get_states()
            for bar, height in zip(bars1, states):
                bar.set_height(height)

            # Update consensus bars
            consensus_vals = self.get_consensus_values()
            for bar, height in zip(bars2, consensus_vals):
                bar.set_height(height)

            ax1.set_title(f'Robot States Over Time (Step {frame})')
            ax2.set_title(f'Consensus Values (Step {frame})')

            return list(bars1) + list(bars2)

        anim = FuncAnimation(fig, update, frames=steps, interval=100, blit=False, repeat=True)
        plt.tight_layout()
        plt.show()
        return anim


def main():
    print("Creating consensus network simulation...")

    # Create network with 8 robots
    network = ConsensusNetwork(8)
    network.create_ring_topology()

    print(f"Network topology: {network.connections}")

    # Print initial states
    initial_states = network.get_states()
    print(f"Initial states: {initial_states}")
    print(f"Initial average: {np.mean(initial_states):.2f}")

    # Run animation
    animation = network.animate_consensus(steps=200)

    # Show final states
    final_states = network.get_states()
    print(f"Final states: {final_states}")
    print(f"Final average: {np.mean(final_states):.2f}")

    return network, animation


if __name__ == '__main__':
    network, anim = main()
```

## ROS 2 Multi-Robot Coordination

### Multi-Robot Communication with ROS 2

```python
import rclpy
from rclpy.node import Node
from std_msgs.msg import String, Float32MultiArray
from geometry_msgs.msg import Pose, Twist
from sensor_msgs.msg import LaserScan
import json
import numpy as np


class MultiRobotCoordinator(Node):
    def __init__(self):
        super().__init__('multi_robot_coordinator')

        # Robot ID (should be set as parameter)
        self.declare_parameter('robot_id', 'robot_001')
        self.robot_id = self.get_parameter('robot_id').value

        # Publishers
        self.coordination_pub = self.create_publisher(String, f'/{self.robot_id}/coordination', 10)
        self.status_pub = self.create_publisher(String, f'/{self.robot_id}/status', 10)
        self.cmd_vel_pub = self.create_publisher(Twist, f'/{self.robot_id}/cmd_vel', 10)

        # Subscribers
        self.coordination_sub = self.create_subscription(
            String,
            '/coordination_broadcast',
            self.coordination_callback,
            10
        )

        self.status_sub = self.create_subscription(
            String,
            '/status_broadcast',
            self.status_callback,
            10
        )

        # Robot state
        self.current_position = np.array([0.0, 0.0])
        self.current_task = None
        self.teammates = {}  # robot_id -> status info
        self.task_assignments = {}  # task_id -> robot_id

        # Timer for coordination
        self.coordination_timer = self.create_timer(1.0, self.coordination_loop)

        self.get_logger().info(f'Multi-robot coordinator for {self.robot_id} initialized')

    def coordination_callback(self, msg):
        """Handle coordination messages from other robots"""
        try:
            coord_data = json.loads(msg.data)
            sender_id = coord_data['sender_id']

            if coord_data['type'] == 'task_assignment':
                task_id = coord_data['task_id']
                assigned_robot = coord_data['assigned_robot']

                # Update task assignments
                self.task_assignments[task_id] = assigned_robot
                self.get_logger().info(f'Task {task_id} assigned to {assigned_robot}')

            elif coord_data['type'] == 'task_request':
                # Handle request for task assignment
                self.handle_task_request(coord_data)

            elif coord_data['type'] == 'formation_request':
                # Handle formation coordination
                self.handle_formation_request(coord_data)

        except Exception as e:
            self.get_logger().error(f'Error processing coordination message: {e}')

    def status_callback(self, msg):
        """Handle status messages from other robots"""
        try:
            status_data = json.loads(msg.data)
            robot_id = status_data['robot_id']

            # Update teammate information
            self.teammates[robot_id] = {
                'position': status_data['position'],
                'status': status_data['status'],
                'battery': status_data['battery'],
                'timestamp': status_data['timestamp']
            }

            self.get_logger().info(f'Updated status for {robot_id}: {status_data["status"]}')

        except Exception as e:
            self.get_logger().error(f'Error processing status message: {e}')

    def coordination_loop(self):
        """Main coordination loop"""
        # Broadcast own status
        self.broadcast_status()

        # Perform coordination tasks based on current state
        if self.current_task is None:
            # Look for available tasks or request assignment
            self.request_task_assignment()

        # Check formation requirements
        self.maintain_formation()

    def broadcast_status(self):
        """Broadcast current robot status to team"""
        status_msg = {
            'robot_id': self.robot_id,
            'position': self.current_position.tolist(),
            'status': 'active' if self.current_task is None else 'working',
            'battery': 0.8,  # Simulated battery level
            'timestamp': self.get_clock().now().nanoseconds / 1e9
        }

        status_string = String()
        status_string.data = json.dumps(status_msg)
        self.status_pub.publish(status_string)

    def broadcast_coordination(self, coord_type, content):
        """Broadcast coordination message to team"""
        coord_msg = {
            'sender_id': self.robot_id,
            'type': coord_type,
            'content': content,
            'timestamp': self.get_clock().now().nanoseconds / 1e9
        }

        coord_string = String()
        coord_string.data = json.dumps(coord_msg)
        self.coordination_pub.publish(coord_string)

    def request_task_assignment(self):
        """Request task assignment from coordinator or other robots"""
        request_content = {
            'capabilities': ['navigation', 'manipulation'],
            'current_position': self.current_position.tolist(),
            'available': True
        }

        self.broadcast_coordination('task_request', request_content)

    def handle_task_request(self, request_data):
        """Handle task request from another robot"""
        requester_id = request_data['sender_id']
        requester_caps = request_data['content']['capabilities']

        # In a real system, this would check for a task coordinator
        # For now, simulate task assignment
        if self.robot_id.startswith('robot_001'):  # Robot 1 acts as coordinator
            # Assign a task to the requesting robot
            task_id = f'task_{int(time.time()) % 1000}'
            task_location = [np.random.uniform(-10, 10), np.random.uniform(-10, 10)]

            assignment = {
                'task_id': task_id,
                'location': task_location,
                'assigned_robot': requester_id
            }

            self.broadcast_coordination('task_assignment', assignment)
            self.get_logger().info(f'Assigned task {task_id} to {requester_id}')

    def handle_formation_request(self, request_data):
        """Handle formation request from another robot"""
        formation_type = request_data['content']['formation_type']
        leader_id = request_data['content']['leader']

        if leader_id == self.robot_id:
            # I'm the leader, coordinate formation
            self.coordinate_formation(formation_type)
        elif self.current_task is None:
            # Join the formation as follower
            self.join_formation(formation_type, leader_id)

    def coordinate_formation(self, formation_type):
        """Coordinate formation as leader"""
        if formation_type == 'line':
            # Calculate positions for line formation
            positions = self.calculate_line_formation()
        elif formation_type == 'circle':
            positions = self.calculate_circle_formation()
        else:
            return

        # Assign positions to team members
        for i, (robot_id, _) in enumerate(self.teammates.items()):
            if i < len(positions):
                assignment = {
                    'formation_type': formation_type,
                    'robot_id': robot_id,
                    'target_position': positions[i]
                }
                self.broadcast_coordination('formation_assignment', assignment)

    def join_formation(self, formation_type, leader_id):
        """Join formation as follower"""
        self.get_logger().info(f'Joining {formation_type} formation led by {leader_id}')

    def calculate_line_formation(self):
        """Calculate positions for line formation"""
        positions = []
        spacing = 2.0
        leader_pos = self.current_position  # In real system, get leader position

        for i in range(len(self.teammates) + 1):  # +1 for self
            pos = [leader_pos[0] + i * spacing, leader_pos[1]]
            positions.append(pos)

        return positions

    def calculate_circle_formation(self):
        """Calculate positions for circle formation"""
        positions = []
        radius = 3.0
        num_robots = len(self.teammates) + 1

        for i in range(num_robots):
            angle = 2 * np.pi * i / num_robots
            pos = [
                self.current_position[0] + radius * np.cos(angle),
                self.current_position[1] + radius * np.sin(angle)
            ]
            positions.append(pos)

        return positions

    def maintain_formation(self):
        """Maintain current formation if in one"""
        # In a real system, this would move robots to maintain formation
        pass


def main(args=None):
    rclpy.init(args=args)

    # Create node with specific robot ID
    import sys
    robot_id = sys.argv[1] if len(sys.argv) > 1 else 'robot_001'

    coordinator = MultiRobotCoordinator()
    coordinator.get_parameter('robot_id').set_value(robot_id)

    try:
        rclpy.spin(coordinator)
    except KeyboardInterrupt:
        pass
    finally:
        coordinator.destroy_node()
        rclpy.shutdown()


if __name__ == '__main__':
    main()
```

## Swarm Intelligence Algorithms

### Ant Colony Optimization for Multi-Robot Path Planning

```python
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation
import random


class AntColonyRobot:
    def __init__(self, robot_id, start_pos, target_pos, world_size):
        self.robot_id = robot_id
        self.position = np.array(start_pos, dtype=float)
        self.target = np.array(target_pos, dtype=float)
        self.world_size = world_size
        self.path = [self.position.copy()]
        self.found_target = False
        self.pheromone_trail = []  # Store pheromone trail
        self.carrying_pheromone = True

    def move(self, pheromone_grid, obstacles, evaporation_rate=0.1):
        """Move robot based on pheromone trails and target direction"""
        if self.found_target:
            return  # Robot has reached target

        # Evaporate some pheromone
        if random.random() < evaporation_rate:
            if self.pheromone_trail:
                self.pheromone_trail.pop(0)  # Remove oldest pheromone

        # Calculate possible moves (8 directions)
        possible_moves = []
        for dx in [-1, 0, 1]:
            for dy in [-1, 0, 1]:
                if dx == 0 and dy == 0:
                    continue  # Skip staying in place

                new_pos = self.position + np.array([dx, dy])

                # Check bounds
                if (0 <= new_pos[0] < self.world_size and
                    0 <= new_pos[1] < self.world_size):

                    # Check obstacles
                    if not any(np.array_equal(new_pos, obs) for obs in obstacles):
                        possible_moves.append(new_pos)

        if not possible_moves:
            return  # No valid moves

        # Calculate probabilities based on pheromone and distance to target
        probabilities = []
        for move in possible_moves:
            # Pheromone factor (higher pheromone = higher probability)
            pheromone_level = pheromone_grid[int(move[0]), int(move[1])] if 0 <= int(move[0]) < self.world_size and 0 <= int(move[1]) < self.world_size else 0

            # Distance factor (closer to target = higher probability)
            distance_to_target = np.linalg.norm(move - self.target)
            distance_factor = 1.0 / (distance_to_target + 1)  # Closer = higher

            probability = pheromone_level + distance_factor
            probabilities.append(probability)

        # Normalize probabilities
        total_prob = sum(probabilities)
        if total_prob > 0:
            probabilities = [p / total_prob for p in probabilities]
        else:
            # If no pheromone, move randomly toward target
            probabilities = [1.0 / len(possible_moves)] * len(possible_moves)

        # Choose move based on probabilities
        chosen_move = possible_moves[np.random.choice(len(possible_moves), p=probabilities)]

        # Update position
        self.position = chosen_move
        self.path.append(self.position.copy())

        # Deposit pheromone
        if 0 <= int(self.position[0]) < self.world_size and 0 <= int(self.position[1]) < self.world_size:
            pheromone_grid[int(self.position[0]), int(self.position[1])] += 0.5

        # Check if reached target
        if np.linalg.norm(self.position - self.target) < 1.0:
            self.found_target = True

    def reset(self, start_pos):
        """Reset robot to new start position"""
        self.position = np.array(start_pos, dtype=float)
        self.path = [self.position.copy()]
        self.found_target = False


class AntColonyMultiRobotSystem:
    def __init__(self, world_size=20, num_robots=5):
        self.world_size = world_size
        self.num_robots = num_robots
        self.pheromone_grid = np.zeros((world_size, world_size))
        self.obstacles = []
        self.robots = []

        # Initialize robots
        for i in range(num_robots):
            start_pos = [random.randint(0, world_size//4), random.randint(0, world_size//4)]
            target_pos = [random.randint(3*world_size//4, world_size-1), random.randint(3*world_size//4, world_size-1)]

            robot = AntColonyRobot(f'robot_{i:02d}', start_pos, target_pos, world_size)
            self.robots.append(robot)

    def add_obstacle(self, pos):
        """Add obstacle to environment"""
        self.obstacles.append(np.array(pos))

    def update(self):
        """Update all robots"""
        for robot in self.robots:
            if not robot.found_target:
                robot.move(self.pheromone_grid, self.obstacles)

    def get_positions(self):
        """Get all robot positions"""
        return [robot.position for robot in self.robots]

    def get_targets(self):
        """Get all target positions"""
        return [robot.target for robot in self.robots]

    def animate_system(self, steps=200):
        """Animate the ant colony system"""
        fig, ax = plt.subplots(figsize=(10, 10))
        ax.set_xlim(0, self.world_size)
        ax.set_ylim(0, self.world_size)
        ax.set_aspect('equal')
        ax.set_title('Ant Colony Optimization for Multi-Robot Path Planning')

        # Initialize scatter plots
        robot_positions = np.array(self.get_positions())
        target_positions = np.array(self.get_targets())

        robot_scatter = ax.scatter(robot_positions[:, 0], robot_positions[:, 1],
                                  c='blue', s=50, label='Robots')
        target_scatter = ax.scatter(target_positions[:, 0], target_positions[:, 1],
                                   c='red', s=100, marker='*', label='Targets')

        # Plot obstacles
        if self.obstacles:
            obstacle_positions = np.array(self.obstacles)
            ax.scatter(obstacle_positions[:, 0], obstacle_positions[:, 1],
                      c='black', s=80, marker='s', label='Obstacles')

        # Plot pheromone heatmap
        im = ax.imshow(self.pheromone_grid, cmap='viridis', alpha=0.3,
                      extent=[0, self.world_size, 0, self.world_size],
                      origin='lower', vmin=0, vmax=1)

        def update(frame):
            self.update()

            # Update robot positions
            robot_positions = np.array(self.get_positions())
            robot_scatter.set_offsets(robot_positions)

            # Update pheromone visualization
            im.set_array(self.pheromone_grid)

            # Update title
            completed = sum(1 for robot in self.robots if robot.found_target)
            ax.set_title(f'Ant Colony Optimization - Step {frame}, Completed: {completed}/{self.num_robots}')

            return [robot_scatter, target_scatter, im]

        anim = FuncAnimation(fig, update, frames=steps, interval=100, blit=False, repeat=True)
        plt.legend()
        plt.show()
        return anim


def main():
    print("Starting Ant Colony Optimization for Multi-Robot Path Planning...")

    # Create system
    ac_system = AntColonyMultiRobotSystem(world_size=20, num_robots=5)

    # Add some obstacles
    for _ in range(10):
        ac_system.add_obstacle([random.randint(5, 15), random.randint(5, 15)])

    print(f"Created system with {ac_system.num_robots} robots and {len(ac_system.obstacles)} obstacles")

    # Run animation
    animation = ac_system.animate_system(steps=300)

    # Count completed robots
    completed = sum(1 for robot in ac_system.robots if robot.found_target)
    print(f"Simulation completed. {completed}/{len(ac_system.robots)} robots reached their targets.")

    return ac_system, animation


if __name__ == '__main__':
    system, anim = main()
```

## Hands-On Lab: Multi-Robot Formation Control

### Objective
Implement a multi-robot formation control system that demonstrates coordination and communication between robots.

### Prerequisites
- Completed Chapter 1-12
- ROS 2 Humble with Gazebo
- Basic understanding of multi-robot systems

### Steps

1. **Create a multi-robot lab package**:
   ```bash
   cd ~/robotics_ws/src
   ros2 pkg create --build-type ament_python multi_robot_lab --dependencies rclpy std_msgs geometry_msgs sensor_msgs nav_msgs tf2_ros
   ```

2. **Create the multi-robot formation controller** (`multi_robot_lab/multi_robot_lab/formation_controller.py`):
   ```python
   #!/usr/bin/env python3
   import rclpy
   from rclpy.node import Node
   from geometry_msgs.msg import Twist, Pose, Point
   from nav_msgs.msg import Odometry
   from std_msgs.msg import String
   import numpy as np
   import math


   class FormationController(Node):
       def __init__(self):
           super().__init__('formation_controller')

           # Robot ID parameter
           self.declare_parameter('robot_id', 'robot_001')
           self.robot_id = self.get_parameter('robot_id').value

           # Formation parameters
           self.declare_parameter('formation_type', 'line')
           self.declare_parameter('formation_spacing', 2.0)
           self.declare_parameter('leader_id', 'robot_001')

           self.formation_type = self.get_parameter('formation_type').value
           self.formation_spacing = self.get_parameter('formation_spacing').value
           self.leader_id = self.get_parameter('leader_id').value

           # Publishers and subscribers
           self.cmd_vel_pub = self.create_publisher(Twist, f'/{self.robot_id}/cmd_vel', 10)
           self.status_pub = self.create_publisher(String, f'/{self.robot_id}/formation_status', 10)

           # Robot state
           self.current_position = np.array([0.0, 0.0])
           self.current_velocity = np.array([0.0, 0.0])
           self.target_position = np.array([0.0, 0.0])
           self.robot_positions = {}  # robot_id -> position
           self.is_leader = (self.robot_id == self.leader_id)

           # Timer for control loop
           self.control_timer = self.create_timer(0.1, self.control_loop)

           # Subscribe to all robot positions
           for i in range(1, 6):  # Assume up to 5 robots
               robot_name = f'robot_{i:03d}'
               sub = self.create_subscription(
                   Odometry,
                   f'/{robot_name}/odom',
                   lambda msg, rn=robot_name: self.odom_callback(msg, rn),
                   10
               )

           self.get_logger().info(f'Formation controller for {self.robot_id} initialized')

       def odom_callback(self, msg, robot_name):
           """Update robot position from odometry"""
           position = np.array([
               msg.pose.pose.position.x,
               msg.pose.pose.position.y
           ])

           self.robot_positions[robot_name] = position

           # Update own position if this is self
           if robot_name == self.robot_id:
               self.current_position = position

       def calculate_formation_position(self):
           """Calculate target position in formation"""
           if self.is_leader:
               # Leader follows a predefined path or user commands
               # For this example, leader moves in a circle
               time_now = self.get_clock().now().nanoseconds / 1e9
               radius = 5.0
               angle = time_now * 0.5  # Rotate at 0.5 rad/s
               target_x = radius * math.cos(angle)
               target_y = radius * math.sin(angle)
               return np.array([target_x, target_y])

           # Follower robots calculate their position based on leader and formation type
           if self.leader_id not in self.robot_positions:
               return self.current_position  # Stay in place if no leader info

           leader_pos = self.robot_positions[self.leader_id]

           # Calculate robot index in formation
           robot_index = self.get_robot_index(self.robot_id)
           if robot_index == -1:
               return self.current_position

           if self.formation_type == 'line':
               # Line formation: robots arranged in a line behind leader
               # Calculate direction from leader to first follower
               direction = np.array([1.0, 0.0])  # Default direction (could be based on leader's heading)
               offset = self.formation_spacing * robot_index * direction
               target_pos = leader_pos - offset

           elif self.formation_type == 'circle':
               # Circle formation: robots arranged in circle around leader
               angle_between_robots = 2 * math.pi / (self.get_total_robots() - 1)  # -1 to exclude leader
               robot_angle = robot_index * angle_between_robots
               offset_x = self.formation_spacing * math.cos(robot_angle)
               offset_y = self.formation_spacing * math.sin(robot_angle)
               target_pos = leader_pos + np.array([offset_x, offset_y])

           elif self.formation_type == 'diamond':
               # Diamond formation: specific positions relative to leader
               positions = [
                   np.array([0.0, 0.0]),      # Leader position
                   np.array([2.0, 0.0]),      # Right
                   np.array([-2.0, 0.0]),     # Left
                   np.array([0.0, 2.0]),      # Front
                   np.array([0.0, -2.0])      # Back
               ]
               target_pos = leader_pos + positions[robot_index]

           else:
               # Default to line formation
               direction = np.array([1.0, 0.0])
               offset = self.formation_spacing * robot_index * direction
               target_pos = leader_pos - offset

           return target_pos

       def get_robot_index(self, robot_id):
           """Get index of robot in formation list"""
           # Extract number from robot ID (e.g., robot_002 -> 2)
           try:
               robot_num = int(robot_id.split('_')[1])
               return robot_num - 1  # 0-indexed
           except:
               return -1

       def get_total_robots(self):
           """Get total number of robots in system"""
           return len(self.robot_positions)

       def calculate_control_command(self, target_pos):
           """Calculate velocity command to reach target position"""
           error = target_pos - self.current_position
           distance = np.linalg.norm(error)

           cmd_vel = Twist()

           if distance > 0.2:  # If not close to target
               # Proportional controller
               kp_pos = 0.5
               desired_velocity = error * kp_pos

               # Limit velocity
               speed = np.linalg.norm(desired_velocity)
               if speed > 1.0:
                   desired_velocity = desired_velocity / speed

               # Calculate angular velocity to face direction of movement
               if speed > 0.01:
                   desired_angle = math.atan2(desired_velocity[1], desired_velocity[0])
                   current_angle = self.get_current_heading()
                   angle_error = desired_angle - current_angle
                   # Normalize angle error to [-pi, pi]
                   while angle_error > math.pi:
                       angle_error -= 2 * math.pi
                   while angle_error < -math.pi:
                       angle_error += 2 * math.pi

                   cmd_vel.linear.x = min(speed, 1.0)
                   cmd_vel.angular.z = angle_error * 2.0  # Proportional angular control
               else:
                   cmd_vel.linear.x = 0.0
                   cmd_vel.angular.z = 0.0
           else:
               cmd_vel.linear.x = 0.0
               cmd_vel.angular.z = 0.0

           return cmd_vel

       def get_current_heading(self):
           """Get current robot heading from odometry (simplified)"""
           # In a real system, you'd extract this from the orientation quaternion
           # For this example, we'll use a simplified approach
           return 0.0  # Default heading

       def control_loop(self):
           """Main control loop"""
           if len(self.robot_positions) < 2:
               # Not enough robots to form formation
               cmd_vel = Twist()
               self.cmd_vel_pub.publish(cmd_vel)
               return

           # Calculate target position in formation
           self.target_position = self.calculate_formation_position()

           # Calculate control command
           cmd_vel = self.calculate_control_command(self.target_position)

           # Publish command
           self.cmd_vel_pub.publish(cmd_vel)

           # Publish status
           status_msg = String()
           status_msg.data = f"Target: ({self.target_position[0]:.2f}, {self.target_position[1]:.2f}), " \
                            f"Current: ({self.current_position[0]:.2f}, {self.current_position[1]:.2f})"
           self.status_pub.publish(status_msg)

           self.get_logger().info(f'Robot {self.robot_id}: Moving to {self.target_position}, Command: ({cmd_vel.linear.x:.2f}, {cmd_vel.angular.z:.2f})')


   def main(args=None):
       rclpy.init(args=args)

       # Get robot ID from command line or use default
       import sys
       robot_id = sys.argv[1] if len(sys.argv) > 1 else 'robot_001'

       formation_controller = FormationController()
       formation_controller.get_parameter('robot_id').set_value(robot_id)

       try:
           rclpy.spin(formation_controller)
       except KeyboardInterrupt:
           pass
       finally:
           formation_controller.destroy_node()
           rclpy.shutdown()


   if __name__ == '__main__':
       main()
   ```

3. **Create a launch file** (`multi_robot_lab/launch/formation_demo.launch.py`):
   ```python
   from launch import LaunchDescription
   from launch.actions import DeclareLaunchArgument, IncludeLaunchDescription
   from launch.substitutions import LaunchConfiguration
   from launch_ros.actions import Node
   from ament_index_python.packages import get_package_share_directory
   import os


   def generate_launch_description():
       # Declare launch arguments
       formation_type = DeclareLaunchArgument(
           'formation_type',
           default_value='line',
           description='Formation type: line, circle, or diamond'
       )

       formation_spacing = DeclareLaunchArgument(
           'formation_spacing',
           default_value='2.0',
           description='Spacing between robots in formation'
       )

       # Launch multiple robot controllers
       robot_controllers = []
       for i in range(1, 6):  # Launch 5 robots
           robot_id = f'robot_{i:03d}'
           controller_node = Node(
               package='multi_robot_lab',
               executable='formation_controller',
               name=f'formation_controller_{robot_id}',
               parameters=[
                   {'robot_id': robot_id},
                   {'formation_type': LaunchConfiguration('formation_type')},
                   {'formation_spacing': LaunchConfiguration('formation_spacing')},
                   {'leader_id': 'robot_001'}  # Robot 1 is leader
               ],
               output='screen'
           )
           robot_controllers.append(controller_node)

       return LaunchDescription([
           formation_type,
           formation_spacing,
       ] + robot_controllers)
   ```

4. **Update setup.py**:
   ```python
   import os
   from glob import glob
   from setuptools import setup
   from setuptools import find_packages

   package_name = 'multi_robot_lab'

   setup(
       name=package_name,
       version='0.0.0',
       packages=find_packages(exclude=['test']),
       data_files=[
           ('share/ament_index/resource_index/packages',
               ['resource/' + package_name]),
           ('share/' + package_name, ['package.xml']),
           (os.path.join('share', package_name, 'launch'), glob('launch/*.launch.py')),
       ],
       install_requires=['setuptools'],
       zip_safe=True,
       maintainer='Your Name',
       maintainer_email='your.email@example.com',
       description='Multi-robot lab for formations and coordination',
       license='Apache License 2.0',
       tests_require=['pytest'],
       entry_points={
           'console_scripts': [
               'formation_controller = multi_robot_lab.formation_controller:main',
           ],
       },
   )
   ```

5. **Build the package**:
   ```bash
   cd ~/robotics_ws
   colcon build --packages-select multi_robot_lab
   source install/setup.bash
   ```

6. **Run the multi-robot formation simulation**:
   ```bash
   ros2 launch multi_robot_lab formation_demo.launch.py formation_type:=circle formation_spacing:=3.0
   ```

### Expected Results
- Multiple robots should form the specified formation (line, circle, or diamond)
- Robots should maintain formation while the leader moves
- Followers should adjust their positions based on leader movement
- Formation should be stable and robust to minor disturbances

### Troubleshooting Tips
- Ensure all robot namespaces are correctly set up
- Check that odometry topics are being published for each robot
- Verify TF frames are properly configured for each robot
- Monitor the logs for formation status and control commands

## Summary

In this chapter, we've explored the fundamental concepts of multi-robot systems and coordination, including:

1. **Multi-Robot Architectures**: Centralized vs decentralized approaches
2. **Communication Protocols**: Robot-to-robot communication and message passing
3. **Swarm Robotics**: Collective behavior algorithms and emergent coordination
4. **Task Allocation**: Auction-based and market-based task assignment
5. **Coordination Algorithms**: Consensus, formation control, and distributed decision making
6. **Implementation**: Practical examples of multi-robot coordination in ROS 2

The hands-on lab provided experience with creating a formation control system that demonstrates coordination between multiple robots. This foundation is essential for more advanced multi-robot applications including cooperative manipulation, distributed sensing, and collective decision making that we'll explore in the upcoming chapters.

============================================================
FILE: book\docs\part-v-integration\chapter-14-deployment\index.md
============================================================
---
sidebar_position: 14
title: "Chapter 14: Real-World Deployment and Safety"
---

# Chapter 14: Real-World Deployment and Safety

## Learning Goals

By the end of this chapter, students will be able to:
- Implement safety protocols for real-world robot deployment
- Apply international safety standards (ISO 10218, ISO/TS 15066, ISO 13482) to robotic systems
- Conduct risk assessments for robotic applications
- Deploy robot systems with proper safety measures and monitoring
- Maintain operational robot systems with safety-first principles

## Key Technologies
- Safety-rated controllers and drives
- ISO safety standards for industrial and service robots
- Risk assessment methodologies (HAZOP, FMEA)
- Emergency stop systems and safety PLCs
- Collision detection and avoidance systems

## Introduction

Deploying robots in real-world environments presents unique challenges that extend far beyond laboratory conditions. While laboratory robots operate in controlled environments with predictable parameters, real-world deployments must account for human interaction, environmental variability, and safety-critical operations. This chapter explores the critical considerations for deploying robots safely and effectively in real-world scenarios.

Real-world robot deployment requires a comprehensive safety framework that encompasses hardware, software, and procedural elements. From initial risk assessment to ongoing maintenance, every aspect of the robot system must be designed with safety as the primary concern. This is particularly crucial when robots interact with humans, operate in public spaces, or perform tasks in safety-critical environments.

## International Safety Standards

### ISO 10218: Industrial Robots

ISO 10218 is the foundational safety standard for industrial robots, covering the safety requirements for the complete robot system. This standard addresses:
- Robot system integration and installation
- Programming and teaching procedures
- Maintenance and repair operations
- Environmental considerations

The standard emphasizes the importance of risk assessment and hazard identification throughout the robot lifecycle. It defines safety functions such as emergency stops, protective stops, and safety-rated monitoring functions that must be implemented in industrial robot applications.

### ISO/TS 15066: Collaborative Robots

ISO/TS 15066 specifically addresses collaborative robots (cobots) that operate alongside humans. This technical specification covers:
- Power and force limiting for human-robot collaboration
- Speed and separation monitoring requirements
- Safety-rated monitored stop functionality
- Testing and validation procedures for collaborative applications

The standard establishes maximum allowable forces and power levels for different contact scenarios, taking into account body regions and potential injury mechanisms. It also provides guidance on workspace design and safety zone definitions for collaborative applications.

### ISO 13482: Service Robots

ISO 13482 addresses personal care robots, cleaning robots, and other service robots that interact with humans in domestic, commercial, and public environments. This standard covers:
- Personal care robot safety requirements
- Cleaning robot safety considerations
- Emergency procedures and user interfaces
- Risk assessment for service robot applications

## Risk Assessment and Management

### Hazard Identification

Effective risk assessment begins with comprehensive hazard identification. Key hazards in robotic systems include:

1. **Mechanical Hazards**: Pinching, crushing, shearing, impact, and entanglement from moving robot parts
2. **Electrical Hazards**: Shock, burns, and fire from electrical components
3. **Thermal Hazards**: Burns and fires from overheating components
4. **Radiation Hazards**: UV, IR, or laser radiation from sensors and communication systems
5. **Chemical Hazards**: Exposure to cooling fluids, lubricants, or battery materials
6. **Behavioral Hazards**: Unexpected robot movements or malfunctions

### Risk Assessment Methodology

A systematic risk assessment follows these steps:

1. **Hazard Identification**: Identify all potential hazards associated with the robot system
2. **Risk Estimation**: Evaluate the probability of occurrence and severity of potential harm
3. **Risk Evaluation**: Compare estimated risks against acceptable risk levels
4. **Risk Reduction**: Implement measures to reduce risks to tolerable levels
5. **Residual Risk Assessment**: Evaluate remaining risks after reduction measures

### Safety Functions and Categories

Robot safety systems typically implement several categories of safety functions:

```python
# Safety-rated controller example
class SafetyRatedController:
    def __init__(self, safety_factors: Dict[str, float] = None):
        """
        Initialize safety-rated controller
        safety_factors: Dictionary of safety margins for different parameters
        """
        self.safety_factors = safety_factors or {
            'velocity': 0.8,    # 80% of maximum velocity allowed
            'acceleration': 0.7, # 70% of maximum acceleration allowed
            'force': 0.9,       # 90% of maximum force allowed
            'distance': 0.95    # 95% of safe distance maintained
        }

        # Robot limits (these would come from robot specifications)
        self.robot_limits = {
            'max_velocity': 1.0,      # m/s
            'max_acceleration': 2.0,  # m/s¬≤
            'max_force': 100.0,       # N
            'min_safe_distance': 0.5  # m
        }

    def calculate_safe_parameters(self, environment_data: dict):
        """
        Calculate safe operating parameters based on environment data
        """
        safe_params = {}

        # Calculate safe velocity based on proximity to obstacles
        closest_obstacle = min(environment_data.get('distances', [float('inf')]))
        if closest_obstacle < self.robot_limits['min_safe_distance']:
            safe_params['velocity'] = 0.0  # Stop if too close
        else:
            # Scale velocity based on safety distance
            distance_ratio = (closest_obstacle - self.robot_limits['min_safe_distance']) / 2.0
            safe_params['velocity'] = min(
                self.robot_limits['max_velocity'] * self.safety_factors['velocity'],
                self.robot_limits['max_velocity'] * distance_ratio
            )

        # Calculate safe acceleration based on current state
        safe_params['acceleration'] = (
            self.robot_limits['max_acceleration'] *
            self.safety_factors['acceleration']
        )

        return safe_params

    def check_safety_zones(self, robot_pose: Pose, environment_map: OccupancyGrid):
        """
        Check if robot is in safe zones and verify no unauthorized access
        """
        # Define safety zones around robot
        safety_radius = self.robot_limits['min_safe_distance']

        # Check for humans or obstacles in safety zone
        for entity in environment_map.entities:
            distance = self.calculate_distance(robot_pose, entity.pose)
            if distance < safety_radius:
                return False, f"Obstacle in safety zone: {entity.type}"

        return True, "Safe to operate"
```

## Safe Robot Operation Protocols

### Pre-Deployment Safety Checks

Before deploying any robot system, comprehensive safety checks must be performed:

1. **Hardware Verification**: Verify all safety-critical components are functioning properly
2. **Software Validation**: Confirm safety functions are correctly implemented and tested
3. **Environmental Assessment**: Evaluate deployment environment for potential hazards
4. **Communication Testing**: Verify all safety-related communication channels
5. **Emergency Procedures**: Test all emergency stop and response systems

### Emergency Response Systems

Robots deployed in real-world environments must implement robust emergency response capabilities:

```python
import threading
import time
from enum import Enum

class EmergencyLevel(Enum):
    NORMAL = 0
    WARNING = 1
    EMERGENCY_STOP = 2
    SYSTEM_SHUTDOWN = 3

class EmergencyResponseSystem:
    def __init__(self):
        self.emergency_level = EmergencyLevel.NORMAL
        self.active_alerts = []
        self.shutdown_procedures = []
        self.emergency_lock = threading.Lock()

    def evaluate_emergency_status(self, sensor_data: dict):
        """
        Evaluate sensor data to determine emergency level
        """
        new_level = EmergencyLevel.NORMAL

        # Check for collision imminent
        if any(dist < 0.3 for dist in sensor_data.get('laser_scan', [])):
            new_level = max(new_level, EmergencyLevel.WARNING)

        # Check for human intrusion in safety zone
        if sensor_data.get('human_detected_in_zone', False):
            new_level = max(new_level, EmergencyLevel.EMERGENCY_STOP)

        # Check for system faults
        if sensor_data.get('critical_fault', False):
            new_level = max(new_level, EmergencyLevel.SYSTEM_SHUTDOWN)

        # Check for excessive force/torque
        if any(abs(torque) > 80 for torque in sensor_data.get('joint_torques', [])):  # 80% of max
            new_level = max(new_level, EmergencyLevel.WARNING)

        return new_level

    def trigger_emergency_response(self, level: EmergencyLevel, reason: str = ""):
        """
        Trigger appropriate emergency response based on level
        """
        with self.emergency_lock:
            self.emergency_level = level

            if level == EmergencyLevel.WARNING:
                self._handle_warning(reason)
            elif level == EmergencyLevel.EMERGENCY_STOP:
                self._execute_emergency_stop(reason)
            elif level == EmergencyLevel.SYSTEM_SHUTDOWN:
                self._execute_system_shutdown(reason)

    def _handle_warning(self, reason: str):
        """
        Handle warning level emergency - slow down operations
        """
        print(f"WARNING: {reason}")
        # Reduce robot speed, increase safety margins
        self.active_alerts.append({
            'level': 'warning',
            'time': time.time(),
            'reason': reason
        })

    def _execute_emergency_stop(self, reason: str):
        """
        Execute emergency stop procedure
        """
        print(f"EMERGENCY STOP: {reason}")
        # Stop all robot motion immediately
        self._stop_robot_immediately()
        self.active_alerts.append({
            'level': 'emergency_stop',
            'time': time.time(),
            'reason': reason
        })

    def _execute_system_shutdown(self, reason: str):
        """
        Execute complete system shutdown
        """
        print(f"SYSTEM SHUTDOWN: {reason}")
        # Execute all shutdown procedures
        for proc in self.shutdown_procedures:
            proc()
        self.active_alerts.append({
            'level': 'shutdown',
            'time': time.time(),
            'reason': reason
        })

    def _stop_robot_immediately(self):
        """
        Send immediate stop command to robot
        """
        # This would interface with robot controller
        # Stop all joints, disable actuators
        pass
```

### Monitoring and Diagnostics

Continuous monitoring is essential for maintaining safe robot operations:

```python
class SafetyMonitor:
    def __init__(self):
        self.safety_metrics = {}
        self.performance_thresholds = {
            'cpu_usage': 85.0,
            'memory_usage': 90.0,
            'temperature': 75.0,
            'communication_latency': 100.0  # ms
        }
        self.alert_history = []

    def monitor_system_health(self):
        """
        Monitor system health and performance metrics
        """
        metrics = {}

        # CPU and memory usage
        import psutil
        metrics['cpu_usage'] = psutil.cpu_percent()
        metrics['memory_usage'] = psutil.virtual_memory().percent
        metrics['disk_usage'] = psutil.disk_usage('/').percent

        # Temperature monitoring (if available)
        try:
            temps = psutil.sensors_temperatures()
            if 'coretemp' in temps:
                metrics['temperature'] = max([t.current for t in temps['coretemp']])
            else:
                metrics['temperature'] = 0.0
        except AttributeError:
            metrics['temperature'] = 0.0

        # Communication monitoring
        metrics['communication_latency'] = self._measure_communication_latency()

        self.safety_metrics.update(metrics)
        return self._check_thresholds(metrics)

    def _measure_communication_latency(self):
        """
        Measure communication latency with robot controller
        """
        # Implementation would measure round-trip time to robot
        return 10.0  # Placeholder

    def _check_thresholds(self, metrics: dict):
        """
        Check if any metrics exceed safety thresholds
        """
        violations = []
        for metric, value in metrics.items():
            threshold = self.performance_thresholds.get(metric)
            if threshold and value > threshold:
                violations.append({
                    'metric': metric,
                    'value': value,
                    'threshold': threshold
                })
        return violations

    def generate_safety_report(self):
        """
        Generate comprehensive safety and performance report
        """
        violations = self._check_thresholds(self.safety_metrics)

        report = {
            'timestamp': time.time(),
            'metrics': self.safety_metrics.copy(),
            'violations': violations,
            'alert_count': len(self.alert_history),
            'last_alert': self.alert_history[-1] if self.alert_history else None
        }

        return report
```

## Deployment Strategies

### Phased Deployment Approach

Successful real-world robot deployment follows a phased approach:

1. **Laboratory Testing**: Extensive testing in controlled laboratory conditions
2. **Pilot Environment**: Limited deployment in a controlled real-world environment
3. **Gradual Expansion**: Incremental expansion of operational scope and autonomy
4. **Full Deployment**: Complete deployment with all planned capabilities

### Environmental Adaptation

Robots must adapt to real-world environmental variations:

- **Lighting Conditions**: Adjust vision systems for varying illumination
- **Surface Variations**: Adapt navigation and manipulation for different surfaces
- **Temperature Fluctuations**: Maintain performance across temperature ranges
- **Humidity and Weather**: Protect systems from moisture and environmental factors

### Human-Robot Interaction Considerations

When deploying robots that interact with humans:

- **Predictable Behavior**: Ensure robot actions are predictable and understandable
- **Clear Communication**: Provide clear feedback about robot intentions and status
- **Escape Routes**: Design interactions that allow humans to maintain escape routes
- **Social Conventions**: Follow social conventions for interaction timing and space

## Maintenance and Operational Safety

### Preventive Maintenance

Regular maintenance schedules ensure continued safe operation:

- **Daily Checks**: Visual inspection, basic functionality tests
- **Weekly Inspections**: Detailed component inspections, calibration verification
- **Monthly Maintenance**: Lubrication, filter replacement, software updates
- **Annual Overhaul**: Comprehensive system inspection and component replacement

### Safety Documentation

Comprehensive documentation supports safe operations:

- **Operating Procedures**: Step-by-step procedures for normal operations
- **Emergency Procedures**: Clear instructions for various emergency scenarios
- **Maintenance Procedures**: Detailed maintenance instructions and safety precautions
- **Training Materials**: Educational resources for operators and maintainers

## Case Study: Safe Deployment of Service Robot in Healthcare

Consider a service robot deployed in a hospital environment for delivering supplies and medications:

### Risk Assessment

**Hazards Identified**:
- Collision with patients, staff, or visitors
- Obstruction of emergency pathways
- Contamination transmission
- System failure leading to delayed medical care

**Safety Measures Implemented**:
- Multiple redundant sensors for obstacle detection
- Speed limitation in high-traffic areas
- Antimicrobial surface treatments
- Emergency stop accessible to hospital staff
- Communication with hospital network for priority routing

### Technical Implementation

```python
class HospitalServiceRobot:
    def __init__(self):
        self.emergency_system = EmergencyResponseSystem()
        self.safety_monitor = SafetyMonitor()
        self.route_planner = HospitalRoutePlanner()

    def execute_delivery(self, destination: str, priority: int = 1):
        """
        Execute delivery with safety protocols
        """
        # Pre-execution safety check
        if not self._pre_execution_safety_check():
            raise RuntimeError("Safety check failed - cannot execute delivery")

        # Plan safe route considering hospital traffic patterns
        route = self.route_planner.plan_route_to(destination, priority)

        # Execute with continuous monitoring
        for waypoint in route:
            if not self._safe_to_proceed(waypoint):
                self.emergency_system.trigger_emergency_response(
                    EmergencyLevel.WARNING,
                    "Unsafe conditions detected"
                )
                continue

            # Move to waypoint with safety monitoring
            self._move_to_waypoint_safely(waypoint)

            # Check system health
            violations = self.safety_monitor.monitor_system_health()
            if violations:
                self.emergency_system.trigger_emergency_response(
                    EmergencyLevel.WARNING,
                    f"Performance violations: {violations}"
                )

    def _pre_execution_safety_check(self):
        """
        Perform comprehensive safety check before execution
        """
        # Check robot systems
        system_status = self._check_robot_systems()
        if not system_status['all_systems_nominal']:
            return False

        # Check environmental conditions
        env_status = self._check_environmental_conditions()
        if not env_status['environment_safe']:
            return False

        # Verify mission parameters
        mission_status = self._validate_mission_parameters()
        if not mission_status['mission_valid']:
            return False

        return True
```

## Conclusion

Real-world robot deployment requires a comprehensive approach to safety that encompasses hardware design, software implementation, operational procedures, and ongoing maintenance. By following established safety standards, conducting thorough risk assessments, and implementing robust safety systems, robots can operate safely and effectively in diverse real-world environments.

The key to successful deployment lies in recognizing that safety is not a one-time consideration but an ongoing process that must be integrated into every aspect of the robot system lifecycle. From initial design through deployment, operation, and maintenance, safety considerations must guide every decision and implementation.

Future developments in robotics safety will likely focus on improving human-robot collaboration, enhancing autonomous decision-making capabilities while maintaining safety, and developing standardized safety frameworks for increasingly complex robotic systems.

## Lab Exercise: Implementing Safety-Rated Control System

### Objective
Implement a safety-rated control system for a mobile robot that monitors environmental conditions and adjusts robot behavior to maintain safe operation.

### Requirements
1. Implement a safety monitor that continuously assesses system health
2. Create an emergency response system that handles different levels of emergencies
3. Develop a safe path execution system that incorporates safety checks
4. Integrate with ROS 2 for real-time monitoring and control

### Implementation Steps

1. Create the safety monitor node:
```bash
ros2 run your_package safety_monitor_node
```

2. Implement the emergency response handler:
```bash
ros2 run your_package emergency_handler_node
```

3. Test the integrated system with various safety scenarios

### Expected Outcomes
Students will understand how to implement comprehensive safety systems for real-world robot deployment and gain practical experience with safety-rated control systems.

============================================================
FILE: book\docs\part-v-integration\chapter-15-future\index.md
============================================================
---
sidebar_position: 15
title: "Chapter 15: Advanced Topics and Future Directions"
---

# Chapter 15: Advanced Topics and Future Directions

## Learning Goals

By the end of this chapter, students will be able to:
- Implement advanced algorithms for next-generation robotics systems
- Analyze ethical considerations in robotics and AI deployment
- Design robotic systems for emerging technologies and applications
- Evaluate the impact of robotics on society and human-robot relationships
- Understand cutting-edge research directions in physical AI and humanoid robotics

## Key Technologies
- Neuromorphic computing for robotics
- Quantum-enhanced algorithms for robot planning
- Advanced machine learning techniques (meta-learning, few-shot learning)
- Brain-computer interfaces for robot control
- Soft robotics and bio-inspired systems
- Digital twins for robot simulation and deployment

## Introduction

The field of robotics stands at an inflection point, with emerging technologies poised to revolutionize how robots perceive, learn, and interact with the world. This chapter explores cutting-edge research areas and future directions that promise to transform robotics from current capabilities to next-generation systems with unprecedented autonomy, adaptability, and intelligence.

As we look toward the future, robotics is evolving beyond traditional pre-programmed behaviors toward systems that can learn, adapt, and collaborate in ways that were previously the realm of science fiction. These advances are driven by breakthroughs in artificial intelligence, materials science, neuroscience, and human-computer interaction, creating opportunities for robots to operate in increasingly complex and unstructured environments.

The convergence of multiple technologies is enabling new possibilities: neuromorphic processors that mimic neural architectures for efficient real-time processing, quantum algorithms that promise to solve complex optimization problems, and bio-inspired materials that enable soft, adaptable robots. These developments are not just incremental improvements but represent paradigm shifts in how we design, build, and deploy robotic systems.

## Advanced Machine Learning for Robotics

### Meta-Learning and Few-Shot Learning

Traditional machine learning approaches in robotics require extensive training on large datasets specific to each task. Meta-learning, or "learning to learn," enables robots to rapidly adapt to new tasks with minimal training data by leveraging knowledge from previous experiences.

```python
import torch
import torch.nn as nn
import numpy as np
from typing import List, Tuple, Dict
import copy

class MetaLearner(nn.Module):
    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int, meta_lr: float = 0.001):
        super(MetaLearner, self).__init__()
        self.network = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, output_dim)
        )
        self.meta_lr = meta_lr
        self.meta_optimizer = torch.optim.Adam(self.parameters(), lr=meta_lr)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.network(x)

    def adapt(self, support_set: Tuple[torch.Tensor, torch.Tensor],
              adaptation_lr: float = 0.01) -> nn.Module:
        """
        Adapt the model to a new task using a support set
        """
        adapted_model = copy.deepcopy(self.network)
        optimizer = torch.optim.SGD(adapted_model.parameters(), lr=adaptation_lr)

        x_support, y_support = support_set
        for _ in range(5):  # Few adaptation steps
            pred = adapted_model(x_support)
            loss = nn.MSELoss()(pred, y_support)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

        return adapted_model

    def meta_update(self, task_batch: List[Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]]):
        """
        Update meta-learner based on batch of tasks
        Each task is (x_support, y_support, x_query, y_query)
        """
        meta_loss = 0.0

        for x_support, y_support, x_query, y_query in task_batch:
            # Adapt to task
            adapted_model = self.adapt((x_support, y_support))

            # Evaluate on query set
            with torch.no_grad():
                query_pred = adapted_model(x_query)
                task_loss = nn.MSELoss()(query_pred, y_query)
                meta_loss += task_loss

        meta_loss /= len(task_batch)

        self.meta_optimizer.zero_grad()
        meta_loss.backward()
        self.meta_optimizer.step()

        return meta_loss.item()

# Example usage for robotic manipulation
class RoboticMetaLearner:
    def __init__(self):
        self.meta_learner = MetaLearner(input_dim=12, hidden_dim=64, output_dim=6)  # 6-DOF control
        self.task_buffer = []

    def learn_new_task(self, demonstration_data: List[Dict]):
        """
        Learn a new manipulation task from few demonstrations
        """
        # Convert demonstration to support/query format
        support_x, support_y = [], []
        query_x, query_y = [], []

        for demo in demonstration_data:
            if len(support_x) < 3:  # Use first 3 as support
                support_x.append(demo['state'])
                support_y.append(demo['action'])
            else:
                query_x.append(demo['state'])
                query_y.append(demo['action'])

        support_set = (torch.stack(support_x), torch.stack(support_y))
        query_set = (torch.stack(query_x), torch.stack(query_y))

        return self.meta_learner.adapt(support_set)
```

### Continual Learning and Catastrophic Forgetting

One of the biggest challenges in deploying lifelong learning robots is catastrophic forgetting‚Äîthe tendency of neural networks to forget previously learned tasks when learning new ones. Continual learning approaches address this challenge:

```python
class ContinualLearner(nn.Module):
    def __init__(self, input_dim: int, output_dim: int, num_tasks: int):
        super(ContinualLearner, self).__init__()
        self.shared_backbone = nn.Sequential(
            nn.Linear(input_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 256),
            nn.ReLU()
        )

        # Task-specific output heads
        self.task_heads = nn.ModuleList([
            nn.Linear(256, output_dim) for _ in range(num_tasks)
        ])

        self.task_id = 0  # Current task
        self.prev_params = {}  # For regularization
        self.ewc_lambda = 1000  # Elastic Weight Consolidation strength

    def forward(self, x: torch.Tensor, task_id: int = 0) -> torch.Tensor:
        features = self.shared_backbone(x)
        output = self.task_heads[task_id](features)
        return output

    def compute_ewc_loss(self) -> torch.Tensor:
        """
        Compute Elastic Weight Consolidation loss to prevent forgetting
        """
        ewc_loss = 0.0
        for name, param in self.named_parameters():
            if name in self.prev_params:
                prev_param = self.prev_params[name]
                fisher_info = torch.diag(self.fisher_information[name]) if hasattr(self, 'fisher_information') else torch.ones_like(param)
                ewc_loss += (fisher_info * (param - prev_param) ** 2).sum()
        return self.ewc_lambda * ewc_loss

    def update_fisher_information(self, dataloader):
        """
        Update Fisher Information Matrix for EWC
        """
        self.zero_grad()
        log_likelihoods = []

        for batch in dataloader:
            x, y = batch
            output = self(x, self.task_id)
            log_likelihood = torch.log_softmax(output, dim=1).gather(1, y.unsqueeze(1)).mean()
            log_likelihoods.append(log_likelihood)

        avg_log_likelihood = torch.stack(log_likelihoods).mean()
        avg_log_likelihood.backward(retain_graph=True)

        self.fisher_information = {}
        for name, param in self.named_parameters():
            self.fisher_information[name] = param.grad.data ** 2
```

### Foundation Models for Robotics

Large-scale foundation models trained on diverse datasets are beginning to show promise for robotics applications, providing general-purpose representations that can be adapted to various robotic tasks:

```python
class RoboticFoundationModel:
    def __init__(self, vision_model, language_model, action_model):
        self.vision_model = vision_model  # Pre-trained vision transformer
        self.language_model = language_model  # Pre-trained language model
        self.action_model = action_model  # Action generation network
        self.fusion_layer = nn.Linear(1024, 512)  # Fuse modalities

    def forward(self, image: torch.Tensor, instruction: str) -> torch.Tensor:
        """
        Generate robot actions from visual input and natural language instruction
        """
        # Extract visual features
        visual_features = self.vision_model(image)

        # Extract language features
        language_features = self.language_model.encode(instruction)

        # Fuse modalities
        combined_features = torch.cat([visual_features, language_features], dim=-1)
        fused_features = self.fusion_layer(combined_features)

        # Generate action
        action = self.action_model(fused_features)
        return action

    def execute_instruction(self, image: torch.Tensor, instruction: str) -> Dict:
        """
        Execute a natural language instruction in the robot's environment
        """
        action = self.forward(image, instruction)

        # Convert to robot command
        robot_command = self._convert_to_robot_command(action)

        return {
            'command': robot_command,
            'confidence': self._estimate_confidence(action),
            'explanation': self._generate_explanation(instruction, action)
        }

    def _convert_to_robot_command(self, action: torch.Tensor) -> Dict:
        """
        Convert neural network output to robot command
        """
        # Convert action tensor to robot joint positions, velocities, or forces
        joint_positions = action[:6].detach().numpy()  # For 6-DOF arm
        gripper_action = action[6].item()  # Gripper command

        return {
            'joint_positions': joint_positions,
            'gripper': 'close' if gripper_action > 0.5 else 'open',
            'duration': 2.0  # seconds
        }
```

## Neuromorphic and Quantum Computing for Robotics

### Neuromorphic Computing

Neuromorphic computing architectures mimic the neural structure of biological brains, offering potential advantages for real-time robotic processing:

```python
import numpy as np
from typing import List, Tuple

class SpikingNeuralNetwork:
    def __init__(self, layers: List[int], time_steps: int = 100):
        self.layers = layers
        self.time_steps = time_steps
        self.weights = []
        self.biases = []

        # Initialize weights for spiking neural network
        for i in range(len(layers) - 1):
            w = np.random.normal(0, np.sqrt(2.0 / layers[i]), (layers[i+1], layers[i]))
            b = np.zeros(layers[i+1])
            self.weights.append(w)
            self.biases.append(b)

    def lif_neuron(self, inputs: np.ndarray, weights: np.ndarray, bias: np.ndarray,
                   membrane_potential: np.ndarray, spike_threshold: float = 1.0) -> Tuple[np.ndarray, np.ndarray]:
        """
        Leaky Integrate-and-Fire neuron model
        """
        # Update membrane potential
        membrane_potential += np.dot(weights, inputs) + bias

        # Generate spikes where potential exceeds threshold
        spikes = (membrane_potential >= spike_threshold).astype(float)

        # Reset membrane potential where spikes occurred
        membrane_potential = np.where(spikes > 0, 0.0, membrane_potential * 0.9)  # Leak factor

        return spikes, membrane_potential

    def forward(self, input_sequence: List[np.ndarray]) -> List[np.ndarray]:
        """
        Process input sequence through spiking neural network
        """
        layer_potentials = [np.zeros(layer_size) for layer_size in self.layers]
        output_sequence = []

        for t, inputs in enumerate(input_sequence):
            current_layer_input = inputs

            for layer_idx in range(len(self.layers) - 1):
                spikes, new_potential = self.lif_neuron(
                    current_layer_input,
                    self.weights[layer_idx],
                    self.biases[layer_idx],
                    layer_potentials[layer_idx]
                )
                layer_potentials[layer_idx] = new_potential
                current_layer_input = spikes

            output_sequence.append(current_layer_input)

        return output_sequence

class NeuromorphicRobotController:
    def __init__(self):
        # SNN for perception (3 layers: input, hidden, output)
        self.perception_snn = SpikingNeuralNetwork([64*64*3, 512, 128])  # Visual input -> features
        # SNN for action selection
        self.action_snn = SpikingNeuralNetwork([128, 256, 6])  # Features -> actions

    def process_sensor_data(self, visual_input: np.ndarray) -> np.ndarray:
        """
        Process visual input using neuromorphic SNN
        """
        # Convert visual input to spike train
        spike_train = self._image_to_spike_train(visual_input)

        # Process through perception network
        perception_output = self.perception_snn.forward(spike_train)

        # Return last output (time-averaged)
        return perception_output[-1]

    def _image_to_spike_train(self, image: np.ndarray) -> List[np.ndarray]:
        """
        Convert image to spike train representation
        """
        # Simple rate coding: brighter pixels fire more frequently
        normalized_image = image.astype(float) / 255.0
        spike_probability = normalized_image.flatten()

        # Generate spike train for time steps
        spike_train = []
        for _ in range(self.perception_snn.time_steps):
            spikes = (np.random.random(len(spike_probability)) < spike_probability).astype(float)
            spike_train.append(spikes)

        return spike_train
```

### Quantum-Enhanced Algorithms

While still in early stages, quantum computing shows promise for specific robotics applications like optimization and planning:

```python
from typing import Dict, List
import numpy as np

class QuantumOptimizationRobot:
    def __init__(self):
        self.qubo_solver = None  # Placeholder for quantum solver
        self.classical_fallback = True

    def solve_path_planning_qubo(self, environment_map: np.ndarray,
                                start: Tuple[int, int], goal: Tuple[int, int]) -> List[Tuple[int, int]]:
        """
        Formulate path planning as Quadratic Unconstrained Binary Optimization (QUBO)
        and solve using quantum or classical methods
        """
        # Convert grid to QUBO formulation
        qubo_matrix = self._create_path_planning_qubo(environment_map, start, goal)

        if self.classical_fallback:
            # Use classical optimization as fallback
            return self._solve_classical_path_planning(qubo_matrix, environment_map.shape, start, goal)
        else:
            # Use quantum solver (simulated here)
            solution = self._quantum_solve(qubo_matrix)
            return self._decode_path_solution(solution, environment_map.shape)

    def _create_path_planning_qubo(self, env_map: np.ndarray, start: Tuple[int, int],
                                  goal: Tuple[int, int]) -> np.ndarray:
        """
        Create QUBO matrix for path planning problem
        """
        height, width = env_map.shape
        n_nodes = height * width

        # Initialize QUBO matrix
        Q = np.zeros((n_nodes, n_nodes))

        # Add obstacle penalties
        for i in range(height):
            for j in range(width):
                if env_map[i, j] == 1:  # Obstacle
                    node_idx = i * width + j
                    Q[node_idx, node_idx] = 1000  # High penalty for obstacles

        # Add connectivity constraints (adjacent nodes)
        for i in range(height):
            for j in range(width):
                current_idx = i * width + j
                if env_map[i, j] == 0:  # Free space
                    # Add connections to adjacent cells
                    neighbors = self._get_neighbors(i, j, height, width)
                    for ni, nj in neighbors:
                        if env_map[ni, nj] == 0:  # Both free
                            neighbor_idx = ni * width + nj
                            Q[current_idx, neighbor_idx] = -1  # Encourage connections

        # Add start and goal constraints
        start_idx = start[0] * width + start[1]
        goal_idx = goal[0] * width + goal[1]
        Q[start_idx, start_idx] -= 100  # Encourage starting at start
        Q[goal_idx, goal_idx] -= 100    # Encourage ending at goal

        return Q

    def _get_neighbors(self, i: int, j: int, height: int, width: int) -> List[Tuple[int, int]]:
        """
        Get valid neighboring cells
        """
        neighbors = []
        for di, dj in [(-1,0), (1,0), (0,-1), (0,1)]:  # 4-connectivity
            ni, nj = i + di, j + dj
            if 0 <= ni < height and 0 <= nj < width:
                neighbors.append((ni, nj))
        return neighbors

    def _solve_classical_path_planning(self, qubo: np.ndarray, shape: Tuple[int, int],
                                      start: Tuple[int, int], goal: Tuple[int, int]) -> List[Tuple[int, int]]:
        """
        Classical fallback for path planning
        """
        # Use A* or Dijkstra as fallback
        from queue import PriorityQueue

        height, width = shape
        pq = PriorityQueue()
        pq.put((0, start))
        came_from = {start: None}
        cost_so_far = {start: 0}

        while not pq.empty():
            _, current = pq.get()

            if current == goal:
                break

            i, j = current
            for ni, nj in self._get_neighbors(i, j, height, width):
                new_cost = cost_so_far[current] + 1  # Uniform cost

                if (ni, nj) not in cost_so_far or new_cost < cost_so_far[(ni, nj)]:
                    cost_so_far[(ni, nj)] = new_cost
                    priority = new_cost + self._heuristic((ni, nj), goal)
                    pq.put((priority, (ni, nj)))
                    came_from[(ni, nj)] = current

        # Reconstruct path
        path = []
        current = goal
        while current != start:
            path.append(current)
            current = came_from[current]
        path.append(start)
        path.reverse()

        return path

    def _heuristic(self, a: Tuple[int, int], b: Tuple[int, int]) -> float:
        """
        Heuristic function for A*
        """
        return abs(a[0] - b[0]) + abs(a[1] - b[1])
```

## Bio-Inspired and Soft Robotics

### Soft Robotics Materials and Actuation

Soft robotics uses compliant materials and novel actuation methods to create robots that can safely interact with humans and adapt to complex environments:

```python
import numpy as np
from scipy import interpolate
from typing import Tuple, List

class SoftRobotArm:
    def __init__(self, segments: int = 5, max_curvature: float = 0.5):
        self.segments = segments
        self.max_curvature = max_curvature
        self.segment_lengths = np.ones(segments) * 0.1  # 10cm per segment
        self.pressure_channels = np.zeros(segments * 2)  # 2 channels per segment (left/right)

    def forward_kinematics(self, curvature_profile: np.ndarray) -> List[Tuple[float, float, float]]:
        """
        Compute end-effector positions given curvature profile
        """
        positions = [(0.0, 0.0, 0.0)]  # Start at origin

        for i, kappa in enumerate(curvature_profile):
            if abs(kappa) < 1e-6:  # Straight segment
                x, y, theta = positions[-1]
                new_x = x + self.segment_lengths[i] * np.cos(theta)
                new_y = y + self.segment_lengths[i] * np.sin(theta)
                positions.append((new_x, new_y, theta))
            else:  # Curved segment
                x, y, theta = positions[-1]
                radius = 1.0 / kappa
                arc_length = self.segment_lengths[i]
                angle_change = arc_length / radius

                # Center of curvature
                center_x = x - radius * np.sin(theta)
                center_y = y + radius * np.cos(theta)

                new_theta = theta + angle_change
                new_x = center_x + radius * np.sin(new_theta)
                new_y = center_y - radius * np.cos(new_theta)

                positions.append((new_x, new_y, new_theta))

        return positions

    def inverse_kinematics(self, target_pos: Tuple[float, float, float],
                          current_pos: List[Tuple[float, float, float]] = None) -> np.ndarray:
        """
        Compute curvature profile to reach target position
        """
        if current_pos is None:
            current_pos = [(0.0, 0.0, 0.0)] * (self.segments + 1)

        # Simplified inverse kinematics using gradient descent
        curvature = np.zeros(self.segments)

        for iteration in range(100):
            current_end_pos = self.forward_kinematics(curvature)[-1]

            # Compute error
            pos_error = np.array(target_pos[:2]) - np.array(current_end_pos[:2])
            angle_error = target_pos[2] - current_end_pos[2]

            if np.linalg.norm(pos_error) < 0.01 and abs(angle_error) < 0.01:
                break

            # Gradient-based update
            gradient = self._compute_jacobian(curvature)
            delta_curvature = np.dot(gradient.T, np.concatenate([pos_error, [angle_error]]))
            curvature -= 0.01 * delta_curvature  # Learning rate

            # Apply constraints
            curvature = np.clip(curvature, -self.max_curvature, self.max_curvature)

        return curvature

    def _compute_jacobian(self, curvature: np.ndarray) -> np.ndarray:
        """
        Compute Jacobian matrix for the soft robot
        """
        epsilon = 1e-6
        jacobian = np.zeros((3, self.segments))  # 3 DOF (x, y, theta) x n segments

        for i in range(self.segments):
            # Perturb curvature
            curvature_plus = curvature.copy()
            curvature_plus[i] += epsilon
            pos_plus = self.forward_kinematics(curvature_plus)[-1]

            curvature_minus = curvature.copy()
            curvature_minus[i] -= epsilon
            pos_minus = self.forward_kinematics(curvature_minus)[-1]

            # Compute finite difference
            dx = (pos_plus[0] - pos_minus[0]) / (2 * epsilon)
            dy = (pos_plus[1] - pos_minus[1]) / (2 * epsilon)
            dtheta = (pos_plus[2] - pos_minus[2]) / (2 * epsilon)

            jacobian[:, i] = [dx, dy, dtheta]

        return jacobian

class PneumaticNetworkActuator:
    def __init__(self, channels: int = 8):
        self.channels = channels
        self.pressures = np.zeros(channels)
        self.max_pressure = 100  # kPa
        self.actuator_length = 0.1  # 10cm

    def control_pressure(self, target_pressures: np.ndarray):
        """
        Control pressure in pneumatic network
        """
        self.pressures = np.clip(target_pressures, 0, self.max_pressure)

    def compute_deformation(self) -> Tuple[float, float]:
        """
        Compute deformation based on pressure distribution
        """
        # Simplified model: deformation proportional to pressure differences
        left_pressure = np.mean(self.pressures[:self.channels//2])
        right_pressure = np.mean(self.pressures[self.channels//2:])

        # Compute bending angle based on pressure difference
        pressure_diff = left_pressure - right_pressure
        max_diff = self.max_pressure

        # Nonlinear response
        bending_angle = (pressure_diff / max_diff) * (np.pi / 3)  # Max 60 degrees

        # Compute elongation based on average pressure
        avg_pressure = np.mean(self.pressures)
        elongation = (avg_pressure / self.max_pressure) * 0.02  # Max 2cm extension

        return bending_angle, elongation
```

## Human-Robot Interaction: Theory of Mind and Social Cognition

### Theory of Mind Systems

Advanced HRI systems incorporate Theory of Mind capabilities to understand and predict human intentions:

```python
import numpy as np
from dataclasses import dataclass
from typing import Dict, List, Optional
import json

@dataclass
class HumanState:
    position: Tuple[float, float]
    velocity: Tuple[float, float]
    goal: Optional[Tuple[float, float]]
    intentions: List[str]
    attention: float  # 0-1, where 1 is focused attention
    emotional_state: str  # happy, neutral, frustrated, etc.

@dataclass
class RobotBelief:
    human_beliefs: Dict[str, any]  # What the human believes about the world
    human_goals: List[str]  # What the human wants to achieve
    human_intentions: List[str]  # What the human plans to do
    human_capabilities: Dict[str, float]  # What the human can do
    uncertainty: float  # Uncertainty in beliefs

class TheoryOfMindSystem:
    def __init__(self):
        self.human_models = {}  # Track multiple humans
        self.robot_model = RobotBelief({}, [], [], {}, 0.0)
        self.belief_update_rate = 10  # Hz

    def update_human_model(self, human_id: str, observation: Dict) -> RobotBelief:
        """
        Update robot's beliefs about a human's mental state
        """
        if human_id not in self.human_models:
            self.human_models[human_id] = RobotBelief({}, [], [], {}, 1.0)

        current_belief = self.human_models[human_id]

        # Update based on observation
        # This is a simplified model - in practice, much more complex inference would be needed

        # Update beliefs about human's beliefs
        if 'object_location' in observation:
            # If human observed object location, they now believe it's there
            current_belief.human_beliefs['object_location'] = observation['object_location']

        # Update beliefs about human's goals
        if 'human_action' in observation:
            action = observation['human_action']
            if action == 'reaching':
                # Likely trying to grasp something
                if 'target_object' in observation:
                    current_belief.human_goals.append(f"grasp_{observation['target_object']}")

        # Update beliefs about human's intentions
        if 'gaze_direction' in observation and 'target' in observation:
            # If human is looking at target, likely intending to interact
            current_belief.human_intentions.append(f"interact_with_{observation['target']}")

        # Update beliefs about human's capabilities
        if 'motion_speed' in observation:
            speed = observation['motion_speed']
            if speed > 0.5:  # High speed
                current_belief.human_capabilities['mobility'] = 1.0
            else:
                current_belief.human_capabilities['mobility'] = 0.3

        # Reduce uncertainty over time with good observations
        current_belief.uncertainty = max(0.1, current_belief.uncertainty * 0.95)

        return current_belief

    def predict_human_action(self, human_id: str) -> str:
        """
        Predict what the human will do next based on beliefs
        """
        if human_id not in self.human_models:
            return "unknown"

        belief = self.human_models[human_id]

        # Simple prediction based on goals and intentions
        if belief.human_goals:
            # If human has goals, predict action toward goal
            goal = belief.human_goals[0]
            if "grasp" in goal:
                return "reach_and_grasp"
            elif "move_to" in goal:
                return "navigate_to_location"

        if belief.human_intentions:
            # If human has intentions, predict corresponding action
            intention = belief.human_intentions[0]
            if "interact_with" in intention:
                return "approach_object"

        return "wait_and_observe"

    def plan_cooperative_action(self, human_id: str) -> Dict:
        """
        Plan robot action that considers human mental state
        """
        belief = self.human_models.get(human_id, RobotBelief({}, [], [], {}, 1.0))

        # Plan based on human's predicted actions and goals
        predicted_action = self.predict_human_action(human_id)

        if predicted_action == "reach_and_grasp":
            # Human wants to grasp something - robot could assist or get out of the way
            if belief.uncertainty < 0.3:  # High confidence in prediction
                return {
                    "action": "assist_grasp",
                    "confidence": 0.8,
                    "explanation": "Human appears to be reaching for object, offering assistance"
                }
            else:
                return {
                    "action": "wait_and_observe",
                    "confidence": 0.6,
                    "explanation": "Uncertain about human's intentions, observing"
                }

        elif predicted_action == "navigate_to_location":
            # Check if robot is blocking path
            return {
                "action": "clear_path",
                "confidence": 0.9,
                "explanation": "Human appears to be navigating, clearing potential path"
            }

        else:
            return {
                "action": "maintain_current_behavior",
                "confidence": 0.7,
                "explanation": "Human behavior unclear, maintaining current state"
            }

class SocialCognitionModule:
    def __init__(self):
        self.theory_of_mind = TheoryOfMindSystem()
        self.social_norms = self._load_social_norms()
        self.cultural_adaptation = True

    def _load_social_norms(self) -> Dict:
        """
        Load social norms and conventions for HRI
        """
        return {
            "personal_space": 0.8,  # meters
            "greeting_protocols": ["wave", "nod", "verbal_greeting"],
            "turn_taking": True,
            "attention_cues": ["eye_contact", "orientation", "gesture"],
            "help_offering": {
                "when": ["struggling", "looking_confused", "reaching_for_high_object"],
                "how": ["offer_assistance", "demonstrate", "provide_information"]
            }
        }

    def evaluate_social_situation(self, environment_state: Dict) -> Dict:
        """
        Evaluate social situation and recommend appropriate behavior
        """
        recommendations = []

        # Check for social norm violations
        for human_id, human_state in environment_state.get('humans', {}).items():
            robot_pos = environment_state.get('robot_position', (0, 0))
            human_pos = human_state.get('position', (0, 0))

            distance = np.sqrt((robot_pos[0] - human_pos[0])**2 + (robot_pos[1] - human_pos[1])**2)

            if distance < self.social_norms["personal_space"]:
                recommendations.append({
                    "action": "increase_distance",
                    "priority": "high",
                    "reason": "Invading personal space"
                })

        # Check for help opportunities
        for human_id, human_state in environment_state.get('humans', {}).items():
            if human_state.get('struggling', False):
                recommendations.append({
                    "action": "offer_assistance",
                    "priority": "high",
                    "reason": "Human appears to be struggling"
                })

        # Check for attention opportunities
        for human_id, human_state in environment_state.get('humans', {}).items():
            if human_state.get('attention_cue_detected', False):
                recommendations.append({
                    "action": "acknowledge_attention",
                    "priority": "medium",
                    "reason": "Human seeking attention"
                })

        return {
            "recommendations": recommendations,
            "social_norm_compliance": len([r for r in recommendations if r["priority"] == "high"]) == 0
        }
```

## Ethics and Societal Impact

### Ethical Frameworks for Robotics

As robots become more autonomous and integrated into society, ethical considerations become paramount:

```python
from enum import Enum
from dataclasses import dataclass
from typing import List, Dict, Any

class EthicalPrinciple(Enum):
    BENEFICENCE = "do_good"
    NON_MALEFICENCE = "do_no_harm"
    AUTONOMY = "respect_human_autonomy"
    JUSTICE = "fair_treatment"
    TRANSPARENCY = "explainable_behavior"
    ACCOUNTABILITY = "responsibility"

@dataclass
class EthicalDecision:
    action: str
    ethical_principles_violated: List[EthicalPrinciple]
    harm_potential: float  # 0-1 scale
    benefit_potential: float  # 0-1 scale
    explainability_score: float  # 0-1 scale
    alternative_actions: List[str]

class EthicalDecisionMaker:
    def __init__(self):
        self.principles = [
            EthicalPrinciple.BENEFICENCE,
            EthicalPrinciple.NON_MALEFICENCE,
            EthicalPrinciple.AUTONOMY,
            EthicalPrinciple.JUSTICE,
            EthicalPrinciple.TRANSPARENCY,
            EthicalPrinciple.ACCOUNTABILITY
        ]
        self.weights = {  # How much to weight each principle
            EthicalPrinciple.BENEFICENCE: 0.8,
            EthicalPrinciple.NON_MALEFICENCE: 1.0,  # Highest priority
            EthicalPrinciple.AUTONOMY: 0.7,
            EthicalPrinciple.JUSTICE: 0.6,
            EthicalPrinciple.TRANSPARENCY: 0.5,
            EthicalPrinciple.ACCOUNTABILITY: 0.4
        }

    def evaluate_action(self, action: str, context: Dict[str, Any]) -> EthicalDecision:
        """
        Evaluate an action against ethical principles
        """
        violations = []
        harm = 0.0
        benefit = 0.0

        # Check for harm
        if context.get('human_in_path', False) and action == 'move_forward':
            violations.append(EthicalPrinciple.NON_MALEFICENCE)
            harm = 0.9

        # Check for autonomy violation
        if context.get('human_expressed_opposition', False) and action == 'continue_despite_opposition':
            violations.append(EthicalPrinciple.AUTONOMY)
            harm = 0.7

        # Check for justice issues
        if context.get('discriminatory_context', False):
            violations.append(EthicalPrinciple.JUSTICE)
            harm = 0.6

        # Calculate benefit
        if context.get('helping_situation', False):
            benefit = 0.8

        # Calculate explainability
        explainability = self._calculate_explainability(action, context)

        # Generate alternatives
        alternatives = self._generate_alternatives(action, context)

        return EthicalDecision(
            action=action,
            ethical_principles_violated=violations,
            harm_potential=harm,
            benefit_potential=benefit,
            explainability_score=explainability,
            alternative_actions=alternatives
        )

    def _calculate_explainability(self, action: str, context: Dict[str, Any]) -> float:
        """
        Calculate how explainable an action is
        """
        # Actions that can be easily explained get higher scores
        explainable_actions = [
            'stop', 'wait', 'ask_for_permission',
            'provide_information', 'offer_assistance'
        ]

        if action in explainable_actions:
            return 0.9
        elif 'assist' in action or 'help' in action:
            return 0.7
        else:
            return 0.3

    def _generate_alternatives(self, action: str, context: Dict[str, Any]) -> List[str]:
        """
        Generate ethically preferable alternative actions
        """
        alternatives = []

        if action == 'move_forward' and context.get('human_in_path', False):
            alternatives.extend(['stop', 'wait_for_clear_path', 'ask_permission'])

        if action == 'continue_despite_opposition':
            alternatives.extend(['stop', 'ask_for_explanation', 'offer_choice'])

        if not alternatives:
            # General ethical alternatives
            alternatives.extend(['wait_and_assess', 'seek_human_input', 'choose_safest_option'])

        return alternatives

    def make_ethical_decision(self, possible_actions: List[str], context: Dict[str, Any]) -> str:
        """
        Choose the most ethical action from possibilities
        """
        decisions = []

        for action in possible_actions:
            decision = self.evaluate_action(action, context)
            decisions.append(decision)

        # Score each decision
        scored_decisions = []
        for decision in decisions:
            # Lower score is better (less ethical violation)
            score = (decision.harm_potential * 2.0 -
                    decision.benefit_potential * 1.0 +
                    (1.0 - decision.explainability_score) * 0.5)

            # Penalty for violating core principles
            for violation in decision.ethical_principles_violated:
                if violation in [EthicalPrinciple.NON_MALEFICENCE, EthicalPrinciple.AUTONOMY]:
                    score += 1.0  # Heavy penalty

            scored_decisions.append((decision, score))

        # Choose action with lowest score (most ethical)
        best_decision = min(scored_decisions, key=lambda x: x[1])
        return best_decision[0].action

class EthicalComplianceMonitor:
    def __init__(self):
        self.ethical_decision_maker = EthicalDecisionMaker()
        self.compliance_log = []
        self.ethical_violations = []

    def monitor_robot_behavior(self, robot_action: str, environment_context: Dict[str, Any]) -> Dict:
        """
        Monitor robot behavior for ethical compliance
        """
        ethical_decision = self.ethical_decision_maker.evaluate_action(
            robot_action, environment_context
        )

        # Log the decision
        self.compliance_log.append({
            'timestamp': time.time(),
            'action': robot_action,
            'ethical_evaluation': ethical_decision,
            'context': environment_context
        })

        # Check for violations
        if ethical_decision.ethical_principles_violated:
            self.ethical_violations.append({
                'timestamp': time.time(),
                'action': robot_action,
                'violations': ethical_decision.ethical_principles_violated,
                'harm_potential': ethical_decision.harm_potential,
                'context': environment_context
            })

        return {
            'action_approved': len(ethical_decision.ethical_principles_violated) == 0,
            'violations': ethical_decision.ethical_principles_violated,
            'suggested_alternatives': ethical_decision.alternative_actions,
            'compliance_score': 1.0 - ethical_decision.harm_potential
        }
```

## Digital Twins and Simulation-to-Reality Transfer

### Digital Twin Architecture

Digital twins enable comprehensive simulation and testing before real-world deployment:

```python
import numpy as np
from typing import Dict, Any, Tuple
import asyncio
import time

class RobotDigitalTwin:
    def __init__(self, robot_spec: Dict[str, Any]):
        self.spec = robot_spec
        self.state = self._initialize_state()
        self.sensors = self._initialize_sensors()
        self.actuators = self._initialize_actuators()
        self.physical_model = self._create_physical_model()
        self.simulation_time = 0.0
        self.real_world_offset = 0.0  # Time offset from real world

    def _initialize_state(self) -> Dict[str, Any]:
        """
        Initialize digital twin state based on robot specification
        """
        return {
            'position': np.array([0.0, 0.0, 0.0]),
            'orientation': np.array([0.0, 0.0, 0.0, 1.0]),  # Quaternion
            'velocity': np.array([0.0, 0.0, 0.0]),
            'angular_velocity': np.array([0.0, 0.0, 0.0]),
            'joint_positions': np.zeros(self.spec['num_joints']),
            'joint_velocities': np.zeros(self.spec['num_joints']),
            'battery_level': 100.0,
            'temperature': 25.0,  # Celsius
            'internal_clock': time.time()
        }

    def _initialize_sensors(self) -> Dict[str, Any]:
        """
        Initialize sensor models with realistic noise and latency
        """
        return {
            'camera': {
                'resolution': self.spec['camera_resolution'],
                'fov': self.spec['camera_fov'],
                'noise_model': self._create_noise_model('gaussian', 0.01),
                'latency': 0.05  # 50ms
            },
            'lidar': {
                'range': self.spec['lidar_range'],
                'resolution': self.spec['lidar_resolution'],
                'noise_model': self._create_noise_model('gaussian', 0.05),
                'latency': 0.02  # 20ms
            },
            'imu': {
                'accelerometer_noise': 0.001,
                'gyro_noise': 0.0001,
                'latency': 0.001  # 1ms
            },
            'force_torque': {
                'noise': 0.1,  # Newtons
                'latency': 0.005  # 5ms
            }
        }

    def _create_noise_model(self, model_type: str, std_dev: float):
        """
        Create noise model for sensors
        """
        def add_noise(value):
            if model_type == 'gaussian':
                return value + np.random.normal(0, std_dev, size=value.shape if hasattr(value, 'shape') else None)
            return value
        return add_noise

    def update_from_real_world(self, real_sensor_data: Dict[str, Any],
                              timestamp: float) -> Dict[str, Any]:
        """
        Update digital twin with real-world sensor data
        """
        # Apply sensor fusion to update state
        self.state['position'] = self._fuse_position_data(
            real_sensor_data.get('position', self.state['position']),
            real_sensor_data.get('imu', {}),
            alpha=0.1  # Low-pass filter coefficient
        )

        # Update other state variables
        for key in ['orientation', 'velocity', 'joint_positions']:
            if key in real_sensor_data:
                # Apply noise model to simulate sensor imperfections
                noisy_data = self.sensors.get(key, {}).get('noise_model', lambda x: x)(real_sensor_data[key])
                self.state[key] = noisy_data

        # Update simulation time to match real world
        self.simulation_time = timestamp
        self.real_world_offset = time.time() - timestamp

        return self.state

    def predict_behavior(self, control_commands: Dict[str, Any],
                        time_horizon: float = 1.0) -> List[Dict[str, Any]]:
        """
        Predict robot behavior given control commands
        """
        predictions = []
        current_state = self.state.copy()
        dt = 0.01  # 10ms simulation steps

        for t in np.arange(0, time_horizon, dt):
            # Apply control commands to physics model
            next_state = self._apply_physics_step(current_state, control_commands, dt)

            # Update sensor models with predicted data
            predicted_sensors = self._predict_sensor_data(next_state)

            predictions.append({
                'time': self.simulation_time + t,
                'state': next_state,
                'predicted_sensors': predicted_sensors
            })

            current_state = next_state

        return predictions

    def _apply_physics_step(self, state: Dict, commands: Dict, dt: float) -> Dict:
        """
        Apply one physics simulation step
        """
        new_state = state.copy()

        # Update joint positions based on commands
        if 'joint_commands' in commands:
            new_state['joint_velocities'] = commands['joint_commands']
            new_state['joint_positions'] += new_state['joint_velocities'] * dt

        # Update base position based on wheel commands or legged locomotion
        if 'base_velocity' in commands:
            linear_vel = commands['base_velocity'][:3]
            angular_vel = commands['base_velocity'][3:]

            new_state['velocity'] = linear_vel
            new_state['angular_velocity'] = angular_vel

            new_state['position'] += linear_vel * dt

            # Update orientation with angular velocity
            orientation_quat = new_state['orientation']
            new_orientation = self._integrate_angular_velocity(
                orientation_quat, angular_vel, dt
            )
            new_state['orientation'] = new_orientation

        # Apply physics constraints and dynamics
        new_state = self._apply_dynamics_constraints(new_state, dt)

        return new_state

    def _apply_dynamics_constraints(self, state: Dict, dt: float) -> Dict:
        """
        Apply physical constraints and dynamics to state
        """
        # Apply gravity, friction, etc.
        # This would include more detailed physics simulation

        # Update battery level based on activity
        activity_level = np.mean(np.abs(state['joint_velocities']))
        battery_drain = activity_level * 0.01 * dt  # Simplified model
        state['battery_level'] = max(0.0, state['battery_level'] - battery_drain)

        # Update temperature based on activity
        state['temperature'] += activity_level * 0.1 * dt
        state['temperature'] = min(80.0, state['temperature'])  # Max operating temp

        return state

    def validate_control_policy(self, policy, environment: Any) -> Dict[str, float]:
        """
        Validate a control policy in simulation before real-world deployment
        """
        success_count = 0
        total_trials = 100
        safety_violations = 0
        performance_metrics = []

        for trial in range(total_trials):
            # Reset simulation
            self.state = self._initialize_state()

            # Run policy in simulation
            trial_result = self._run_policy_trial(policy, environment)

            if trial_result['success']:
                success_count += 1
                performance_metrics.append(trial_result['performance'])

            if trial_result['safety_violation']:
                safety_violations += 1

        return {
            'success_rate': success_count / total_trials,
            'safety_violation_rate': safety_violations / total_trials,
            'avg_performance': np.mean(performance_metrics) if performance_metrics else 0.0,
            'std_performance': np.std(performance_metrics) if performance_metrics else 0.0
        }

    def _run_policy_trial(self, policy, environment) -> Dict[str, Any]:
        """
        Run a single policy trial in simulation
        """
        # Implementation would run the policy for a task
        # and return success/failure and performance metrics
        pass

class SimulationToRealityTransfer:
    def __init__(self):
        self.domain_randomization = True
        self.system_identification = True
        self.adaptive_control = True

    def prepare_for_real_world(self, simulation_policy, robot_digital_twin: RobotDigitalTwin):
        """
        Prepare simulation-trained policy for real-world deployment
        """
        # Apply domain randomization during training simulation
        if self.domain_randomization:
            randomized_params = self._randomize_simulation_parameters()
            robot_digital_twin.physical_model.update(randomized_params)

        # System identification to match simulation to reality
        if self.system_identification:
            system_params = self._identify_system_parameters(robot_digital_twin)
            robot_digital_twin.physical_model.calibrate(system_params)

        # Adaptive control to handle reality gaps
        if self.adaptive_control:
            adaptive_policy = self._create_adaptive_policy(simulation_policy)
            return adaptive_policy

        return simulation_policy

    def _randomize_simulation_parameters(self) -> Dict[str, Any]:
        """
        Randomize simulation parameters to improve transfer
        """
        return {
            'friction_coefficients': np.random.uniform(0.1, 0.9, size=10),
            'mass_variations': np.random.uniform(0.95, 1.05, size=5),  # ¬±5% mass variation
            'sensor_noise': np.random.uniform(0.001, 0.01, size=5),
            'actuator_dynamics': np.random.uniform(0.8, 1.2, size=5)  # Response time variation
        }

    def _identify_system_parameters(self, robot_digital_twin: RobotDigitalTwin) -> Dict[str, Any]:
        """
        Identify real-world system parameters through excitation
        """
        # This would involve running specific excitation maneuvers
        # and identifying parameters from input-output data
        pass

    def _create_adaptive_policy(self, base_policy) -> Any:
        """
        Create adaptive version of policy that can adjust to reality gaps
        """
        # Implementation would wrap base policy with adaptation mechanisms
        pass
```

## Future Research Directions

### Open Challenges and Opportunities

The field of robotics faces several significant challenges that represent opportunities for future research:

1. **Generalization**: Creating robots that can generalize across diverse tasks and environments without extensive retraining
2. **Common Sense Reasoning**: Endowing robots with human-like common sense understanding of the physical world
3. **Long-term Autonomy**: Developing systems that can operate reliably for months or years without human intervention
4. **Human-Robot Collaboration**: Creating truly collaborative robots that can work seamlessly with humans
5. **Ethical AI**: Ensuring that autonomous robots make ethically sound decisions

### Emerging Technologies

Several emerging technologies are likely to shape the future of robotics:

- **Edge AI**: Bringing advanced AI capabilities to resource-constrained robotic platforms
- **5G/6G Communication**: Enabling real-time coordination of robot swarms and remote operation
- **Advanced Materials**: New materials that enable better actuation, sensing, and human-robot interaction
- **Brain-Computer Interfaces**: Direct neural interfaces for robot control and feedback
- **Swarm Intelligence**: Coordinated behavior of large numbers of simple robots

## Conclusion

The future of robotics is incredibly promising, with advances in AI, materials science, neuroscience, and human-computer interaction converging to create increasingly capable and intelligent robotic systems. The robots of tomorrow will not just be tools that execute pre-programmed tasks, but intelligent agents that can learn, adapt, and collaborate with humans in unprecedented ways.

However, with these advances come significant responsibilities. As robots become more autonomous and integrated into society, we must ensure that they are developed and deployed ethically, safely, and in ways that benefit humanity as a whole. This requires not just technical excellence, but also thoughtful consideration of the societal implications of robotic technology.

The journey from current capabilities to the robots of the future will require continued innovation across multiple disciplines, from fundamental research in AI and robotics to applied work in human factors and social acceptance. The challenges are significant, but so are the opportunities to create a future where robots enhance human capabilities and improve quality of life.

As we stand at this exciting frontier, the next generation of roboticists will have the opportunity to shape not just the technology, but the society in which it operates. The work begun today in laboratories and classrooms around the world will determine whether the future of robotics is one of human-robot collaboration and mutual benefit, or one of missed opportunities and unintended consequences.

## Lab Exercise: Implementing a Foundation Model for Robot Control

### Objective
Implement a multimodal foundation model that can interpret natural language instructions and visual input to generate robot actions, incorporating ethical decision-making capabilities.

### Requirements
1. Create a model that processes both visual and language inputs
2. Implement action generation from multimodal inputs
3. Integrate ethical decision-making into the action selection process
4. Test the system with various instruction scenarios

### Implementation Steps

1. Set up the multimodal model architecture:
```bash
python -m pip install transformers torch torchvision
```

2. Implement the multimodal processing pipeline

3. Integrate with a robot simulator for testing

4. Evaluate performance across different scenarios

### Expected Outcomes
Students will understand how to implement advanced multimodal systems for robotics and incorporate ethical considerations into autonomous decision-making.

============================================================
FILE: book\docusaurus-plugin-chatbot\index.js
============================================================
// @ts-check

/** @type {import('@docusaurus/types').Plugin} */
module.exports = function chatbotPlugin(context, options) {
  return {
    name: 'docusaurus-plugin-chatbot',

    configureWebpack(config, isServer, utils) {
      return {
        resolve: {
          alias: {
            '@chatbot': '../rag_frontend',
          },
        },
      };
    },

    getThemePath() {
      return './theme';
    },

    getClientModules() {
      return [require.resolve('./src/client-modules/chatbot')];
    },
  };
};

============================================================
FILE: book\rag_backend\ingest_docs.py
============================================================
import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from data_ingestion.markdown_parser import MarkdownParser
from embeddings.openrouter_client import OpenRouterEmbeddingClient
from vector_db.qdrant_client import QdrantVectorDB


def ingest_documents():
    """Main function to ingest documents into the vector database."""
    print("Starting document ingestion process...")
    
    # Initialize components
    parser = MarkdownParser(docs_path="../docs")  # Relative to rag_backend
    embedding_client = OpenRouterEmbeddingClient()
    vector_db = QdrantVectorDB()
    
    # Parse all documents
    print("Parsing markdown documents...")
    chunks = parser.parse_all_documents()
    print(f"Found {len(chunks)} text chunks to process.")
    
    # Generate embeddings for all chunks
    print("Generating embeddings...")
    embeddings = []
    processed_count = 0
    
    for i, chunk in enumerate(chunks):
        text = chunk['text']
        
        # Chunk if too large
        token_count = embedding_client.num_tokens_from_text(text)
        if token_count > embedding_client.max_tokens_per_chunk:
            text_chunks = embedding_client.chunk_text(text, embedding_client.max_tokens_per_chunk)
            for subchunk_text in text_chunks:
                embedding = embedding_client.get_embedding(subchunk_text)
                if embedding:
                    embeddings.append(embedding)
                    processed_count += 1
                    print(f"Processed embedding {processed_count}/{len(text_chunks)} for chunk {i+1}")
        else:
            embedding = embedding_client.get_embedding(text)
            if embedding:
                embeddings.append(embedding)
                processed_count += 1
                print(f"Processed embedding {processed_count}/{len(chunks)}")
    
    print(f"Generated {len(embeddings)} embeddings from {len(chunks)} text chunks.")
    
    # Store embeddings in Qdrant
    print("Storing embeddings in vector database...")
    vector_db.store_embeddings(chunks, embeddings)
    
    print("Document ingestion completed successfully!")


if __name__ == "__main__":
    ingest_documents()

============================================================
FILE: book\rag_backend\main.py
============================================================
from fastapi import FastAPI, HTTPException, Request
from pydantic import BaseModel
from typing import List, Optional, Dict
import openai
import os
from dotenv import load_dotenv
from embeddings.openrouter_client import OpenRouterEmbeddingClient
from vector_db.qdrant_client import QdrantVectorDB


# Load environment variables
load_dotenv()

# Configure OpenAI client to use OpenRouter
openai.base_url = "https://openrouter.ai/api/v1/"
openai.api_key = os.getenv("OPENROUTER_API_KEY")

# Initialize global components for RAG
embedding_client = OpenRouterEmbeddingClient()
vector_db = QdrantVectorDB()

if not openai.api_key:
    raise ValueError("OPENROUTER_API_KEY environment variable is not set")


class ChatRequest(BaseModel):
    message: str
    history: Optional[List[dict]] = []
    temperature: Optional[float] = 0.7


class SelectedTextChatRequest(ChatRequest):
    selected_text: str


app = FastAPI(title="Book RAG Chatbot API",
              description="Retrieval-Augmented Generation chatbot for the Physical AI & Humanoid Robotics book",
              version="1.0.0")


def format_context_for_llm(retrieved_chunks: List[Dict]) -> str:
    """Format retrieved document chunks for LLM context."""
    formatted_context = ""
    sources = []
    
    for i, chunk in enumerate(retrieved_chunks):
        text = chunk['text']
        metadata = chunk['metadata']
        
        formatted_context += f"\n--- Context Chunk {i+1} ---\n{text}\n"
        
        # Track sources
        source_info = {
            'title': metadata.get('title', 'Unknown'),
            'file_path': metadata.get('file_path', 'Unknown'),
            'heading': metadata.get('heading', ''),
            'score': chunk.get('score', 0.0)
        }
        sources.append(source_info)
    
    return formatted_context, sources


@app.get("/")
async def root():
    return {"message": "Book RAG Chatbot API is running!"}


@app.post("/chat")
async def chat_full_book(request: ChatRequest):
    """
    Chat endpoint that uses full book content for RAG.
    """
    try:
        # Generate embedding for the user query
        query_embedding = embedding_client.get_embedding(request.message)
        
        if not query_embedding:
            raise HTTPException(status_code=500, detail="Failed to generate embedding for query")
        
        # Search for relevant chunks in the vector database
        retrieved_chunks = vector_db.search_similar(query_embedding, top_k=5)
        
        if not retrieved_chunks:
            # If no relevant chunks found, respond accordingly
            return {
                "response": "I couldn't find relevant information in the book to answer your question.",
                "sources": []
            }
        
        # Format retrieved context for the LLM
        context, sources = format_context_for_llm(retrieved_chunks)
        
        # Construct the system message with context
        system_message = f"""
        You are an expert assistant for the Physical AI & Humanoid Robotics book. 
        Use the following context from the book to answer the user's question.
        Be accurate, detailed, and cite sources when possible based on the context provided.
        
        Context from the book:
        {context}
        """
        
        messages = [
            {"role": "system", "content": system_message},
        ]
        
        # Add history if available
        for msg in request.history:
            messages.append({"role": msg["role"], "content": msg["content"]})
        
        # Add user message
        messages.append({"role": "user", "content": request.message})

        # Call OpenRouter API
        response = openai.chat.completions.create(
            model=os.getenv("CHAT_MODEL", "qwen/qwen-2-7b-instruct:free"),
            messages=messages,
            temperature=request.temperature,
            max_tokens=1024,
        )

        return {
            "response": response.choices[0].message.content,
            "sources": sources
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/chat/selected")
async def chat_selected_text(request: SelectedTextChatRequest):
    """
    Chat endpoint that uses only user-selected text for context.
    """
    try:
        # Validate that selected_text exists
        if not request.selected_text.strip():
            raise HTTPException(status_code=400, detail="Selected text cannot be empty")
        
        # Construct the prompt with the selected text as context
        system_message = f"""
        You are an expert assistant for the Physical AI & Humanoid Robotics book. 
        Answer only based on the following selected text from the book.
        If the question cannot be answered from the provided text, clearly state that the information is not available in the selected text.
        
        Selected text: {request.selected_text}
        """
        
        messages = [
            {"role": "system", "content": system_message},
        ]
        
        # Add history if available
        for msg in request.history:
            messages.append({"role": msg["role"], "content": msg["content"]})
        
        # Add user message
        messages.append({"role": "user", "content": request.message})

        # Call OpenRouter API
        response = openai.chat.completions.create(
            model=os.getenv("CHAT_MODEL", "qwen/qwen-2-7b-instruct:free"),
            messages=messages,
            temperature=request.temperature,
            max_tokens=1024,
        )

        # Check if response indicates information is not in selected text
        response_text = response.choices[0].message.content
        if "information is not available" in response_text.lower() or "cannot answer" in response_text.lower():
            return {
                "response": response_text,
                "sources": [],
                "warning": "The requested information may not be available in the selected text."
            }
        
        return {
            "response": response_text,
            "sources": [{"text": request.selected_text[:200] + "..."}]  # Truncated for brevity
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/health")
async def health_check():
    """Health check endpoint."""
    return {"status": "healthy"}

============================================================
FILE: book\rag_backend\quick_test.py
============================================================
import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from data_ingestion.markdown_parser import MarkdownParser
from embeddings.openrouter_client import OpenRouterEmbeddingClient
from vector_db.qdrant_client import QdrantVectorDB
import time

def quick_test_ingestion():
    """Quick test to verify the ingestion pipeline works."""
    print("Starting quick ingestion test...")
    
    # Initialize components
    print("1. Initializing components...")
    parser = MarkdownParser(docs_path="../docs")  # Relative to rag_backend
    embedding_client = OpenRouterEmbeddingClient()
    vector_db = QdrantVectorDB()
    
    # Parse just a single small document for testing
    print("2. Parsing a sample document...")
    from pathlib import Path
    sample_file = Path("../docs/intro.md")  # Use Path object
    if sample_file.exists():
        chunks = parser.parse_document(sample_file)
        print(f"Parsed {len(chunks)} chunks from sample document")
        
        if chunks:
            # Test embedding generation for first chunk only
            print("3. Testing embedding generation...")
            sample_text = chunks[0]['text'][:500]  # Use first 500 chars for test
            print(f"Sample text (first 100 chars): {sample_text[:100]}...")
            
            start_time = time.time()
            embedding = embedding_client.get_embedding(sample_text)
            embedding_time = time.time() - start_time
            
            if embedding:
                print(f"‚úì Embedding generated successfully! Length: {len(embedding)}, Time: {embedding_time:.2f}s")
                
                # Test storing in vector DB
                print("4. Testing storage in vector database...")
                try:
                    vector_db.store_embeddings([chunks[0]], [embedding])
                    print("‚úì Successfully stored in vector database!")
                    
                    # Test retrieval
                    print("5. Testing retrieval...")
                    search_results = vector_db.search_similar(embedding, top_k=1)
                    print(f"‚úì Retrieved {len(search_results)} results from vector database")
                    
                    print("\n‚úì All tests passed! The ingestion pipeline is working.")
                    return True
                except Exception as e:
                    print(f"‚úó Storage/retrieval test failed: {e}")
                    return False
            else:
                print("‚úó Failed to generate embedding")
                return False
        else:
            print("‚úó No chunks found in sample document")
            return False
    else:
        print(f"‚úó Sample file {sample_file} does not exist")
        # Try to find any markdown file
        for root, dirs, files in os.walk("../docs"):
            for file in files:
                if file.endswith(".md"):
                    sample_file = Path(os.path.join(root, file))
                    print(f"Found sample file: {sample_file}")
                    chunks = parser.parse_document(sample_file)
                    if chunks:
                        print(f"Test with first chunk of {file}")
                        sample_text = chunks[0]['text'][:500]
                        embedding = embedding_client.get_embedding(sample_text)
                        if embedding:
                            print("‚úì Embedding works with actual document!")
                            return True
                    break
        return False

if __name__ == "__main__":
    quick_test_ingestion()

============================================================
FILE: book\rag_backend\README.md
============================================================
# RAG Chatbot for Docusaurus Book

This repository contains a Retrieval-Augmented Generation (RAG) chatbot implementation for the Physical AI & Humanoid Robotics book, integrated with Docusaurus.

## Features

- **Full Book RAG**: Ask questions about the entire book content
- **Strict Context Mode**: Get answers based only on selected text
- **Docusaurus Integration**: Seamless integration with existing Docusaurus setup
- **Qwen LLM**: Using OpenRouter's Qwen model for responses
- **Vector Database**: Qdrant for efficient document retrieval

## Getting Started

### Prerequisites

- Python 3.8+
- Node.js and npm
- OpenRouter API key
- Qdrant Cloud account (free tier)

### Installation

1. Clone or integrate this code with your Docusaurus book
2. Install backend dependencies:
   ```bash
   cd rag_backend
   pip install -r requirements.txt
   ```

3. Set up environment variables:
   ```bash
   cp .env.example .env
   # Edit .env with your API keys
   ```

4. Ingest book content into vector database:
   ```bash
   python ingest_docs.py
   ```

5. Start the backend server:
   ```bash
   # On Windows
   start_backend.bat
   # Or on Unix
   chmod +x start_backend.sh
   ./start_backend.sh
   ```

6. Start your Docusaurus site:
   ```bash
   npm run start
   ```

## Architecture

### Backend Structure
```
rag_backend/
‚îú‚îÄ‚îÄ api/                 # FastAPI endpoints
‚îú‚îÄ‚îÄ data_ingestion/      # Markdown parsing
‚îú‚îÄ‚îÄ embeddings/          # OpenRouter embedding client
‚îú‚îÄ‚îÄ vector_db/           # Qdrant integration
‚îú‚îÄ‚îÄ main.py             # FastAPI application
‚îú‚îÄ‚îÄ ingest_docs.py      # Document ingestion script
‚îî‚îÄ‚îÄ requirements.txt    # Python dependencies
```

### Frontend Integration
```
src/
‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îú‚îÄ‚îÄ Chatbot.js      # Chat interface
‚îÇ   ‚îú‚îÄ‚îÄ Chatbot.css     # Styling
‚îÇ   ‚îî‚îÄ‚îÄ ChatbotAPI.js   # API utilities
‚îî‚îÄ‚îÄ theme/
    ‚îî‚îÄ‚îÄ Layout.js       # Docusaurus theme override
```

## API Endpoints

- `GET /` - Health check
- `POST /chat` - Full book RAG chat
- `POST /chat/selected` - Strict context mode chat
- `GET /health` - Health check

## Environment Variables

Create a `.env` file in the `rag_backend/` directory:

```env
# OpenRouter API Configuration
OPENROUTER_API_KEY=your_openrouter_api_key_here

# Qdrant Vector Database Configuration
QDRANT_URL=your_qdrant_url_here
QDRANT_API_KEY=your_qdrant_api_key_here
QDRANT_COLLECTION_NAME=book_embeddings

# Application Configuration
EMBEDDING_MODEL=qwen/qwen-2-7b-instruct:free
MAX_TOKENS_PER_CHUNK=512
```

## Production Deployment

1. Deploy the FastAPI backend to a cloud platform
2. Update the frontend to point to your deployed backend URL
3. Ensure all environment variables are properly configured in production

## Contributing

Feel free to open issues or submit pull requests for improvements.

## License

This project is licensed under the MIT License.

============================================================
FILE: book\rag_backend\requirements.txt
============================================================
fastapi==0.104.1
uvicorn[standard]==0.24.0
openai==1.3.7
qdrant-client==1.7.0
python-dotenv==1.0.0
PyYAML==6.0.1
tiktoken==0.5.2
langchain==0.0.336
markdownify==0.11.6
aiofiles==23.2.1

============================================================
FILE: book\rag_backend\server.py
============================================================
import os
import sys

# Set environment to use local Qdrant before importing anything
os.environ['QDRANT_URL'] = ''  # Empty to trigger local mode

# Add current directory to path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# Import and run the FastAPI app
if __name__ == "__main__":
    print("Starting the RAG API server...")
    print("Using local Qdrant mode")
    
    from main import app
    import uvicorn
    
    print("Server ready! Access at http://127.0.0.1:8000")
    print("Docs available at http://127.0.0.1:8000/docs")
    
    uvicorn.run(app, host="127.0.0.1", port=8000)

============================================================
FILE: book\rag_backend\test_qdrant.py
============================================================
import os
# Make sure we're using local mode
os.environ['QDRANT_URL'] = ''  # Empty string to trigger local mode

from vector_db.qdrant_client import QdrantVectorDB

try:
    print("Creating QdrantVectorDB instance...")
    db = QdrantVectorDB()
    print("SUCCESS: Qdrant client created in local mode")
    print(f"Collection: {db.collection_name}")
    
    # Test if we can count items (should be 0 initially)
    count = db.client.count(db.collection_name).count if db.client.collection_exists(db.collection_name) else 0
    print(f"Current items in collection: {count}")
    
except Exception as e:
    print(f"ERROR: {str(e)}")
    import traceback
    traceback.print_exc()

============================================================
FILE: book\rag_backend\data_ingestion\markdown_parser.py
============================================================
import os
import re
from pathlib import Path
from typing import List, Dict, Tuple
import yaml


class MarkdownParser:
    """Parse markdown files from the Docusaurus docs directory."""
    
    def __init__(self, docs_path: str = "./docs"):
        self.docs_path = Path(docs_path)
        
    def extract_frontmatter(self, content: str) -> Tuple[str, Dict]:
        """Extract YAML frontmatter from markdown content."""
        if content.startswith("---"):
            try:
                end_marker = content.find("---", 3)
                if end_marker != -1:
                    frontmatter = content[3:end_marker].strip()
                    remaining_content = content[end_marker + 3:].strip()
                    metadata = yaml.safe_load(frontmatter) or {}
                    return remaining_content, metadata
            except yaml.YAMLError:
                pass
        
        return content, {}
    
    def split_by_headings(self, content: str) -> List[Dict[str, str]]:
        """Split content by headings into sections."""
        # Regular expression to match markdown headings (# ## ### ####)
        heading_pattern = r'^(#{1,6})\s+(.+)$'
        
        lines = content.split('\n')
        sections = []
        current_section = {
            'heading': '',
            'content': [],
            'level': 0
        }
        
        for line in lines:
            match = re.match(heading_pattern, line.strip())
            
            if match:
                # Save previous section if it has content
                if current_section['content']:
                    sections.append({
                        'heading': current_section['heading'],
                        'content': '\n'.join(current_section['content']).strip(),
                        'level': current_section['level']
                    })
                
                # Start new section
                heading_level = len(match.group(1))
                heading_text = match.group(2)
                current_section = {
                    'heading': heading_text,
                    'content': [],
                    'level': heading_level
                }
            else:
                current_section['content'].append(line)
        
        # Add the last section
        if current_section['content']:
            sections.append({
                'heading': current_section['heading'],
                'content': '\n'.join(current_section['content']).strip(),
                'level': current_section['level']
            })
        
        return sections
    
    def parse_document(self, file_path: Path) -> List[Dict]:
        """Parse a markdown document into structured chunks."""
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        content_without_frontmatter, metadata = self.extract_frontmatter(content)
        
        # Extract filename without extension as title if not in metadata
        if 'title' not in metadata:
            metadata['title'] = file_path.stem.replace('-', ' ').title()
        
        # Add file path info to metadata
        metadata['file_path'] = str(file_path.relative_to(self.docs_path))
        
        sections = self.split_by_headings(content_without_frontmatter)
        
        chunks = []
        for section in sections:
            # Combine heading with content
            chunk_content = f"{section['heading']}\n\n{section['content']}".strip() if section['heading'] else section['content']
            
            if chunk_content.strip():  # Only add non-empty chunks
                chunks.append({
                    'text': chunk_content,
                    'metadata': {
                        **metadata,
                        'heading': section['heading'],
                        'heading_level': section['level']
                    }
                })
                
        return chunks
    
    def parse_all_documents(self) -> List[Dict]:
        """Parse all markdown documents in the docs directory."""
        all_chunks = []
        
        for md_file in self.docs_path.rglob("*.md"):
            try:
                chunks = self.parse_document(md_file)
                all_chunks.extend(chunks)
                print(f"Parsed {len(chunks)} chunks from {md_file}")
            except Exception as e:
                print(f"Error parsing {md_file}: {str(e)}")
                
        return all_chunks

============================================================
FILE: book\rag_backend\embeddings\openrouter_client.py
============================================================
import os
import openai
from dotenv import load_dotenv
import tiktoken


class OpenRouterEmbeddingClient:
    """Client for generating embeddings using OpenRouter API."""
    
    def __init__(self):
        load_dotenv()
        self.api_key = os.getenv("OPENROUTER_API_KEY")
        if not self.api_key:
            raise ValueError("OPENROUTER_API_KEY environment variable is not set")

        # Configure OpenAI client to use OpenRouter
        openai.base_url = "https://openrouter.ai/api/v1/"
        openai.api_key = self.api_key

        # Use a proper embedding model instead of a chat model
        self.model = os.getenv("EMBEDDING_MODEL", "text-embedding-ada-002")  # OpenRouter format
        self.max_tokens_per_chunk = int(os.getenv("MAX_TOKENS_PER_CHUNK", "512"))

        # Initialize tokenizer for chunking
        self.tokenizer = tiktoken.encoding_for_model("gpt-3.5-turbo")  # Close approximation
    
    def get_embedding(self, text: str) -> list[float]:
        """Get embedding for a text using OpenRouter API."""
        try:
            response = openai.embeddings.create(
                model=self.model,
                input=text
            )
            return response.data[0].embedding
        except Exception as e:
            print(f"Error getting embedding: {str(e)}")
            return []
    
    def num_tokens_from_text(self, text: str) -> int:
        """Return the number of tokens in a text string."""
        return len(self.tokenizer.encode(text))
    
    def chunk_text(self, text: str, max_tokens: int = None) -> list[str]:
        """
        Split text into chunks that are no larger than max_tokens.
        """
        if max_tokens is None:
            max_tokens = self.max_tokens_per_chunk
            
        tokens = self.tokenizer.encode(text)
        chunks = []
        
        # Split into chunks of max_tokens
        for i in range(0, len(tokens), max_tokens):
            chunk_tokens = tokens[i:i + max_tokens]
            chunk_text = self.tokenizer.decode(chunk_tokens)
            chunks.append(chunk_text)
        
        return chunks

============================================================
FILE: book\rag_backend\qdrant_data\meta.json
============================================================
{"collections": {"book_embeddings": {"vectors": {"size": 1536, "distance": "Cosine", "hnsw_config": null, "quantization_config": null, "on_disk": null, "datatype": null, "multivector_config": null}, "shard_number": null, "sharding_method": null, "replication_factor": null, "write_consistency_factor": null, "on_disk_payload": null, "hnsw_config": null, "wal_config": null, "optimizers_config": null, "quantization_config": null, "sparse_vectors": null, "strict_mode_config": null, "metadata": null}}, "aliases": {}}

============================================================
FILE: book\rag_backend\vector_db\qdrant_client.py
============================================================
import os
from typing import List, Dict, Optional
from qdrant_client import QdrantClient
from qdrant_client.http.models import PointStruct, VectorParams, Distance, PayloadSchemaType
from dotenv import load_dotenv


class QdrantVectorDB:
    """Integration with Qdrant vector database for storing and searching embeddings."""
    
    def __init__(self):
        load_dotenv()
        self.url = os.getenv("QDRANT_URL")
        self.api_key = os.getenv("QDRANT_API_KEY")
        self.collection_name = os.getenv("QDRANT_COLLECTION_NAME", "book_embeddings")

        # Initialize Qdrant client - use local mode if no URL provided or URL is empty/default
        if self.url and self.url != "your_qdrant_url_here":
            self.client = QdrantClient(
                url=self.url,
                api_key=self.api_key,
                prefer_grpc=False  # Using REST API for simplicity
            )
        else:
            # Use local/disk mode with a local path
            from pathlib import Path
            local_path = Path("./qdrant_data").resolve()
            local_path.mkdir(exist_ok=True)
            self.client = QdrantClient(path=str(local_path))  # Local mode
        
        # Create collection if it doesn't exist
        self._create_collection_if_not_exists()
    
    def _create_collection_if_not_exists(self):
        """Create the collection if it doesn't exist already."""
        try:
            # Check if collection exists
            collections = self.client.get_collections()
            collection_names = [coll.name for coll in collections.collections]
            
            if self.collection_name not in collection_names:
                # Create collection - dimension is based on embedding size (typically 384 for smaller models, 1536 for larger)
                # Using 1536 as it's common for embedding models - adjust based on actual embedding size
                self.client.create_collection(
                    collection_name=self.collection_name,
                    vectors_config={"size": 1536, "distance": "Cosine"},
                )
                
                # Create payload index for metadata searching
                self.client.create_payload_index(
                    collection_name=self.collection_name,
                    field_name="file_path",
                    field_schema=PayloadSchemaType.KEYWORD
                )
                
                print(f"Created collection '{self.collection_name}' successfully.")
            else:
                print(f"Collection '{self.collection_name}' already exists.")
                
        except Exception as e:
            print(f"Error creating collection: {str(e)}")
            raise e
    
    def store_embeddings(self, texts: List[Dict], embeddings: List[List[float]]):
        """
        Store embeddings with their associated text and metadata.
        
        Args:
            texts: List of dictionaries containing 'text' and 'metadata'
            embeddings: List of embedding vectors
        """
        if len(texts) != len(embeddings):
            raise ValueError("Number of texts must match number of embeddings")
        
        points = []
        for i, (text_obj, embedding) in enumerate(zip(texts, embeddings)):
            point = PointStruct(
                id=i,
                vector=embedding,
                payload={
                    "text": text_obj['text'],
                    "metadata": text_obj['metadata']
                }
            )
            points.append(point)
        
        # Upload points to Qdrant
        self.client.upload_points(
            collection_name=self.collection_name,
            points=points
        )
        
        print(f"Stored {len(points)} embeddings in Qdrant collection '{self.collection_name}'.")
    
    def search_similar(self, query_embedding: List[float], top_k: int = 5) -> List[Dict]:
        """
        Search for similar embeddings in the vector database.
        
        Args:
            query_embedding: The embedding to search for similarities
            top_k: Number of top results to return
            
        Returns:
            List of dictionaries containing similar text snippets and metadata
        """
        search_result = self.client.search(
            collection_name=self.collection_name,
            query_vector=query_embedding,
            limit=top_k
        )
        
        results = []
        for hit in search_result:
            result = {
                'text': hit.payload['text'],
                'metadata': hit.payload['metadata'],
                'score': hit.score
            }
            results.append(result)
        
        return results
    
    def search_by_metadata(self, metadata_filter: Dict, top_k: int = 5) -> List[Dict]:
        """
        Search for embeddings with specific metadata filters.
        """
        from qdrant_client.http.models import FieldCondition, MatchValue
        
        conditions = []
        for key, value in metadata_filter.items():
            conditions.append(
                FieldCondition(
                    key=f"metadata.{key}",
                    match=MatchValue(value=value)
                )
            )
        
        if conditions:
            # Create a filter combining all conditions with AND logic
            from qdrant_client.http.models import Filter
            search_filter = Filter(must=conditions)
        else:
            search_filter = None
        
        search_result = self.client.search(
            collection_name=self.collection_name,
            query_filter=search_filter,
            limit=top_k
        )
        
        results = []
        for hit in search_result:
            result = {
                'text': hit.payload['text'],
                'metadata': hit.payload['metadata'],
                'score': hit.score
            }
            results.append(result)
        
        return results

============================================================
FILE: book\src\components\Chatbot.css
============================================================
.chatbot-container {
  position: fixed;
  bottom: 20px;
  right: 20px;
  z-index: 1000;
}

.chatbot-toggle-btn {
  width: 60px;
  height: 60px;
  border-radius: 50%;
  background: linear-gradient(135deg, #2C3E50 0%, #00BCD4 100%);
  color: white;
  border: none;
  display: flex;
  align-items: center;
  justify-content: center;
  cursor: pointer;
  box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
  transition: all 0.3s ease;
  font-size: 1.5rem;
}

.chatbot-toggle-btn:hover {
  transform: scale(1.05) translateY(-2px);
  box-shadow: 0 6px 16px rgba(0, 0, 0, 0.2);
}

.chatbot-window {
  width: 400px;
  height: 600px;
  background-color: var(--ifm-background-surface-color);
  border-radius: 12px;
  box-shadow: var(--ifm-global-shadow-lg);
  display: flex;
  flex-direction: column;
  overflow: hidden;
  border: 1px solid var(--ifm-border-color);
}

.chatbot-header {
  background: linear-gradient(135deg, #2C3E50 0%, #34495E 100%);
  color: white;
  padding: 15px;
  display: flex;
  justify-content: space-between;
  align-items: center;
  border-bottom: 1px solid var(--ifm-border-color);
}

.chatbot-header h3 {
  margin: 0;
  font-size: 1.1rem;
  font-weight: 600;
}

.header-controls {
  display: flex;
  gap: 10px;
  align-items: center;
}

.strict-mode-toggle {
  display: flex;
  align-items: center;
  gap: 5px;
  font-size: 0.8rem;
  color: var(--ifm-font-color-secondary);
}

.strict-mode-toggle input {
  margin: 0;
}

.clear-chat-btn, .close-chat-btn {
  background: none;
  border: none;
  color: white;
  cursor: pointer;
  font-size: 1.2rem;
  padding: 0;
  width: 28px;
  height: 28px;
  display: flex;
  align-items: center;
  justify-content: center;
  border-radius: 4px;
  transition: background-color 0.2s ease;
}

.clear-chat-btn:hover, .close-chat-btn:hover {
  background-color: rgba(255, 255, 255, 0.2);
}

.selected-text-preview {
  background-color: rgba(0, 188, 212, 0.1);
  padding: 8px 15px;
  font-size: 0.8rem;
  border-bottom: 1px solid var(--ifm-border-color);
  color: var(--ifm-color-accent);
}

.selected-text-warning {
  background-color: rgba(255, 193, 7, 0.1);
  padding: 8px 15px;
  font-size: 0.8rem;
  border-bottom: 1px solid var(--ifm-border-color);
  color: #FF9800;
}

.chatbot-messages {
  flex: 1;
  overflow-y: auto;
  padding: 15px;
  display: flex;
  flex-direction: column;
  gap: 15px;
  background-color: var(--ifm-background-color);
}

.welcome-message {
  text-align: center;
  color: var(--ifm-font-color-secondary);
  font-style: italic;
  padding: 20px 0;
  font-size: 0.9rem;
}

.message {
  max-width: 85%;
  word-wrap: break-word;
  padding: 12px 16px;
  border-radius: 18px;
  line-height: 1.5;
  animation: fadeIn 0.3s ease;
}

@keyframes fadeIn {
  from { opacity: 0; transform: translateY(10px); }
  to { opacity: 1; transform: translateY(0); }
}

.message.user {
  align-self: flex-end;
  background-color: var(--ifm-color-accent);
  color: white;
  border-bottom-right-radius: 5px;
}

.message.bot {
  align-self: flex-start;
  background-color: var(--ifm-color-emphasis-200);
  color: var(--ifm-font-color-base);
  border-bottom-left-radius: 5px;
}

html[data-theme='dark'] .message.bot {
  background-color: var(--ifm-color-emphasis-800);
}

.message-content {
  line-height: 1.5;
}

.sources {
  margin-top: 8px;
  font-size: 0.85em;
}

.sources ul {
  margin: 5px 0 0 0;
  padding-left: 15px;
  font-size: 0.8rem;
}

.sources li {
  margin-bottom: 3px;
  color: var(--ifm-color-accent);
}

.sources summary {
  cursor: pointer;
  color: var(--ifm-color-accent);
  font-weight: 500;
}

.typing-indicator {
  display: flex;
  gap: 5px;
  justify-content: center;
}

.typing-indicator span {
  width: 10px;
  height: 10px;
  background-color: var(--ifm-font-color-secondary);
  border-radius: 50%;
  animation: bounce 1.4s infinite ease-in-out;
}

.typing-indicator span:nth-child(2) {
  animation-delay: 0.2s;
}

.typing-indicator span:nth-child(3) {
  animation-delay: 0.4s;
}

@keyframes bounce {
  0%, 80%, 100% { transform: translateY(0); }
  40% { transform: translateY(-5px); }
}

.chatbot-input-area {
  padding: 15px;
  border-top: 1px solid var(--ifm-border-color);
  display: flex;
  gap: 10px;
  background-color: var(--ifm-background-surface-color);
}

.chatbot-input-area textarea {
  flex: 1;
  padding: 12px 15px;
  border: 1px solid var(--ifm-border-color);
  border-radius: 8px;
  resize: none;
  font-family: inherit;
  font-size: 0.9rem;
  background-color: var(--ifm-background-color);
  color: var(--ifm-font-color-base);
  transition: border-color 0.2s ease;
}

.chatbot-input-area textarea:focus {
  outline: none;
  border-color: var(--ifm-color-accent);
  box-shadow: 0 0 0 2px rgba(0, 188, 212, 0.2);
}

.disabled-input {
  background-color: var(--ifm-color-emphasis-100);
  color: var(--ifm-font-color-secondary);
  cursor: not-allowed;
}

.send-button {
  background: linear-gradient(135deg, #2C3E50 0%, #34495E 100%);
  color: white;
  border: none;
  border-radius: 8px;
  padding: 0 20px;
  cursor: pointer;
  font-weight: 600;
  transition: all 0.2s ease;
  display: flex;
  align-items: center;
  justify-content: center;
}

.send-button:hover:not(:disabled) {
  background: linear-gradient(135deg, #34495E 0%, #2C3E50 100%);
  transform: translateY(-1px);
  box-shadow: var(--ifm-global-shadow-md);
}

.send-button:disabled {
  background: var(--ifm-color-emphasis-300);
  cursor: not-allowed;
  transform: none;
  box-shadow: none;
}

@media (max-width: 768px) {
  .chatbot-container {
    bottom: 15px;
    right: 15px;
  }

  .chatbot-window {
    width: calc(100vw - 30px);
    height: 50vh;
    max-height: 400px;
  }

  .message {
    max-width: 90%;
  }
}

@media (max-width: 480px) {
  .chatbot-container {
    bottom: 10px;
    right: 10px;
  }

  .chatbot-window {
    width: calc(100vw - 20px);
    height: 45vh;
    max-height: 350px;
  }

  .chatbot-header h3 {
    font-size: 1rem;
  }

  .chatbot-header {
    padding: 12px;
  }

  .chatbot-messages {
    padding: 10px;
  }

  .message {
    padding: 10px 14px;
    font-size: 0.9rem;
  }
}

============================================================
FILE: book\src\components\Chatbot.js
============================================================
import React, { useState, useEffect, useRef } from 'react';
import './Chatbot.css';
import { chatApi } from './ChatbotAPI';

const Chatbot = () => {
  const [isOpen, setIsOpen] = useState(false);
  const [messages, setMessages] = useState([]);
  const [inputMessage, setInputMessage] = useState('');
  const [isLoading, setIsLoading] = useState(false);
  const [selectedText, setSelectedText] = useState('');
  const [useStrictContext, setUseStrictContext] = useState(false);
  const messagesEndRef = useRef(null);

  // Function to get selected text from the page
  const getSelectedText = () => {
    const selectedText = window.getSelection?.toString()?.trim() || '';
    setSelectedText(selectedText);
    return selectedText;
  };

  // Auto-scroll to bottom of messages
  useEffect(() => {
    messagesEndRef.current?.scrollIntoView({ behavior: 'smooth' });
  }, [messages]);

  // Handle sending a message
  const sendMessage = async () => {
    if (!inputMessage.trim() || isLoading) return;

    const userMessage = { role: 'user', content: inputMessage };
    const newMessages = [...messages, userMessage];
    setMessages(newMessages);
    setInputMessage('');
    setIsLoading(true);

    try {
      let response;

      if (useStrictContext && selectedText) {
        // Use strict context mode with selected text
        response = await chatApi.chatSelectedText(
          inputMessage,
          selectedText,
          newMessages.filter(msg => msg.role !== 'assistant').map(msg => ({
            role: msg.role,
            content: msg.content
          }))
        );
      } else {
        // Use full book RAG mode
        response = await chatApi.chatFullBook(
          inputMessage,
          newMessages.filter(msg => msg.role !== 'assistant').map(msg => ({
            role: msg.role,
            content: msg.content
          }))
        );
      }

      const botMessage = {
        role: 'assistant',
        content: response.response,
        sources: response.sources || []
      };

      setMessages(prev => [...prev, botMessage]);
    } catch (error) {
      console.error('Error sending message:', error);
      const errorMessage = {
        role: 'assistant',
        content: 'Sorry, I encountered an error. Please try again.'
      };
      setMessages(prev => [...prev, errorMessage]);
    } finally {
      setIsLoading(false);
    }
  };

  // Handle Enter key press
  const handleKeyPress = (e) => {
    if (e.key === 'Enter' && !e.shiftKey) {
      e.preventDefault();
      sendMessage();
    }
  };

  // Toggle chat window
  const toggleChat = () => {
    setIsOpen(!isOpen);
    if (!isOpen) {
      getSelectedText(); // Capture any selected text when opening
    }
  };

  // Clear chat history
  const clearChat = () => {
    setMessages([]);
    setInputMessage('');
  };

  return (
    <div className={`chatbot-container ${isOpen ? 'open' : ''}`}>
      {/* Floating button to open chat */}
      {!isOpen && (
        <button className="chatbot-toggle-btn" onClick={toggleChat}>
          üí¨ AI Assistant
        </button>
      )}

      {isOpen && (
        <div className="chatbot-window">
          {/* Header */}
          <div className="chatbot-header">
            <h3>Book AI Assistant</h3>
            <div className="header-controls">
              <label className="strict-mode-toggle">
                <input
                  type="checkbox"
                  checked={useStrictContext}
                  onChange={(e) => setUseStrictContext(e.target.checked)}
                />
                Strict Context Mode
              </label>
              <button className="clear-chat-btn" onClick={clearChat}>Clear</button>
              <button className="close-chat-btn" onClick={toggleChat}>√ó</button>
            </div>
          </div>

          {/* Selected text indicator */}
          {useStrictContext && selectedText && (
            <div className="selected-text-preview">
              <strong>Selected text:</strong> "{selectedText.substring(0, 100)}..."
            </div>
          )}
          
          {useStrictContext && !selectedText && (
            <div className="selected-text-warning">
              No text selected. Select text on the page to use strict context mode.
            </div>
          )}

          {/* Messages container */}
          <div className="chatbot-messages">
            {messages.length === 0 ? (
              <div className="welcome-message">
                <p>Hello! I'm your AI assistant for the Physical AI & Humanoid Robotics book.</p>
                <p>You can ask me anything about the book content!</p>
                <p>In strict context mode, I'll only answer based on the text you've selected.</p>
              </div>
            ) : (
              messages.map((message, index) => (
                <div key={index} className={`message ${message.role}`}>
                  <div className="message-content">
                    {message.content}
                    {message.sources && message.sources.length > 0 && (
                      <div className="sources">
                        <details>
                          <summary>Sources</summary>
                          <ul>
                            {message.sources.slice(0, 3).map((source, idx) => (
                              <li key={idx}>
                                {source.file_path ? `${source.title} (${source.file_path})` : source.text}
                              </li>
                            ))}
                          </ul>
                        </details>
                      </div>
                    )}
                  </div>
                </div>
              ))
            )}
            {isLoading && (
              <div className="message bot">
                <div className="message-content">
                  <div className="typing-indicator">
                    <span></span>
                    <span></span>
                    <span></span>
                  </div>
                </div>
              </div>
            )}
            <div ref={messagesEndRef} />
          </div>

          {/* Input area */}
          <div className="chatbot-input-area">
            <textarea
              value={inputMessage}
              onChange={(e) => setInputMessage(e.target.value)}
              onKeyDown={handleKeyPress}
              placeholder={useStrictContext && !selectedText 
                ? "Select text on the page first, then type your question..." 
                : "Ask a question about the book content..."}
              rows="2"
              disabled={isLoading}
              className={useStrictContext && !selectedText ? "disabled-input" : ""}
            />
            <button 
              onClick={sendMessage} 
              disabled={!inputMessage.trim() || isLoading || (useStrictContext && !selectedText)}
              className="send-button"
            >
              Send
            </button>
          </div>
        </div>
      )}
    </div>
  );
};

export default Chatbot;

============================================================
FILE: book\src\components\ChatbotAPI.js
============================================================
// API utility functions for the chatbot
// Using the backend server
const API_BASE = 'http://localhost:8000';

export const chatApi = {
  async chatFullBook(message, history = [], temperature = 0.7) {
    try {
      const response = await fetch(`${API_BASE}/chat`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          message,
          history,
          temperature
        })
      });

      if (!response.ok) {
        throw new Error(`HTTP error! status: ${response.status}`);
      }

      return await response.json();
    } catch (error) {
      console.error('Error in chatFullBook:', error);
      throw error;
    }
  },

  async chatSelectedText(message, selectedText, history = [], temperature = 0.7) {
    try {
      const response = await fetch(`${API_BASE}/chat/selected`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          message,
          selected_text: selectedText,
          history,
          temperature
        })
      });

      if (!response.ok) {
        throw new Error(`HTTP error! status: ${response.status}`);
      }

      return await response.json();
    } catch (error) {
      console.error('Error in chatSelectedText:', error);
      throw error;
    }
  }
};

============================================================
FILE: book\src\components\ChatbotIntegration.js
============================================================
import React, { useState, useEffect } from 'react';
import Head from '@docusaurus/Head';
import useDocusaurusContext from '@docusaurus/useDocusaurusContext';

const ChatbotIntegration = () => {
  const [isOpen, setIsOpen] = useState(false);
  const [isLoaded, setIsLoaded] = useState(false);
  const { siteConfig } = useDocusaurusContext();

  const toggleChatbot = () => {
    setIsOpen(!isOpen);
  };

  const closeChatbot = () => {
    setIsOpen(false);
  };

  // Load the chatbot script dynamically when component mounts
  useEffect(() => {
    if (!isLoaded) {
      setIsLoaded(true);
    }
  }, [isLoaded]);

  return (
    <>
      <Head>
        <script>
          {`
            // Chatbot script logic would go here if needed
          `}
        </script>
      </Head>
      
      <div id="chatbot-container">
        <div id="chatbot-panel" className={isOpen ? 'open' : ''}>
          <div id="chatbot-header">
            <h3 id="chatbot-title">AI Assistant</h3>
            <button id="chatbot-close" onClick={closeChatbot} aria-label="Close chatbot">
              √ó
            </button>
          </div>
          <iframe
            id="chatbot-iframe"
            src="/../../rag_frontend/index.html"
            title="AI Textbook Assistant"
            allow="microphone"
            style={{ width: '100%', height: '400px', border: 'none' }}
          ></iframe>
        </div>
        <button
          id="chatbot-toggle"
          onClick={toggleChatbot}
          aria-label={isOpen ? "Close AI Assistant" : "Open AI Assistant"}
          aria-expanded={isOpen}
        >
          üí¨
        </button>
      </div>
    </>
  );
};

export default ChatbotIntegration;

============================================================
FILE: book\src\components\CustomLayout.js
============================================================
import React from 'react';
import Layout from '@theme/Layout';
import Chatbot from '../components/Chatbot';

// Custom layout wrapper that includes the chatbot
export default function CustomLayout(props) {
  return (
    <>
      <Layout {...props}>
        {props.children}
      </Layout>
      <Chatbot />
    </>
  );
}

============================================================
FILE: book\src\components\index.js
============================================================
// Export Chatbot as the main component
export { default } from './Chatbot';

============================================================
FILE: book\src\css\custom.css
============================================================
/**
 * Custom CSS for Physical AI & Humanoid Robotics textbook
 */

/* Color Variables */
:root {
  /* Light mode colors - keeping for reference but focusing on dark theme */
  --ifm-color-primary: #2C3E50;
  --ifm-color-primary-dark: #1A252F;
  --ifm-color-primary-darker: #0F1419;
  --ifm-color-primary-darkest: #0A0D10;
  --ifm-color-primary-light: #34495E;
  --ifm-color-primary-lighter: #3D566E;
  --ifm-color-primary-lightest: #4D6480;

  /* Accent color (cyan/teal) */
  --ifm-color-accent: #00BCD4;
  --ifm-color-accent-dark: #008BA3;
  --ifm-color-accent-light: #26C6DA;

  /* Background colors */
  --ifm-background-color: #F8F9FA;
  --ifm-background-surface-color: #FFFFFF;

  /* Text colors */
  --ifm-font-color-base: #2C3E50;
  --ifm-font-color-secondary: #7F8C8D;

  /* Code block colors */
  --ifm-code-background: #ECEFF1;
  --ifm-code-color: #3B4252;

  /* Border colors */
  --ifm-border-color: #E1E8ED;

  /* Shadow */
  --ifm-global-shadow: 0 1px 2px rgba(0,0,0,0.08), 0 2px 4px rgba(0,0,0,0.08);
  --ifm-global-shadow-md: 0 4px 6px rgba(0,0,0,0.07);
  --ifm-global-shadow-lg: 0 10px 15px rgba(0,0,0,0.08);

  /* Spacing */
  --ifm-spacing-horizontal: 1.5rem;
  --ifm-spacing-vertical: 1.5rem;

  /* Container width */
  --ifm-container-width: 1200px;
}

/* Dark mode colors - Enhanced premium dark theme */
html[data-theme='dark'] {
  /* Premium dark theme colors */
  --ifm-color-primary: #00E6FF;         /* Electric cyan for primary actions */
  --ifm-color-primary-dark: #00B8CC;    /* Darker cyan */
  --ifm-color-primary-darker: #008BA3;  /* Even darker cyan */
  --ifm-color-primary-darkest: #005F7A; /* Darkest cyan */
  --ifm-color-primary-light: #33EEFF;   /* Lighter cyan */
  --ifm-color-primary-lighter: #66F6FF; /* Lightest cyan */
  --ifm-color-primary-lightest: #99FDFF; /* Almost white cyan */

  /* Complementary accent colors */
  --ifm-color-accent: #00E6FF;          /* Electric cyan */
  --ifm-color-accent-dark: #00B894;     /* Teal */
  --ifm-color-accent-light: #6C5CE7;    /* Violet */

  /* Background colors */
  --ifm-background-color: #0A0F18;      /* Deep charcoal background */
  --ifm-background-surface-color: #121826; /* Dark surface for cards/panels */

  /* Text colors */
  --ifm-font-color-base: #E6E6E6;       /* Bright text for readability */
  --ifm-font-color-secondary: #A0AEC0;  /* Softer secondary text */
  --ifm-font-color-tertiary: #718096;   /* Muted tertiary text */

  /* Code block colors */
  --ifm-code-background: #1A202C;       /* Dark blue-gray for code */
  --ifm-code-color: #F0F0F0;            /* Bright code text */

  /* Border colors */
  --ifm-border-color: #2D3748;          /* Subtle border color */
  --ifm-border-color-light: #4A5568;    /* Lighter border option */

  /* Enhanced shadows for depth */
  --ifm-global-shadow: 0 1px 3px rgba(0,0,0,0.2), 0 2px 6px rgba(0,0,0,0.15);
  --ifm-global-shadow-md: 0 4px 12px rgba(0,0,0,0.25);
  --ifm-global-shadow-lg: 0 10px 25px rgba(0,0,0,0.3);

  /* Gradient definitions */
  --ifm-gradient-primary: linear-gradient(135deg, #00E6FF 0%, #00B894 100%);
  --ifm-gradient-secondary: linear-gradient(135deg, #6C5CE7 0%, #00B894 100%);
  --ifm-gradient-dark: linear-gradient(135deg, #0A0F18 0%, #121826 100%);
}

/* Typography */
html {
  font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Open Sans', 'Helvetica Neue', sans-serif;
  font-size: 16px;
  line-height: 1.7;
  letter-spacing: 0.2px;
}

/* Body styles - uniform dark background */
body {
  font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Open Sans', 'Helvetica Neue', sans-serif;
  color: var(--ifm-font-color-base);
  background-color: #0f172a; /* Consistent dark blue-gray background */
  transition: background-color 0.3s ease, color 0.3s ease;
  min-height: 100vh;
}

html[data-theme='dark'] body {
  background-color: #0f172a; /* Consistent dark blue-gray background */
}

/* Enhanced container and layout */
.container {
  max-width: var(--ifm-container-width);
  margin: 0 auto;
  padding: 0 var(--ifm-spacing-horizontal);
}

/* Header and navigation enhancements */
.navbar {
  background-color: #0f172a; /* Consistent with page background */
  backdrop-filter: blur(10px);
  border-bottom: 1px solid #1e293b; /* Darker border */
  box-shadow: var(--ifm-global-shadow-md);
  padding-top: 0.75rem;
  padding-bottom: 0.75rem;
  position: sticky;
  top: 0;
  z-index: 100;
  min-height: 64px; /* Increased height for better spacing */
  align-items: center;
  justify-content: space-between; /* Ensure proper spacing between items */
}

.navbar__inner {
  display: flex;
  flex-wrap: nowrap;
  justify-content: space-between;
  align-items: center;
  width: 100%;
  max-width: var(--ifm-container-width);
  margin: 0 auto;
  padding: 0 var(--ifm-spacing-horizontal);
}

.navbar__items {
  flex-wrap: nowrap; /* Prevent wrapping */
  align-items: center;
}

.navbar__items--left {
  flex: 1;
  display: flex;
  align-items: center;
  min-width: 0; /* Allow flex item to shrink */
}

.navbar__items--right {
  flex: 0 0 auto;
  display: flex;
  align-items: center;
  margin-left: 1rem;
}

.navbar__brand {
  color: #00E6FF; /* Bright cyan for brand */
  font-weight: 700;
  font-size: 1.2rem; /* Slightly larger font */
  display: flex;
  align-items: center;
  flex: 0 0 auto; /* Don't grow or shrink */
  margin-right: 2rem; /* Add space between brand and nav items */
}

.navbar__brand svg, .navbar__logo {
  margin-right: 12px;
  width: 32px; /* Increased logo size */
  height: 32px; /* Increased logo size */
}

.navbar__title {
  font-size: 0; /* Hide the title text */
  display: none; /* Remove title from navbar */
  font-weight: 600;
}

.navbar__item {
  flex: 0 0 auto; /* Don't grow or shrink */
  margin: 0 0.25rem; /* Add consistent spacing between items */
}

.navbar__link {
  color: #e2e8f0; /* Brighter text for better visibility */
  font-weight: 500;
  transition: color 0.2s ease;
  position: relative;
  padding: 0.6rem 0.9rem;
  border-radius: 8px;
  white-space: nowrap; /* Prevent text wrapping */
  display: flex;
  align-items: center;
  font-size: 0.95rem;
  margin: 0 0.125rem; /* Small margin for click area */
}

.navbar__link:hover {
  color: #00E6FF;
  background-color: rgba(0, 230, 255, 0.1);
}

/* Navbar toggle button styling */
.navbar__toggle {
  margin-right: 1rem;
  background: transparent;
  border: none;
  color: #e2e8f0; /* Brighter color for better visibility */
  padding: 0.5rem;
  border-radius: 6px;
  transition: all 0.2s ease;
}

.navbar__toggle:hover {
  background-color: rgba(0, 230, 255, 0.1);
  color: #00E6FF;
}

/* Color mode toggle styling */
.colorModeToggle_DEke {
  margin-left: 0.5rem;
}

/* Navbar responsive behavior */
@media (max-width: 996px) {
  .navbar__inner {
    padding: 0 1rem;
  }

  .navbar-sidebar__brand {
    display: flex;
    align-items: center;
    padding: 1rem;
  }

  .navbar-sidebar__brand .navbar__title {
    font-size: 1.25rem;
    display: block;
  }

  .navbar__items--left {
    margin-right: 0;
  }
}

/* Hide title in main navbar but keep in sidebar */
.navbar__title {
  font-size: 0;
  display: none;
}

/* Enhanced navbar link active state */
.navbar__link--active {
  color: #00E6FF;
  background-color: rgba(0, 230, 255, 0.15);
  font-weight: 600;
  position: relative;
}

.navbar__link--active::after {
  content: '';
  position: absolute;
  bottom: -1px;
  left: 0;
  width: 100%;
  height: 3px;
  background: linear-gradient(90deg, #00E6FF, #00B894);
  border-radius: 2px;
}

/* Enhanced dropdown menu styling */
.dropdown__menu {
  background-color: #121826;
  border: 1px solid #1e293b;
  border-radius: 8px;
  box-shadow: var(--ifm-global-shadow-lg);
  padding: 0.5rem 0;
  min-width: 200px;
  margin-top: 0.5rem;
}

.dropdown__link {
  color: #e2e8f0; /* Brighter text for better visibility */
  padding: 0.7rem 1.2rem;
  transition: all 0.2s ease;
  border-radius: 0;
}

.dropdown__link:hover {
  color: #00E6FF;
  background-color: rgba(0, 230, 255, 0.1);
}

.dropdown__link--active {
  color: #00E6FF;
  background-color: rgba(0, 230, 255, 0.15);
  font-weight: 600;
}

/* Enhanced navbar search container */
.navbar__search-input {
  border-radius: 8px;
  border: 1px solid var(--ifm-border-color);
  padding: 0.5rem 1rem;
  background-color: rgba(255, 255, 255, 0.05);
  color: #e2e8f0; /* Brighter text for better visibility */
  transition: all 0.2s ease;
  font-size: 0.9rem;
  min-width: 200px;
}

.navbar__search-input:focus {
  outline: none;
  border-color: var(--ifm-color-accent);
  box-shadow: 0 0 0 2px rgba(0, 230, 255, 0.2);
  background-color: var(--ifm-background-surface-color);
  color: #e2e8f0; /* Ensure text remains visible when focused */
}

/* Improve text visibility in main content */
.main-wrapper {
  background-color: #0f172a; /* Consistent with page background */
  min-height: calc(100vh - 200px);
}

article {
  padding: 2rem var(--ifm-spacing-horizontal);
  background-color: #111827; /* Dark background for content */
  border-radius: 12px;
  margin: 1.5rem 0;
  border: 1px solid #1e293b; /* Darker border */
  box-shadow: var(--ifm-global-shadow);
}

/* Improve text readability */
.markdown {
  color: #e2e8f0; /* Brighter text for better readability */
}

.markdown p {
  color: #e2e8f0; /* Brighter text for better readability */
  line-height: 1.8; /* Better line spacing for readability */
}

.markdown li {
  color: #e2e8f0; /* Brighter text for better readability */
  line-height: 1.8; /* Better line spacing for readability */
}

/* Enhanced code block styling - High contrast and readability */
.markdown code {
  background-color: #1e293b; /* Dark background for inline code */
  color: #f8fafc; /* Very light text for code */
  padding: 3px 6px;
  border-radius: 4px;
  font-size: 0.95em; /* Slightly larger font */
  border: 1px solid #334155; /* Subtle border */
  font-family: 'Fira Code', 'JetBrains Mono', Consolas, monospace;
}

.markdown pre code {
  background-color: #1e293b; /* Dark background for code blocks */
  color: #f8fafc; /* Very light text for code */
  padding: 1rem;
  border-radius: 8px;
  border: 1px solid #334155; /* Slightly lighter border */
  font-size: 1rem; /* Larger readable size */
  line-height: 1.6; /* Better line spacing for code */
  display: block;
  overflow-x: auto; /* Scrollable on small screens */
  white-space: pre; /* Preserve formatting */
}

/* Code block container styling */
.codeBlockContainer_node_modules-\@docusaurus-theme-classic-lib-theme-CodeBlock-Container-styles-module {
  border-radius: 8px;
  overflow: hidden;
  box-shadow: var(--ifm-global-shadow);
  margin: 1.5rem 0;
  border: 1px solid #334155; /* Consistent border */
  background-color: #1e293b; /* Dark background */
}

.prism-code {
  font-family: 'Fira Code', 'JetBrains Mono', Consolas, monospace;
  font-size: 1rem; /* Larger readable size */
  line-height: 1.6;
  padding: 1.2rem !important;
  background-color: #1e293b !important; /* Dark background */
  color: #f8fafc !important; /* Light text */
  overflow-x: auto; /* Scrollable on small screens */
  white-space: pre; /* Preserve formatting */
}

/* Additional code block styling */
.docusaurus-highlight-code-line {
  background-color: rgba(0, 0, 0, 0.3); /* Darker highlight for current line */
  display: block;
  margin: 0 calc(-1 * var(--ifm-pre-padding));
  padding: 0 var(--ifm-pre-padding);
}

/* Ensure proper spacing between navbar items */
.navbar__items--right {
  flex: 0 0 auto;
}

.navbar__items--left {
  flex: 1 1 auto;
}

/* Sidebar enhancements */
.menu {
  background-color: #0f172a; /* Consistent with page background */
  border-right: 1px solid #1e293b; /* Darker border */
  padding: 1rem 0;
  border-radius: 0;
  box-shadow: none;
}

.menu__list {
  padding: 0 0.5rem;
}

.menu__list-item {
  margin-bottom: 0.25rem;
}

.menu__link {
  color: var(--ifm-font-color-secondary);
  font-weight: 500;
  border-radius: 6px;
  padding: 0.6rem 1rem;
  transition: all 0.2s ease;
  margin: 0.15rem 0;
}

.menu__link:hover {
  color: #00E6FF; /* Bright cyan accent */
  background-color: rgba(0, 230, 255, 0.1);
  transform: translateX(3px);
}

.menu__link--active {
  color: #00E6FF; /* Bright cyan accent */
  background-color: rgba(0, 230, 255, 0.15);
  font-weight: 600;
  border-left: 3px solid #00E6FF;
}

.theme-doc-sidebar-menu {
  padding: 0 0.5rem;
}

/* Remove scrollbars for sidebar */
.menu::-webkit-scrollbar {
  display: none; /* Hide scrollbar */
}

.menu {
  -ms-overflow-style: none;  /* IE and Edge */
  scrollbar-width: none;  /* Firefox */
}

/* Remove scroll down arrow styling */
.menu .menu__list-item:last-child {
  margin-bottom: 0;
}

/* Sidebar collapse button */
.menu__caret,
.menu__link--sublist-caret {
  color: #00E6FF;
}

.menu__link--sublist-caret:hover {
  color: #33EEFF;
}

/* Main content enhancements */
.main-wrapper {
  background-color: #0f172a; /* Consistent dark background */
  min-height: calc(100vh - 200px);
}

article {
  padding: 2rem var(--ifm-spacing-horizontal);
  background-color: #111827; /* Dark background for content */
  border-radius: 12px;
  margin: 1.5rem 0;
  border: 1px solid #1e293b; /* Darker border */
  box-shadow: var(--ifm-global-shadow);
}

/* Improve text readability */
.markdown {
  color: #e2e8f0; /* Light gray for better readability */
}

.markdown p {
  color: #e2e8f0; /* Light gray for better readability */
  line-height: 1.8; /* Better line spacing for readability */
}

.markdown li {
  color: #e2e8f0; /* Light gray for better readability */
  line-height: 1.8; /* Better line spacing for readability */
}

/* Enhanced code block styling - High contrast and readability */
.markdown code {
  background-color: #1e293b; /* Dark background for inline code */
  color: #f8fafc; /* Very light text for code */
  padding: 3px 6px;
  border-radius: 4px;
  font-size: 0.95em; /* Slightly larger font */
  border: 1px solid #334155; /* Subtle border */
  font-family: 'Fira Code', 'JetBrains Mono', Consolas, monospace;
}

.markdown pre code {
  background-color: #1e293b; /* Dark background for code blocks */
  color: #f8fafc; /* Very light text for code */
  padding: 1rem;
  border-radius: 8px;
  border: 1px solid #334155; /* Slightly lighter border */
  font-size: 1rem; /* Larger readable size */
  line-height: 1.6; /* Better line spacing for code */
  display: block;
  overflow-x: auto; /* Scrollable on small screens */
  white-space: pre; /* Preserve formatting */
}

/* Code block container styling */
.codeBlockContainer_node_modules-\@docusaurus-theme-classic-lib-theme-CodeBlock-Container-styles-module {
  border-radius: 8px;
  overflow: hidden;
  box-shadow: var(--ifm-global-shadow);
  margin: 1.5rem 0;
  border: 1px solid #334155; /* Consistent border */
  background-color: #1e293b; /* Dark background */
}

.prism-code {
  font-family: 'Fira Code', 'JetBrains Mono', Consolas, monospace;
  font-size: 1rem; /* Larger readable size */
  line-height: 1.6;
  padding: 1.2rem !important;
  background-color: #1e293b !important; /* Dark background */
  color: #f8fafc !important; /* Light text */
  overflow-x: auto; /* Scrollable on small screens */
  white-space: pre; /* Preserve formatting */
}

/* Additional code block styling */
.docusaurus-highlight-code-line {
  background-color: rgba(0, 0, 0, 0.3); /* Darker highlight for current line */
  display: block;
  margin: 0 calc(-1 * var(--ifm-pre-padding));
  padding: 0 var(--ifm-pre-padding);
}

.markdown {
  max-width: 850px;
  margin: 0 auto;
  padding: 0 1rem;
}

/* Enhanced headings with premium styling */
h1, h2, h3, h4, h5, h6 {
  color: #00E6FF; /* Bright blue-cyan accent */
  font-weight: 700;
  line-height: 1.3;
  position: relative;
}

h1 {
  font-size: 2.75rem;
  margin: 2rem 0 1.5rem 0;
  padding-bottom: 1rem;
  border-bottom: 3px solid rgba(0, 230, 255, 0.3);
  background: linear-gradient(to right, #00E6FF, #6C5CE7);
  -webkit-background-clip: text;
  -webkit-text-fill-color: transparent;
  background-clip: text;
}

h2 {
  font-size: 2.2rem;
  margin: 2.5rem 0 1.5rem 0;
  padding-left: 1.2rem;
  position: relative;
}

h2::before {
  content: '';
  position: absolute;
  left: 0;
  top: 50%;
  transform: translateY(-50%);
  width: 6px;
  height: 1.5rem;
  background: linear-gradient(135deg, #00E6FF 0%, #00B894 100%);
  border-radius: 3px;
  box-shadow: 0 0 10px rgba(0, 230, 255, 0.3);
}

h3 {
  font-size: 1.6rem;
  margin: 2rem 0 1.2rem 0;
  color: #00E6FF; /* Bright blue-cyan accent */
}

h4 {
  font-size: 1.3rem;
  margin: 1.8rem 0 1rem 0;
  color: #00E6FF; /* Bright blue-cyan accent */
}

/* Enhanced links with premium styling */
a {
  color: var(--ifm-color-accent);
  text-decoration: none;
  position: relative;
  padding-bottom: 2px;
  transition: color 0.2s ease, transform 0.2s ease;
}

a::after {
  content: '';
  position: absolute;
  bottom: 0;
  left: 0;
  width: 0;
  height: 2px;
  background: var(--ifm-color-accent);
  transition: width 0.3s ease;
}

a:hover {
  color: var(--ifm-color-primary-light);
  transform: translateY(-1px);
}

a:hover::after {
  width: 100%;
}

/* Enhanced buttons with premium styling */
.button {
  border-radius: 8px;
  padding: 0.7rem 1.5rem;
  font-weight: 600;
  transition: all 0.3s ease;
  border: none;
  cursor: pointer;
  position: relative;
  overflow: hidden;
  z-index: 1;
}

.button::before {
  content: '';
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  background: var(--ifm-gradient-primary);
  z-index: -1;
  transition: opacity 0.3s ease;
  opacity: 0;
}

.button:hover::before {
  opacity: 1;
}

.button--primary {
  background: var(--ifm-gradient-primary);
  color: white;
  border: none;
}

.button--primary:hover {
  color: white;
  transform: translateY(-2px);
  box-shadow: var(--ifm-global-shadow-lg);
}

/* Premium code block styling */
.codeBlockContainer_node_modules-\@docusaurus-theme-classic-lib-theme-CodeBlock-Container-styles-module {
  border-radius: 12px;
  overflow: hidden;
  box-shadow: var(--ifm-global-shadow-md);
  margin: 1.8rem 0;
  border: 1px solid var(--ifm-border-color);
  background-color: var(--ifm-code-background);
}

.prism-code {
  font-family: 'Fira Code', 'JetBrains Mono', Consolas, monospace;
  font-size: 0.9rem;
  line-height: 1.6;
  padding: 1.2rem !important;
  background-color: var(--ifm-code-background) !important;
  color: var(--ifm-code-color) !important;
  border-left: 4px solid var(--ifm-color-accent);
}

/* Enhanced blockquotes with premium styling */
blockquote {
  border-left: 5px solid var(--ifm-color-accent);
  background: linear-gradient(to bottom, rgba(0, 230, 255, 0.05), rgba(108, 92, 231, 0.05));
  padding: 1.5rem 1.8rem;
  margin: 2rem 0;
  border-radius: 0 8px 8px 0;
  position: relative;
  overflow: hidden;
  box-shadow: var(--ifm-global-shadow);
}

blockquote::before {
  content: '';
  position: absolute;
  top: 0;
  right: 0;
  width: 0;
  height: 0;
  border-style: solid;
  border-width: 0 40px 40px 0;
  border-color: transparent var(--ifm-color-accent) transparent transparent;
  opacity: 0.2;
}

/* Enhanced tables with premium styling */
table {
  border-radius: 12px;
  overflow: hidden;
  box-shadow: var(--ifm-global-shadow-md);
  margin: 1.8rem 0;
  background-color: var(--ifm-background-surface-color);
  border: 1px solid var(--ifm-border-color);
  width: 100%;
  max-width: 100%;
}

table th,
table td {
  padding: 0.9rem 1.2rem;
  border-bottom: 1px solid var(--ifm-border-color);
  border-right: 1px solid var(--ifm-border-color);
}

table th {
  background: var(--ifm-gradient-primary);
  color: white;
  font-weight: 600;
  text-transform: uppercase;
  font-size: 0.85rem;
  letter-spacing: 0.5px;
}

table tr:last-child td {
  border-bottom: none;
}

table td:last-child,
table th:last-child {
  border-right: none;
}

/* Premium card styling */
.card {
  border-radius: 16px;
  box-shadow: var(--ifm-global-shadow-md);
  transition: all 0.3s ease;
  overflow: hidden;
  border: none;
  background-color: var(--ifm-background-surface-color);
  border: 1px solid var(--ifm-border-color);
}

.card:hover {
  transform: translateY(-8px);
  box-shadow: var(--ifm-global-shadow-lg);
  border: 1px solid var(--ifm-color-accent);
}

.card__header {
  background: var(--ifm-gradient-primary);
  color: white;
  padding: 1rem 1.5rem;
}

.card__body {
  padding: 1.5rem;
}

/* Enhanced footer */
.footer {
  background: #0f172a; /* Consistent with page background */
  border-top: 1px solid #1e293b; /* Darker border */
  padding: 3rem 0 2rem;
  margin-top: 4rem;
}

.footer__title {
  color: var(--ifm-color-primary);
  font-weight: 600;
  margin-bottom: 1.5rem;
  position: relative;
  padding-bottom: 0.5rem;
}

.footer__title::after {
  content: '';
  position: absolute;
  bottom: 0;
  left: 0;
  width: 50px;
  height: 3px;
  background: var(--ifm-gradient-primary);
  border-radius: 2px;
}

.footer__links {
  margin-bottom: 2rem;
}

.footer__item a {
  color: var(--ifm-font-color-secondary);
  transition: color 0.2s ease;
}

.footer__item a:hover {
  color: var(--ifm-color-accent);
}

.footer--dark {
  --ifm-footer-background-color: var(--ifm-gradient-dark);
}

/* Enhanced sidebar TOC */
.table-of-contents {
  border-left: 3px solid var(--ifm-color-accent);
  padding-left: 1rem;
}

.table-of-contents li {
  margin-bottom: 0.5rem;
}

.table-of-contents__link {
  color: var(--ifm-font-color-secondary);
  transition: all 0.2s ease;
}

.table-of-contents__link:hover,
.table-of-contents__link--active {
  color: var(--ifm-color-primary);
  transform: translateX(3px);
}

/* Theme selection toggle */
.react-toggle-track {
  background-color: var(--ifm-background-surface-color) !important;
  border: 1px solid var(--ifm-border-color) !important;
}

.react-toggle-thumb {
  background-color: var(--ifm-color-accent) !important;
}

/* Code tabs */
.tabs__item {
  border-radius: 6px;
  margin: 0 3px;
  padding: 0.6rem 1.2rem;
  transition: all 0.2s ease;
}

.tabs__item:hover {
  background-color: rgba(0, 230, 255, 0.1);
}

.tabs__item--active {
  background: var(--ifm-gradient-primary);
  color: white;
  font-weight: 600;
}

/* Pagination controls */
.pagination-nav__link {
  border-radius: 12px;
  border: 1px solid var(--ifm-border-color);
  transition: all 0.3s ease;
  overflow: hidden;
  position: relative;
  z-index: 1;
}

.pagination-nav__link:hover {
  border-color: var(--ifm-color-accent);
  transform: translateY(-3px);
  box-shadow: var(--ifm-global-shadow-lg);
}

/* Search bar enhancements */
.navbar__search-input {
  border-radius: 8px;
  border: 1px solid var(--ifm-border-color);
  padding: 0.5rem 1rem;
  background-color: rgba(255, 255, 255, 0.05);
  color: var(--ifm-font-color-base);
  transition: all 0.2s ease;
}

.navbar__search-input:focus {
  outline: none;
  border-color: var(--ifm-color-accent);
  box-shadow: 0 0 0 2px rgba(0, 230, 255, 0.2);
  background-color: var(--ifm-background-surface-color);
}

/* Chatbot container styling to match the theme */
.chatbot-container {
  position: fixed;
  bottom: 25px;
  right: 25px;
  z-index: 1000;
}

.chatbot-toggle-btn {
  width: 70px;
  height: 70px;
  border-radius: 50%;
  background: var(--ifm-gradient-primary);
  color: white;
  border: none;
  display: flex;
  align-items: center;
  justify-content: center;
  cursor: pointer;
  box-shadow: var(--ifm-global-shadow-lg);
  transition: all 0.3s cubic-bezier(0.23, 1, 0.32, 1);
  font-size: 1.6rem;
  font-weight: bold;
  overflow: hidden;
  position: relative;
}

.chatbot-toggle-btn::before {
  content: '';
  position: absolute;
  top: -50%;
  left: -50%;
  width: 200%;
  height: 200%;
  background: radial-gradient(circle, rgba(255,255,255,0.4) 0%, transparent 70%);
  transform: scale(0);
  transition: transform 0.5s ease;
}

.chatbot-toggle-btn:hover {
  transform: translateY(-3px) scale(1.05);
  box-shadow: 0 10px 25px rgba(0, 230, 255, 0.3);
}

.chatbot-toggle-btn:hover::before {
  transform: scale(1);
}

.chatbot-window {
  width: 400px;
  height: 600px;
  background-color: #111827; /* Consistent dark background */
  border-radius: 16px;
  box-shadow: var(--ifm-global-shadow-lg);
  display: flex;
  flex-direction: column;
  overflow: hidden;
  border: 1px solid #1e293b; /* Consistent border */
  backdrop-filter: blur(10px);
}

.chatbot-header {
  background: var(--ifm-gradient-primary);
  color: white;
  padding: 18px 20px;
  display: flex;
  justify-content: space-between;
  align-items: center;
  border-bottom: 1px solid var(--ifm-border-color);
}

/* Hide sidebar expand/collapse arrows */
.menu__caret,
.menu__link--sublist::after {
  display: none !important;
}

============================================================
FILE: book\src\pages\index.md
============================================================
---
title: Physical AI & Humanoid Robotics
---

# Physical AI & Humanoid Robotics

Welcome to the comprehensive guide to Physical AI and Humanoid Robotics. This book provides a hands-on, project-driven approach to understanding and implementing embodied intelligence in humanoid robotic systems.

## About This Book

This book is designed for students and practitioners with intermediate Python skills and basic AI knowledge who want to understand the intersection of artificial intelligence and physical systems. You'll learn how to build intelligent robots that can perceive, reason, and act in the real world.

## What You'll Learn

- Fundamentals of Robot Operating System (ROS 2)
- Robot modeling and simulation techniques
- Sensor integration and perception systems
- Motion planning and control algorithms
- Reinforcement learning for robotics
- Vision-language-action models (VLA)
- Human-robot interaction principles
- Multi-robot coordination and deployment

## How to Use This Book

Each chapter follows a consistent structure:
- **Learning Goals**: What you'll understand by the end of the chapter
- **Key Technologies**: Tools and frameworks covered
- **Practical Outcomes**: Hands-on projects to implement
- **Code Samples**: Ready-to-run examples
- **Labs**: Step-by-step exercises

## Getting Started

Before diving into the content, ensure you have the following prerequisites installed:
- Python 3.8+
- ROS 2 Humble Hawksbill
- Gazebo simulation environment
- Basic knowledge of linear algebra and calculus

Start with [Chapter 1: Introduction to Physical AI & Embodied Intelligence](./docs/part-i-foundations/chapter-1-introduction/index.md) to begin your journey into physical AI.

============================================================
FILE: book\src\pages\LayoutWrapper.js
============================================================
import React from 'react';
import Layout from '@theme/Layout';
import ChatbotIntegration from '@site/src/components/ChatbotIntegration';

export default function LayoutWrapper(props) {
  return (
    <Layout {...props}>
      {props.children}
      <ChatbotIntegration />
    </Layout>
  );
}

============================================================
FILE: book\src\pages\mdxPage.js
============================================================
import React from 'react';
import clsx from 'clsx';
import Layout from '@theme/Layout';
import { ChatbotIntegration } from '../components/ChatbotIntegration';

export default function MDXPage(props) {
  const { content: MDXContent } = props;
  const {
    metadata: { title, description },
  } = MDXContent;

  return (
    <Layout title={title} description={description}>
      <main className="container margin-vert--lg">
        <div className="row">
          <div className={clsx('col', 'col--8', 'col--offset-2')}>
            <MDXContent />
          </div>
        </div>
      </main>
      <ChatbotIntegration />
    </Layout>
  );
}

============================================================
FILE: book\src\theme\Layout.js
============================================================
import React from 'react';
import OriginalLayout from '@theme-original/Layout';
import Chatbot from '@site/src/components/Chatbot';

export default function Layout(props) {
  return (
    <>
      <OriginalLayout {...props}>
        {props.children}
      </OriginalLayout>
      <Chatbot />
    </>
  );
}

============================================================
FILE: book\src\utils\proxyConfig.js
============================================================
// Development server configuration with proxy
const path = require('path');

// Proxy configuration for development
const proxyConfig = {
  '/api/**': {
    target: 'http://localhost:8000',
    changeOrigin: true,
    pathRewrite: {
      '^/api': '', // Remove /api prefix when forwarding
    },
  },
};

module.exports = {
  proxyConfig
};

============================================================
FILE: history\prompts\book-structure\0002-book-structure-spec.spec.prompt.md
============================================================
---
id: 2
title: "Book Structure Specification"
stage: spec
date_iso: 2025-12-18
surface: agent
model: claude-sonnet-4-5-20250929
feature: book-structure
branch: 1-book-structure
user: Dell
command: "/sp.specify"
labels: ["specification", "robotics", "ai", "curriculum", "education"]
links:
  spec: "specs/1-book-structure/spec.md"
  ticket: null
  adr: null
  pr: null
files_yaml:
  - "specs/1-book-structure/spec.md"
  - "specs/1-book-structure/checklists/requirements.md"
tests_yaml:
  - "none"
---

# Book Structure Specification

## Prompt Text

Specify the full book structure based on the provided curriculum.
Break it into:
- Parts
- Chapters
- Sections
For each chapter, define:
- Learning goals
- Key technologies
- Practical outcomes
Align with ROS 2, Gazebo, NVIDIA Isaac, and VLA.

## Response Text

Created a comprehensive specification for the Physical AI & Humanoid Robotics book structure with 5 parts, 15 chapters covering foundations, perception, motion control, intelligence, and integration. Each chapter includes specific learning goals, key technologies, and practical outcomes aligned with ROS 2, Gazebo, NVIDIA Isaac, and VLA.

## Outcome Impact

Successfully specified a complete book structure that aligns with the curriculum goals, providing a comprehensive learning path from foundational concepts to advanced implementations in humanoid robotics.

## Tests Summary

none

## Files Summary

- specs/1-book-structure/spec.md: Complete specification for the book structure with 5 parts and 15 chapters
- specs/1-book-structure/checklists/requirements.md: Quality checklist for the specification

## Next Prompts

- Create implementation plan for the book structure
- Develop detailed tasks for each chapter
- Design project-based learning activities

## Reflection Note

The specification provides a comprehensive and well-structured curriculum that progresses logically from basic concepts to advanced implementations in humanoid robotics.

## Failure Modes Observed

None

## Next Experiment to Improve Prompt Quality

N/A

============================================================
FILE: history\prompts\book-structure\0003-book-structure-clarify.clarify.prompt.md
============================================================
---
id: 3
title: "Book Structure Clarifications"
stage: clarify
date_iso: 2025-12-18
surface: agent
model: claude-sonnet-4-5-20250929
feature: book-structure
branch: 1-book-structure
user: Dell
command: "/sp.clarify"
labels: ["clarification", "robotics", "ai", "curriculum", "education"]
links:
  spec: "specs/1-book-structure/spec.md"
  ticket: null
  adr: null
  pr: null
files_yaml:
  - "specs/1-book-structure/spec.md"
tests_yaml:
  - "none"
---

# Book Structure Clarifications

## Prompt Text

Review the specification and clarify ambiguities.
Answer:
- What assumptions are made about student background?
- What is simulated vs real hardware?
- What tools are mandatory vs optional?
- Where cloud vs local compute is used?
Refine scope where needed.

## Response Text

Successfully clarified key ambiguities in the book structure specification: student prerequisites (intermediate Python skills and basic ML knowledge), simulation vs hardware approach (primarily simulation-based with optional real hardware extensions), tool requirements (ROS 2 and Gazebo mandatory; NVIDIA Isaac and VLA optional), and computing approach (primarily local with optional cloud resources).

## Outcome Impact

The specification now clearly defines student prerequisites, mandatory vs optional tools, simulation vs hardware approach, and computing requirements, enabling precise implementation planning.

## Tests Summary

none

## Files Summary

- specs/1-book-structure/spec.md: Updated with clarifications about student background, simulation vs hardware, mandatory vs optional tools, and computing approach

## Next Prompts

- Create implementation plan for the clarified book structure
- Develop detailed tasks for each chapter
- Design project-based learning activities

## Reflection Note

The clarifications significantly improved the specification by removing key ambiguities about prerequisites, tools, and approach, making it ready for implementation planning.

## Failure Modes Observed

None

## Next Experiment to Improve Prompt Quality

N/A

============================================================
FILE: history\prompts\book-structure\0004-book-implementation-plan.plan.prompt.md
============================================================
---
id: 4
title: "Book Implementation Plan"
stage: plan
date_iso: 2025-12-18
surface: agent
model: claude-sonnet-4-5-20250929
feature: book-structure
branch: 1-book-structure
user: Dell
command: "/sp.plan"
labels: ["planning", "robotics", "ai", "curriculum", "docusaurus"]
links:
  spec: "specs/1-book-structure/spec.md"
  ticket: null
  adr: null
  pr: null
files_yaml:
  - "specs/1-book-structure/plan.md"
  - "specs/1-book-structure/research.md"
  - "specs/1-book-structure/quickstart.md"
tests_yaml:
  - "none"
---

# Book Implementation Plan

## Prompt Text

Create a writing and build plan for the book.
Include:
- Chapter writing order
- Tool usage (Docusaurus, GitHub Pages)
- Where code samples, diagrams, and labs appear
- Milestones for draft, review, and publish
Optimize for solo author workflow.

## Response Text

Created comprehensive implementation plan for the Physical AI & Humanoid Robotics book including writing order, Docusaurus/GitHub Pages tool usage, content structure, milestones, and solo author workflow optimization. Included supporting research and quickstart guide.

## Outcome Impact

Successfully planned the complete implementation approach for the robotics book with clear workflow, tooling, and timeline optimized for solo author.

## Tests Summary

none

## Files Summary

- specs/1-book-structure/plan.md: Complete implementation plan with architecture, workflow, and milestones
- specs/1-book-structure/research.md: Research document supporting the implementation decisions
- specs/1-book-structure/quickstart.md: Quickstart guide for setting up and working with the book project

## Next Prompts

- Create detailed tasks for book implementation
- Develop specific chapter templates
- Design review and quality assurance process

## Reflection Note

The implementation plan provides a comprehensive roadmap for creating the robotics book with attention to both technical requirements and workflow optimization for a solo author.

## Failure Modes Observed

None

## Next Experiment to Improve Prompt Quality

N/A

============================================================
FILE: history\prompts\book-structure\0005-book-tasks-breakdown.tasks.prompt.md
============================================================
---
id: 5
title: "Book Tasks Breakdown"
stage: tasks
date_iso: 2025-12-18
surface: agent
model: claude-sonnet-4-5-20250929
feature: book-structure
branch: 1-book-structure
user: Dell
command: "/sp.tasks"
labels: ["tasks", "robotics", "ai", "curriculum", "docusaurus"]
links:
  spec: "specs/1-book-structure/spec.md"
  plan: "specs/1-book-structure/plan.md"
  ticket: null
  adr: null
  pr: null
files_yaml:
  - "specs/1-book-structure/tasks.md"
tests_yaml:
  - "none"
---

# Book Tasks Breakdown

## Prompt Text

Break the plan into executable tasks.
For each task include:
- Task description
- Input
- Output
- Tool used (Claude Code, Spec-Kit, GitHub, Docusaurus)
Tasks should be small, clear, and sequential.

## Response Text

Created comprehensive task breakdown for the Physical AI & Humanoid Robotics book with 95 specific tasks across 8 phases, including setup, foundational, and chapter-specific tasks for all 15 chapters. Each task follows the required format with clear descriptions and tool specifications.

## Outcome Impact

Successfully decomposed the book implementation plan into executable tasks with clear dependencies, parallel execution opportunities, and quality gates for systematic development.

## Tests Summary

none

## Files Summary

- specs/1-book-structure/tasks.md: Complete task breakdown with 95 specific tasks organized by phase and user story

## Next Prompts

- Begin implementation of setup tasks (T001-T005)
- Create initial Docusaurus project structure
- Start foundational content development

## Reflection Note

The task breakdown provides a comprehensive roadmap for implementing the robotics book with clear sequential and parallel execution paths optimized for solo author workflow.

## Failure Modes Observed

None

## Next Experiment to Improve Prompt Quality

N/A

============================================================
FILE: history\prompts\book-structure\0006-book-implementation.green.prompt.md
============================================================
---
id: 6
title: "Book Implementation Started"
stage: green
date_iso: 2025-12-18
surface: agent
model: claude-sonnet-4-5-20250929
feature: book-structure
branch: 1-book-structure
user: Dell
command: "/sp.implement"
labels: ["implementation", "robotics", "ai", "curriculum", "docusaurus"]
links:
  spec: "specs/1-book-structure/spec.md"
  plan: "specs/1-book-structure/plan.md"
  tasks: "specs/1-book-structure/tasks.md"
  ticket: null
  adr: null
  pr: null
files_yaml:
  - "book/docusaurus.config.js"
  - "book/sidebars.js"
  - "book/src/pages/index.md"
  - "book/docs/part-i-foundations/chapter-1-introduction/index.md"
  - "book/src/css/custom.css"
  - "book/package.json"
  - "book/README.md"
  - "book/.github/workflows/deploy.yml"
tests_yaml:
  - "none"
---

# Book Implementation Started

## Prompt Text

Start implementing the book.
Generate:
- Docusaurus folder structure
- Initial docs markdown files
- Home page (index.md)
- First chapter content:
  "Introduction to Physical AI & Embodied Intelligence"
Use clean markdown and production-ready content.

## Response Text

Successfully implemented the initial Docusaurus project structure for the Physical AI & Humanoid Robotics book, including configuration files, home page, and the complete first chapter on Introduction to Physical AI & Embodied Intelligence. All requested components were created with production-ready content.

## Outcome Impact

Successfully established the foundation for the robotics book with complete Docusaurus setup, navigation structure, and initial content, enabling continued development of the remaining chapters.

## Tests Summary

none

## Files Summary

- book/docusaurus.config.js: Docusaurus configuration with proper settings for the book
- book/sidebars.js: Navigation sidebar with complete structure for all 15 chapters
- book/src/pages/index.md: Home page with book overview and getting started information
- book/docs/part-i-foundations/chapter-1-introduction/index.md: Complete first chapter with learning goals, code samples, and lab exercise
- book/src/css/custom.css: Custom styling for the book
- book/package.json: Dependencies and scripts for the Docusaurus project
- book/README.md: Project documentation and setup instructions
- book/.github/workflows/deploy.yml: GitHub Actions workflow for deployment

## Next Prompts

- Continue with remaining chapters following the same structure
- Create diagrams for Chapter 1 concepts
- Implement Chapter 2: Robot Operating System (ROS 2) Fundamentals

## Reflection Note

The implementation successfully created a complete foundation for the robotics book with proper structure, navigation, and initial content that follows the specified learning objectives.

## Failure Modes Observed

None

## Next Experiment to Improve Prompt Quality

N/A

============================================================
FILE: history\prompts\constitution\0001-physical-ai-humanoid-robotics-constitution.constitution.prompt.md
============================================================
---
id: 1
title: "Physical AI & Humanoid Robotics Constitution"
stage: constitution
date_iso: 2025-12-18
surface: agent
model: claude-sonnet-4-5-20250929
feature: none
branch: master
user: Dell
command: "/sp.constitution"
labels: ["constitution", "robotics", "ai", "education", "curriculum"]
links:
  spec: null
  ticket: null
  adr: null
  pr: null
files_yaml:
  - ".specify/memory/constitution.md"
tests_yaml:
  - "none"
---

# Physical AI & Humanoid Robotics Constitution

## Prompt Text

Act as an expert curriculum designer and technical author.
Create a clear constitution for a book titled:
"Physical AI & Humanoid Robotics".

Principles:
- Technically accurate, industry-aligned
- Beginner-to-advanced flow
- Hands-on, project-driven learning
- Open-source friendly
- Written for students with Python & AI background
Define tone, audience, scope, and non-goals.

## Response Text

Created a comprehensive constitution for the "Physical AI & Humanoid Robotics" book with detailed principles, target audience, scope definition, and development workflow.

## Outcome Impact

Successfully created a project constitution that defines the core principles, audience, scope, and standards for a Physical AI & Humanoid Robotics book. The constitution includes technical accuracy requirements, learning flow structure, hands-on project approach, and open-source focus.

## Tests Summary

none

## Files Summary

- .specify/memory/constitution.md: Updated with the complete constitution for the Physical AI & Humanoid Robotics book

## Next Prompts

- Create initial specification for the first chapter
- Develop learning objectives for core concepts
- Design project-based curriculum structure

## Reflection Note

The constitution successfully captures the key requirements for a Physical AI & Humanoid Robotics curriculum, balancing technical accuracy with accessibility for students who have Python and AI backgrounds.

## Failure Modes Observed

None

## Next Experiment to Improve Prompt Quality

N/A


============================================================
FILE: rag_backend\check_qdrant.py
============================================================
from qdrant_client import QdrantClient
import inspect

# Create a local Qdrant client
client = QdrantClient(':memory:')

# Print all methods
all_attrs = dir(client)
methods = []
for attr in all_attrs:
    if not attr.startswith('_'):
        obj = getattr(client, attr)
        if callable(obj):
            methods.append(attr)

print("Available methods:", methods)

# Check specifically for search-related methods
search_methods = [m for m in methods if 'search' in m.lower()]
print("Search-related methods:", search_methods)

# Check if 'search' method exists
has_search = hasattr(client, 'search')
print(f"Has 'search' method: {has_search}")

============================================================
FILE: rag_backend\debug_rag.py
============================================================
import sys
import os
import traceback

# Add current directory to path
sys.path.insert(0, os.path.dirname(os.path.abspath('.')))

print("Testing RAGService initialization...")

try:
    from api.rag_service import RAGService
    print("Import successful")
    
    print("Creating RAGService instance...")
    rag_service = RAGService()
    print("RAGService created successfully")
    
    print("Testing get_answer method...")
    import asyncio
    
    async def test():
        try:
            result = await rag_service.get_answer('test query')
            print(f"get_answer result: {result}")
        except Exception as e:
            print(f"Error in get_answer: {e}")
            traceback.print_exc()
    
    asyncio.run(test())
    print("Test completed successfully")
    
except Exception as e:
    print(f"Error during initialization: {e}")
    traceback.print_exc()

============================================================
FILE: rag_backend\main.py
============================================================
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import Dict, Optional
import logging
import os
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Import the RAG service
from api.rag_service import RAGService

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class QuestionRequest(BaseModel):
    question: Optional[str] = None
    query: Optional[str] = None
    strict_mode: bool = False

    @property
    def text(self) -> str:
        return self.question or self.query or ""


app = FastAPI()

# Add CORS middleware to allow requests from the frontend
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000", "http://127.0.0.1:3000", "http://localhost:8000", "http://127.0.0.1:8000", "http://127.0.0.1:3001", "http://localhost:3001"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Initialize the RAG service globally
rag_service = RAGService()

@app.post("/chat")
async def chat(request: QuestionRequest) -> Dict[str, str]:
    """
    Chat endpoint that accepts a question and returns a response using RAG.
    """
    try:
        # Initialize documents if not already done
        await rag_service.initialize_docs()

        # Get the answer using RAG
        result = await rag_service.get_answer(request.text, strict_mode=request.strict_mode)

        return {"response": result["answer"]}
    except Exception as e:
        logger.error(f"Error processing chat request: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error processing request: {str(e)}")


@app.post("/ask")
async def ask(request: QuestionRequest) -> Dict[str, str]:
    """
    Ask endpoint that accepts a question and returns a response using RAG.
    """
    try:
        # Initialize documents if not already done
        await rag_service.initialize_docs()

        # Get the answer using RAG
        result = await rag_service.get_answer(request.text, strict_mode=request.strict_mode)

        return {"response": result["answer"]}
    except Exception as e:
        logger.error(f"Error processing ask request: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error processing request: {str(e)}")


@app.get("/")
async def root():
    return {"message": "Robotics AI Backend is running!"}


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="127.0.0.1", port=8000)

============================================================
FILE: rag_backend\README.md
============================================================
# Physical AI & Humanoid Robotics - RAG System

Complete Retrieval-Augmented Generation (RAG) system for the "Physical AI & Humanoid Robotics" textbook.

## Overview

This system consists of two main components:
1. **Backend**: Python FastAPI application with document indexing and Qwen-powered chat
2. **Frontend**: Simple web interface for interacting with the RAG system

## Architecture

```
rag_backend/
‚îú‚îÄ‚îÄ main.py                 # FastAPI application entry point
‚îú‚îÄ‚îÄ requirements.txt        # Python dependencies
‚îú‚îÄ‚îÄ .env.example           # Environment variables template
‚îú‚îÄ‚îÄ data_ingestion/        # Document loading and chunking
‚îÇ   ‚îî‚îÄ‚îÄ loader.py
‚îú‚îÄ‚îÄ embeddings/            # Embedding generation
‚îÇ   ‚îî‚îÄ‚îÄ generator.py
‚îú‚îÄ‚îÄ vector_db/             # Qdrant vector database interface
‚îÇ   ‚îî‚îÄ‚îÄ qdrant.py
‚îî‚îÄ‚îÄ api/                   # API endpoints and RAG service
    ‚îî‚îÄ‚îÄ rag_service.py

rag_frontend/
‚îú‚îÄ‚îÄ index.html             # Main web interface
‚îî‚îÄ‚îÄ README.md              # Frontend documentation
```

## Setup Instructions

### Backend Setup

1. Install Python dependencies:
```bash
cd rag_backend
pip install -r requirements.txt
```

2. Copy and configure environment variables:
```bash
cp .env.example .env
# Edit .env with your API keys and configurations
```

3. Run the backend server:
```bash
python main.py
# Server will start on http://localhost:8000
```

### Frontend Setup

1. Simply open `rag_frontend/index.html` in your web browser
2. Ensure the backend server is running on `http://localhost:8000`

## Configuration

### Environment Variables

- `OPENROUTER_API_KEY`: Your OpenRouter API key for accessing Qwen models
- `CHAT_MODEL`: Model to use for chat (default: `qwen/qwen-2-72b-instruct`)
- `EMBEDDING_MODEL`: Model to use for embeddings (default: `nvidia/nv-embed-v1`)
- `QDRANT_URL`: URL for Qdrant vector database (default: `http://localhost:6333`)
- `QDRANT_API_KEY`: API key for Qdrant (if using cloud version)
- `QDRANT_COLLECTION_NAME`: Name of the collection to store embeddings (default: `book_docs`)

### Document Sources

The system automatically loads documents from `../../book/docs/` relative to the backend directory. Ensure your textbook content is available in this location as markdown files.

## API Endpoints

- `GET /` - Health check
- `POST /chat` - Main chat endpoint
  - Request: `{"query": "your question", "top_k": 5}`
  - Response: `{"answer": "the answer", "sources": ["source1", "source2"], "query": "your question"}`

## Usage Notes

- On first run, the system will index all documents from the book/docs directory
- Subsequent queries will use the indexed information to generate answers
- If an answer is not found in the book, the system responds with "Answer not found in the book."
- The system uses cosine similarity for document retrieval from the vector database

## Troubleshooting

- If documents don't load, verify the `book/docs/` path is accessible
- Check that your OpenRouter API key is valid and has sufficient credits
- Ensure Qdrant is running if using a local instance
- Check logs in the backend for detailed error information

============================================================
FILE: rag_backend\requirements.txt
============================================================
fastapi==0.104.1
uvicorn[standard]==0.24.0
qdrant-client==1.8.0
python-dotenv==1.0.0
numpy
aiohttp==3.9.0
pydantic
requests==2.31.0

============================================================
FILE: rag_backend\run_server.py
============================================================
import sys
import os
sys.path.insert(0, os.path.dirname(os.path.abspath('.')))

from simple_server import app
import uvicorn

print('Starting server on http://127.0.0.1:8000')
print('Server is running. Press Ctrl+C to stop.')
try:
    uvicorn.run(app, host='127.0.0.1', port=8000, log_level='info')
except KeyboardInterrupt:
    print("Server stopped by user")
except Exception as e:
    print(f"Error running server: {e}")
    import traceback
    traceback.print_exc()

============================================================
FILE: rag_backend\simple_server.py
============================================================
from fastapi import FastAPI, HTTPException, Depends
from pydantic import BaseModel
from typing import List, Optional
import logging
import asyncio
import os
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

from fastapi.middleware.cors import CORSMiddleware

app = FastAPI(
    title="Physical AI & Humanoid Robotics RAG API",
    description="RAG system for answering questions about Physical AI & Humanoid Robotics textbook",
    version="1.0.0"
)

# Add CORS middleware to allow requests from the frontend
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000", "http://127.0.0.1:3000", "http://localhost:8000", "http://127.0.0.1:8000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Import after setting up logging and environment
from api.rag_service import RAGService

# Initialize the RAG service
rag_service = RAGService()

class QuestionRequest(BaseModel):
    question: str
    top_k: Optional[int] = 5


class ChatRequest(QuestionRequest):
    pass

class ChatResponse(BaseModel):
    answer: str
    sources: List[str]
    query: str

@app.get("/")
def health_check():
    """Health check endpoint"""
    return {"status": "healthy", "service": "RAG API"}

@app.post("/chat", response_model=ChatResponse)
async def chat(request: QuestionRequest):
    """
    Main chat endpoint that processes user queries using RAG
    """
    try:
        result = await rag_service.get_answer(request.question, request.top_k)
        return ChatResponse(**result)
    except HTTPException:
        # Re-raise HTTP exceptions as-is
        raise
    except Exception as e:
        logger.error(f"Unexpected error in chat endpoint: {str(e)}")
        raise HTTPException(status_code=500, detail="Internal server error")

@app.get("/docs/sources")
def get_document_sources():
    """Get list of all document sources in the vector database"""
    return {"sources": [], "message": "Source listing not implemented in this version"}

if __name__ == "__main__":
    import uvicorn
    port = int(os.getenv("PORT", 8000))
    host = os.getenv("HOST", "127.0.0.1")
    print(f"Starting server on http://{host}:{port}")
    print("Server is running. Press Ctrl+C to stop.")
    uvicorn.run(app, host=host, port=port)

============================================================
FILE: rag_backend\start_server.py
============================================================
import uvicorn
import sys
import os
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

try:
    # Import after loading environment - use simple_server which has full RAG implementation
    from simple_server import app

    # Get port from environment variable, default to 8000
    port = int(os.getenv("PORT", 8000))
    host = os.getenv("HOST", "0.0.0.0")  # Use 0.0.0.0 for Railway deployment

    print("Application imported successfully")
    print(f"Starting server on http://{host}:{port}")

    # Start the server with dynamic port
    uvicorn.run(app, host=host, port=port, log_level='info')

except Exception as e:
    print(f"Error starting server: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)

============================================================
FILE: rag_backend\test_api.py
============================================================
import requests
import json

# Test the chat API endpoint
url = "http://localhost:8000/chat"
headers = {
    "Content-Type": "application/json"
}

# Test payload - using the correct field name according to QuestionRequest model
payload = {
    "question": "What is Physical AI?"
}

try:
    response = requests.post(url, headers=headers, data=json.dumps(payload))
    print(f"Status Code: {response.status_code}")
    print(f"Response: {response.text}")
except Exception as e:
    print(f"Error: {e}")

# Also test the /ask endpoint
url = "http://localhost:8000/ask"
try:
    response = requests.post(url, headers=headers, data=json.dumps(payload))
    print(f"Status Code (/ask): {response.status_code}")
    print(f"Response (/ask): {response.text}")
except Exception as e:
    print(f"Error (/ask): {e}")

============================================================
FILE: rag_backend\test_connection.py
============================================================
import requests
import json

# Test the chat API endpoint with timeout
url = 'http://localhost:8000/chat'
headers = {'Content-Type': 'application/json'}
payload = {'query': 'Hello', 'top_k': 5}

try:
    print("Sending request to:", url)
    response = requests.post(url, headers=headers, data=json.dumps(payload), timeout=10)
    print(f'Status Code: {response.status_code}')
    print(f'Response: {response.text}')
except requests.exceptions.Timeout:
    print('Request timed out - server might not be responding')
except requests.exceptions.ConnectionError as e:
    print(f'Connection error - server might not be running: {e}')
except Exception as e:
    print(f'Error: {e}')

============================================================
FILE: rag_backend\test_inner_qdrant.py
============================================================
from qdrant_client import QdrantClient
from qdrant_client.local.qdrant_local import QdrantLocal

# Create a local Qdrant client
client = QdrantClient(':memory:')

# Check the actual type
print(f"Client type: {type(client)}")
print(f"Client _client type: {type(client._client)}")

# Check if _client has search
if hasattr(client._client, 'search'):
    print("client._client has search method")
else:
    print("client._client does NOT have search method")

# Check methods of the inner client
inner_methods = [m for m in dir(client._client) if not m.startswith('_') and callable(getattr(client._client, m))]
print(f"Inner client methods: {inner_methods[:10]}")

# Check specifically for search in inner client
search_methods = [m for m in inner_methods if 'search' in m.lower()]
print(f"Search methods in inner client: {search_methods}")

============================================================
FILE: rag_backend\test_qdrant.py
============================================================
from qdrant_client import QdrantClient
from qdrant_client.http.models import PointStruct, VectorParams, Distance
import uuid

# Create a local Qdrant client
client = QdrantClient(
    url="https://499fc875-1fc3-46b9-8d0c-ebc329798849.us-east4-0.gcp.cloud.qdrant.io",
    api_key="eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.1gabqxivh6-T706S8O8YrrO6haY1nm1crNdD-YUl1SU"
)


# Create a collection
collection_name ="book_docs"
vector_size = 1536

try:
    client.create_collection(
        collection_name=collection_name,
        vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE),
    )
    print("Collection created successfully")
except Exception as e:
    print(f"Error creating collection: {e}")

# Add a test point
test_embedding = [0.1] * vector_size  # Simple test embedding
test_point = PointStruct(
    id=str(uuid.uuid4()),
    vector=test_embedding,
    payload={
        "content": "Test content",
        "source": "test_source",
        "title": "test_title",
        "chunk_id": "test_chunk"
    }
)

try:
    client.upsert(
        collection_name=collection_name,
        points=[test_point]
    )
    print("Point added successfully")
except Exception as e:
    print(f"Error adding point: {e}")

# Try to search
try:
    results = client.search(
        collection_name=collection_name,
        query_vector=test_embedding,
        limit=5
    )
    print(f"Search successful, found {len(results)} results")
except Exception as e:
    print(f"Error during search: {e}")
    print("Available methods on client object:")
    # Try to get methods in a safer way
    safe_attrs = []
    for attr in dir(client):
        if not attr.startswith('_'):
            try:
                obj = getattr(client, attr)
                if callable(obj):
                    safe_attrs.append(attr)
            except:
                continue
    print(safe_attrs[:20])  # First 20 attributes

============================================================
FILE: rag_backend\test_server.py
============================================================
from fastapi import FastAPI, HTTPException, Depends
from pydantic import BaseModel
from typing import List, Optional
import logging
import asyncio
import os
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(
    title="Physical AI & Humanoid Robotics RAG API",
    description="RAG system for answering questions about Physical AI & Humanoid Robotics textbook",
    version="1.0.0"
)

# Import after setting up logging and environment
from api.rag_service import RAGService

# Initialize the RAG service
rag_service = RAGService()

class ChatRequest(BaseModel):
    query: str
    top_k: Optional[int] = 5

class ChatResponse(BaseModel):
    answer: str
    sources: List[str]
    query: str

@app.get("/")
def health_check():
    """Health check endpoint"""
    return {"status": "healthy", "service": "RAG API"}

@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """
    Main chat endpoint that processes user queries using RAG
    """
    try:
        result = await rag_service.get_answer(request.query, request.top_k)
        return ChatResponse(**result)
    except HTTPException:
        # Re-raise HTTP exceptions as-is
        raise
    except Exception as e:
        logger.error(f"Unexpected error in chat endpoint: {str(e)}")
        raise HTTPException(status_code=500, detail="Internal server error")

@app.get("/docs/sources")
def get_document_sources():
    """Get list of all document sources in the vector database"""
    return {"sources": [], "message": "Source listing not implemented in this version"}

if __name__ == "__main__":
    import uvicorn
    port = int(os.getenv("PORT", 8000))
    host = os.getenv("HOST", "0.0.0.0")
    
    # Don't run initialization on startup to avoid connection errors
    uvicorn.run(app, host=host, port=port)

============================================================
FILE: rag_backend\__init__.py
============================================================
  


============================================================
FILE: rag_backend\api\rag_service.py
============================================================
# # rag_backend\api\rag_service.py
# from fastapi import HTTPException
# import logging
# from typing import List, Dict
# import asyncio
# import os
# from dotenv import load_dotenv

# from data_ingestion.loader import DocumentLoader
# from embeddings.generator import EmbeddingGenerator
# from vector_db.qdrant import VectorDB

# load_dotenv()

# logger = logging.getLogger(__name__)

# class RAGService:
#     """Main service class for RAG functionality"""
    
#     def __init__(self):
#         try:
#             self.embedding_generator = EmbeddingGenerator()
#         except Exception as e:
#             logger.warning(f"Could not initialize embedding generator: {e}")
#             # Create a mock embedding generator that returns dummy embeddings
#             class MockEmbeddingGenerator:
#                 def generate_embeddings(self, texts):
#                     import numpy as np
#                     return [[0.01] * 1536 for _ in texts]

#                 def get_embedding(self, text):
#                     import numpy as np
#                     return [0.01] * 1536

#             self.embedding_generator = MockEmbeddingGenerator()

#         try:
#             self.vector_db = VectorDB()
#         except Exception as e:
#             logger.warning(f"Could not initialize vector database: {e}")
#             # Create a mock vector database
#             class MockVectorDB:
#                 def search_similar(self, query_embedding, top_k=5):
#                     return []

#                 def store_embeddings(self, chunks, embeddings):
#                     pass

#             self.vector_db = MockVectorDB()

#         self.docs_loaded = False
#         self.initialization_error = None

#     async def initialize_docs(self):
#         """Load and index documents if not already loaded"""
#         if self.docs_loaded:
#             return

#         if self.initialization_error:
#             # Reset the error flag to allow retry if conditions have changed (e.g., new API key)
#             logger.info(f"Retrying document initialization after previous error: {self.initialization_error}")
#             self.initialization_error = None

#         try:
#             # Load documents
#             loader = DocumentLoader()
#             documents = loader.load_documents()

#             if not documents:
#                 logger.warning("No documents found to load")
#                 # Still mark as initialized so the system can work with LLM directly
#                 self.docs_loaded = True
#                 return

#             # Chunk documents
#             chunks = loader.chunk_documents(documents)

#             if not chunks:
#                 logger.warning("No chunks created from documents")
#                 # Still mark as initialized so the system can work with LLM directly
#                 self.docs_loaded = True
#                 return

#             # Generate embeddings for all chunks
#             texts = [chunk["content"] for chunk in chunks]
#             embeddings = self.embedding_generator.generate_embeddings(texts)

#             # Store in vector database
#             self.vector_db.store_embeddings(chunks, embeddings)
#             self.docs_loaded = True

#             logger.info(f"Successfully initialized {len(chunks)} document chunks in vector database")
#         except Exception as e:
#             self.initialization_error = str(e)
#             logger.error(f"Error initializing documents: {str(e)}")
#             # Still mark as loaded so the system can work with LLM directly
#             self.docs_loaded = True
#             # Don't raise the exception during startup, just log it
#             # The system can still work if the LLM is available but documents are not indexed yet

#     def get_collection_info(self):
#         """Get information about the vector database collection for debugging"""
#         try:
#             return self.vector_db.get_collection_info()
#         except Exception as e:
#             logger.error(f"Error getting collection info: {str(e)}")
#             return None
    
#     async def get_answer(self, query: str, top_k: int = 5, strict_mode: bool = False) -> Dict:
#         """
#         Get answer to query using RAG (Retrieval Augmented Generation)
#         """
#         try:
#             # Generate embedding for the query
#             query_embedding = self.embedding_generator.get_embedding(query)

#             # Search for similar documents in the vector database
#             similar_docs = self.vector_db.search_similar(query_embedding, top_k=top_k)

#             if similar_docs:
#                 # Format the context from retrieved documents
#                 context_parts = []
#                 for doc in similar_docs:
#                     context_parts.append(f"Source: {doc['source']}\nContent: {doc['content']}")

#                 context = "\n\n".join(context_parts)
#             else:
#                 context = "No relevant documents found in the knowledge base."
#         except Exception as e:
#             logger.warning(f"Error during document retrieval: {str(e)}. Using empty context.")
#             similar_docs = []
#             context = "No relevant documents found in the knowledge base."

#         # Generate answer using the context and query
#         answer = await self._generate_answer_with_llm(query, context, strict_mode)

#         return {
#             "answer": answer,
#             "sources": similar_docs,
#             "query": query
#         }
    
#         async def _generate_answer_with_llm(self, query: str, context: str, strict_mode: bool = False) -> str:
#             """
#             Generate answer using direct OpenAI API
#             """
#             from openai import AsyncOpenAI

#             api_key = os.getenv("OPENAI_API_KEY")
#             model = os.getenv("CHAT_MODEL", "gpt-4o-mini")

#             if not api_key:
#                 return "API key not configured. Please set OPENAI_API_KEY in environment variables."

#             client = AsyncOpenAI(api_key=api_key)

#             if strict_mode:
#                 system_prompt = "You are an expert assistant for the textbook 'Physical AI & Humanoid Robotics'. Answer ONLY based on the provided context. If the answer is not in the context, say 'Answer not found in the book.'"
#             else:
#                 system_prompt = "You are an expert assistant for the textbook 'Physical AI & Humanoid Robotics'. Use the provided context to answer accurately. If context is irrelevant, use your knowledge."

#             user_prompt = f"""Context:
#     {context}

#     Question: {query}

#     Answer:"""

#             try:
#                 response = await client.chat.completions.create(
#                     model=model,
#                     messages=[
#                         {"role": "system", "content": system_prompt},
#                         {"role": "user", "content": user_prompt}
#                     ],
#                     max_tokens=1000,
#                     temperature=0.3
#                 )
#                 return response.choices[0].message.content.strip()
#             except Exception as e:
#                 logger.error(f"Error calling OpenAI API: {str(e)}")
#                 if "not found in the book" in context.lower():
#                     return "Answer not found in the book."
#                 return f"Sorry, I couldn't process your question due to a service error."

#     def cleanup_project_files(self):
#         """Call the file cleanup utility"""
#         from ..utils.file_cleanup import cleanup_project_files
#         cleanup_project_files()










# rag_backend/api/rag_service.py

from fastapi import HTTPException
import logging
from typing import List, Dict
import asyncio
import os
from dotenv import load_dotenv
from data_ingestion.loader import DocumentLoader
from embeddings.generator import EmbeddingGenerator
from vector_db.qdrant import VectorDB

load_dotenv()

logger = logging.getLogger(__name__)

class RAGService:
    """Main service class for RAG functionality"""

    def __init__(self):
        try:
            self.embedding_generator = EmbeddingGenerator()
        except Exception as e:
            logger.warning(f"Could not initialize embedding generator: {e}")
            # Create a mock embedding generator that returns dummy embeddings
            class MockEmbeddingGenerator:
                def generate_embeddings(self, texts):
                    import numpy as np
                    vector_size = int(os.getenv("VECTOR_SIZE", 1536))
                    return [[0.01] * vector_size for _ in texts]

                def get_embedding(self, text):
                    import numpy as np
                    vector_size = int(os.getenv("VECTOR_SIZE", 1536))
                    return [0.01] * vector_size

            self.embedding_generator = MockEmbeddingGenerator()

        try:
            self.vector_db = VectorDB()
        except Exception as e:
            logger.warning(f"Could not initialize vector database: {e}")
            # Create a mock vector database
            class MockVectorDB:
                def search_similar(self, query_embedding, top_k=5):
                    return []

                def store_embeddings(self, chunks, embeddings):
                    pass

                def get_collection_info(self):
                    return None

            self.vector_db = MockVectorDB()

        self.docs_loaded = False
        self.initialization_error = None

    async def initialize_docs(self):
        """Load and index documents if not already loaded"""
        if self.docs_loaded:
            return

        if self.initialization_error:
            logger.info(f"Retrying document initialization after previous error: {self.initialization_error}")
            self.initialization_error = None

        try:
            # Load documents
            loader = DocumentLoader()
            documents = loader.load_documents()

            if not documents:
                logger.warning("No documents found to load")
                self.docs_loaded = True
                return

            # Chunk documents
            chunks = loader.chunk_documents(documents)

            if not chunks:
                logger.warning("No chunks created from documents")
                self.docs_loaded = True
                return

            # Generate embeddings for all chunks
            texts = [chunk["content"] for chunk in chunks]
            embeddings = self.embedding_generator.generate_embeddings(texts)

            # Store in vector database
            self.vector_db.store_embeddings(chunks, embeddings)

            self.docs_loaded = True
            logger.info(f"Successfully initialized {len(chunks)} document chunks in vector database")

        except Exception as e:
            self.initialization_error = str(e)
            logger.error(f"Error initializing documents: {str(e)}")
            self.docs_loaded = True  # Allow LLM fallback

    def get_collection_info(self):
        """Get information about the vector database collection for debugging"""
        try:
            return self.vector_db.get_collection_info()
        except Exception as e:
            logger.error(f"Error getting collection info: {str(e)}")
            return None

    async def get_answer(self, query: str, top_k: int = 5, strict_mode: bool = False) -> Dict:
        """
        Get answer to query using RAG (Retrieval Augmented Generation)
        """
        try:
            # Generate embedding for the query
            query_embedding = self.embedding_generator.get_embedding(query)

            # Search for similar documents
            similar_docs = self.vector_db.search_similar(query_embedding, top_k=top_k)

            if similar_docs:
                context_parts = []
                for doc in similar_docs:
                    context_parts.append(f"Source: {doc['source']}\nContent: {doc['content']}")
                context = "\n\n".join(context_parts)
            else:
                context = "No relevant documents found in the knowledge base."

        except Exception as e:
            logger.warning(f"Error during document retrieval: {str(e)}. Using empty context.")
            similar_docs = []
            context = "No relevant documents found in the knowledge base."

        # Generate answer using LLM
        answer = await self._generate_answer_with_llm(query, context, strict_mode)

        return {
            "answer": answer,
            "sources": similar_docs,
            "query": query
        }

    async def _generate_answer_with_llm(self, query: str, context: str, strict_mode: bool = False) -> str:
        """
        Generate answer using direct OpenAI API
        """
        from openai import AsyncOpenAI

        api_key = os.getenv("OPENAI_API_KEY")
        model = os.getenv("CHAT_MODEL", "gpt-4o-mini")

        if not api_key:
            return "API key not configured. Please set OPENAI_API_KEY in environment variables."

        client = AsyncOpenAI(api_key=api_key)

        if strict_mode:
            system_prompt = (
                "You are an expert assistant for the textbook 'Physical AI & Humanoid Robotics'. "
                "Answer ONLY based on the provided context. "
                "If the answer is not in the context, respond with 'Answer not found in the book.'"
            )
        else:
            system_prompt = (
                "You are an expert assistant for the textbook 'Physical AI & Humanoid Robotics'. "
                "Use the provided context to answer accurately. "
                "If the context is irrelevant or insufficient, use your general knowledge to provide a helpful response."
            )

        user_prompt = f"""Context:
{context}

Question: {query}

Answer:"""

        try:
            response = await client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                max_tokens=1000,
                temperature=0.3
            )
            return response.choices[0].message.content.strip()

        except Exception as e:
            logger.error(f"Error calling OpenAI API: {str(e)}")
            if "not found" in context.lower():
                return "Answer not found in the book."
            return "Sorry, I couldn't process your question due to a service error."

    def cleanup_project_files(self):
        """Call the file cleanup utility"""
        try:
            from ..utils.file_cleanup import cleanup_project_files
            cleanup_project_files()
        except Exception as e:
            logger.warning(f"Could not run cleanup: {e}")

============================================================
FILE: rag_backend\api\__init__.py
============================================================
  


============================================================
FILE: rag_backend\data_ingestion\loader.py
============================================================
import os
from typing import List, Dict
import logging
from pathlib import Path

logger = logging.getLogger(__name__)

class DocumentLoader:
    """Load documents from the book/docs/ directory"""
    
    def __init__(self, docs_path: str = "../../book/docs"):
        # Try multiple possible paths for flexibility
        possible_paths = [
            Path(docs_path),  # Original relative path
            Path("../../../book/docs"),  # Alternative relative path
            Path("../../book/docs"),  # Another relative path
            Path("../book/docs"),  # Closer relative path
            Path("./book/docs"),  # Local path
            Path("C:/Users/Dell/ai-book/book/docs")  # Absolute path
        ]

        self.docs_path = None
        for path in possible_paths:
            if path.exists():
                self.docs_path = path
                logger.info(f"Found docs at: {path}")
                break

        if self.docs_path is None:
            # If none of the paths work, default to original path and let it fail gracefully
            self.docs_path = Path(docs_path)
            logger.warning(f"Docs path not found: {self.docs_path}")
        
    def load_documents(self) -> List[Dict[str, str]]:
        """
        Load all markdown documents from the docs directory
        Returns a list of dictionaries with 'content' and 'source' keys
        """
        documents = []
        
        if not self.docs_path.exists():
            logger.error(f"Docs path does not exist: {self.docs_path}")
            return documents
            
        # Look for markdown files in the docs directory and subdirectories
        for md_file in self.docs_path.rglob("*.md"):
            try:
                with open(md_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                    
                # Create document object
                doc = {
                    "content": content,
                    "source": str(md_file.relative_to(self.docs_path)),
                    "title": md_file.stem  # filename without extension
                }
                
                documents.append(doc)
                logger.info(f"Loaded document: {doc['source']}")
                
            except Exception as e:
                logger.error(f"Error loading document {md_file}: {str(e)}")
                
        logger.info(f"Loaded {len(documents)} documents from {self.docs_path}")
        return documents

    def chunk_documents(self, documents: List[Dict], chunk_size: int = 1000, overlap: int = 100) -> List[Dict]:
        """
        Split documents into chunks for better retrieval
        """
        chunks = []
        
        for doc in documents:
            content = doc["content"]
            source = doc["source"]
            title = doc["title"]
            
            # Simple sliding window approach to chunk the content
            start = 0
            while start < len(content):
                end = start + chunk_size
                
                # If we're near the end, make sure to include the remainder
                if end > len(content):
                    end = len(content)
                    
                chunk_text = content[start:end]
                
                chunk = {
                    "content": chunk_text,
                    "source": source,
                    "title": title,
                    "chunk_id": f"{source}_{start}_{end}"
                }
                
                chunks.append(chunk)
                
                # Move to next chunk with overlap
                start += chunk_size - overlap
                
                # If start is beyond the length, break
                if start >= len(content):
                    break
                    
        logger.info(f"Created {len(chunks)} chunks from {len(documents)} documents")
        return chunks

============================================================
FILE: rag_backend\data_ingestion\__init__.py
============================================================
  


============================================================
FILE: rag_backend\embeddings\generator.py
============================================================
# # rag_backend\embeddings\generator.py
# import numpy as np
# from typing import List
# import logging
# import os
# import requests
# from dotenv import load_dotenv

# load_dotenv()

# logger = logging.getLogger(__name__)

# class EmbeddingGenerator:
#     """Generate embeddings using OpenAI-compatible API for Qwen"""
    
#     def __init__(self):
#         self.api_key = os.getenv("OPENROUTER_API_KEY")

#         # Try to get the embedding model, with fallbacks
#         configured_model = os.getenv("EMBEDDING_MODEL", "nvidia/nv-embed-v1")

#         # Define fallback models in case the primary one doesn't work
#         self.possible_models = [
#             configured_model,
#             "nomic-ai/nomic-embed-text-v1.5",
#             "text-embedding-3-small",
#             "openai/text-embedding-3-small"
#         ]

#         # Use the first working model
#         self.model = self._find_working_model()

#         self.base_url = os.getenv("OPENROUTER_BASE_URL", "https://openrouter.ai/api/v1")

#         if not self.api_key:
#             raise ValueError("OPENROUTER_API_KEY environment variable is required")

#     def _find_working_model(self):
#         """Try each model in the list until one works"""
#         headers = {
#             "Authorization": f"Bearer {self.api_key}",
#             "Content-Type": "application/json"
#         }

#         for model in self.possible_models:
#             try:
#                 # Test the model with a simple request
#                 payload = {
#                     "model": model,
#                     "input": ["test"],
#                     "encoding_format": "float"
#                 }

#                 response = requests.post(
#                     f"{self.base_url}/embeddings",
#                     headers=headers,
#                     json=payload
#                 )

#                 if response.status_code == 200:
#                     response_data = response.json()
#                     if 'data' in response_data:
#                         logger.info(f"Successfully verified embedding model: {model}")
#                         return model
#                     else:
#                         logger.warning(f"Model {model} response missing 'data' key: {response_data}")
#                 else:
#                     logger.warning(f"Model {model} failed with status {response.status_code}: {response.text}")
#             except Exception as e:
#                 logger.warning(f"Model {model} failed with error: {str(e)}")

#         # If all models fail, use the first one and let it fail later with proper error
#         logger.warning(f"No embedding models worked, defaulting to: {self.possible_models[0]}")
#         return self.possible_models[0]
    
#     def generate_embeddings(self, texts: List[str]) -> List[List[float]]:
#         """
#         Generate embeddings for a list of texts using OpenRouter API
#         """
#         headers = {
#             "Authorization": f"Bearer {self.api_key}",
#             "Content-Type": "application/json"
#         }

#         # Prepare the request payload
#         payload = {
#             "model": self.model,
#             "input": texts,
#             "encoding_format": "float"
#         }

#         try:
#             response = requests.post(
#                 f"{self.base_url}/embeddings",
#                 headers=headers,
#                 json=payload
#             )

#             if response.status_code != 200:
#                 logger.error(f"Embedding API request failed with status {response.status_code}: {response.text}")
#                 # Return mock embeddings instead of raising an exception
#                 # This allows the system to continue working even if embeddings fail
#                 return [[0.01] * 1536 for _ in texts]

#             response_data = response.json()

#             # Check if the response has the expected 'data' key
#             if 'data' not in response_data:
#                 logger.error(f"Error generating embeddings: 'data' key not found in response. Full response: {response_data}")
#                 return [[0.01] * 1536 for _ in texts]

#             embeddings = [item['embedding'] for item in response_data['data']]

#             logger.info(f"Generated embeddings for {len(texts)} texts")
#             return embeddings

#         except Exception as e:
#             logger.error(f"Error generating embeddings: {str(e)}")
#             # Return mock embeddings instead of raising an exception
#             # This allows the system to continue working even if embeddings fail
#             return [[0.01] * 1536 for _ in texts]
    
#     def get_embedding(self, text: str) -> List[float]:
#         """
#         Get embedding for a single text
#         """
#         return self.generate_embeddings([text])[0]








# rag_backend/embeddings/generator.py

import os
import logging
from typing import List
from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()
logger = logging.getLogger(__name__)

class EmbeddingGenerator:
    """Generate embeddings using direct OpenAI API"""

    def __init__(self):
        self.api_key = os.getenv("OPENAI_API_KEY")
        if not self.api_key:
            raise ValueError("OPENAI_API_KEY environment variable is required for embeddings")

        # Use direct OpenAI client ‚Äî no base_url needed
        self.client = OpenAI(api_key=self.api_key)

        # Get model from env, fallback to a reliable one
        self.model = os.getenv("EMBEDDING_MODEL", "text-embedding-3-small")
        logger.info(f"Using embedding model: {self.model}")

    def generate_embeddings(self, texts: List[str]) -> List[List[float]]:
        """
        Generate embeddings for a list of texts using OpenAI API
        """
        try:
            response = self.client.embeddings.create(
                model=self.model,
                input=texts
            )
            embeddings = [item.embedding for item in response.data]
            logger.info(f"Generated {len(embeddings)} embeddings using {self.model}")
            return embeddings
        except Exception as e:
            logger.error(f"Error generating embeddings: {str(e)}")
            # Return mock embeddings to prevent total failure
            vector_size = 1536 if "small" in self.model else 3076
            return [[0.01] * vector_size for _ in texts]

    def get_embedding(self, text: str) -> List[float]:
        """
        Get embedding for a single text
        """
        return self.generate_embeddings([text])[0]

============================================================
FILE: rag_backend\embeddings\__init__.py
============================================================
  


============================================================
FILE: rag_backend\utils\file_cleanup.py
============================================================
import os
import hashlib
from pathlib import Path


def cleanup_project_files():
    """
    Cleans up duplicate markdown files and empty directories in the docs folder.
    - Traverses the '../book/docs' directory
    - Identifies and removes duplicate .md or .mdx files based on content hash
    - Deletes any empty folders left behind
    """
    docs_path = Path("../book/docs")
    
    if not docs_path.exists():
        print(f"Directory {docs_path} does not exist")
        return
    
    # Dictionary to store file hashes and their paths
    file_hashes = {}
    files_to_remove = []
    
    # Walk through all markdown files
    for file_path in docs_path.rglob("*"):
        if file_path.suffix.lower() in ['.md', '.mdx']:
            # Calculate MD5 hash of file content
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                file_hash = hashlib.md5(content.encode('utf-8')).hexdigest()
            
            if file_hash in file_hashes:
                # Duplicate found, mark for removal
                print(f"Duplicate found: {file_path} (duplicate of {file_hashes[file_hash]})")
                files_to_remove.append(file_path)
            else:
                # New unique file
                file_hashes[file_hash] = file_path
    
    # Remove duplicate files
    for file_path in files_to_remove:
        try:
            file_path.unlink()
            print(f"Removed duplicate file: {file_path}")
        except Exception as e:
            print(f"Error removing file {file_path}: {e}")
    
    # Remove empty directories
    for dir_path in sorted(docs_path.rglob("*"), reverse=True):  # Reverse to remove child directories first
        if dir_path.is_dir() and not any(dir_path.iterdir()):
            try:
                dir_path.rmdir()
                print(f"Removed empty directory: {dir_path}")
            except Exception as e:
                print(f"Error removing directory {dir_path}: {e}")
    
    print("Cleanup completed.")


if __name__ == "__main__":
    cleanup_project_files()

============================================================
FILE: rag_backend\vector_db\qdrant.py
============================================================
import uuid
from typing import List, Dict, Optional
import logging
from qdrant_client import QdrantClient
from qdrant_client.http.models import PointStruct, VectorParams, Distance
from qdrant_client.http import models
import os
from dotenv import load_dotenv

load_dotenv()

logger = logging.getLogger(__name__)

class VectorDB:
    """Qdrant vector database wrapper for storing and retrieving document embeddings"""
    
    def __init__(self):
        # Configure Qdrant client
        qdrant_url = os.getenv("QDRANT_URL", "http://localhost:6333")
        qdrant_api_key = os.getenv("QDRANT_API_KEY")

        # Check if we're using local mode
        if qdrant_url == "local":
            # Use local in-memory mode
            self.client = QdrantClient(":memory:")
            self.qdrant_url = "local"
        else:
            # Initialize Qdrant client with URL
            if qdrant_api_key:
                self.client = QdrantClient(url=qdrant_url, api_key=qdrant_api_key)
            else:
                self.client = QdrantClient(url=qdrant_url)
            self.qdrant_url = qdrant_url

        self.collection_name = os.getenv("QDRANT_COLLECTION_NAME", "book_docs")
        # Get embedding size based on the model being used
        embedding_model = os.getenv("EMBEDDING_MODEL", "text-embedding-3-small")
        if "nvidia" in embedding_model.lower() or "nv-embed" in embedding_model.lower():
            self.vector_size = 1024  # NVIDIA embedding size
        elif "text-embedding-3-small" in embedding_model:
            self.vector_size = 1536  # OpenAI embedding size
        elif "text-embedding-3-large" in embedding_model:
            self.vector_size = 3072  # OpenAI large embedding size
        else:
            # Default to 1536 but allow for dynamic sizing
            self.vector_size = int(os.getenv("VECTOR_SIZE", "1536"))

        # Create collection if it doesn't exist - will be done when first needed
        self.collection_initialized = False
    
    def _create_collection(self):
        """Create the collection if it doesn't exist"""
        try:
            # Check if collection exists
            collection_info = self.client.get_collection(self.collection_name)
            # Verify the vector size matches
            if collection_info.config.params.vectors.size != self.vector_size:
                logger.warning(f"Vector size mismatch: expected {self.vector_size}, got {collection_info.config.params.vectors.size}. This may cause issues.")
            logger.info(f"Collection '{self.collection_name}' already exists with {collection_info.config.params.vectors.size} dimensions")
            self.collection_initialized = True
        except:
            # Collection doesn't exist, create it
            self.client.create_collection(
                collection_name=self.collection_name,
                vectors_config=VectorParams(size=self.vector_size, distance=Distance.COSINE),
            )
            logger.info(f"Created collection '{self.collection_name}' with {self.vector_size} dimensions")
            self.collection_initialized = True

    def _ensure_collection_exists(self):
        """Ensure the collection exists before performing operations"""
        if not self.collection_initialized:
            self._create_collection()
    
    def store_embeddings(self, chunks: List[Dict], embeddings: List[List[float]]):
        """
        Store document chunks with their embeddings in the vector database
        """
        try:
            self._ensure_collection_exists()
        except Exception as e:
            logger.error(f"Failed to connect to Qdrant at {self.qdrant_url}: {str(e)}")
            raise

        points = []

        for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
            point = PointStruct(
                id=str(uuid.uuid4()),
                vector=embedding,
                payload={
                    "content": chunk["content"],
                    "source": chunk["source"],
                    "title": chunk["title"],
                    "chunk_id": chunk.get("chunk_id", f"chunk_{i}")
                }
            )
            points.append(point)

        # Upload points to Qdrant using the newer API
        self.client.upsert(
            collection_name=self.collection_name,
            points=points
        )

        logger.info(f"Stored {len(points)} embeddings in collection '{self.collection_name}'")
    
    def search_similar(self, query_embedding: List[float], top_k: int = 5) -> List[Dict]:
        """
        Search for similar documents based on the query embedding
        """
        try:
            self._ensure_collection_exists()
        except Exception as e:
            logger.error(f"Failed to connect to Qdrant at {self.qdrant_url}: {str(e)}")
            # Return empty list if Qdrant is not available
            return []

        # Use the newer query_points method for modern Qdrant client
        search_result = self.client.query_points(
            collection_name=self.collection_name,
            query=query_embedding,
            limit=top_k
        )
        results = search_result.points

        # Extract the payload from search results
        similar_docs = []
        for result in results:
            # Check if result has the expected structure for the newer API
            if hasattr(result, 'payload'):
                content = result.payload.get("content", "")
                source = result.payload.get("source", "")
                title = result.payload.get("title", "")
                score = getattr(result, 'score', 0.0)
            else:
                # Fallback for different API response structure
                content = result.get("payload", {}).get("content", "")
                source = result.get("payload", {}).get("source", "")
                title = result.get("payload", {}).get("title", "")
                score = result.get("score", 0.0)

            doc = {
                "content": content,
                "source": source,
                "title": title,
                "score": score
            }
            similar_docs.append(doc)

        logger.info(f"Found {len(similar_docs)} similar documents with scores: {[round(doc['score'], 3) for doc in similar_docs[:3]]}")
        return similar_docs
    
    def get_collection_info(self):
        """
        Get information about the collection for debugging
        """
        try:
            collection_info = self.client.get_collection(self.collection_name)
            return {
                "points_count": collection_info.points_count,
                "config": {
                    "vector_size": collection_info.config.params.vectors.size,
                    "distance": collection_info.config.params.vectors.distance
                }
            }
        except Exception as e:
            logger.error(f"Error getting collection info: {str(e)}")
            return None

    def clear_collection(self):
        """
        Clear all points from the collection (useful for reindexing)
        """
        try:
            self.client.delete_collection(self.collection_name)
            logger.info(f"Cleared collection '{self.collection_name}'")
            self._create_collection()  # Recreate empty collection
        except Exception as e:
            logger.error(f"Error clearing collection: {str(e)}")

============================================================
FILE: rag_backend\vector_db\__init__.py
============================================================
 


============================================================
FILE: rag_frontend\index.html
============================================================
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Textbook Assistant</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 20px;
            background-color: #f5f5f5;
        }
        
        .chat-container {
            max-width: 800px;
            margin: 0 auto;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            overflow: hidden;
        }
        
        .chat-header {
            background: #4f46e5;
            color: white;
            padding: 15px;
            text-align: center;
        }
        
        .chat-messages {
            height: 400px;
            overflow-y: auto;
            padding: 15px;
            background: #fafafa;
        }
        
        .message {
            margin-bottom: 15px;
            padding: 10px;
            border-radius: 8px;
            max-width: 80%;
        }
        
        .user-message {
            background: #e0e7ff;
            margin-left: auto;
            text-align: right;
        }
        
        .bot-message {
            background: #f3f4f6;
            margin-right: auto;
        }
        
        .input-area {
            display: flex;
            padding: 15px;
            background: white;
            border-top: 1px solid #e5e7eb;
        }
        
        #user-input {
            flex: 1;
            padding: 10px;
            border: 1px solid #d1d5db;
            border-radius: 4px;
            margin-right: 10px;
        }
        
        #send-button {
            padding: 10px 20px;
            background: #4f46e5;
            color: white;
            border: none;
            border-radius: 4px;
            cursor: pointer;
        }
        
        #send-button:hover {
            background: #4338ca;
        }
        
        #send-button:disabled {
            background: #d1d5db;
            cursor: not-allowed;
        }
        
        .typing-indicator {
            color: #6b7280;
            font-style: italic;
            padding: 10px;
        }
    </style>
</head>
<body>
    <div class="chat-container">
        <div class="chat-header">
            <h2>AI Textbook Assistant</h2>
            <p>Ask questions about Physical AI & Humanoid Robotics</p>
        </div>
        
        <div class="chat-messages" id="chat-messages">
            <div class="message bot-message">
                Hello! I'm your AI assistant for the Physical AI & Humanoid Robotics textbook. Ask me anything about the content!
            </div>
        </div>
        
        <div class="input-area">
            <input type="text" id="user-input" placeholder="Ask a question about the textbook..." />
            <button id="send-button">Send</button>
        </div>
    </div>

    <script>
        const chatMessages = document.getElementById('chat-messages');
        const userInput = document.getElementById('user-input');
        const sendButton = document.getElementById('send-button');
        
        // Function to add a message to the chat
        function addMessage(text, isUser) {
            const messageDiv = document.createElement('div');
            messageDiv.classList.add('message');
            messageDiv.classList.add(isUser ? 'user-message' : 'bot-message');
            messageDiv.textContent = text;
            chatMessages.appendChild(messageDiv);
            
            // Scroll to the bottom
            chatMessages.scrollTop = chatMessages.scrollHeight;
        }
        
        // Function to show typing indicator
        function showTyping() {
            const typingDiv = document.createElement('div');
            typingDiv.id = 'typing-indicator';
            typingDiv.classList.add('typing-indicator');
            typingDiv.textContent = 'AI is thinking...';
            chatMessages.appendChild(typingDiv);
            chatMessages.scrollTop = chatMessages.scrollHeight;
            return typingDiv;
        }
        
        // Function to remove typing indicator
        function removeTyping(typingDiv) {
            if (typingDiv && typingDiv.parentNode) {
                typingDiv.parentNode.removeChild(typingDiv);
            }
        }
        
        // Function to send message to backend
        async function sendMessage() {
            const message = userInput.value.trim();
            if (!message) return;
            
            // Add user message to chat
            addMessage(message, true);
            userInput.value = '';
            
            // Disable input and button while waiting for response
            userInput.disabled = true;
            sendButton.disabled = true;
            
            // Show typing indicator
            const typingDiv = showTyping();
            
            try {
                // Send request to backend
                const response = await fetch('http://127.0.0.1:8000/chat', {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json',
                    },
                    body: JSON.stringify({ question: message })
                });
                
                // Remove typing indicator
                removeTyping(typingDiv);
                
                if (response.ok) {
                    const data = await response.json();
                    addMessage(data.response, false);
                } else {
                    addMessage('Sorry, I encountered an error processing your request.', false);
                }
            } catch (error) {
                console.error('Error:', error);
                // Remove typing indicator
                removeTyping(typingDiv);
                addMessage('Sorry, I encountered an error connecting to the AI service.', false);
            } finally {
                // Re-enable input and button
                userInput.disabled = false;
                sendButton.disabled = false;
                userInput.focus();
            }
        }
        
        // Event listeners
        sendButton.addEventListener('click', sendMessage);
        
        userInput.addEventListener('keypress', function(e) {
            if (e.key === 'Enter') {
                sendMessage();
            }
        });
        
        // Focus on input when page loads
        userInput.focus();
    </script>
</body>
</html>

============================================================
FILE: specs\1-book-structure\plan.md
============================================================
# Physical AI & Humanoid Robotics Book Implementation Plan

## Technical Context

**Problem**: Create a comprehensive writing and build plan for a Physical AI & Humanoid Robotics book that follows the specified structure with 15 chapters across 5 parts.

**Solution**: Develop a systematic approach for writing, building, and publishing the book using Docusaurus and GitHub Pages, optimized for a solo author workflow.

**Technology Stack**:
- Writing: Markdown format with Docusaurus documentation framework
- Deployment: GitHub Pages
- Code samples: Python, ROS 2, Gazebo, NVIDIA Isaac, VLA
- Diagrams: Mermaid, Draw.io, or similar
- Version control: Git with GitHub

**Architecture Style**: Documentation-as-code with modular, maintainable content structure.

## Constitution Check

Based on the project constitution:
- ‚úÖ Technically accurate and industry-aligned content
- ‚úÖ Beginner-to-advanced learning flow maintained
- ‚úÖ Hands-on, project-driven learning approach
- ‚úÖ Open-source friendly tools (Docusaurus, GitHub)
- ‚úÖ Written for students with Python & AI background
- ‚úÖ Accessible hardware & simulation focus

## Phase 0: Research & Requirements

### Research Tasks

#### Writing Order & Dependencies
**Decision**: Follow logical progression with parallel writing approach
**Rationale**: Part I-III form foundation for Parts IV-V; some parallel development possible with stubs
**Alternatives considered**:
- Sequential (1-15): Too linear, blocks parallel work
- Reverse (15-1): Foundation issues
- Modular with cross-references: Best balance

#### Tool Stack Selection
**Decision**: Docusaurus + GitHub Pages for documentation, with GitHub for version control
**Rationale**:
- Docusaurus provides excellent documentation features (search, navigation, versioning)
- GitHub Pages offers free hosting with custom domains
- Markdown format supports both documentation and code samples
- Integrates well with Git workflow
**Alternatives considered**:
- GitBook: Proprietary, limited customization
- Sphinx: Python-focused, less flexible for multi-language content
- Hugo: Static site generator, good but less documentation-focused

#### Content Structure
**Decision**: Modular chapter structure with reusable components
**Rationale**: Enables parallel writing, easier maintenance, consistent formatting
**Alternatives considered**: Monolithic approach vs modular

## Phase 1: Design & Architecture

### Content Architecture

#### Directory Structure
```
book/
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îú‚îÄ‚îÄ part-i-foundations/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chapter-1-introduction/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chapter-2-ros-fundamentals/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ chapter-3-robot-modeling/
‚îÇ   ‚îú‚îÄ‚îÄ part-ii-perception/
‚îÇ   ‚îú‚îÄ‚îÄ part-iii-motion/
‚îÇ   ‚îú‚îÄ‚îÄ part-iv-intelligence/
‚îÇ   ‚îî‚îÄ‚îÄ part-v-integration/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îú‚îÄ‚îÄ css/
‚îÇ   ‚îî‚îÄ‚îÄ pages/
‚îú‚îÄ‚îÄ notebooks/ (Jupyter notebooks for code samples)
‚îú‚îÄ‚îÄ diagrams/ (PlantUML, Mermaid, or other diagram sources)
‚îú‚îÄ‚îÄ assets/ (images, videos, additional resources)
‚îî‚îÄ‚îÄ docusaurus.config.js
```

#### Writing Workflow
1. **Draft Phase**: Write chapter content in markdown with placeholders for code/diagrams
2. **Implementation Phase**: Add code samples, diagrams, and labs
3. **Review Phase**: Technical review and content validation
4. **Publish Phase**: Deploy to GitHub Pages

### Writing Order & Dependencies

#### Recommended Writing Sequence
**Foundation Block (Required First)**:
1. Chapter 2: Robot Operating System (ROS 2) Fundamentals
2. Chapter 3: Robot Modeling and Simulation Fundamentals
3. Chapter 1: Introduction to Physical AI and Humanoid Robotics

**Perception Block**:
4. Chapter 4: Sensor Integration and Data Processing
5. Chapter 5: Computer Vision for Robotics
6. Chapter 6: 3D Perception and Scene Understanding

**Motion Block**:
7. Chapter 7: Kinematics and Dynamics
8. Chapter 8: Locomotion and Balance Control
9. Chapter 9: Motion Planning and Navigation

**Intelligence Block**:
10. Chapter 10: Reinforcement Learning for Robotics
11. Chapter 11: Imitation Learning and VLA
12. Chapter 12: Human-Robot Interaction

**Integration Block (Dependent on previous blocks)**:
13. Chapter 13: Multi-Robot Systems and Coordination
14. Chapter 14: Real-World Deployment and Safety
15. Chapter 15: Advanced Topics and Future Directions

### Tool Usage Plan

#### Docusaurus Configuration
- **Navigation**: Part-based sidebar with chapter grouping
- **Search**: Built-in Algolia search or local search
- **Versioning**: If needed for draft vs final versions
- **Code blocks**: Syntax highlighting for Python, C++, launch files, etc.
- **Math support**: LaTeX for mathematical equations
- **Diagram rendering**: Mermaid for sequence diagrams, architecture diagrams

#### GitHub Pages Deployment
- **Workflow**: GitHub Actions for automated build and deployment
- **Branch**: Deploy from `gh-pages` branch or `docs/` folder
- **Custom domain**: Configurable through CNAME file
- **Analytics**: Optional Google Analytics integration

### Content Components

#### Code Samples Strategy
- **Location**: Embedded in chapter markdown files with proper syntax highlighting
- **Organization**: In `notebooks/` directory as Jupyter notebooks for interactive examples
- **Testing**: Ensure all code samples are tested and functional
- **Variants**: Different complexity levels where appropriate (basic ‚Üí advanced)

#### Diagrams Strategy
- **Location**: In `diagrams/` directory with source files and rendered images
- **Format**: PlantUML, Mermaid, or Draw.io for version control
- **Types**: Architecture diagrams, flow charts, robot kinematics, system interactions
- **Integration**: Embedded in markdown files with alt text and descriptions

#### Labs & Practical Activities
- **Location**: Integrated within chapters as "Hands-On" sections
- **Format**: Step-by-step instructions with expected outcomes
- **Dependencies**: Clear requirements for software/hardware setup
- **Validation**: Checkpoints with expected results

### Milestones & Timeline

#### Milestone 1: Foundation (Chapters 1-3) - Weeks 1-4
- Complete draft of foundational chapters
- Basic Docusaurus setup and configuration
- Initial code samples for ROS 2 basics
- Basic diagrams for robot modeling

#### Milestone 2: Core Systems (Chapters 4-9) - Weeks 5-12
- Complete draft of perception and motion chapters
- Advanced code samples for simulation and control
- Detailed diagrams for kinematics and navigation
- Initial lab exercises

#### Milestone 3: Intelligence & Interaction (Chapters 10-12) - Weeks 13-18
- Complete draft of AI and interaction chapters
- Advanced code samples for learning and interaction
- Complex diagrams for RL and VLA systems
- Advanced lab exercises

#### Milestone 4: Integration & Deployment (Chapters 13-15) - Weeks 19-22
- Complete draft of integration chapters
- End-to-end code samples and examples
- System architecture diagrams
- Capstone lab project

#### Milestone 5: Review & Polish - Weeks 23-26
- Technical review of all content
- Code sample testing and validation
- Diagram refinement and accessibility
- Final content editing

#### Milestone 6: Publish - Week 27
- GitHub Pages deployment
- Final quality assurance
- Launch and feedback collection

### Solo Author Workflow Optimization

#### Git Workflow
- **Branching**: Feature branches for each chapter
- **Commits**: Logical chunks (sections or concepts)
- **Review**: Self-review with checklist before merging
- **Backup**: Regular pushes to GitHub for safety

#### Writing Tools
- **Editor**: VS Code with Markdown extensions
- **Preview**: Live preview with Docusaurus dev server
- **Assets**: Organized in dedicated directories
- **References**: BibTeX or markdown for citations

#### Quality Assurance
- **Checklist**: Consistency, accuracy, and completeness
- **Testing**: All code samples tested in clean environment
- **Accessibility**: Alt text for diagrams, readable code formatting
- **Validation**: Cross-references and internal consistency

## Phase 2: Implementation Plan

### Chapter-by-Chapter Implementation

#### Part I: Foundations of Physical AI and Robotics

**Chapter 1: Introduction to Physical AI and Humanoid Robotics**
- Week 1: Draft content, setup basic ROS 2 environment
- Week 2: Add code samples for basic ROS 2 nodes
- Week 3: Create diagrams for AI vs Physical AI concepts
- Week 4: Labs for environment setup and basic simulation

**Chapter 2: Robot Operating System (ROS 2) Fundamentals**
- Week 1: Draft core concepts and architecture
- Week 2: Code samples for nodes, topics, services
- Week 3: Diagrams for ROS 2 architecture
- Week 4: Labs for multi-node systems

**Chapter 3: Robot Modeling and Simulation Fundamentals**
- Week 1: Draft URDF and SDF concepts
- Week 2: Code samples for URDF creation
- Week 3: Diagrams for robot kinematics
- Week 4: Labs for simulation setup

#### Part II: Perception and Understanding

**Chapter 4: Sensor Integration and Data Processing**
- Week 5: Draft sensor concepts and data streams
- Week 6: Code samples for sensor processing
- Week 7: Diagrams for sensor fusion
- Week 8: Labs for sensor integration

**Chapter 5: Computer Vision for Robotics**
- Week 5: Draft vision concepts and SLAM
- Week 6: Code samples for object detection
- Week 7: Diagrams for visual processing pipeline
- Week 8: Labs for vision integration

**Chapter 6: 3D Perception and Scene Understanding**
- Week 5: Draft 3D concepts and point clouds
- Week 6: Code samples for 3D processing
- Week 7: Diagrams for spatial reasoning
- Week 8: Labs for 3D mapping

#### Part III: Motion and Control

**Chapter 7: Kinematics and Dynamics**
- Week 9: Draft kinematics concepts
- Week 10: Code samples for kinematic solvers
- Week 11: Diagrams for robot kinematics
- Week 12: Labs for trajectory generation

**Chapter 8: Locomotion and Balance Control**
- Week 9: Draft locomotion concepts
- Week 10: Code samples for balance control
- Week 11: Diagrams for walking patterns
- Week 12: Labs for locomotion simulation

**Chapter 9: Motion Planning and Navigation**
- Week 9: Draft planning concepts
- Week 10: Code samples for path planning
- Week 11: Diagrams for navigation systems
- Week 12: Labs for navigation implementation

#### Part IV: Intelligence and Learning

**Chapter 10: Reinforcement Learning for Robotics**
- Week 13: Draft RL concepts
- Week 14: Code samples for RL agents
- Week 15: Diagrams for RL systems
- Week 16: Labs for RL training

**Chapter 11: Imitation Learning and VLA**
- Week 13: Draft imitation learning concepts
- Week 14: Code samples for VLA systems
- Week 15: Diagrams for multi-modal learning
- Week 16: Labs for VLA implementation

**Chapter 12: Human-Robot Interaction**
- Week 13: Draft interaction concepts
- Week 14: Code samples for interaction systems
- Week 15: Diagrams for HRI systems
- Week 16: Labs for interaction implementation

#### Part V: Integration and Applications

**Chapter 13: Multi-Robot Systems and Coordination**
- Week 17: Draft multi-robot concepts
- Week 18: Code samples for coordination
- Week 19: Diagrams for multi-robot systems
- Week 20: Labs for multi-robot coordination

**Chapter 14: Real-World Deployment and Safety**
- Week 17: Draft deployment concepts
- Week 18: Code samples for safety systems
- Week 19: Diagrams for deployment architecture
- Week 20: Labs for safety implementation

**Chapter 15: Advanced Topics and Future Directions**
- Week 17: Draft advanced concepts
- Week 18: Code samples for advanced systems
- Week 19: Diagrams for future systems
- Week 20: Labs for advanced implementation

### Build & Deployment Pipeline

#### Local Development
1. **Install Docusaurus**: `npm install -g @docusaurus/init`
2. **Initialize site**: `npx @docusaurus init website --typescript`
3. **Start dev server**: `cd website && npm run start`

#### GitHub Actions Workflow
```yaml
name: Deploy to GitHub Pages

on:
  push:
    branches: [main]
  workflow_dispatch:

jobs:
  deploy:
    name: Deploy Docusaurus
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-node@v3
        with:
          node-version: 18
          cache: npm

      - name: Install dependencies
        run: npm ci
      - name: Build website
        run: npm run build

      - name: Deploy to GitHub Pages
        uses: peaceiris/actions-gh-pages@v3
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: ./build
```

#### Content Review Process
1. **Self-review**: Author reviews each chapter against learning objectives
2. **Technical validation**: All code samples tested in clean environment
3. **Peer review**: Optional external review for technical accuracy
4. **Iteration**: Refinement based on feedback

## Risk Mitigation

### Technical Risks
- **ROS 2 compatibility**: Test across different distributions
- **Simulation dependencies**: Ensure examples work with standard installations
- **Hardware access**: Focus on simulation with optional hardware extensions

### Schedule Risks
- **Complexity underestimation**: Include buffer time for complex topics
- **Tool issues**: Have backup approaches for critical tools
- **Research dependencies**: Complete foundational research early

### Quality Risks
- **Inconsistent quality**: Regular review checkpoints
- **Outdated information**: Regular updates and versioning
- **Accessibility**: Include alt text and multiple learning modalities

============================================================
FILE: specs\1-book-structure\quickstart.md
============================================================
# Physical AI & Humanoid Robotics Book Quickstart Guide

## Getting Started

This guide will help you set up the development environment for writing and building the Physical AI & Humanoid Robotics book.

### Prerequisites

- Node.js (v16 or higher)
- Git
- Python 3.8+ (for code sample testing)
- ROS 2 Humble Hawksbill (for testing examples)
- Basic familiarity with Markdown

### Setup Instructions

1. **Clone the repository**:
   ```bash
   git clone <repository-url>
   cd <repository-name>
   ```

2. **Install Docusaurus**:
   ```bash
   npm install
   ```

3. **Start the development server**:
   ```bash
   npm run start
   ```
   This will start a local server at `http://localhost:3000` with live reloading.

### Project Structure

```
book/
‚îú‚îÄ‚îÄ docs/               # Chapter content
‚îÇ   ‚îú‚îÄ‚îÄ part-i-foundations/
‚îÇ   ‚îú‚îÄ‚îÄ part-ii-perception/
‚îÇ   ‚îú‚îÄ‚îÄ part-iii-motion/
‚îÇ   ‚îú‚îÄ‚îÄ part-iv-intelligence/
‚îÇ   ‚îî‚îÄ‚îÄ part-v-integration/
‚îú‚îÄ‚îÄ src/                # Custom components and styling
‚îú‚îÄ‚îÄ notebooks/          # Jupyter notebooks for code samples
‚îú‚îÄ‚îÄ diagrams/           # Diagram source files
‚îú‚îÄ‚îÄ assets/             # Images and other assets
‚îú‚îÄ‚îÄ docusaurus.config.js # Docusaurus configuration
‚îî‚îÄ‚îÄ package.json        # Dependencies and scripts
```

### Writing a New Chapter

1. **Create the chapter directory** in the appropriate part:
   ```bash
   mkdir docs/part-i-foundations/chapter-x-topic-name
   ```

2. **Add the chapter content** as `index.md`:
   ```markdown
   ---
   title: Chapter X: Topic Name
   sidebar_position: X
   ---

   # Chapter X: Topic Name

   ## Learning Goals
   - Goal 1
   - Goal 2

   ## Key Technologies
   - Technology 1
   - Technology 2

   ## Content...
   ```

3. **Update the sidebar configuration** in the appropriate `_category_.json` file.

### Adding Code Samples

1. **For simple code snippets**, use standard Markdown syntax:
   ```python
   # Python code example
   import rospy
   from std_msgs.msg import String
   ```

2. **For complex examples**, create Jupyter notebooks in the `notebooks/` directory and reference them in the chapter.

### Adding Diagrams

1. **Create diagram source files** in the `diagrams/` directory
2. **Export as images** to the `assets/` directory
3. **Reference in markdown**:
   ```markdown
   ![Diagram Description](/assets/diagram-name.png)
   ```

### Building for Production

```bash
npm run build
```

This creates an optimized build in the `build/` directory that can be served statically.

### Deploying to GitHub Pages

The deployment happens automatically via GitHub Actions when changes are pushed to the main branch. Ensure your `docusaurus.config.js` is properly configured for your GitHub Pages URL.

============================================================
FILE: specs\1-book-structure\research.md
============================================================
# Physical AI & Humanoid Robotics Book Research

## Writing Order & Dependencies Analysis

### Decision: Modular Writing with Foundation-First Approach
**Rationale**: Allows parallel development while ensuring foundational concepts are established first. Part I-III provide necessary background for Parts IV-V.

**Implementation Strategy**:
- Establish core concepts in Parts I-III first
- Develop Parts IV-V with references to established concepts
- Create reusable content modules that can be referenced across chapters

## Tool Stack Selection

### Decision: Docusaurus + GitHub Pages
**Rationale**:
- Docusaurus provides excellent documentation features (search, navigation, versioning)
- GitHub Pages offers free hosting with custom domains
- Markdown format supports both documentation and code samples
- Integrates well with Git workflow
- Supports mathematical notation and diagrams

**Alternatives Considered**:
1. **GitBook**: Proprietary, limited customization options
2. **Sphinx**: Python-focused, less flexible for multi-language content
3. **Hugo**: Static site generator, good but less documentation-focused
4. **MkDocs**: Good alternative but less mature ecosystem than Docusaurus

## Content Structure & Architecture

### Decision: Modular Chapter Structure
**Rationale**: Enables parallel writing, easier maintenance, consistent formatting, and logical navigation.

**Components**:
- Part-based organization with clear progression
- Chapter-level independence with cross-references
- Reusable content modules (setup guides, common concepts)
- Consistent format across all chapters

## Solo Author Workflow Optimization

### Decision: Iterative Development with Quality Gates
**Rationale**: Maintains quality while allowing steady progress. Quality gates ensure consistency and accuracy without blocking forward momentum.

**Workflow Stages**:
1. **Draft**: Focus on content and structure
2. **Implement**: Add code samples, diagrams, labs
3. **Review**: Quality assurance and validation
4. **Refine**: Polish based on feedback/testing

## Deployment Strategy

### Decision: GitHub Actions for Automated Deployment
**Rationale**: Seamless integration with version control, automatic deployment on updates, reliable hosting.

**Configuration Elements**:
- Build process automation
- Error handling and notifications
- Custom domain support
- Analytics integration (optional)

============================================================
FILE: specs\1-book-structure\spec.md
============================================================
# Physical AI & Humanoid Robotics Book Structure Specification

## Feature Description

Create a comprehensive book structure for "Physical AI & Humanoid Robotics" that breaks down the content into parts, chapters, and sections. Each chapter should define learning goals, key technologies (ROS 2, Gazebo, NVIDIA Isaac, VLA), and practical outcomes to ensure a cohesive learning experience.

## Problem Statement

Students and practitioners need a structured curriculum that bridges the gap between theoretical AI knowledge and practical implementation in humanoid robotics. The current fragmented approach to learning robotics concepts makes it difficult to build comprehensive understanding and practical skills.

## System Intent

**Target Users**:
- Computer science and engineering students
- AI/robotics researchers and practitioners
- Software engineers transitioning to robotics
- Graduate students in robotics programs

**Core Value Proposition**: A comprehensive, hands-on curriculum that teaches physical AI and humanoid robotics through project-based learning using industry-standard tools (ROS 2, Gazebo with NVIDIA Isaac and VLA as advanced options).

**Key Capabilities**:
- Progressive learning from fundamentals to advanced concepts
- Practical implementation with real-world tools
- Project-based outcomes with tangible results
- Integration of perception, planning, and control systems

**Student Prerequisites**: Students need intermediate Python skills and basic ML knowledge to successfully complete the curriculum.

## Clarifications

### Session 2025-12-18

- Q: What assumptions are made about student background? ‚Üí A: Students need intermediate Python skills and basic ML knowledge
- Q: What is simulated vs real hardware? ‚Üí A: Primarily simulation-based with optional real hardware extensions
- Q: What tools are mandatory vs optional? ‚Üí A: ROS 2 and Gazebo mandatory; NVIDIA Isaac and VLA as advanced options
- Q: Where cloud vs local compute is used? ‚Üí A: Primarily local compute with optional cloud resources for heavy processing

## Functional Requirements

### Part I: Foundations of Physical AI and Robotics

#### Chapter 1: Introduction to Physical AI and Humanoid Robotics
- **Learning Goals**:
  - Understand the scope and applications of humanoid robotics
  - Distinguish between physical AI and traditional AI
  - Identify key challenges in humanoid robotics
- **Key Technologies**: ROS 2 ecosystem overview
- **Practical Outcomes**:
  - Set up ROS 2 development environment
  - Create first ROS 2 package and nodes
  - Launch basic simulation environment
- **Acceptance Criteria**: Student can successfully install ROS 2, create a simple publisher/subscriber node, and launch a basic simulation.

#### Chapter 2: Robot Operating System (ROS 2) Fundamentals
- **Learning Goals**:
  - Master ROS 2 architecture and communication patterns
  - Understand nodes, topics, services, and actions
  - Learn about parameter management and launch files
- **Key Technologies**: ROS 2 (Humble Hawksbill), rclpy, rclcpp
- **Practical Outcomes**:
  - Build a multi-node system for robot control
  - Implement custom message types and services
  - Create launch files for complex robot systems
- **Acceptance Criteria**: Student can create a distributed ROS 2 system with multiple nodes communicating through various communication patterns.

#### Chapter 3: Robot Modeling and Simulation Fundamentals
- **Learning Goals**:
  - Understand URDF and SDF robot description formats
  - Learn kinematic and dynamic modeling concepts
  - Master simulation environment setup
- **Key Technologies**: Gazebo, URDF, Xacro, RViz
- **Practical Outcomes**:
  - Create URDF model of a simple humanoid robot
  - Simulate robot in Gazebo environment
  - Visualize robot in RViz
- **Acceptance Criteria**: Student can create a complete URDF model, simulate it in Gazebo, and visualize it in RViz with proper joint constraints.

### Part II: Perception and Understanding

#### Chapter 4: Sensor Integration and Data Processing
- **Learning Goals**:
  - Understand various robot sensors and their applications
  - Learn to process sensor data streams
  - Master sensor fusion techniques
- **Key Technologies**: ROS 2 sensor interfaces, OpenCV, Point Cloud Library
- **Practical Outcomes**:
  - Integrate cameras, LIDAR, IMU, and other sensors
  - Process and visualize sensor data streams
  - Implement basic sensor fusion
- **Acceptance Criteria**: Student can integrate multiple sensors, process their data streams, and implement basic fusion algorithms.

#### Chapter 5: Computer Vision for Robotics
- **Learning Goals**:
  - Apply computer vision techniques to robotic perception
  - Understand visual SLAM and object recognition
  - Learn real-time image processing
- **Key Technologies**: OpenCV, ROS 2 vision modules, NVIDIA Isaac
- **Practical Outcomes**:
  - Implement object detection and tracking
  - Create visual SLAM pipeline
  - Integrate vision with robot control
- **Acceptance Criteria**: Student can detect and track objects in real-time, perform visual SLAM, and integrate vision data with robot control systems.

#### Chapter 6: 3D Perception and Scene Understanding
- **Learning Goals**:
  - Process 3D point cloud data
  - Understand spatial reasoning and mapping
  - Learn scene segmentation and understanding
- **Key Technologies**: PCL, NVIDIA Isaac, ROS 2 perception stack
- **Practical Outcomes**:
  - Process and visualize 3D point clouds
  - Create 3D maps of environments
  - Implement scene segmentation
- **Acceptance Criteria**: Student can process 3D point clouds, create spatial maps, and implement scene understanding algorithms.

### Part III: Motion and Control

#### Chapter 7: Kinematics and Dynamics
- **Learning Goals**:
  - Master forward and inverse kinematics
  - Understand robot dynamics and motion planning
  - Learn trajectory generation techniques
- **Key Technologies**: KDL, MoveIt!, ROS 2 control
- **Practical Outcomes**:
  - Implement kinematic solvers for robotic arms
  - Generate smooth trajectories for robot motion
  - Control robot joints with precise positioning
- **Acceptance Criteria**: Student can solve forward and inverse kinematics problems, generate smooth trajectories, and control robot joints accurately.

#### Chapter 8: Locomotion and Balance Control
- **Learning Goals**:
  - Understand bipedal locomotion principles
  - Learn balance control and stabilization
  - Master walking pattern generation
- **Key Technologies**: ROS 2 control, Gazebo simulation, NVIDIA Isaac
- **Practical Outcomes**:
  - Implement balance control algorithms
  - Generate walking patterns for humanoid robots
  - Simulate stable locomotion in various terrains
- **Acceptance Criteria**: Student can implement balance control, generate stable walking patterns, and simulate locomotion in simulation environments.

#### Chapter 9: Motion Planning and Navigation
- **Learning Goals**:
  - Master path planning algorithms
  - Understand navigation in dynamic environments
  - Learn obstacle avoidance techniques
- **Key Technologies**: Navigation2, MoveIt!, Gazebo
- **Practical Outcomes**:
  - Implement A*, Dijkstra, and RRT path planning
  - Navigate robots in complex environments
  - Handle dynamic obstacles and replanning
- **Acceptance Criteria**: Student can implement path planning algorithms, navigate robots in complex environments, and handle dynamic obstacle scenarios.

### Part IV: Intelligence and Learning

#### Chapter 10: Reinforcement Learning for Robotics
- **Learning Goals**:
  - Apply RL algorithms to robotic control
  - Understand simulation-to-reality transfer
  - Learn policy optimization techniques
- **Key Technologies**: NVIDIA Isaac, ROS 2, PyTorch/TensorFlow
- **Practical Outcomes**:
  - Train RL agents for basic robotic tasks
  - Transfer policies from simulation to real robots
  - Optimize control policies
- **Acceptance Criteria**: Student can train RL agents for robotic tasks, transfer policies between simulation and reality, and optimize control policies.

#### Chapter 11: Imitation Learning and VLA (Vision-Language-Action)
- **Learning Goals**:
  - Understand vision-language-action models
  - Learn imitation learning techniques
  - Master multi-modal learning
- **Key Technologies**: NVIDIA VLA, ROS 2, Transformer models
- **Practical Outcomes**:
  - Implement VLA-based robot control
  - Train imitation learning models
  - Control robots using vision and language inputs
- **Acceptance Criteria**: Student can implement VLA-based control systems, train imitation learning models, and control robots using multi-modal inputs.

#### Chapter 12: Human-Robot Interaction
- **Learning Goals**:
  - Design intuitive human-robot interfaces
  - Understand social robotics principles
  - Learn collaborative robotics concepts
- **Key Technologies**: ROS 2, NVIDIA Isaac, Speech recognition APIs
- **Practical Outcomes**:
  - Implement natural language interfaces
  - Create gesture recognition systems
  - Design collaborative robot behaviors
- **Acceptance Criteria**: Student can implement natural interfaces, create recognition systems, and design collaborative behaviors.

### Part V: Integration and Applications

#### Chapter 13: Multi-Robot Systems and Coordination
- **Learning Goals**:
  - Understand distributed robotics systems
  - Learn coordination and communication protocols
  - Master swarm robotics concepts
- **Key Technologies**: ROS 2 multi-robot systems, DDS, Gazebo multi-robot simulation
- **Practical Outcomes**:
  - Implement multi-robot communication
  - Coordinate multiple robots for tasks
  - Simulate swarm behaviors
- **Acceptance Criteria**: Student can implement multi-robot systems, coordinate robot teams, and simulate swarm behaviors.

#### Chapter 14: Real-World Deployment and Safety
- **Learning Goals**:
  - Understand safety protocols for physical robots
  - Learn deployment strategies and monitoring
  - Master error handling and recovery
- **Key Technologies**: ROS 2 safety frameworks, monitoring tools, NVIDIA Isaac safety features
- **Practical Outcomes**:
  - Implement safety checks and limits
  - Deploy robot systems in controlled environments
  - Monitor and maintain robot systems
- **Acceptance Criteria**: Student can implement safety protocols, deploy robot systems safely, and maintain operational robot systems.

#### Chapter 15: Advanced Topics and Future Directions
- **Learning Goals**:
  - Explore cutting-edge research in humanoid robotics
  - Understand ethical considerations
  - Learn about emerging technologies
- **Key Technologies**: Latest NVIDIA Isaac features, research frameworks
- **Practical Outcomes**:
  - Implement research-level algorithms
  - Analyze ethical implications of robotics
  - Design for future technology integration
- **Acceptance Criteria**: Student can implement advanced algorithms, analyze ethical considerations, and design for future technology integration.

## Non-Functional Requirements

### Performance
- Simulations must run in real-time or faster
- Control loops must maintain 50Hz minimum frequency
- Vision processing should achieve 30fps for real-time applications

### Reliability
- Systems must handle sensor failures gracefully
- Control systems must have safe fallback behaviors
- Simulation environments must be reproducible

### Scalability
- Architecture must support various robot morphologies
- Systems should be extensible for additional sensors
- Code should be modular and reusable

### Usability
- All examples should be reproducible with standard hardware
- Documentation must be clear and comprehensive
- Code examples should be well-commented and tested

## System Constraints

### External Dependencies
- ROS 2 Humble Hawksbill (or latest LTS) - MANDATORY
- Gazebo Garden (or compatible simulation) - MANDATORY
- NVIDIA Isaac ROS (for GPU-accelerated processing) - ADVANCED/OPTIONAL
- VLA models and frameworks - ADVANCED/OPTIONAL

### Technical Constraints
- All code must be Python 3.8+ compatible
- Simulation environments must run on consumer hardware
- Primarily local compute with optional cloud resources for heavy processing
- Examples work primarily in simulation with optional real hardware extensions

### Educational Constraints
- Each chapter must be completable in 1-2 weeks
- Projects must have clear deliverables
- Content must build progressively in complexity
- Students need intermediate Python skills and basic ML knowledge

## Non-Goals & Out of Scope

**Explicitly excluded**:
- Low-level embedded systems programming
- Mechanical design and CAD modeling
- Detailed control theory mathematics beyond practical application
- Proprietary commercial robotics platforms
- Real hardware implementation as mandatory requirement (simulation-based approach with optional hardware extensions)

## Known Gaps & Technical Debt

### Gap 1: Hardware-Specific Implementation
- **Issue**: Some concepts may require specific hardware not universally available
- **Evidence**: Certain NVIDIA Isaac features may require specific GPU hardware
- **Impact**: Students with different hardware may face limitations
- **Recommendation**: Curriculum designed as primarily simulation-based with optional real hardware extensions to ensure accessibility

### Gap 2: Advanced Mathematical Foundations
- **Issue**: Deep mathematical understanding of kinematics and dynamics
- **Evidence**: Some students may lack advanced math background
- **Impact**: Difficulty understanding core concepts
- **Recommendation**: Include mathematical appendices with prerequisites

## Success Criteria

### Functional Success
- [ ] All 15 chapters have defined learning goals, technologies, and outcomes
- [ ] All chapters include practical, implementable projects
- [ ] Content flows logically from basic to advanced concepts
- [ ] Core technologies (ROS 2, Gazebo) are fully integrated; advanced technologies (NVIDIA Isaac, VLA) are available as extensions

### Non-Functional Success
- [ ] Each chapter can be completed in 1-2 weeks by target audience with intermediate Python and basic ML knowledge
- [ ] All projects are reproducible with standard development environments using primarily local compute
- [ ] Content assumes Python and AI background as specified in constitution
- [ ] Simulation-based learning is prioritized for accessibility with optional real hardware extensions

### Educational Success
- [ ] 80% of students with intermediate Python and basic ML knowledge can complete the first 5 chapters successfully
- [ ] Students can implement basic humanoid robot control by chapter 10
- [ ] Students can integrate perception, planning, and control by final chapters
- [ ] Students can build end-to-end robotic systems by course completion

## Acceptance Tests

### Test 1: Curriculum Completeness
**Given**: Student with intermediate Python skills and basic ML knowledge
**When**: Following the complete curriculum (primarily simulation-based with optional real hardware)
**Then**: Student can build a functioning humanoid robot system with perception, planning, and control

### Test 2: Technology Integration
**Given**: Standard development environment with ROS 2 and Gazebo (with NVIDIA Isaac and VLA as optional advanced tools)
**When**: Implementing projects from each chapter using primarily local compute
**Then**: All projects successfully run and demonstrate the intended concepts

### Test 3: Progressive Learning
**Given**: Student with intermediate Python skills and basic ML knowledge
**When**: Progressing through chapters sequentially
**Then**: Student demonstrates increasing competency in physical AI and humanoid robotics concepts

============================================================
FILE: specs\1-book-structure\tasks.md
============================================================
# Physical AI & Humanoid Robotics Book - Implementation Tasks

## Feature Overview

Create a comprehensive Physical AI & Humanoid Robotics book with 15 chapters across 5 parts using Docusaurus and GitHub Pages, optimized for a solo author workflow.

## Phase 1: Setup Tasks

### Project Initialization
- [X] T001 Initialize Docusaurus project in book/ directory using Claude Code
- [X] T002 Configure docusaurus.config.js with navigation structure per plan using Docusaurus
- [X] T003 Create directory structure per implementation plan using GitHub
- [X] T004 Set up GitHub Actions workflow for deployment using GitHub
- [ ] T005 Install required dependencies (Python, ROS 2, Gazebo) using Spec-Kit

## Phase 2: Foundational Tasks

### Core Infrastructure
- [X] T006 Create basic markdown templates for chapters using Claude Code
- [X] T007 Set up code sample directory structure in notebooks/ using Claude Code
- [X] T008 Create diagrams directory structure with template files using Claude Code
- [X] T009 Configure syntax highlighting for Python, C++, and launch files using Docusaurus
- [X] T010 Set up math notation support (LaTeX) in Docusaurus config using Docusaurus

## Phase 3: Part I - Foundations of Physical AI and Robotics

### Chapter 1: Introduction to Physical AI and Humanoid Robotics [US1]
**Goal**: Create introductory chapter covering scope and applications of humanoid robotics
**Independent Test Criteria**: Student can understand the difference between physical AI and traditional AI, and identify key challenges in humanoid robotics

- [X] T011 [US1] Draft Chapter 1 content with learning goals and outcomes using Claude Code
- [ ] T012 [US1] Create diagrams comparing Physical AI vs Traditional AI using Claude Code
- [X] T013 [US1] Write code samples for basic ROS 2 environment setup using Claude Code
- [X] T014 [US1] Create lab exercise for ROS 2 installation and basic simulation using Claude Code
- [X] T015 [US1] Add key technologies overview for ROS 2 ecosystem using Claude Code

### Chapter 2: Robot Operating System (ROS 2) Fundamentals [US2]
**Goal**: Create chapter covering ROS 2 architecture and communication patterns
**Independent Test Criteria**: Student can create a distributed ROS 2 system with multiple nodes communicating through various communication patterns

- [X] T016 [US2] Draft Chapter 2 content covering ROS 2 architecture using Claude Code
- [ ] T017 [US2] Create diagrams for ROS 2 architecture and communication patterns using Claude Code
- [X] T018 [US2] Write code samples for nodes, topics, services, and actions using Claude Code
- [X] T019 [US2] Create lab exercise for multi-node system using Claude Code
- [X] T020 [US2] Add content about parameter management and launch files using Claude Code

### Chapter 3: Robot Modeling and Simulation Fundamentals [US3]
**Goal**: Create chapter covering URDF/SDF formats and simulation setup
**Independent Test Criteria**: Student can create a complete URDF model, simulate it in Gazebo, and visualize it in RViz with proper joint constraints

- [X] T021 [US3] Draft Chapter 3 content covering URDF and SDF concepts using Claude Code
- [ ] T022 [US3] Create diagrams for robot kinematics and joint constraints using Claude Code
- [X] T023 [US3] Write code samples for URDF creation and modeling using Claude Code
- [X] T024 [US3] Create lab exercise for Gazebo simulation setup using Claude Code
- [X] T025 [US3] Add RViz visualization content and examples using Claude Code

## Phase 4: Part II - Perception and Understanding

### Chapter 4: Sensor Integration and Data Processing [US4]
**Goal**: Create chapter covering various robot sensors and data processing
**Independent Test Criteria**: Student can integrate multiple sensors, process their data streams, and implement basic fusion algorithms

- [X] T026 [US4] Draft Chapter 4 content covering sensor types and applications using Claude Code
- [ ] T027 [US4] Create diagrams for sensor fusion architecture using Claude Code
- [X] T028 [US4] Write code samples for sensor data processing using Claude Code
- [X] T029 [US4] Create lab exercise for sensor integration using Claude Code
- [X] T030 [US4] Add content about ROS 2 sensor interfaces using Claude Code

### Chapter 5: Computer Vision for Robotics [US5]
**Goal**: Create chapter covering computer vision techniques for robotic perception
**Independent Test Criteria**: Student can detect and track objects in real-time, perform visual SLAM, and integrate vision data with robot control systems

- [X] T031 [US5] Draft Chapter 5 content covering vision techniques and SLAM using Claude Code
- [ ] T032 [US5] Create diagrams for visual processing pipeline using Claude Code
- [X] T033 [US5] Write code samples for object detection and tracking using Claude Code
- [X] T034 [US5] Create lab exercise for vision integration with robot control using Claude Code
- [X] T035 [US5] Add content about OpenCV and ROS 2 vision modules using Claude Code

### Chapter 6: 3D Perception and Scene Understanding [US6]
**Goal**: Create chapter covering 3D point cloud processing and scene understanding
**Independent Test Criteria**: Student can process 3D point clouds, create spatial maps, and implement scene understanding algorithms

- [X] T036 [US6] Draft Chapter 6 content covering 3D perception concepts using Claude Code
- [ ] T037 [US6] Create diagrams for spatial reasoning and mapping using Claude Code
- [X] T038 [US6] Write code samples for point cloud processing using Claude Code
- [X] T039 [US6] Create lab exercise for 3D mapping and scene segmentation using Claude Code
- [X] T040 [US6] Add content about PCL and 3D perception stack using Claude Code

## Phase 5: Part III - Motion and Control

### Chapter 7: Kinematics and Dynamics [US7]
**Goal**: Create chapter covering forward/inverse kinematics and trajectory generation
**Independent Test Criteria**: Student can solve forward and inverse kinematics problems, generate smooth trajectories, and control robot joints accurately

- [X] T041 [US7] Draft Chapter 7 content covering kinematics and dynamics using Claude Code
- [ ] T042 [US7] Create diagrams for robot kinematics and joint relationships using Claude Code
- [X] T043 [US7] Write code samples for kinematic solvers using Claude Code
- [X] T044 [US7] Create lab exercise for trajectory generation using Claude Code
- [X] T045 [US7] Add content about KDL and MoveIt! integration using Claude Code

### Chapter 8: Locomotion and Balance Control [US8]
**Goal**: Create chapter covering bipedal locomotion and balance control
**Independent Test Criteria**: Student can implement balance control, generate stable walking patterns, and simulate locomotion in simulation environments

- [X] T046 [US8] Draft Chapter 8 content covering locomotion principles using Claude Code
- [ ] T047 [US8] Create diagrams for walking patterns and balance control using Claude Code
- [X] T048 [US8] Write code samples for balance control algorithms using Claude Code
- [X] T049 [US8] Create lab exercise for locomotion simulation using Claude Code
- [X] T050 [US8] Add content about Gazebo simulation for locomotion using Claude Code

### Chapter 9: Motion Planning and Navigation [US9]
**Goal**: Create chapter covering path planning and navigation algorithms
**Independent Test Criteria**: Student can implement path planning algorithms, navigate robots in complex environments, and handle dynamic obstacle scenarios

- [X] T051 [US9] Draft Chapter 9 content covering path planning algorithms using Claude Code
- [ ] T052 [US9] Create diagrams for navigation systems and obstacle avoidance using Claude Code
- [X] T053 [US9] Write code samples for A*, Dijkstra, and RRT algorithms using Claude Code
- [X] T054 [US9] Create lab exercise for navigation implementation using Claude Code
- [X] T055 [US9] Add content about Navigation2 and MoveIt! integration using Claude Code

## Phase 6: Part IV - Intelligence and Learning

### Chapter 10: Reinforcement Learning for Robotics [US10]
**Goal**: Create chapter covering RL algorithms applied to robotic control
**Independent Test Criteria**: Student can train RL agents for robotic tasks, transfer policies between simulation and reality, and optimize control policies

- [X] T056 [US10] Draft Chapter 10 content covering RL for robotics using Claude Code
- [ ] T057 [US10] Create diagrams for RL systems and training pipelines using Claude Code
- [X] T058 [US10] Write code samples for RL agents and training using Claude Code
- [X] T059 [US10] Create lab exercise for RL training in simulation using Claude Code
- [X] T060 [US10] Add content about simulation-to-reality transfer using Claude Code

### Chapter 11: Imitation Learning and VLA [US11]
**Goal**: Create chapter covering vision-language-action models and imitation learning
**Independent Test Criteria**: Student can implement VLA-based control systems, train imitation learning models, and control robots using multi-modal inputs

- [X] T061 [US11] Draft Chapter 11 content covering VLA and imitation learning using Claude Code
- [ ] T062 [US11] Create diagrams for multi-modal learning architectures using Claude Code
- [X] T063 [US11] Write code samples for VLA-based robot control using Claude Code
- [X] T064 [US11] Create lab exercise for VLA implementation using Claude Code
- [X] T065 [US11] Add content about Transformer models for robotics using Claude Code

### Chapter 12: Human-Robot Interaction [US12]
**Goal**: Create chapter covering intuitive human-robot interfaces and social robotics
**Independent Test Criteria**: Student can implement natural interfaces, create recognition systems, and design collaborative behaviors

- [X] T066 [US12] Draft Chapter 12 content covering HRI principles using Claude Code
- [ ] T067 [US12] Create diagrams for HRI systems and interfaces using Claude Code
- [X] T068 [US12] Write code samples for natural language and gesture interfaces using Claude Code
- [X] T069 [US12] Create lab exercise for collaborative robot behaviors using Claude Code
- [X] T070 [US12] Add content about speech recognition APIs using Claude Code

## Phase 7: Part V - Integration and Applications

### Chapter 13: Multi-Robot Systems and Coordination [US13]
**Goal**: Create chapter covering distributed robotics and coordination protocols
**Independent Test Criteria**: Student can implement multi-robot systems, coordinate robot teams, and simulate swarm behaviors

- [X] T071 [US13] Draft Chapter 13 content covering multi-robot systems using Claude Code
- [ ] T072 [US13] Create diagrams for multi-robot coordination architecture using Claude Code
- [X] T073 [US13] Write code samples for multi-robot communication using Claude Code
- [X] T074 [US13] Create lab exercise for swarm behavior simulation using Claude Code
- [X] T075 [US13] Add content about DDS and communication protocols using Claude Code

### Chapter 14: Real-World Deployment and Safety [US14]
**Goal**: Create chapter covering safety protocols and deployment strategies
**Independent Test Criteria**: Student can implement safety protocols, deploy robot systems safely, and maintain operational robot systems

- [X] T076 [US14] Draft Chapter 14 content covering safety protocols using Claude Code
- [ ] T077 [US14] Create diagrams for safety architecture and deployment systems using Claude Code
- [X] T078 [US14] Write code samples for safety checks and limits using Claude Code
- [X] T079 [US14] Create lab exercise for safe deployment implementation using Claude Code
- [X] T080 [US14] Add content about monitoring and error handling using Claude Code

### Chapter 15: Advanced Topics and Future Directions [US15]
**Goal**: Create chapter exploring cutting-edge research and future technologies
**Independent Test Criteria**: Student can implement advanced algorithms, analyze ethical considerations, and design for future technology integration

- [X] T081 [US15] Draft Chapter 15 content covering advanced research topics using Claude Code
- [ ] T082 [US15] Create diagrams for future robotics systems and technologies using Claude Code
- [X] T083 [US15] Write code samples for research-level algorithms using Claude Code
- [X] T084 [US15] Create lab exercise for ethical robotics analysis using Claude Code
- [X] T085 [US15] Add content about emerging technologies and ethical considerations using Claude Code

## Phase 8: Polish & Cross-Cutting Concerns

### Quality Assurance and Integration
- [ ] T086 Review and validate all code samples in clean environments using Claude Code
- [ ] T087 Test all diagrams for accessibility and clarity using Claude Code
- [ ] T088 Verify all lab exercises are reproducible with standard hardware using Claude Code
- [ ] T089 Update cross-references and internal consistency across all chapters using Claude Code
- [ ] T090 Final proofreading and content editing for all chapters using Claude Code

### Deployment and Launch
- [ ] T091 Deploy complete book to GitHub Pages using GitHub
- [ ] T092 Verify all links, navigation, and search functionality using Docusaurus
- [ ] T093 Set up custom domain if needed using GitHub
- [ ] T094 Document feedback collection process using Claude Code
- [ ] T095 Create launch announcement and initial marketing content using Claude Code

## Dependencies

- **US2 (Chapter 2)** and **US3 (Chapter 3)** should be completed before **US4-6** (Part II)
- **US4-6** (Part II) should be completed before **US7-9** (Part III)
- **US7-9** (Part III) should be completed before **US10-12** (Part IV)
- **US1-12** (Parts I-IV) should be completed before **US13-15** (Part V)

## Parallel Execution Opportunities

- Chapters within Part II (US4-6) can be developed in parallel after US2-3 completion
- Chapters within Part III (US7-9) can be developed in parallel after US4-6 completion
- Chapters within Part IV (US10-12) can be developed in parallel after US7-9 completion
- Diagrams and code samples can be created in parallel with content writing

## Implementation Strategy

1. **MVP Scope**: Complete US1-US3 (Part I) as the minimum viable product
2. **Incremental Delivery**: Deliver content in part-based increments (I, II, III, IV, V)
3. **Quality Gates**: Validate code samples and diagrams at the end of each part
4. **Iterative Refinement**: Gather feedback after each part and incorporate improvements

============================================================
FILE: specs\1-book-structure\checklists\requirements.md
============================================================
# Specification Quality Checklist: Physical AI & Humanoid Robotics Book Structure

**Purpose**: Validate specification completeness and quality before proceeding to planning
**Created**: 2025-12-18
**Feature**: [Link to spec.md](../1-book-structure/spec.md)

## Content Quality

- [x] No implementation details (languages, frameworks, APIs)
- [x] Focused on user value and business needs
- [x] Written for non-technical stakeholders
- [x] All mandatory sections completed

## Requirement Completeness

- [x] No [NEEDS CLARIFICATION] markers remain
- [x] Requirements are testable and unambiguous
- [x] Success criteria are measurable
- [x] Success criteria are technology-agnostic (no implementation details)
- [x] All acceptance scenarios are defined
- [x] Edge cases are identified
- [x] Scope is clearly bounded
- [x] Dependencies and assumptions identified

## Feature Readiness

- [x] All functional requirements have clear acceptance criteria
- [x] User scenarios cover primary flows
- [x] Feature meets measurable outcomes defined in Success Criteria
- [x] No implementation details leak into specification

## Notes

- Items marked complete after thorough review of the book structure specification
- Specification aligns well with the constitution principles for the Physical AI & Humanoid Robotics book
